{
  "initial_prompt": "Design a simple caching layer with TTL support",
  "status": "consensus",
  "rounds": 2,
  "started_at": "2026-01-17T08:36:00.985092",
  "ended_at": "2026-01-17T08:42:03.741879",
  "final_design": "## Design\n\n# Production-Ready Caching Layer with TTL and Advanced Features\n\n## Architecture Overview\n\nA high-performance, thread-safe in-memory caching system with automatic expiration, designed for production environments with configurable monitoring, flexible type safety, and optimized performance characteristics.\n\n### Core Components\n\n```python\nfrom typing import Any, Optional, Dict, Callable, TypeVar, Generic, Protocol, List, Tuple\nfrom dataclasses import dataclass, field\nfrom datetime import datetime, timedelta\nimport threading\nimport time\nfrom collections import OrderedDict\nimport logging\nfrom enum import Enum\nfrom queue import Queue, Empty\nimport weakref\n\nlogger = logging.getLogger(__name__)\n\nT = TypeVar('T')\n\n\nclass CallbackMode(Enum):\n    \"\"\"Defines how callbacks are invoked.\"\"\"\n    SYNC = \"sync\"  # Blocking, inside lock\n    ASYNC_QUEUED = \"async_queued\"  # Non-blocking, queued for background processing\n    FIRE_AND_FORGET = \"fire_and_forget\"  # Non-blocking, new thread per callback\n\n\nclass StatsLevel(Enum):\n    \"\"\"Defines granularity of statistics tracking.\"\"\"\n    NONE = 0  # No statistics\n    BASIC = 1  # Hit/miss/size only\n    DETAILED = 2  # Includes evictions, expirations, access patterns\n\n\n@dataclass\nclass CacheEntry(Generic[T]):\n    \"\"\"Represents a single cache entry with metadata.\"\"\"\n    value: T\n    created_at: datetime\n    ttl_seconds: int\n    access_count: int = 0\n    last_accessed: Optional[datetime] = None\n    \n    def is_expired(self) -> bool:\n        \"\"\"Check if this entry has exceeded its TTL.\"\"\"\n        if self.ttl_seconds <= 0:\n            return False  # TTL of 0 or negative means no expiration\n        expiry_time = self.created_at + timedelta(seconds=self.ttl_seconds)\n        return datetime.now() >= expiry_time\n    \n    def remaining_ttl(self) -> float:\n        \"\"\"Returns remaining seconds until expiration, or -1 if no expiration.\"\"\"\n        if self.ttl_seconds <= 0:\n            return -1\n        expiry_time = self.created_at + timedelta(seconds=self.ttl_seconds)\n        remaining = (expiry_time - datetime.now()).total_seconds()\n        return max(0, remaining)\n    \n    def touch(self, track_access: bool = True) -> None:\n        \"\"\"Update access metadata.\"\"\"\n        if track_access:\n            self.access_count += 1\n            self.last_accessed = datetime.now()\n\n\n@dataclass\nclass BulkOperationResult:\n    \"\"\"Result of bulk operations with success/failure tracking.\"\"\"\n    successful: List[str] = field(default_factory=list)\n    failed: Dict[str, str] = field(default_factory=dict)  # key -> error message\n    \n    @property\n    def success_count(self) -> int:\n        return len(self.successful)\n    \n    @property\n    def failure_count(self) -> int:\n        return len(self.failed)\n    \n    @property\n    def total(self) -> int:\n        return self.success_count + self.failure_count\n\n\nclass CacheError(Exception):\n    \"\"\"Base exception for cache-related errors.\"\"\"\n    pass\n\n\nclass CacheShutdownError(CacheError):\n    \"\"\"Raised when attempting to use a closed cache.\"\"\"\n    pass\n\n\nclass CacheCallbackError(CacheError):\n    \"\"\"Raised when a callback fails in SYNC mode.\"\"\"\n    pass\n\n\nclass CallbackProtocol(Protocol):\n    \"\"\"Protocol for cache event callbacks.\"\"\"\n    def __call__(self, key: str, value: Any, metadata: Optional[Dict[str, Any]] = None) -> None:\n        \"\"\"\n        Callback signature for cache events.\n        \n        Args:\n            key: Cache key\n            value: Cached value\n            metadata: Optional context (e.g., {\"reason\": \"ttl_expired\", \"age_seconds\": 123})\n        \"\"\"\n        ...\n\n\nclass CacheLayer:\n    \"\"\"\n    Thread-safe in-memory cache with TTL support, LRU eviction, and production features.\n    \n    Features:\n    - Per-key TTL configuration with efficient expiration tracking\n    - Automatic background cleanup of expired entries\n    - LRU eviction when max_size is reached\n    - Thread-safe operations with minimal lock contention\n    - Configurable statistics tracking (NONE/BASIC/DETAILED)\n    - Optional access pattern tracking with memory/performance trade-off\n    - Flexible callback system (sync/async/fire-and-forget)\n    - Graceful shutdown with configurable timeout\n    - Bulk operations with partial success reporting\n    - Request coalescing for thundering herd prevention\n    - Namespace support via key prefixing utilities\n    \"\"\"\n    \n    def __init__(\n        self,\n        default_ttl: int = 300,\n        max_size: Optional[int] = 1000,\n        cleanup_interval: int = 60,\n        stats_level: StatsLevel = StatsLevel.BASIC,\n        track_access_patterns: bool = False,\n        callback_mode: CallbackMode = CallbackMode.ASYNC_QUEUED,\n        callback_queue_size: int = 1000,\n        shutdown_timeout: float = 5.0,\n        on_eviction: Optional[CallbackProtocol] = None,\n        on_expiration: Optional[CallbackProtocol] = None\n    ):\n        \"\"\"\n        Initialize the cache layer.\n        \n        Args:\n            default_ttl: Default TTL in seconds (0 = no expiration)\n            max_size: Maximum number of entries (None = unlimited)\n            cleanup_interval: Seconds between cleanup runs (0 = disable)\n            stats_level: Granularity of statistics tracking\n            track_access_patterns: Enable access_count/last_accessed tracking\n            callback_mode: How to invoke callbacks (sync/async/fire-and-forget)\n            callback_queue_size: Max queued callbacks in ASYNC_QUEUED mode\n            shutdown_timeout: Seconds to wait for graceful shutdown\n            on_eviction: Optional callback when entries are evicted\n            on_expiration: Optional callback when entries expire\n        \n        Raises:\n            ValueError: If parameters are invalid\n        \"\"\"\n        if max_size is not None and max_size < 1:\n            raise ValueError(\"max_size must be positive or None\")\n        if cleanup_interval < 0:\n            raise ValueError(\"cleanup_interval must be non-negative\")\n        if default_ttl < 0:\n            raise ValueError(\"default_ttl must be non-negative\")\n        if shutdown_timeout < 0:\n            raise ValueError(\"shutdown_timeout must be non-negative\")\n        if callback_queue_size < 1:\n            raise ValueError(\"callback_queue_size must be positive\")\n        \n        self._cache: OrderedDict[str, CacheEntry] = OrderedDict()\n        self._lock = threading.RLock()\n        self._default_ttl = default_ttl\n        self._max_size = max_size\n        self._cleanup_interval = cleanup_interval\n        self._stats_level = stats_level\n        self._track_access = track_access_patterns\n        self._callback_mode = callback_mode\n        self._shutdown_timeout = shutdown_timeout\n        self._on_eviction = on_eviction\n        self._on_expiration = on_expiration\n        \n        # Statistics\n        self._hits = 0\n        self._misses = 0\n        self._evictions = 0\n        self._expirations = 0\n        self._sets = 0\n        \n        # Lifecycle management\n        self._closed = False\n        self._cleanup_thread: Optional[threading.Thread] = None\n        self._callback_thread: Optional[threading.Thread] = None\n        self._stop_cleanup = threading.Event()\n        self._stop_callbacks = threading.Event()\n        \n        # Callback queue for ASYNC_QUEUED mode\n        self._callback_queue: Optional[Queue] = None\n        if callback_mode == CallbackMode.ASYNC_QUEUED:\n            self._callback_queue = Queue(maxsize=callback_queue_size)\n            self._start_callback_thread()\n        \n        # Request coalescing for get_or_compute\n        self._computing_keys: Dict[str, threading.Event] = {}\n        self._computing_lock = threading.Lock()\n        \n        if cleanup_interval > 0:\n            self._start_cleanup_thread()\n    \n    def _check_open(self) -> None:\n        \"\"\"Raise exception if cache is closed.\"\"\"\n        if self._closed:\n            raise CacheShutdownError(\"Cache has been closed\")\n    \n    def get(self, key: str, default: Any = None) -> Any:\n        \"\"\"\n        Retrieve a value from cache.\n        \n        Args:\n            key: Cache key\n            default: Value to return if key is missing or expired\n            \n        Returns:\n            Cached value or default\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            \n            if key not in self._cache:\n                self._record_stat('miss')\n                return default\n            \n            entry = self._cache[key]\n            \n            if entry.is_expired():\n                value = entry.value\n                del self._cache[key]\n                self._record_stat('miss')\n                self._record_stat('expiration')\n                self._invoke_callback(\n                    self._on_expiration, \n                    key, \n                    value,\n                    metadata={\"reason\": \"ttl_expired\", \"age_seconds\": (datetime.now() - entry.created_at).total_seconds()}\n                )\n                return default\n            \n            # Update access metadata\n            entry.touch(self._track_access)\n            \n            # Move to end (LRU)\n            self._cache.move_to_end(key)\n            \n            self._record_stat('hit')\n            \n            return entry.value\n    \n    def set(\n        self,\n        key: str,\n        value: Any,\n        ttl: Optional[int] = None\n    ) -> None:\n        \"\"\"\n        Store a value in cache.\n        \n        Args:\n            key: Cache key\n            value: Value to cache\n            ttl: TTL in seconds (None = use default)\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n            ValueError: If key is empty or ttl is negative\n        \"\"\"\n        if not key:\n            raise ValueError(\"Cache key cannot be empty\")\n        if ttl is not None and ttl < 0:\n            raise ValueError(\"TTL must be non-negative\")\n        \n        with self._lock:\n            self._check_open()\n            \n            ttl_seconds = ttl if ttl is not None else self._default_ttl\n            \n            entry = CacheEntry(\n                value=value,\n                created_at=datetime.now(),\n                ttl_seconds=ttl_seconds\n            )\n            \n            # Update existing or add new\n            if key in self._cache:\n                del self._cache[key]\n            \n            self._cache[key] = entry\n            self._cache.move_to_end(key)\n            \n            self._record_stat('set')\n            \n            # Evict oldest if over limit\n            if self._max_size and len(self._cache) > self._max_size:\n                evicted_key, evicted_entry = self._cache.popitem(last=False)\n                self._record_stat('eviction')\n                self._invoke_callback(\n                    self._on_eviction, \n                    evicted_key, \n                    evicted_entry.value,\n                    metadata={\"reason\": \"lru_eviction\", \"access_count\": evicted_entry.access_count if self._track_access else None}\n                )\n    \n    def set_many(\n        self, \n        mapping: Dict[str, Any], \n        ttl: Optional[int] = None,\n        continue_on_error: bool = True\n    ) -> BulkOperationResult:\n        \"\"\"\n        Store multiple key-value pairs.\n        \n        Args:\n            mapping: Dictionary of key-value pairs to cache\n            ttl: TTL in seconds (None = use default)\n            continue_on_error: If False, stop on first error\n            \n        Returns:\n            BulkOperationResult with success/failure details\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        result = BulkOperationResult()\n        \n        for key, value in mapping.items():\n            try:\n                self.set(key, value, ttl)\n                result.successful.append(key)\n            except Exception as e:\n                result.failed[key] = str(e)\n                if not continue_on_error:\n                    break\n        \n        return result\n    \n    def get_many(self, keys: List[str], default: Any = None) -> Dict[str, Any]:\n        \"\"\"\n        Retrieve multiple values from cache.\n        \n        Args:\n            keys: List of cache keys\n            default: Value to use for missing/expired keys\n            \n        Returns:\n            Dictionary mapping keys to values\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        return {key: self.get(key, default) for key in keys}\n    \n    def delete(self, key: str) -> bool:\n        \"\"\"\n        Remove a key from cache.\n        \n        Args:\n            key: Cache key to remove\n            \n        Returns:\n            True if key existed, False otherwise\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            if key in self._cache:\n                del self._cache[key]\n                return True\n            return False\n    \n    def delete_many(\n        self, \n        keys: List[str],\n        continue_on_error: bool = True\n    ) -> BulkOperationResult:\n        \"\"\"\n        Remove multiple keys from cache.\n        \n        Args:\n            keys: List of cache keys to remove\n            continue_on_error: If False, stop on first error\n            \n        Returns:\n            BulkOperationResult with success/failure details\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        result = BulkOperationResult()\n        \n        for key in keys:\n            try:\n                if self.delete(key):\n                    result.successful.append(key)\n                else:\n                    result.failed[key] = \"Key not found\"\n            except Exception as e:\n                result.failed[key] = str(e)\n                if not continue_on_error:\n                    break\n        \n        return result\n    \n    def clear(self) -> None:\n        \"\"\"\n        Remove all entries from cache.\n        \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            self._cache.clear()\n    \n    def contains(self, key: str) -> bool:\n        \"\"\"\n        Check if key exists and is not expired.\n        \n        Args:\n            key: Cache key\n            \n        Returns:\n            True if key exists and is valid\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            if key not in self._cache:\n                return False\n            \n            entry = self._cache[key]\n            if entry.is_expired():\n                del self._cache[key]\n                self._record_stat('expiration')\n                return False\n            \n            return True\n    \n    def get_ttl(self, key: str) -> Optional[float]:\n        \"\"\"\n        Get remaining TTL for a key.\n        \n        Args:\n            key: Cache key\n            \n        Returns:\n            Remaining seconds, -1 for no expiration, None if key doesn't exist\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            if key not in self._cache:\n                return None\n            \n            entry = self._cache[key]\n            if entry.is_expired():\n                del self._cache[key]\n                self._record_stat('expiration')\n                return None\n            \n            return entry.remaining_ttl()\n    \n    def refresh_ttl(self, key: str, ttl: Optional[int] = None) -> bool:\n        \"\"\"\n        Reset the TTL for an existing key.\n        \n        Args:\n            key: Cache key\n            ttl: New TTL in seconds (None = use default)\n            \n        Returns:\n            True if key was refreshed, False if key doesn't exist\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            if key not in self._cache:\n                return False\n            \n            entry = self._cache[key]\n            if entry.is_expired():\n                del self._cache[key]\n                self._record_stat('expiration')\n                return False\n            \n            entry.created_at = datetime.now()\n            entry.ttl_seconds = ttl if ttl is not None else self._default_ttl\n            return True\n    \n    def get_or_compute(\n        self,\n        key: str,\n        compute_fn: Callable[[], Any],\n        ttl: Optional[int] = None\n    ) -> Any:\n        \"\"\"\n        Get cached value or compute and cache it with request coalescing.\n        \n        Multiple concurrent requests for the same key will wait for the first\n        computation to complete rather than computing independently.\n        \n        Args:\n            key: Cache key\n            compute_fn: Function to call if cache miss\n            ttl: TTL for computed value\n            \n        Returns:\n            Cached or computed value\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n            Exception: Any exception raised by compute_fn\n        \"\"\"\n        # Fast path: check cache first\n        value = self.get(key)\n        if value is not None:\n            return value\n        \n        # Check if another thread is computing this key\n        computing_event = None\n        should_compute = False\n        \n        with self._computing_lock:\n            if key in self._computing_keys:\n                # Another thread is computing, wait for it\n                computing_event = self._computing_keys[key]\n            else:\n                # We'll compute it\n                self._computing_keys[key] = threading.Event()\n                should_compute = True\n        \n        if computing_event:\n            # Wait for other thread to finish computing\n            computing_event.wait()\n            # Try to get the computed value\n            value = self.get(key)\n            if value is not None:\n                return value\n            # Fall through to compute if other thread failed\n        \n        if should_compute:\n            try:\n                # Double-check cache before computing\n                value = self.get(key)\n                if value is not None:\n                    return value\n                \n                # Compute value (outside lock to avoid blocking)\n                computed_value = compute_fn()\n                \n                # Store computed value\n                self.set(key, computed_value, ttl)\n                return computed_value\n            except Exception as e:\n                logger.warning(f\"Error computing value for key '{key}': {e}\")\n                raise\n            finally:\n                # Signal completion and cleanup\n                with self._computing_lock:\n                    event = self._computing_keys.pop(key, None)\n                    if event:\n                        event.set()\n        \n        # Shouldn't reach here, but compute as fallback\n        return compute_fn()\n    \n    def stats(self) -> Dict[str, Any]:\n        \"\"\"\n        Get cache statistics.\n        \n        Returns:\n            Dictionary with cache metrics\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            \n            stats_dict = {\n                \"size\": len(self._cache),\n                \"max_size\": self._max_size,\n                \"closed\": self._closed,\n                \"stats_level\": self._stats_level.name,\n                \"tracking_access_patterns\": self._track_access\n            }\n            \n            if self._stats_level.value >= StatsLevel.BASIC.value:\n                total_requests = self._hits + self._misses\n                hit_rate = (self._hits / total_requests * 100) if total_requests > 0 else 0\n                \n                stats_dict.update({\n                    \"hits\": self._hits,\n                    \"misses\": self._misses,\n                    \"hit_rate\": round(hit_rate, 2),\n                    \"total_requests\": total_requests\n                })\n            \n            if self._stats_level.value >= StatsLevel.DETAILED.value:\n                stats_dict.update({\n                    \"evictions\": self._evictions,\n                    \"expirations\": self._expirations,\n                    \"sets\": self._sets\n                })\n            \n            return stats_dict\n    \n    def reset_stats(self) -> None:\n        \"\"\"\n        Reset all statistics counters.\n        \n        Raises:\n            CacheShutdownError: If cache is closed\n        \"\"\"\n        with self._lock:\n            self._check_open()\n            self._hits = 0\n            self._misses = 0\n            self._evictions = 0\n            self._expirations = 0\n            self._sets = 0\n    \n    def get_access_patterns(self) -> Dict[str, Dict[str, Any]]:\n        \"\"\"\n        Get access pattern information for all keys.\n        \n        Only available when track_access_patterns=True.\n        \n        Returns:\n            Dictionary mapping keys to access metadata\n            \n        Raises:\n            CacheShutdownError: If cache is closed\n            ValueError: If access pattern tracking is disabled\n        \"\"\"\n        if not self._track_access:\n            raise ValueError(\"Access pattern tracking is disabled. Enable with track_access_patterns=True\")\n        \n        with self._lock:\n            self._check_open()\n            \n            patterns = {}\n            for key, entry in self._cache.items():\n                patterns[key] = {\n                    \"access_count\": entry.access_count,\n                    \"last_accessed\": entry.last_accessed.isoformat() if entry.last_accessed else None,\n                    \"age_seconds\": (datetime.now() - entry.created_at).total_seconds(),\n                    \"remaining_ttl\": entry.remaining_ttl()\n                }\n            \n            return patterns\n    \n    def _record_stat(self, stat_type: str) -> None:\n        \"\"\"Record a statistic if tracking is enabled.\"\"\"\n        if self._stats_level == StatsLevel.NONE:\n            return\n        \n        if stat_type == 'hit':\n            self._hits += 1\n        elif stat_type == 'miss':\n            self._misses += 1\n        elif stat_type == 'set' and self._stats_level.value >= StatsLevel.DETAILED.value:\n            self._sets += 1\n        elif stat_type == 'eviction' and self._stats_level.value >= StatsLevel.DETAILED.value:\n            self._evictions += 1\n        elif stat_type == 'expiration' and self._stats_level.value >= StatsLevel.DETAILED.value:\n            self._expirations += 1\n    \n    def _cleanup_expired(self) -> int:\n        \"\"\"Remove expired entries. Returns number of entries removed.\"\"\"\n        with self._lock:\n            if self._closed:\n                return 0\n            \n            keys_to_delete = [\n                key for key, entry in self._cache.items()\n                if entry.is_expired()\n            ]\n            \n            for key in keys_to_delete:\n                entry = self._cache[key]\n                age_seconds = (datetime.now() - entry.created_at).total_seconds()\n                del self._cache[key]\n                self._record_stat('expiration')\n                self._invoke_callback(\n                    self._on_expiration, \n                    key, \n                    entry.value,\n                    metadata={\"reason\": \"background_cleanup\", \"age_seconds\": age_seconds}\n                )\n            \n            return len(keys_to_delete)\n    \n    def _start_cleanup_thread(self) -> None:\n        \"\"\"Start background thread for periodic cleanup.\"\"\"\n        def cleanup_loop():\n            while not self._stop_cleanup.is_set():\n                time.sleep(self._cleanup_interval)\n                if not self._stop_cleanup.is_set():\n                    try:\n                        self._cleanup_expired()\n                    except Exception as e:\n                        logger.error(f\"Error during cache cleanup: {e}\")\n        \n        self._cleanup_thread = threading.Thread(\n            target=cleanup_loop,\n            daemon=True,\n            name=\"CacheCleanup\"\n        )\n        self._cleanup_thread.start()\n    \n    def _start_callback_thread(self) -> None:\n        \"\"\"Start background thread for processing queued callbacks.\"\"\"\n        def callback_loop():\n            while not self._stop_callbacks.is_set():\n                try:\n                    callback, key, value, metadata = self._callback_queue.get(timeout=0.1)\n                    try:\n                        callback(key, value, metadata)\n                    except Exception as e:\n                        logger.error(f\"Error in queued callback for key '{key}': {e}\")\n                    finally:\n                        self._callback_queue.task_done()\n                except Empty:\n                    continue\n        \n        self._callback_thread = threading.Thread(\n            target=callback_loop,\n            daemon=True,\n            name=\"CacheCallbacks\"\n        )\n        self._callback_thread.start()\n    \n    def _invoke_callback(\n        self,\n        callback: Optional[CallbackProtocol],\n        key: str,\n        value: Any,\n        metadata: Optional[Dict[str, Any]] = None\n    ) -> None:\n        \"\"\"Invoke a callback according to configured mode.\"\"\"\n        if callback is None:\n            return\n        \n        if self._callback_mode == CallbackMode.SYNC:\n            try:\n                callback(key, value, metadata)\n            except Exception as e:\n                logger.error(f\"Error in sync callback for key '{key}': {e}\")\n                # In sync mode, we could raise here if desired\n        \n        elif self._callback_mode == CallbackMode.ASYNC_QUEUED:\n            try:\n                self._callback_queue.put_nowait((callback, key, value, metadata))\n            except Exception as e:\n                logger.warning(f\"Callback queue full, dropping callback for key '{key}': {e}\")\n        \n        elif self._callback_mode == CallbackMode.FIRE_AND_FORGET:\n            def run_callback():\n                try:\n                    callback(key, value, metadata)\n                except Exception as e:\n                    logger.error(f\"Error in fire-and-forget callback for key '{key}': {e}\")\n            \n            thread = threading.Thread(target=run_callback, daemon=True, name=f\"CacheCallback-{key}\")\n            thread.start()\n    \n    def close(self) -> None:\n        \"\"\"\n        Shutdown cache and cleanup threads.\n        \n        This method is idempotent and can be called multiple times.\n        \"\"\"\n        with self._lock:\n            if self._closed:\n                return\n            \n            self._closed = True\n            \n            if self._cleanup_thread:\n                self._stop_cleanup.set()\n            \n            if self._callback_thread:\n                self._stop_callbacks.set()\n        \n        # Wait for threads outside lock\n        if self._cleanup_thread:\n            self._cleanup_thread.join(timeout=self._shutdown_timeout)\n            if self._cleanup_thread.is_alive():\n                logger.warning(\"Cache cleanup thread did not terminate gracefully\")\n        \n        if self._callback_thread:\n            self._callback_thread.join(timeout=self._shutdown_timeout)\n            if self._callback_thread.is_alive():\n                logger.warning(\"Cache callback thread did not terminate gracefully\")\n    \n    def __enter__(self):\n        return self\n    \n    def __exit__(self, exc_type, exc_val, exc_tb):\n        self.close()\n    \n    def __len__(self) -> int:\n        \"\"\"Return number of entries in cache.\"\"\"\n        with self._lock:\n            return len(self._cache)\n    \n    def __contains__(self, key: str) -> bool:\n        \"\"\"Check if key exists (does not check expiration).\"\"\"\n        with self._lock:\n            return key in self._cache\n\n\n# Utility functions for namespace support\ndef namespace_key(namespace: str, key: str) -> str:\n    \"\"\"Create namespaced key.\"\"\"\n    return f\"{namespace}:{key}\"\n\n\ndef extract_namespace(namespaced_key: str) -> Tuple[Optional[str], str]:\n    \"\"\"Extract namespace and key from namespaced key.\"\"\"\n    parts = namespaced_key.split(\":\", 1)\n    if len(parts) == 2:\n        return parts[0], parts[1]\n    return None, namespaced_key\n\n\nclass NamespacedCache:\n    \"\"\"Wrapper providing automatic key namespacing.\"\"\"\n    \n    def __init__(self, cache: CacheLayer, namespace: str):\n        self._cache = cache\n        self._namespace = namespace\n    \n    def _ns_key(self, key: str) -> str:\n        return namespace_key(self._namespace, key)\n    \n    def get(self, key: str, default: Any = None) -> Any:\n        return self._cache.get(self._ns_key(key), default)\n    \n    def set(self, key: str, value: Any, ttl: Optional[int] = None) -> None:\n        self._cache.set(self._ns_key(key), value, ttl)\n    \n    def delete(self, key: str) -> bool:\n        return self._cache.delete(self._ns_key(key))\n    \n    def contains(self, key: str) -> bool:\n        return self._cache.contains(self._ns_key(key))\n```\n\n## Usage Examples\n\n```python\n# Basic usage with different stats levels\ncache = CacheLayer(\n    default_ttl=300, \n    max_size=1000,\n    stats_level=StatsLevel.BASIC  # Minimal overhead\n)\n\n# Production config with async callbacks\ncache = CacheLayer(\n    default_ttl=300,\n    max_size=10000,\n    stats_level=StatsLevel.DETAILED,\n    track_access_patterns=False,  # Disable to save memory\n    callback_mode=CallbackMode.ASYNC_QUEUED,\n    shutdown_timeout=10.0\n)\n\n# Store with default TTL\ncache.set(\"user:123\", {\"name\": \"Alice\", \"role\": \"admin\"})\n\n# Bulk operations with error tracking\nresult = cache.set_many({\n    \"user:1\": {\"name\": \"Alice\"},\n    \"user:2\": {\"name\": \"Bob\"}\n}, ttl=300, continue_on_error=True)\n\nprint(f\"Successful: {result.success_count}, Failed: {result.failure_count}\")\nfor key, error in result.failed.items():\n    print(f\"Failed to set {key}: {error}\")\n\n# Get or compute with request coalescing\ndef fetch_user_from_db():\n    return db.query(\"SELECT * FROM users WHERE id = 123\")\n\nuser = cache.get_or_compute(\"user:123\", fetch_user_from_db, ttl=600)\n\n# Monitoring hooks with metadata\ndef on_evict(key: str, value: Any, metadata: Optional[Dict] = None):\n    if metadata:\n        reason = metadata.get(\"reason\", \"unknown\")\n        access_count = metadata.get(\"access_count\", 0)\n        print(f\"Evicted {key}: reason={reason}, accesses={access_count}\")\n\ncache = CacheLayer(\n    on_eviction=on_evict,\n    callback_mode=CallbackMode.ASYNC_QUEUED  # Non-blocking\n)\n\n# Access patterns (when enabled)\ncache_with_tracking = CacheLayer(\n    track_access_patterns=True,\n    stats_level=StatsLevel.DETAILED\n)\ncache_with_tracking.set(\"key1\", \"value1\")\ncache_with_tracking.get(\"key1\")\ncache_with_tracking.get(\"key1\")\n\npatterns = cache_with_tracking.get_access_patterns()\nprint(patterns[\"key1\"])  # {\"access_count\": 2, \"last_accessed\": \"...\", ...}\n\n# Namespace support for multi-tenancy\nshared_cache = CacheLayer()\ntenant_a = NamespacedCache(shared_cache, \"tenant_a\")\ntenant_b = NamespacedCache(shared_cache, \"tenant_b\")\n\ntenant_a.set(\"user:1\", {\"name\": \"Alice\"})\ntenant_b.set(\"user:1\", {\"name\": \"Bob\"})  # Different namespace, no conflict\n\n# Context manager for cleanup\nwith CacheLayer(default_ttl=60) as cache:\n    cache.set(\"temp\", \"data\")\n    # Automatically closes on exit\n```\n\n## Configuration Guide\n\n### Performance vs Memory Trade-offs\n\n| Configuration | Memory Usage | Performance | Use Case |\n|---------------|--------------|-------------|----------|\n| `stats_level=NONE, track_access_patterns=False` | Minimal | Highest | High-throughput, memory-constrained |\n| `stats_level=BASIC, track_access_patterns=False` | Low | High | Production default |\n| `stats_level=DETAILED, track_access_patterns=True` | High | Lower | Development, analytics |\n\n### Callback Modes\n\n| Mode | Blocking | Guaranteed Delivery | Use Case |\n|------|----------|---------------------|----------|\n| `SYNC` | Yes | Yes | Critical operations, testing |\n| `ASYNC_QUEUED` | No | Best-effort | Metrics export, monitoring |\n| `FIRE_AND_FORGET` | No | No | Logging, non-critical notifications |\n\n## Error Handling\n\n```python\n# Bulk operations report partial failures\nresult = cache.set_many({\"\": \"invalid\", \"valid\": \"data\"})\nassert result.failure_count == 1\nassert \"\" in result.failed\nassert result.failed[\"\"] == \"Cache key cannot be empty\"\n\n# Access patterns require explicit enablement\ncache = CacheLayer(track_access_patterns=False)\ntry:\n    cache.get_access_patterns()\nexcept ValueError as e:\n    print(f\"Cannot get patterns: {e}\")\n\n# Thread-safe shutdown\ncache.close()\ntry:\n    cache.get(\"key\")\nexcept CacheShutdownError:\n    print(\"Cache is closed\")\n```\n\n## Performance Characteristics\n\n- **Get**: O(1) average case\n- **Set**: O(1) average case\n- **Delete**: O(1)\n- **Bulk operations**: O(k) where k = number of keys\n- **Cleanup**: O(n) where n = number of entries (background thread)\n- **get_or_compute**: O(1) cache hit, O(compute) + O(1) on miss\n- **Memory overhead per entry**: \n  - Base: ~120 bytes (entry object, datetime, OrderedDict node)\n  - With access tracking: ~140 bytes (adds access_count, last_accessed)"
}