{
  "initial_prompt": "Design a caching layer with Redis",
  "status": "consensus",
  "rounds": 2,
  "started_at": "2026-01-17T08:36:07.602757",
  "ended_at": "2026-01-17T08:44:36.861399",
  "final_design": "## Design\n\n# Redis Caching Layer\n\n## Architecture Overview\n\nA production-ready caching layer implementing write-through/write-behind patterns with Redis, featuring TTL-based expiration, cache warming, connection pooling, and comprehensive observability. Designed for high-throughput scenarios (100k+ req/s) with optimizations for scalability and memory efficiency.\n\n### Core Components\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Application Layer                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                  \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    CacheManager                              \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n\u2502  \u2502 L1: In-Memory LRU (hot data, <1ms latency)            \u2502 \u2502\n\u2502  \u2502   - Adaptive size limits based on memory pressure      \u2502 \u2502\n\u2502  \u2502   - Probabilistic early refresh for hot keys           \u2502 \u2502\n\u2502  \u2502   - Bloom filter for negative caching                  \u2502 \u2502\n\u2502  \u2502 L2: Redis (warm data, ~1-5ms latency)                 \u2502 \u2502\n\u2502  \u2502 Operations: get, set, delete, invalidate               \u2502 \u2502\n\u2502  \u2502 Patterns: TTL, sliding expiration, tags, write-behind  \u2502 \u2502\n\u2502  \u2502 Metrics: hits, misses, latency per tier                \u2502 \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502                                         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510              \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   RedisClient          \u2502              \u2502  FallbackStrategy   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502              \u2502  - L1 cache only    \u2502\n\u2502  \u2502 Connection Pool  \u2502  \u2502              \u2502  - Degraded mode    \u2502\n\u2502  \u2502 Retry Logic      \u2502  \u2502              \u2502  - Circuit breaker  \u2502\n\u2502  \u2502 Pipelining       \u2502  \u2502              \u2502  - Backpressure     \u2502\n\u2502  \u2502 Serialization    \u2502  \u2502              \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25bc\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     Redis Cluster                           \u2502\n\u2502  Master/Replica Setup with Sentinel/Cluster Mode            \u2502\n\u2502  + Message broker (Kafka/NATS) for distributed invalidation\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Data Models\n\n### Cache Entry Structure\n\n```typescript\ninterface CacheEntry<T> {\n  key: string;\n  value: T;\n  metadata: {\n    createdAt: number;\n    expiresAt: number;\n    lastAccessedAt: number;\n    accessCount: number;\n    version: number;\n    tags: string[];\n    size: number;\n    compressionType?: 'gzip' | 'lz4' | 'none';\n    durability?: 'immediate' | 'deferred';\n  };\n}\n\ninterface CacheKey {\n  namespace: string;\n  identifier: string;\n  version?: string;\n}\n```\n\n### Key Naming Convention\n\n```\nFormat: {namespace}:{entity_type}:{identifier}:{version}\nExamples:\n  - user:profile:12345:v1\n  - product:details:abc-789:v2\n  - session:data:token-xyz\n  - query:results:hash-abc123\n```\n\n## Core Interfaces\n\n### CacheManager Interface\n\n```typescript\ninterface ICacheManager {\n  // Basic operations\n  get<T>(key: CacheKey, options?: GetOptions): Promise<T | null>;\n  set<T>(key: CacheKey, value: T, options?: SetOptions): Promise<void>;\n  delete(key: CacheKey): Promise<boolean>;\n  exists(key: CacheKey): Promise<boolean>;\n  \n  // Batch operations\n  mget<T>(keys: CacheKey[]): Promise<Map<string, T>>;\n  mset<T>(entries: Map<CacheKey, T>, options?: SetOptions): Promise<void>;\n  \n  // Pattern-based operations\n  invalidateByTag(tag: string): Promise<number>;\n  invalidateByPattern(pattern: string): Promise<number>;\n  invalidateByPrefix(prefix: string): Promise<number>;\n  \n  // Cache warming and preloading\n  warm<T>(key: CacheKey, loader: () => Promise<T>, options?: WarmOptions): Promise<void>;\n  warmBatch<T>(loaders: Map<CacheKey, () => Promise<T>>, options?: WarmOptions): Promise<void>;\n  \n  // Distributed cache invalidation\n  broadcastInvalidation(keys: CacheKey[], options?: BroadcastOptions): Promise<void>;\n  \n  // Observability\n  getStats(): CacheStats;\n  getStatsPerTier(): TieredCacheStats;\n  healthCheck(): Promise<HealthStatus>;\n  \n  // Lifecycle\n  flush(): Promise<void>;\n  close(): Promise<void>;\n}\n\ninterface GetOptions {\n  ttl?: number;\n  refresh?: boolean;\n  fallback?: () => Promise<any>;\n  skipL1?: boolean;\n  promoteToL1?: boolean;\n  staleIfError?: boolean;\n  maxStaleTime?: number;\n}\n\ninterface SetOptions {\n  ttl?: number;\n  nx?: boolean;\n  xx?: boolean;\n  tags?: string[];\n  compress?: boolean;\n  compressionType?: 'gzip' | 'lz4';\n  writeThrough?: boolean;\n  writeBehind?: boolean;\n  durability?: 'immediate' | 'deferred' | 'eventual';\n  l1Only?: boolean;\n  l2Only?: boolean;\n}\n\ninterface WarmOptions {\n  ttl?: number;\n  background?: boolean;\n  force?: boolean;\n  batchSize?: number;\n  concurrency?: number;\n  priority?: 'high' | 'normal' | 'low';\n}\n\ninterface BroadcastOptions {\n  reliable?: boolean;\n  timeout?: number;\n}\n```\n\n### Redis Client Interface\n\n```typescript\ninterface IRedisClient {\n  // Connection management\n  connect(): Promise<void>;\n  disconnect(): Promise<void>;\n  isConnected(): boolean;\n  getConnectionPoolStats(): PoolStats;\n  \n  // Core commands\n  get(key: string): Promise<string | null>;\n  set(key: string, value: string, options?: RedisSetOptions): Promise<void>;\n  del(keys: string[]): Promise<number>;\n  exists(keys: string[]): Promise<number>;\n  ttl(key: string): Promise<number>;\n  \n  // Batch operations\n  mget(keys: string[]): Promise<(string | null)[]>;\n  mset(entries: Record<string, string>): Promise<void>;\n  \n  // Advanced commands\n  pipeline(): RedisPipeline;\n  multi(): RedisTransaction;\n  scan(cursor: number, pattern?: string, count?: number): Promise<ScanResult>;\n  \n  // Set operations for tag indexing\n  sadd(key: string, members: string[]): Promise<number>;\n  smembers(key: string): Promise<string[]>;\n  srem(key: string, members: string[]): Promise<number>;\n  \n  // Memory management\n  memory(subcommand: 'USAGE', key: string): Promise<number>;\n}\n\ninterface RedisSetOptions {\n  ex?: number;\n  px?: number;\n  nx?: boolean;\n  xx?: boolean;\n  keepTtl?: boolean;\n}\n\ninterface PoolStats {\n  totalConnections: number;\n  activeConnections: number;\n  idleConnections: number;\n  waitingClients: number;\n  avgWaitTime: number;\n}\n```\n\n## Implementation Details\n\n### Two-Tier Cache Manager with Enhancements\n\n```typescript\nclass CacheManager implements ICacheManager {\n  private l1Cache: AdaptiveLRUCache<string, any>;\n  private redis: IRedisClient;\n  private serializer: ISerializer;\n  private metrics: MetricsCollector;\n  private fallback: FallbackStrategy;\n  private config: CacheConfig;\n  private writeQueue: WriteQueue;\n  private invalidationBroker: InvalidationBroker;\n  private bloomFilter: BloomFilter;\n  private hotKeyDetector: HotKeyDetector;\n\n  constructor(config: CacheConfig) {\n    this.config = config;\n    this.redis = new RedisClient(config.redis);\n    this.serializer = this.createSerializer(config.serialization);\n    this.metrics = new MetricsCollector(config.metrics);\n    this.fallback = new FallbackStrategy(config.fallback);\n    \n    // L1 cache with adaptive sizing\n    this.l1Cache = new AdaptiveLRUCache<string, any>({\n      maxItems: config.l1.maxItems,\n      maxSize: config.l1.maxSize,\n      ttl: config.l1.ttl * 1000,\n      adaptiveThreshold: config.l1.adaptiveThreshold,\n      onEvict: (value, key) => this.metrics.recordL1Eviction(key)\n    });\n    \n    // Write-behind queue with durability options\n    if (config.writeBehind.enabled) {\n      this.writeQueue = new WriteQueue({\n        batchSize: config.writeBehind.batchSize,\n        flushInterval: config.writeBehind.flushInterval,\n        redis: this.redis,\n        serializer: this.serializer,\n        persistenceLog: config.writeBehind.persistenceLog,\n        onError: (error) => this.metrics.recordWriteBehindError(error)\n      });\n    }\n    \n    // Distributed invalidation via message broker\n    if (config.distributed.enabled) {\n      this.invalidationBroker = new InvalidationBroker({\n        type: config.distributed.brokerType,\n        config: config.distributed.brokerConfig,\n        onInvalidation: (keys) => this.handleDistributedInvalidation(keys)\n      });\n    }\n    \n    // Bloom filter for negative caching\n    if (config.negativeCache.enabled) {\n      this.bloomFilter = new BloomFilter({\n        expectedItems: config.negativeCache.expectedItems,\n        falsePositiveRate: config.negativeCache.falsePositiveRate\n      });\n    }\n    \n    // Hot key detection and probabilistic refresh\n    this.hotKeyDetector = new HotKeyDetector({\n      threshold: config.hotKey.threshold,\n      window: config.hotKey.window,\n      refreshProbability: config.hotKey.refreshProbability\n    });\n  }\n\n  async get<T>(key: CacheKey, options?: GetOptions): Promise<T | null> {\n    const startTime = performance.now();\n    const redisKey = this.buildRedisKey(key);\n    \n    try {\n      // Check bloom filter for negative cache\n      if (this.bloomFilter && !this.bloomFilter.mightContain(redisKey)) {\n        this.metrics.recordBloomFilterHit(redisKey);\n        return null;\n      }\n      \n      // L1 cache check\n      if (!options?.skipL1) {\n        const l1Value = this.l1Cache.get(redisKey);\n        if (l1Value !== undefined) {\n          this.metrics.recordHit('L1', redisKey, performance.now() - startTime);\n          \n          // Probabilistic early refresh for hot keys\n          if (this.shouldProbabilisticRefresh(redisKey, l1Value)) {\n            this.refreshKeyInBackground(key, options?.fallback);\n          }\n          \n          return l1Value as T;\n        }\n      }\n      \n      // L2 cache check\n      const cached = await this.redis.get(redisKey);\n      \n      if (cached) {\n        this.metrics.recordHit('L2', redisKey, performance.now() - startTime);\n        const entry: CacheEntry<T> = await this.deserialize(cached);\n        \n        // Update bloom filter\n        if (this.bloomFilter) {\n          this.bloomFilter.add(redisKey);\n        }\n        \n        // Adaptive promotion based on access patterns\n        if (this.shouldPromoteToL1(entry, redisKey)) {\n          this.l1Cache.set(redisKey, entry.value, {\n            ttl: Math.min(entry.metadata.expiresAt - Date.now(), this.config.l1.ttl * 1000)\n          });\n        }\n        \n        // Sliding expiration\n        if (options?.ttl) {\n          await this.redis.set(redisKey, cached, { ex: options.ttl, keepTtl: false });\n        }\n        \n        return entry.value;\n      }\n      \n      this.metrics.recordMiss(redisKey, performance.now() - startTime);\n      \n      // Fallback to loader\n      if (options?.fallback) {\n        const value = await options.fallback();\n        if (value !== null) {\n          await this.set(key, value, { ttl: options.ttl });\n        }\n        return value;\n      }\n      \n      return null;\n    } catch (error) {\n      this.metrics.recordError(redisKey, error);\n      \n      // Stale-if-error fallback\n      if (options?.staleIfError) {\n        const staleValue = this.l1Cache.getStale(redisKey, options.maxStaleTime);\n        if (staleValue !== undefined) {\n          this.metrics.recordStaleServed(redisKey);\n          return staleValue as T;\n        }\n      }\n      \n      return this.fallback.handle<T>(key, error, this.l1Cache);\n    }\n  }\n\n  async set<T>(key: CacheKey, value: T, options?: SetOptions): Promise<void> {\n    const redisKey = this.buildRedisKey(key);\n    const startTime = performance.now();\n    \n    try {\n      // Store in L1 if not L2-only\n      if (!options?.l2Only) {\n        this.l1Cache.set(redisKey, value);\n      }\n      \n      // Update bloom filter\n      if (this.bloomFilter) {\n        this.bloomFilter.add(redisKey);\n      }\n      \n      // Skip L2 if L1-only\n      if (options?.l1Only) {\n        this.metrics.recordSet('L1', redisKey, performance.now() - startTime);\n        return;\n      }\n      \n      const entry: CacheEntry<T> = {\n        key: redisKey,\n        value,\n        metadata: {\n          createdAt: Date.now(),\n          expiresAt: Date.now() + (options?.ttl || this.config.defaultTtl) * 1000,\n          lastAccessedAt: Date.now(),\n          accessCount: 0,\n          version: this.config.version,\n          tags: options?.tags || [],\n          size: this.estimateSize(value),\n          compressionType: 'none',\n          durability: options?.durability || 'immediate'\n        }\n      };\n\n      let serialized = await this.serialize(entry);\n      \n      // Adaptive compression\n      if (this.shouldCompress(serialized, options)) {\n        const compressionType = options?.compressionType || 'lz4';\n        serialized = await this.compress(serialized, compressionType);\n        entry.metadata.compressionType = compressionType;\n      }\n\n      // Handle durability guarantees\n      if (options?.durability === 'immediate' || !options?.writeBehind) {\n        await this.redis.set(redisKey, serialized, {\n          ex: options?.ttl || this.config.defaultTtl,\n          nx: options?.nx,\n          xx: options?.xx\n        });\n      } else if (options?.durability === 'deferred' && this.writeQueue) {\n        this.writeQueue.enqueue(redisKey, serialized, {\n          ex: options?.ttl || this.config.defaultTtl,\n          nx: options?.nx,\n          xx: options?.xx\n        }, 'normal');\n      } else if (options?.durability === 'eventual' && this.writeQueue) {\n        this.writeQueue.enqueue(redisKey, serialized, {\n          ex: options?.ttl || this.config.defaultTtl\n        }, 'low');\n      }\n\n      // Store tags\n      if (options?.tags?.length) {\n        await this.indexTags(redisKey, options.tags);\n      }\n\n      this.metrics.recordSet('L2', redisKey, performance.now() - startTime);\n    } catch (error) {\n      this.metrics.recordError(redisKey, error);\n      throw new CacheWriteError(`Failed to set cache key ${redisKey}`, error);\n    }\n  }\n\n  async mget<T>(keys: CacheKey[]): Promise<Map<string, T>> {\n    const startTime = performance.now();\n    const results = new Map<string, T>();\n    const redisKeys = keys.map(k => this.buildRedisKey(k));\n    const missingKeys: string[] = [];\n    \n    // Check L1 first\n    for (const redisKey of redisKeys) {\n      const l1Value = this.l1Cache.get(redisKey);\n      if (l1Value !== undefined) {\n        results.set(redisKey, l1Value);\n        this.metrics.recordHit('L1', redisKey, 0);\n      } else {\n        missingKeys.push(redisKey);\n      }\n    }\n    \n    // Batch fetch from Redis\n    if (missingKeys.length > 0) {\n      const values = await this.redis.mget(missingKeys);\n      \n      for (let i = 0; i < missingKeys.length; i++) {\n        if (values[i]) {\n          const entry: CacheEntry<T> = await this.deserialize(values[i]!);\n          results.set(missingKeys[i], entry.value);\n          \n          if (this.shouldPromoteToL1(entry, missingKeys[i])) {\n            this.l1Cache.set(missingKeys[i], entry.value);\n          }\n          \n          this.metrics.recordHit('L2', missingKeys[i], 0);\n        } else {\n          this.metrics.recordMiss(missingKeys[i], 0);\n        }\n      }\n    }\n    \n    this.metrics.recordBatchOperation('mget', keys.length, performance.now() - startTime);\n    return results;\n  }\n\n  async broadcastInvalidation(keys: CacheKey[], options?: BroadcastOptions): Promise<void> {\n    if (!this.invalidationBroker) {\n      throw new Error('Distributed invalidation not enabled');\n    }\n    \n    const redisKeys = keys.map(k => this.buildRedisKey(k));\n    await this.invalidationBroker.broadcast(redisKeys, {\n      reliable: options?.reliable ?? false,\n      timeout: options?.timeout ?? 5000\n    });\n  }\n\n  async invalidateByPrefix(prefix: string): Promise<number> {\n    let cursor = 0;\n    let deletedCount = 0;\n    const keysToDelete: string[] = [];\n    \n    do {\n      const result = await this.redis.scan(cursor, `${prefix}*`, 100);\n      cursor = result.cursor;\n      keysToDelete.push(...result.keys);\n      \n      if (keysToDelete.length >= 100) {\n        deletedCount += await this.deleteBatch(keysToDelete);\n        keysToDelete.length = 0;\n      }\n    } while (cursor !== 0);\n    \n    if (keysToDelete.length > 0) {\n      deletedCount += await this.deleteBatch(keysToDelete);\n    }\n    \n    this.metrics.recordInvalidation('prefix', prefix, deletedCount);\n    return deletedCount;\n  }\n\n  private async deleteBatch(keys: string[]): Promise<number> {\n    // Invalidate L1\n    keys.forEach(key => this.l1Cache.delete(key));\n    \n    // Delete from Redis\n    const deleted = await this.redis.del(keys);\n    \n    // Broadcast invalidation\n    if (this.invalidationBroker) {\n      await this.invalidationBroker.broadcast(keys, { reliable: false });\n    }\n    \n    return deleted;\n  }\n\n  private shouldPromoteToL1(entry: CacheEntry<any>, key: string): boolean {\n    // Adaptive promotion based on size, access patterns, and memory pressure\n    if (entry.metadata.size > this.config.l1.maxItemSize) {\n      return false;\n    }\n    \n    // Check hot key detector\n    const isHot = this.hotKeyDetector.isHot(key);\n    \n    // Consider memory pressure\n    const memoryPressure = this.l1Cache.getMemoryPressure();\n    \n    if (memoryPressure > 0.9 && !isHot) {\n      return false;\n    }\n    \n    return isHot || entry.metadata.accessCount > 5;\n  }\n\n  private shouldProbabilisticRefresh(key: string, value: any): boolean {\n    return this.hotKeyDetector.shouldRefresh(key);\n  }\n\n  private async refreshKeyInBackground(key: CacheKey, loader?: () => Promise<any>): Promise<void> {\n    if (!loader) return;\n    \n    // Non-blocking background refresh\n    setImmediate(async () => {\n      try {\n        const newValue = await loader();\n        await this.set(key, newValue);\n        this.metrics.recordBackgroundRefresh(this.buildRedisKey(key));\n      } catch (error) {\n        this.metrics.recordBackgroundRefreshError(this.buildRedisKey(key), error);\n      }\n    });\n  }\n\n  private shouldCompress(serialized: string, options?: SetOptions): boolean {\n    if (options?.compress === false) return false;\n    if (options?.compress === true) return true;\n    return serialized.length > this.config.compressionThreshold;\n  }\n\n  private async indexTags(key: string, tags: string[]): Promise<void> {\n    const pipeline = this.redis.pipeline();\n    tags.forEach(tag => {\n      pipeline.sadd(`tag:${tag}`, [key]);\n      pipeline.expire(`tag:${tag}`, this.config.tagIndexTtl);\n    });\n    await pipeline.exec();\n  }\n\n  private handleDistributedInvalidation(keys: string[]): void {\n    keys.forEach(key => this.l1Cache.delete(key));\n    this.metrics.recordDistributedInvalidation(keys.length);\n  }\n\n  private buildRedisKey(key: CacheKey): string {\n    const parts = [key.namespace, key.identifier];\n    if (key.version) parts.push(key.version);\n    return parts.join(':');\n  }\n\n  private estimateSize(value: any): number {\n    return JSON.stringify(value).length * 2;\n  }\n\n  private createSerializer(config: SerializationConfig): ISerializer {\n    return config.format === 'msgpack' \n      ? new MsgPackSerializer() \n      : new JSONSerializer();\n  }\n\n  private async serialize<T>(value: T): Promise<string> {\n    return this.serializer.serialize(value);\n  }\n\n  private async deserialize<T>(data: string): Promise<T> {\n    return this.serializer.deserialize(data);\n  }\n\n  private async compress(data: string, type: 'gzip' | 'lz4'): Promise<string> {\n    if (type === 'lz4') {\n      return lz4.compress(Buffer.from(data)).toString('base64');\n    } else {\n      return zlib.gzipSync(data).toString('base64');\n    }\n  }\n\n  getStatsPerTier(): TieredCacheStats {\n    return {\n      l1: {\n        size: this.l1Cache.size,\n        maxSize: this.l1Cache.max,\n        hits: this.metrics.getL1Hits(),\n        misses: this.metrics.getL1Misses(),\n        evictions: this.metrics.getL1Evictions(),\n        hitRate: this.metrics.getL1HitRate(),\n        memoryPressure: this.l1Cache.getMemoryPressure()\n      },\n      l2: {\n        hits: this.metrics.getL2Hits(),\n        misses: this.metrics.getL2Misses(),\n        hitRate: this.metrics.getL2HitRate(),\n        avgLatency: this.metrics.getL2AvgLatency()\n      }\n    };\n  }\n\n  async close(): Promise<void> {\n    if (this.writeQueue) {\n      await this.writeQueue.flush();\n    }\n    if (this.invalidationBroker) {\n      await this.invalidationBroker.close();\n    }\n    await this.redis.disconnect();\n  }\n}\n```\n\n### Adaptive LRU Cache\n\n```typescript\nclass AdaptiveLRUCache<K, V> {\n  private cache: LRUCache<K, V>;\n  private currentSize: number = 0;\n  private maxSize: number;\n  private adaptiveThreshold: number;\n\n  constructor(options: AdaptiveLRUOptions) {\n    this.maxSize = options.maxSize;\n    this.adaptiveThreshold = options.adaptiveThreshold;\n    \n    this.cache = new LRUCache<K, V>({\n      max: options.maxItems,\n      sizeCalculation: (value) => this.calculateSize(value),\n      ttl: options.ttl,\n      updateAgeOnGet: true,\n      dispose: (value, key) => {\n        this.currentSize -= this.calculateSize(value);\n        options.onEvict?.(value, key);\n      }\n    });\n  }\n\n  get(key: K): V | undefined {\n    return this.cache.get(key);\n  }\n\n  getStale(key: K, maxStaleTime?: number): V | undefined {\n    const entry = this.cache.getRemainingTTL(key);\n    if (entry < 0 && maxStaleTime && Math.abs(entry) < maxStaleTime) {\n      return this.cache.get(key, { allowStale: true });\n    }\n    return undefined;\n  }\n\n  set(key: K, value: V, options?: { ttl?: number }): void {\n    const size = this.calculateSize(value);\n    \n    // Adaptive eviction based on memory pressure\n    if (this.getMemoryPressure() > this.adaptiveThreshold) {\n      this.evictLowPriorityItems();\n    }\n    \n    this.cache.set(key, value, { ttl: options?.ttl });\n    this.currentSize += size;\n  }\n\n  delete(key: K): boolean {\n    const value = this.cache.get(key);\n    if (value !== undefined) {\n      this.currentSize -= this.calculateSize(value);\n    }\n    return this.cache.delete(key);\n  }\n\n  getMemoryPressure(): number {\n    return this.currentSize / this.maxSize;\n  }\n\n  private evictLowPriorityItems(): void {\n    // Evict items with lowest access count\n    const entries = Array.from(this.cache.entries());\n    entries.sort((a, b) => {\n      const aAccess = (a[1] as any).metadata?.accessCount || 0;\n      const bAccess = (b[1] as any).metadata?.accessCount || 0;\n      return aAccess - bAccess;\n    });\n    \n    for (let i = 0; i < Math.min(10, entries.length); i++) {\n      this.cache.delete(entries[i][0]);\n    }\n  }\n\n  private calculateSize(value: V): number {\n    return JSON.stringify(value).length * 2;\n  }\n\n  get size(): number {\n    return this.cache.size;\n  }\n\n  get max(): number {\n    return this.cache.max;\n  }\n}\n\ninterface AdaptiveLRUOptions {\n  maxItems: number;\n  maxSize: number;\n  ttl: number;\n  adaptiveThreshold: number;\n  onEvict?: (value: any, key: any) => void;\n}\n```\n\n### Write-Behind Queue with Durability\n\n```typescript\nclass WriteQueue {\n  private highPriorityQueue: Map<string, QueueEntry> = new Map();\n  private normalPriorityQueue: Map<string, QueueEntry> = new Map();\n  private lowPriorityQueue: Map<string, QueueEntry> = new Map();\n  private timer: NodeJS.Timeout | null = null;\n  private processing = false;\n  private wal: WriteAheadLog | null = null;\n\n  constructor(private config: WriteQueueConfig) {\n    if (config.persistenceLog) {\n      this.wal = new WriteAheadLog(config.persistenceLog);\n    }\n    this.startFlushTimer();\n  }\n\n  enqueue(key: string, value: string, options: RedisSetOptions, priority: 'high' | 'normal' | 'low' = 'normal'): void {\n    const entry = { value, options, enqueuedAt: Date.now() };\n    \n    // Log to WAL for durability\n    if (this.wal) {\n      this.wal.append(key, value, options);\n    }\n    \n    const queue = this.getQueue(priority);\n    queue.set(key, entry);\n    \n    if (this.getTotalQueueSize() >= this.config.batchSize) {\n      this.flush();\n    }\n  }\n\n  async flush(): Promise<void> {\n    if (this.processing) return;\n    \n    this.processing = true;\n    \n    try {\n      // Flush high priority first\n      await this.flushQueue(this.highPriorityQueue);\n      await this.flushQueue(this.normalPriorityQueue);\n      await this.flushQueue(this.lowPriorityQueue);\n      \n      // Clear WAL\n      if (this.wal) {\n        await this.wal.clear();\n      }\n    } catch (error) {\n      this.config.onError(error);\n    } finally {\n      this.processing = false;\n    }\n  }\n\n  private async flushQueue(queue: Map<string, QueueEntry>): Promise<void> {\n    if (queue.size === 0) return;\n    \n    const batch = new Map(queue);\n    queue.clear();\n    \n    try {\n      const pipeline = this.config.redis.pipeline();\n      \n      for (const [key, entry] of batch) {\n        pipeline.set(key, entry.value, entry.options);\n      }\n      \n      await pipeline.exec();\n    } catch (error) {\n      // Re-queue failed writes to high priority\n      for (const [key, entry] of batch) {\n        this.highPriorityQueue.set(key, entry);\n      }\n      throw error;\n    }\n  }\n\n  private getQueue(priority: 'high' | 'normal' | 'low'): Map<string, QueueEntry> {\n    switch (priority) {\n      case 'high': return this.highPriorityQueue;\n      case 'low': return this.lowPriorityQueue;\n      default: return this.normalPriorityQueue;\n    }\n  }\n\n  private getTotalQueueSize(): number {\n    return this.highPriorityQueue.size + this.normalPriorityQueue.size + this.lowPriorityQueue.size;\n  }\n\n  private startFlushTimer(): void {\n    this.timer = setInterval(() => {\n      this.flush();\n    }, this.config.flushInterval);\n  }\n\n  async close(): Promise<void> {\n    if (this.timer) clearInterval(this.timer);\n    await this.flush();\n    if (this.wal) {\n      await this.wal.close();\n    }\n  }\n}\n\ninterface QueueEntry {\n  value: string;\n  options: RedisSetOptions;\n  enqueuedAt: number;\n}\n\ninterface WriteQueueConfig {\n  batchSize: number;\n  flushInterval: number;\n  redis: IRedisClient;\n  serializer: ISerializer;\n  persistenceLog?: string;\n  onError: (error: Error) => void;\n}\n```\n\n### Write-Ahead Log for Durability\n\n```typescript\nclass WriteAheadLog {\n  private fd: number | null = null;\n  private buffer: Buffer[] = [];\n\n  constructor(private logPath: string) {\n    this.fd = fs.openSync(logPath, 'a');\n  }\n\n  append(key: string, value: string, options: RedisSetOptions): void {\n    const entry = JSON.stringify({ key, value, options, timestamp: Date.now() });\n    const line = Buffer.from(entry + '\\n');\n    this.buffer.push(line);\n    \n    if (this.buffer.length >= 100) {\n      this.flush();\n    }\n  }\n\n  flush(): void {\n    if (this.fd === null || this.buffer.length === 0) return;\n    \n    const data = Buffer.concat(this.buffer);\n    fs.writeSync(this.fd, data);\n    this.buffer = [];\n  }\n\n  async clear(): Promise<void> {\n    if (this.fd !== null) {\n      fs.ftruncateSync(this.fd, 0);\n    }\n  }\n\n  async close(): Promise<void> {\n    this.flush();\n    if (this.fd !== null) {\n      fs.closeSync(this.fd);\n      this.fd = null;\n    }\n  }\n}\n```\n\n### Distributed Invalidation Broker\n\n```typescript\nclass InvalidationBroker {\n  private client: any;\n  private subscribed = false;\n\n  constructor(private config: InvalidationBrokerConfig) {\n    this.initialize();\n  }\n\n  private async initialize(): Promise<void> {\n    if (this.config.type === 'kafka') {\n      const { Kafka } = require('kafkajs');\n      const kafka = new Kafka(this.config.config);\n      \n      this.client = {\n        producer: kafka.producer(),\n        consumer: kafka.consumer({ groupId: 'cache-invalidation' })\n      };\n      \n      await this.client.producer.connect();\n      await this.client.consumer.connect();\n      await this.client.consumer.subscribe({ topic: 'cache-invalidation', fromBeginning: false });\n      \n      await this.client.consumer.run({\n        eachMessage: async ({ message }) => {\n          const keys = JSON.parse(message.value.toString());\n          this.config.onInvalidation(keys);\n        }\n      });\n      \n    } else if (this.config.type === 'nats') {\n      const { connect } = require('nats');\n      this.client = await connect(this.config.config);\n      \n      const sub = this.client.subscribe('cache.invalidation');\n      (async () => {\n        for await (const msg of sub) {\n          const keys = JSON.parse(msg.data);\n          this.config.onInvalidation(keys);\n        }\n      })();\n    } else if (this.config.type === 'redis-pubsub') {\n      // Fallback to Redis Pub/Sub\n      const Redis = require('ioredis');\n      this.client = new Redis(this.config.config);\n      \n      await this.client.subscribe('cache:invalidation', (err: Error) => {\n        if (err) throw err;\n      });\n      \n      this.client.on('message', (channel: string, message: string) => {\n        if (channel === 'cache:invalidation') {\n          const keys = JSON.parse(message);\n          this.config.onInvalidation(keys);\n        }\n      });\n    }\n    \n    this.subscribed = true;\n  }\n\n  async broadcast(keys: string[], options?: { reliable?: boolean; timeout?: number }): Promise<void> {\n    if (!this.subscribed) return;\n    \n    const message = JSON.stringify(keys);\n    \n    if (this.config.type === 'kafka') {\n      await this.client.producer.send({\n        topic: 'cache-invalidation',\n        messages: [{ value: message }],\n        timeout: options?.timeout\n      });\n    } else if (this.config.type === 'nats') {\n      this.client.publish('cache.invalidation', message);\n    } else if (this.config.type === 'redis-pubsub') {\n      await this.client.publish('cache:invalidation', message);\n    }\n  }\n\n  async close(): Promise<void> {\n    if (!this.subscribed) return;\n    \n    if (this.config.type === 'kafka') {\n      await this.client.producer.disconnect();\n      await this.client.consumer.disconnect();\n    } else if (this.config.type === 'nats') {\n      await this.client.close();\n    } else if (this.config.type === 'redis-pubsub') {\n      await this.client.quit();\n    }\n    \n    this.subscribed = false;\n  }\n}\n\ninterface InvalidationBrokerConfig {\n  type: 'kafka' | 'nats' | 'redis-pubsub';\n  config: any;\n  onInvalidation: (keys: string[]) => void;\n}\n```\n\n### Bloom Filter for Negative Caching\n\n```typescript\nclass BloomFilter {\n  private bits: Uint8Array;\n  private numBits: number;\n  private numHashes: number;\n\n  constructor(options: BloomFilterOptions) {\n    this.numBits = this.calculateBitSize(options.expectedItems, options.falsePositiveRate);\n    this.numHashes = this.calculateNumHashes(this.numBits, options.expectedItems);\n    this.bits = new Uint8Array(Math.ceil(this.numBits / 8));\n  }\n\n  add(key: string): void {\n    const hashes = this.getHashes(key);\n    hashes.forEach(hash => {\n      const index = hash % this.numBits;\n      this.bits[Math.floor(index / 8)] |= (1 << (index % 8));\n    });\n  }\n\n  mightContain(key: string): boolean {\n    const hashes = this.getHashes(key);\n    return hashes.every(hash => {\n      const index = hash % this.numBits;\n      return (this.bits[Math.floor(index / 8)] & (1 << (index % 8))) !== 0;\n    });\n  }\n\n  private getHashes(key: string): number[] {\n    const hashes: number[] = [];\n    for (let i = 0; i < this.numHashes; i++) {\n      const hash = this.hash(key + i);\n      hashes.push(hash);\n    }\n    return hashes;\n  }\n\n  private hash(str: string): number {\n    let hash = 0;\n    for (let i = 0; i < str.length; i++) {\n      hash = ((hash << 5) - hash) + str.charCodeAt(i);\n      hash |= 0;\n    }\n    return Math.abs(hash);\n  }\n\n  private calculateBitSize(n: number, p: number): number {\n    return Math.ceil(-(n * Math.log(p)) / (Math.log(2) ** 2));\n  }\n\n  private calculateNumHashes(m: number, n: number): number {\n    return Math.ceil((m / n) * Math.log(2));\n  }\n}\n\ninterface BloomFilterOptions {\n  expectedItems: number;\n  falsePositiveRate: number;\n}\n```\n\n### Hot Key Detection\n\n```typescript\nclass HotKeyDetector {\n  private accessCounts: Map<string, AccessInfo> = new Map();\n  private threshold: number;\n  private window: number;\n  private refreshProbability: number;\n\n  constructor(config: HotKeyConfig) {\n    this.threshold = config.threshold;\n    this.window = config.window;\n    this.refreshProbability = config.refreshProbability;\n    \n    // Cleanup old entries periodically\n    setInterval(() => this.cleanup(), this.window);\n  }\n\n  isHot(key: string): boolean {\n    const info = this.accessCounts.get(key);\n    if (!info) {\n      this.accessCounts.set(key, { count: 1, firstAccess: Date.now(), lastAccess: Date.now() });\n      return false;\n    }\n    \n    info.count++;\n    info.lastAccess = Date.now();\n    \n    const windowAge = Date.now() - info.firstAccess;\n    return windowAge < this.window && info.count >= this.threshold;\n  }\n\n  shouldRefresh(key: string): boolean {\n    if (!this.isHot(key)) {\n      return false;\n    }\n    \n    return Math.random() < this.refreshProbability;\n  }\n\n  private cleanup(): void {\n    const now = Date.now();\n    for (const [key, info] of this.accessCounts.entries()) {\n      if (now - info.lastAccess > this.window) {\n        this.accessCounts.delete(key);\n      }\n    }\n  }\n}\n\ninterface AccessInfo {\n  count: number;\n  firstAccess: number;\n  lastAccess: number;\n}\n\ninterface HotKeyConfig {\n  threshold: number;\n  window: number;\n  refreshProbability: number;\n}\n```\n\n### Connection Pool Sizing Strategy\n\n```typescript\nclass ConnectionPoolManager {\n  static calculatePoolSize(config: PoolSizingConfig): PoolConfig {\n    // Formula: pool_size = (avg_request_rate * avg_latency) / 1000 + buffer\n    const baseSize = Math.ceil((config.avgRequestRate * config.avgLatencyMs) / 1000);\n    \n    // Add 20% buffer for spikes\n    const buffer = Math.ceil(baseSize * 0.2);\n    \n    // Minimum pool size for low-traffic scenarios\n    const minSize = Math.max(10, Math.ceil(baseSize * 0.2));\n    \n    // Maximum pool size with upper bound\n    const maxSize = Math.min(config.maxConnections || 500, baseSize + buffer);\n    \n    return {\n      min: minSize,\n      max: maxSize,\n      acquireTimeout: config.acquireTimeout || 10000\n    };\n  }\n}\n\ninterface PoolSizingConfig {\n  avgRequestRate: number;\n  avgLatencyMs: number;\n  maxConnections?: number;\n  acquireTimeout?: number;\n}\n\ninterface PoolConfig {\n  min: number;\n  max: number;\n  acquireTimeout: number;\n}\n```\n\n## Configuration\n\n```typescript\ninterface CacheConfig {\n  redis: {\n    host: string;\n    port: number;\n    password?: string;\n    db?: number;\n    tls?: boolean;\n    cluster?: boolean;\n    sentinels?: Array<{ host: string; port: number }>;\n    retryStrategy?: (times: number) => number;\n    maxRetriesPerRequest?: number;\n    connectTimeout?: number;\n    commandTimeout?: number;\n    connectionPool: {\n      min: number;\n      max: number;\n      acquireTimeout: number;\n    };\n  };\n  \n  defaultTtl: number;\n  compressionThreshold: number;\n  tagIndexTtl: number;\n  version: number;\n  \n  l1: {\n    enabled: boolean;\n    maxItems: number;\n    maxSize: number;\n    maxItemSize: number;\n    ttl: number;\n    adaptiveThreshold: number;\n  };\n  \n  writeBehind: {\n    enabled: boolean;\n    batchSize: number;\n    flushInterval: number;\n    persistenceLog?: string;\n  };\n  \n  distributed: {\n    enabled: boolean;\n    brokerType: 'kafka' | 'nats' | 'redis-pubsub';\n    brokerConfig: any;\n  };\n  \n  serialization: {\n    format: 'json' | 'msgpack';\n  };\n  \n  fallback: {\n    enabled: boolean;\n    failureThreshold: number;\n    successThreshold: number;\n    resetTimeout: number;\n    halfOpenRequests: number;\n    rateLimitRequests: number;\n    rateLimitWindow: number;\n  };\n  \n  metrics: {\n    enabled: boolean;\n    exporter: 'prometheus' | 'statsd' | 'datadog';\n    labels: Record<string, string>;\n    collectPerKeyMetrics: boolean;\n  };\n  \n  negativeCache: {\n    enabled: boolean;\n    expectedItems: number;\n    falsePositiveRate: number;\n  };\n  \n  hotKey: {\n    threshold: number;\n    window: number;\n    refreshProbability: number;\n  };\n}\n```\n\n## Usage Examples\n\n```typescript\n// Initialize with adaptive pool sizing\nconst poolConfig = ConnectionPoolManager.calculatePoolSize({\n  avgRequestRate: 10000,\n  avgLatencyMs: 5,\n  maxConnections: 200\n});\n\nconst cacheManager = new CacheManager({\n  redis: {\n    host: 'localhost',\n    port: 6379,\n    password: process.env.REDIS_PASSWORD,\n    retryStrategy: (times) => Math.min(times * 50, 2000),\n    connectionPool: poolConfig\n  },\n  defaultTtl: 3600,\n  compressionThreshold: 1024,\n  tagIndexTtl: 86400,\n  version: 1,\n  l1: {\n    enabled: true,\n    maxItems: 10000,\n    maxSize: 50 * 1024 * 1024,\n    maxItemSize: 100 * 1024,\n    ttl: 300,\n    adaptiveThreshold: 0.85\n  },\n  writeBehind: {\n    enabled: true,\n    batchSize: 100,\n    flushInterval: 1000,\n    persistenceLog: '/var/log/cache-wal.log'\n  },\n  distributed: {\n    enabled: true,\n    brokerType: 'kafka',\n    brokerConfig: {\n      brokers: ['localhost:9092']\n    }\n  },\n  serialization: {\n    format: 'msgpack'\n  },\n  fallback: {\n    enabled: true,\n    failureThreshold: 5,\n    successThreshold: 2,\n    resetTimeout: 60000,\n    halfOpenRequests: 3,\n    rateLimitRequests: 1000,\n    rateLimitWindow: 1000\n  },\n  metrics: {\n    enabled: true,\n    exporter: 'prometheus',\n    labels: { service: 'api' },\n    collectPerKeyMetrics: false\n  },\n  negativeCache: {\n    enabled: true,\n    expectedItems: 100000,\n    falsePositiveRate: 0.01\n  },\n  hotKey: {\n    threshold: 100,\n    window: 60000,\n    refreshProbability: 0.1\n  }\n});\n\n// Durability levels\nawait cacheManager.set(\n  { namespace: 'user', identifier: userId },\n  userData,\n  { durability: 'immediate', ttl: 300 }\n);\n\nawait cacheManager.set(\n  { namespace: 'counter', identifier: 'views' },\n  viewCount,\n  { durability: 'eventual', writeBehind: true, ttl: 60 }\n);\n\n// Stale-if-error fallback\nconst user = await cacheManager.get<User>(\n  { namespace: 'user', identifier: userId },\n  { staleIfError: true, maxStaleTime: 60000 }\n);\n```"
}