{
  "initial_prompt": "Design a task queue system with priority support",
  "status": "debating",
  "rounds": 2,
  "started_at": "2026-01-17T09:20:59.057362",
  "ended_at": "2026-01-17T09:27:26.840012",
  "final_design": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system implements a **priority-based task scheduling system** with proven starvation prevention and pragmatic failure recovery. It uses a **worker-pull model** with three main layers:\n\n1. **API Layer**: REST endpoints for task lifecycle management\n2. **Scheduling Layer**: Multi-queue priority system with age-based promotion\n3. **Execution Layer**: Worker pool with health monitoring and graceful degradation\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels (high, medium, low)\n  - Maintain separate priority queues with periodic age-based promotion\n  - Route tasks to workers based on simple capability matching\n  - Orchestrate task lifecycle (submitted \u2192 queued \u2192 executing \u2192 terminal state)\n  - Handle task timeouts and retries\n\n### 2. Multi-Queue Priority System with Age-Based Promotion\n\n**Structure**: Three separate Redis sorted sets, scored by submission time\n```\nqueue:high    - High priority tasks (scored by created_at timestamp)\nqueue:medium  - Medium priority tasks (scored by created_at timestamp)\nqueue:low     - Low priority tasks (scored by created_at timestamp)\n```\n\n**Starvation Prevention**: Periodic promotion via background job\n```python\n# Runs every 60 seconds\ndef promote_aged_tasks():\n    now = time.time()\n    \n    # Promote low \u2192 medium if waiting > 10 minutes\n    aged_low = redis.zrangebyscore(\n        'queue:low', \n        0, \n        now - 600,  # 10 minutes\n        start=0, \n        num=100\n    )\n    for task_id in aged_low:\n        task = get_task(task_id)\n        task.effective_priority = 'medium'\n        redis.zrem('queue:low', task_id)\n        redis.zadd('queue:medium', {task_id: task.created_at})\n    \n    # Promote medium \u2192 high if waiting > 20 minutes\n    aged_medium = redis.zrangebyscore(\n        'queue:medium',\n        0,\n        now - 1200,  # 20 minutes\n        start=0,\n        num=50\n    )\n    for task_id in aged_medium:\n        task = get_task(task_id)\n        task.effective_priority = 'high'\n        redis.zrem('queue:medium', task_id)\n        redis.zadd('queue:high', {task_id: task.created_at})\n```\n\n**Guarantees**:\n- Low priority tasks promoted to medium after 10 minutes\n- Medium priority tasks promoted to high after 20 minutes\n- Maximum wait time: 30 minutes for any task\n- Within each queue: strict FIFO ordering\n\n**Rationale**: This approach is simpler to reason about, debug, and maintain than dynamic scoring. The discrete promotion thresholds are easy to explain to users (\"your low priority task will be treated as medium after 10 minutes\") and the behavior is predictable. The 60-second promotion interval is frequent enough for practical fairness while being cheap to execute.\n\n### 3. Worker Pool with Simple Health Monitoring\n\n**Architecture**: Fixed-size worker pool with capability tags\n- **Worker States**: `active`, `draining`, `dead`\n- **Health Checks**: \n  - **Heartbeat only**: Every 30 seconds, 3 missed heartbeats (90s) = worker dead\n  - **Task timeout**: Independent per-task timeout, triggers recovery on expiry\n- **Concurrency**: Per-worker capacity (default: 5 concurrent tasks)\n- **Graceful Shutdown**: `draining` state prevents new assignments\n\n**Rationale for Simplified Health Monitoring**: \n- Active health probes add complexity and create new failure modes (probe timeouts, probe service overload)\n- Task-level timeouts already detect hung workers effectively\n- Heartbeat + task timeout covers 99% of failure scenarios\n- Simpler system = fewer edge cases = more reliable\n\n### 4. Task State Machine\n\n**States**: `queued` \u2192 `executing` \u2192 [`completed` | `failed` | `cancelled` | `dead_letter`]\n\n**Simplified from 7 states to 5 states**: \n- Removed `submitted` (immediately becomes `queued`)\n- Removed `assigned` (worker atomically transitions `queued` \u2192 `executing` during poll)\n- Removed `initializing` worker state (workers register as `active`)\n\n**Rationale**: Each state adds complexity in recovery logic. The key insight is that if a worker has the task, the worker is responsible for it. We don't need fine-grained tracking of \"assigned but not started\" - the task timeout handles this case.\n\n### 5. Dead Letter Queue & Core Observability\n\n**DLQ**: Separate Redis sorted set `queue:dlq` for tasks exceeding retry limits\n- Score: timestamp when moved to DLQ\n- TTL: 30 days\n- Manual replay: Admin endpoint to requeue with reset retries\n\n**Observability**: \n- **Metrics**: Queue depth, task duration, worker utilization, error rates\n- **Logging**: Structured JSON logs for state transitions\n- **Tracing**: Trace ID propagated through task lifecycle\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"effective_priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\",\n    \"required_capabilities\": [\"gpu\"]\n  },\n  \"status\": \"queued|executing|completed|failed|cancelled|dead_letter\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker_id\": \"worker-id|null\",\n  \"result\": \"json-object|null\",\n  \"error\": {\n    \"message\": \"string\",\n    \"retriable\": true\n  },\n  \"timeout_seconds\": 300,\n  \"idempotency_key\": \"string|null\",\n  \"trace_id\": \"string\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"active|draining|dead\",\n  \"current_tasks\": [\"task-id-1\", \"task-id-2\"],\n  \"last_heartbeat\": \"timestamp\",\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"version\": \"string\"\n}\n```\n\n### Priority Configuration\n```json\n{\n  \"high\": {\n    \"max_retries\": 5,\n    \"timeout_seconds\": 300\n  },\n  \"medium\": {\n    \"max_retries\": 3,\n    \"timeout_seconds\": 600\n  },\n  \"low\": {\n    \"max_retries\": 2,\n    \"timeout_seconds\": 900\n  },\n  \"starvation_prevention\": {\n    \"low_to_medium_seconds\": 600,\n    \"medium_to_high_seconds\": 1200,\n    \"promotion_interval_seconds\": 60,\n    \"promotion_batch_size\": 100\n  }\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"required_capabilities\": [\"gpu\"],\n  \"timeout_seconds\": 600,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"queued\",\n  \"queue_position\": 5\n}\n```\n\n#### Task Status\n```\nGET /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"executing\",\n  \"priority\": \"high\",\n  \"effective_priority\": \"high\",\n  \"created_at\": \"...\",\n  \"assigned_worker_id\": \"worker-uuid\",\n  \"result\": null\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\"\n}\n```\n\n#### Queue Statistics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"queues\": {\n    \"high\": {\"depth\": 12, \"oldest_age_seconds\": 45},\n    \"medium\": {\"depth\": 45, \"oldest_age_seconds\": 320},\n    \"low\": {\"depth\": 123, \"oldest_age_seconds\": 580}\n  },\n  \"workers\": {\n    \"active\": 18,\n    \"draining\": 2,\n    \"dead\": 0,\n    \"total_capacity\": 100,\n    \"used_capacity\": 67\n  },\n  \"throughput\": {\n    \"completed_last_minute\": 120,\n    \"failed_last_minute\": 3\n  }\n}\n```\n\n### Worker Interface (Internal)\n\n#### Worker Registration\n```\nPOST /internal/workers/register\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"capacity\": 5\n}\nResponse: {\n  \"worker_id\": \"uuid\",\n  \"poll_interval_ms\": 1000\n}\n```\n\n#### Poll for Tasks\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"available_capacity\": 3\n}\nResponse: {\n  \"tasks\": [\n    {\n      \"task_id\": \"uuid\",\n      \"task_type\": \"process_video\",\n      \"parameters\": {...},\n      \"timeout_seconds\": 600,\n      \"trace_id\": \"...\"\n    }\n  ]\n}\n```\n\n**Poll Algorithm**:\n```python\ndef poll_tasks(worker_id, capabilities, available_capacity):\n    tasks = []\n    \n    # Try each queue in priority order\n    for queue_name in ['queue:high', 'queue:medium', 'queue:low']:\n        if len(tasks) >= available_capacity:\n            break\n        \n        # Get oldest tasks from this queue\n        candidate_ids = redis.zrange(queue_name, 0, available_capacity * 2)\n        \n        for task_id in candidate_ids:\n            if len(tasks) >= available_capacity:\n                break\n            \n            # Atomic claim: remove from queue and update task\n            success = try_claim_task(task_id, worker_id, queue_name)\n            if success:\n                task = get_task(task_id)\n                \n                # Check capabilities\n                if task.required_capabilities.issubset(capabilities):\n                    tasks.append(task)\n                else:\n                    # Return to queue if capabilities don't match\n                    return_task_to_queue(task_id, queue_name)\n    \n    return tasks\n\ndef try_claim_task(task_id, worker_id, queue_name):\n    # Atomic Lua script\n    result = redis.eval(\"\"\"\n        local task_id = ARGV[1]\n        local worker_id = ARGV[2]\n        local queue_name = ARGV[3]\n        \n        -- Remove from queue\n        local removed = redis.call('ZREM', queue_name, task_id)\n        if removed == 0 then\n            return 0  -- Already claimed\n        end\n        \n        -- Update task status\n        local task_key = 'tasks:' .. task_id\n        redis.call('HSET', task_key, 'status', 'executing')\n        redis.call('HSET', task_key, 'assigned_worker_id', worker_id)\n        redis.call('HSET', task_key, 'started_at', ARGV[4])\n        \n        return 1\n    \"\"\", 0, task_id, worker_id, queue_name, time.time())\n    \n    return result == 1\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n```\n\n#### Worker Heartbeat\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"active\"\n}\nResponse: {\n  \"acknowledged\": true\n}\n```\n\n## Error Handling & Recovery\n\n### Worker Failure Detection & Recovery\n\n**Detection**: Background job runs every 30 seconds\n```python\ndef detect_dead_workers():\n    now = time.time()\n    cutoff = now - 90  # 3 missed heartbeats\n    \n    all_workers = get_all_workers()\n    for worker in all_workers:\n        if worker.status == 'dead':\n            continue\n        \n        if worker.last_heartbeat < cutoff:\n            handle_worker_failure(worker)\n\ndef handle_worker_failure(worker):\n    # Mark worker as dead\n    worker.status = 'dead'\n    save_worker(worker)\n    \n    # Find all tasks assigned to this worker\n    tasks = find_tasks_by_worker(worker.worker_id)\n    \n    for task in tasks:\n        if task.status == 'executing':\n            # Requeue with incremented retry\n            requeue_task(task)\n```\n\n**Task Timeout Detection**: Separate background job runs every 30 seconds\n```python\ndef detect_timed_out_tasks():\n    now = time.time()\n    \n    # Find tasks in 'executing' state past their timeout\n    timed_out = query_tasks(\n        status='executing',\n        started_at__lt=now - max_task_timeout\n    )\n    \n    for task in timed_out:\n        # This handles hung workers AND legitimately long tasks\n        handle_task_timeout(task)\n```\n\n### Retry Strategy\n\n```python\ndef requeue_task(task):\n    task.retry_count += 1\n    \n    # Check retry limit\n    if task.retry_count > task.max_retries:\n        move_to_dlq(task, reason='max_retries_exceeded')\n        return\n    \n    # Reset state\n    task.status = 'queued'\n    task.assigned_worker_id = None\n    task.started_at = None\n    \n    # Increase timeout by 50% on retry (handle slow tasks)\n    task.timeout_seconds *= 1.5\n    \n    # Return to original priority queue (NOT effective_priority)\n    # This ensures retries don't skip ahead unfairly\n    queue_name = f'queue:{task.priority}'\n    redis.zadd(queue_name, {task.task_id: task.created_at})\n    \n    save_task(task)\n```\n\n### Idempotency\n\n```python\ndef submit_task(payload, idempotency_key=None):\n    if idempotency_key:\n        # Check if already exists (24-hour window)\n        cache_key = f'idempotent:{idempotency_key}'\n        existing_id = redis.get(cache_key)\n        \n        if existing_id:\n            return get_task(existing_id)\n    \n    # Create new task\n    task = create_task(payload)\n    \n    if idempotency_key:\n        # Store mapping with 24-hour TTL\n        redis.setex(cache_key, 86400, task.task_id)\n    \n    # Add to appropriate queue\n    queue_name = f'queue:{task.priority}'\n    redis.zadd(queue_name, {task.task_id: task.created_at})\n    \n    return task\n```\n\n## Monitoring & Observability\n\n### Key Metrics (Prometheus)\n\n**Queue Health**:\n- `queue_depth{priority=\"high|medium|low\"}` - Tasks waiting\n- `queue_age_seconds{priority,percentile=\"p50|p95|p99\"}` - Wait time distribution\n- `tasks_promoted_total{from,to}` - Starvation prevention activity\n\n**Task Lifecycle**:\n- `task_duration_seconds{priority,status}` - End-to-end time\n- `task_retries_total{priority,reason}` - Retry patterns\n- `task_state_total{status}` - Current task distribution\n\n**Worker Health**:\n- `workers_total{status}` - Worker pool state\n- `worker_capacity_used_ratio` - Utilization\n- `worker_heartbeat_failures_total` - Health issues\n\n**Error Rates**:\n- `tasks_failed_total{reason}` - Failure classification\n- `tasks_dlq_total` - Unrecoverable failures\n\n### Alerting Rules\n\n**Critical**:\n- No active workers for >2 minutes\n- High priority queue depth >1000\n- Task failure rate >25%\n- DLQ growth >10 tasks/minute\n\n**Warning**:\n- Worker utilization >90% for >10 minutes\n- Low priority tasks waiting >25 minutes (promotion not working)\n- Any queue depth >5000\n\n## Deployment Architecture\n\n**Single-Region (Recommended)**:\n- Redis Sentinel (3 nodes) for high availability\n- PostgreSQL for durable task history\n- Worker pool: 10-100 instances depending on load\n- All components in same VPC for <5ms latency\n\n**Scaling Characteristics**:\n- Queue operations: O(log N) where N = queue depth\n- Worker polling: O(1) - fixed batch size\n- Promotion job: O(P) where P = tasks to promote (bounded by batch_size)\n- Expected throughput: 1000+ tasks/second per Redis instance"
}