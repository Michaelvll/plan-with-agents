{
  "initial_prompt": "Design a task queue system with priority support",
  "rounds": [
    {
      "round_number": 1,
      "agent_a": {
        "content": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system follows a **producer-consumer pattern** with priority-based scheduling. It consists of three main layers:\n\n1. **API Layer**: REST/gRPC endpoints for task submission and management\n2. **Queue Layer**: Priority queue implementation with persistence\n3. **Worker Layer**: Consumer pool that processes tasks based on priority\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels\n  - Maintain multiple priority queues (high, medium, low)\n  - Distribute tasks to available workers\n  - Handle task lifecycle (pending \u2192 processing \u2192 completed/failed)\n\n### 2. Priority Queue Implementation\n- **Structure**: Min-heap based priority queue per priority level\n- **Storage**: Redis for in-memory performance + PostgreSQL for durability\n- **Keys**:\n  - `queue:high`, `queue:medium`, `queue:low` (sorted sets in Redis)\n  - Task metadata stored in hash structures\n\n### 3. Worker Pool\n- **Architecture**: Dynamic worker pool with auto-scaling\n- **Worker States**: idle, busy, dead\n- **Health Checks**: Heartbeat mechanism every 30 seconds\n- **Concurrency**: Configurable max concurrent tasks per worker\n\n### 4. Task Registry\n- **Purpose**: Track all tasks across their lifecycle\n- **Storage**: PostgreSQL for persistence and queryability\n- **Indexing**: By task_id, status, priority, created_at, user_id\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\"\n  },\n  \"status\": \"pending|processing|completed|failed|retrying\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker\": \"worker-id|null\",\n  \"result\": \"json-object|null\",\n  \"error\": \"string|null\",\n  \"timeout_seconds\": 300,\n  \"user_id\": \"string\",\n  \"idempotency_key\": \"string|null\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"idle|busy|dead\",\n  \"current_task_id\": \"uuid-v4|null\",\n  \"last_heartbeat\": \"timestamp\",\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"started_at\": \"timestamp\",\n  \"tags\": [\"gpu\", \"cpu-intensive\"]\n}\n```\n\n### Priority Levels\n```json\n{\n  \"high\": {\"score\": 1000, \"max_retry_delay_seconds\": 10},\n  \"medium\": {\"score\": 500, \"max_retry_delay_seconds\": 30},\n  \"low\": {\"score\": 100, \"max_retry_delay_seconds\": 60}\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"timeout_seconds\": 600,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"pending\",\n  \"position_in_queue\": 5\n}\n```\n\n#### Task Status\n```\nGET /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"created_at\": \"...\",\n  \"started_at\": \"...\"\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\"\n}\n```\n\n#### Queue Statistics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"high\": {\"pending\": 12, \"processing\": 3},\n  \"medium\": {\"pending\": 45, \"processing\": 10},\n  \"low\": {\"pending\": 123, \"processing\": 5},\n  \"total_workers\": 20,\n  \"idle_workers\": 2\n}\n```\n\n### Worker Interface (Internal)\n\n#### Poll for Tasks\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"worker_capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"capacity\": 5\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"task_type\": \"process_video\",\n  \"parameters\": {...},\n  \"timeout_seconds\": 600\n}\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n```\n\n#### Worker Heartbeat\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"busy\"\n}\n```\n\n## Priority Scheduling Algorithm\n\n### Task Selection Strategy\n1. **Priority-first**: Always select highest priority queue with pending tasks\n2. **FIFO within priority**: Within same priority, oldest task first\n3. **Starvation prevention**: If low-priority tasks wait >10 minutes, boost to medium\n4. **Worker affinity**: Match task requirements with worker capabilities\n\n### Scoring Formula\n```\ntask_score = priority_base_score - (current_time - created_time) / aging_factor\n```\n- Higher score = higher urgency\n- Aging factor prevents indefinite starvation\n\n## Error Handling\n\n### Retry Strategy\n1. **Exponential backoff**: delay = min(2^retry_count * base_delay, max_delay)\n2. **Max retries**: Configurable per priority level (default: 3)\n3. **Dead Letter Queue**: Failed tasks after max retries go to DLQ for manual inspection\n\n### Failure Scenarios\n\n#### Worker Failure\n- **Detection**: Missed heartbeat (>90 seconds)\n- **Recovery**: Mark worker as dead, requeue assigned tasks with priority boost\n\n#### Task Timeout\n- **Detection**: Execution time exceeds timeout_seconds\n- **Action**: Cancel task, increment retry_count, requeue if retries remaining\n\n#### System Crash\n- **Recovery**: On startup, scan for tasks in \"processing\" state with no live worker\n- **Action**: Reset to \"pending\" and requeue with original priority\n\n#### Queue Overflow\n- **Detection**: Queue size exceeds threshold (10,000 tasks)\n- **Action**: Return 429 Too Many Requests, implement rate limiting per user\n\n### Data Consistency\n\n#### Idempotency\n- Use `idempotency_key` to prevent duplicate task submission\n- Store key-to-task_id mapping in Redis with TTL of 24 hours\n\n#### Transaction Guarantees\n- Task state transitions use optimistic locking (version field)\n- Worker assignment uses Redis distributed locks (redlock algorithm)\n- PostgreSQL transactions for critical state changes\n\n#### Visibility Timeout\n- When worker polls task, task becomes invisible for timeout_seconds\n- If worker doesn't complete/fail within timeout, task becomes visible again\n\n## Monitoring & Observability\n\n### Metrics\n- Queue depth per priority level\n- Task processing latency (p50, p95, p99)\n- Worker utilization rate\n- Task success/failure rates\n- Retry rates per priority\n\n### Logging\n- Structured JSON logs with trace_id propagation\n- Log all state transitions with timestamps\n- Worker assignments and reassignments\n\n### Alerting\n- High queue depth (>1000 pending tasks)\n- Low worker availability (<20% idle)\n- High failure rate (>10% in 5 minutes)\n- Stale tasks (pending >1 hour)\n\n## Rationale\n\n**Why multiple priority queues instead of single sorted queue?**\n- Simpler implementation and reasoning about priority\n- Easier to prevent starvation with clear boundaries\n- Better performance for high-priority tasks (no sorting overhead)\n\n**Why Redis + PostgreSQL hybrid?**\n- Redis provides low-latency queue operations\n- PostgreSQL ensures durability and queryability for task history\n- Async replication from Redis to PostgreSQL balances performance and reliability\n\n**Why worker pull model vs push?**\n- Workers control their own load\n- Natural backpressure mechanism\n- Easier to handle heterogeneous worker capabilities\n\n**Why optimistic locking?**\n- Better performance than pessimistic locks\n- Conflicts are rare in this workload\n- Graceful degradation under contention\n\n## What I Changed\n\nThis is the initial design proposal, so no changes from previous iterations.\n\n## What I Kept\n\nN/A - initial proposal.\n\n## Open Questions\n\n1. **Distributed deployment**: Should we support multi-datacenter deployment with geo-distributed queues, or assume single-region deployment?\n\n2. **Task dependencies**: Should tasks support dependencies (task B waits for task A), or keep it simple with independent tasks only?\n\n3. **Priority levels**: Three levels (high/medium/low) sufficient, or should we support numeric priorities (1-10)?\n\n4. **Real-time notifications**: Should we add WebSocket support for real-time task status updates, or polling-only?\n\n## Convergence Status\n**ITERATING**: This is the initial design and needs critical review before refinement.\n\n## Prompt for Collaborator",
        "prompt_for_other": "Focus on the priority scheduling algorithm and starvation prevention mechanism. Are there edge cases where low-priority tasks could still be starved? Also evaluate if the error handling for worker failures is robust enough for production use.",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system follows a **producer-consumer pattern** with priority-based scheduling. It consists of three main layers:\n\n1. **API Layer**: REST/gRPC endpoints for task submission and management\n2. **Queue Layer**: Priority queue implementation with persistence\n3. **Worker Layer**: Consumer pool that processes tasks based on priority\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels\n  - Maintain multiple priority queues (high, medium, low)\n  - Distribute tasks to available workers\n  - Handle task lifecycle (pending \u2192 processing \u2192 completed/failed)\n\n### 2. Priority Queue Implementation\n- **Structure**: Min-heap based priority queue per priority level\n- **Storage**: Redis for in-memory performance + PostgreSQL for durability\n- **Keys**:\n  - `queue:high`, `queue:medium`, `queue:low` (sorted sets in Redis)\n  - Task metadata stored in hash structures\n\n### 3. Worker Pool\n- **Architecture**: Dynamic worker pool with auto-scaling\n- **Worker States**: idle, busy, dead\n- **Health Checks**: Heartbeat mechanism every 30 seconds\n- **Concurrency**: Configurable max concurrent tasks per worker\n\n### 4. Task Registry\n- **Purpose**: Track all tasks across their lifecycle\n- **Storage**: PostgreSQL for persistence and queryability\n- **Indexing**: By task_id, status, priority, created_at, user_id\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\"\n  },\n  \"status\": \"pending|processing|completed|failed|retrying\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker\": \"worker-id|null\",\n  \"result\": \"json-object|null\",\n  \"error\": \"string|null\",\n  \"timeout_seconds\": 300,\n  \"user_id\": \"string\",\n  \"idempotency_key\": \"string|null\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"idle|busy|dead\",\n  \"current_task_id\": \"uuid-v4|null\",\n  \"last_heartbeat\": \"timestamp\",\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"started_at\": \"timestamp\",\n  \"tags\": [\"gpu\", \"cpu-intensive\"]\n}\n```\n\n### Priority Levels\n```json\n{\n  \"high\": {\"score\": 1000, \"max_retry_delay_seconds\": 10},\n  \"medium\": {\"score\": 500, \"max_retry_delay_seconds\": 30},\n  \"low\": {\"score\": 100, \"max_retry_delay_seconds\": 60}\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"timeout_seconds\": 600,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"pending\",\n  \"position_in_queue\": 5\n}\n```\n\n#### Task Status\n```\nGET /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"processing\",\n  \"progress\": 45,\n  \"created_at\": \"...\",\n  \"started_at\": \"...\"\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\"\n}\n```\n\n#### Queue Statistics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"high\": {\"pending\": 12, \"processing\": 3},\n  \"medium\": {\"pending\": 45, \"processing\": 10},\n  \"low\": {\"pending\": 123, \"processing\": 5},\n  \"total_workers\": 20,\n  \"idle_workers\": 2\n}\n```\n\n### Worker Interface (Internal)\n\n#### Poll for Tasks\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"worker_capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"capacity\": 5\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"task_type\": \"process_video\",\n  \"parameters\": {...},\n  \"timeout_seconds\": 600\n}\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n```\n\n#### Worker Heartbeat\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"busy\"\n}\n```\n\n## Priority Scheduling Algorithm\n\n### Task Selection Strategy\n1. **Priority-first**: Always select highest priority queue with pending tasks\n2. **FIFO within priority**: Within same priority, oldest task first\n3. **Starvation prevention**: If low-priority tasks wait >10 minutes, boost to medium\n4. **Worker affinity**: Match task requirements with worker capabilities\n\n### Scoring Formula\n```\ntask_score = priority_base_score - (current_time - created_time) / aging_factor\n```\n- Higher score = higher urgency\n- Aging factor prevents indefinite starvation\n\n## Error Handling\n\n### Retry Strategy\n1. **Exponential backoff**: delay = min(2^retry_count * base_delay, max_delay)\n2. **Max retries**: Configurable per priority level (default: 3)\n3. **Dead Letter Queue**: Failed tasks after max retries go to DLQ for manual inspection\n\n### Failure Scenarios\n\n#### Worker Failure\n- **Detection**: Missed heartbeat (>90 seconds)\n- **Recovery**: Mark worker as dead, requeue assigned tasks with priority boost\n\n#### Task Timeout\n- **Detection**: Execution time exceeds timeout_seconds\n- **Action**: Cancel task, increment retry_count, requeue if retries remaining\n\n#### System Crash\n- **Recovery**: On startup, scan for tasks in \"processing\" state with no live worker\n- **Action**: Reset to \"pending\" and requeue with original priority\n\n#### Queue Overflow\n- **Detection**: Queue size exceeds threshold (10,000 tasks)\n- **Action**: Return 429 Too Many Requests, implement rate limiting per user\n\n### Data Consistency\n\n#### Idempotency\n- Use `idempotency_key` to prevent duplicate task submission\n- Store key-to-task_id mapping in Redis with TTL of 24 hours\n\n#### Transaction Guarantees\n- Task state transitions use optimistic locking (version field)\n- Worker assignment uses Redis distributed locks (redlock algorithm)\n- PostgreSQL transactions for critical state changes\n\n#### Visibility Timeout\n- When worker polls task, task becomes invisible for timeout_seconds\n- If worker doesn't complete/fail within timeout, task becomes visible again\n\n## Monitoring & Observability\n\n### Metrics\n- Queue depth per priority level\n- Task processing latency (p50, p95, p99)\n- Worker utilization rate\n- Task success/failure rates\n- Retry rates per priority\n\n### Logging\n- Structured JSON logs with trace_id propagation\n- Log all state transitions with timestamps\n- Worker assignments and reassignments\n\n### Alerting\n- High queue depth (>1000 pending tasks)\n- Low worker availability (<20% idle)\n- High failure rate (>10% in 5 minutes)\n- Stale tasks (pending >1 hour)\n\n## Rationale\n\n**Why multiple priority queues instead of single sorted queue?**\n- Simpler implementation and reasoning about priority\n- Easier to prevent starvation with clear boundaries\n- Better performance for high-priority tasks (no sorting overhead)\n\n**Why Redis + PostgreSQL hybrid?**\n- Redis provides low-latency queue operations\n- PostgreSQL ensures durability and queryability for task history\n- Async replication from Redis to PostgreSQL balances performance and reliability\n\n**Why worker pull model vs push?**\n- Workers control their own load\n- Natural backpressure mechanism\n- Easier to handle heterogeneous worker capabilities\n\n**Why optimistic locking?**\n- Better performance than pessimistic locks\n- Conflicts are rare in this workload\n- Graceful degradation under contention\n\n## What I Changed\n\nThis is the initial design proposal, so no changes from previous iterations.\n\n## What I Kept\n\nN/A - initial proposal.\n\n## Open Questions\n\n1. **Distributed deployment**: Should we support multi-datacenter deployment with geo-distributed queues, or assume single-region deployment?\n\n2. **Task dependencies**: Should tasks support dependencies (task B waits for task A), or keep it simple with independent tasks only?\n\n3. **Priority levels**: Three levels (high/medium/low) sufficient, or should we support numeric priorities (1-10)?\n\n4. **Real-time notifications**: Should we add WebSocket support for real-time task status updates, or polling-only?\n\n## Convergence Status\n**ITERATING**: This is the initial design and needs critical review before refinement.\n\n## Prompt for Collaborator\nPROMPT_FOR_CRITIC:\nFocus on the priority scheduling algorithm and starvation prevention mechanism. Are there edge cases where low-priority tasks could still be starved? Also evaluate if the error handling for worker failures is robust enough for production use."
      },
      "agent_b": {
        "content": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system implements a **priority-based task scheduling system** with strong guarantees against starvation and robust failure recovery. It uses a **hybrid push-pull model** with three main layers:\n\n1. **API Layer**: REST endpoints for task lifecycle management\n2. **Scheduling Layer**: Dynamic priority queue with aging-based starvation prevention\n3. **Execution Layer**: Worker pool with capability-based routing and failure detection\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels\n  - Maintain a unified priority queue with dynamic scoring\n  - Route tasks to capable workers based on requirements\n  - Orchestrate task lifecycle (submitted \u2192 queued \u2192 assigned \u2192 executing \u2192 terminal state)\n  - Implement circuit breakers for cascading failure prevention\n\n### 2. Unified Priority Queue Implementation\n- **Structure**: Single Redis sorted set with dynamic composite scores\n- **Score Calculation**: \n  ```\n  score = (priority_weight \u00d7 1000) - (age_seconds \u00d7 aging_factor)\n  \n  priority_weight: high=1000, medium=100, low=10\n  aging_factor: 0.5 (low priority tasks gain 2 points per second)\n  \n  Example after 600 seconds:\n  - High priority task: 1000000 - (600 \u00d7 0.5) = 999700\n  - Low priority task: 10000 - (600 \u00d7 0.5) = 9700\n  ```\n- **Storage**: \n  - Redis sorted set `queue:active` for fast priority-based polling\n  - Redis hash `tasks:{task_id}` for task metadata\n  - PostgreSQL for durable task history and analytics\n- **Guaranteed Starvation Prevention**: Mathematical guarantee that any task will eventually reach highest effective priority\n\n### 3. Worker Pool with Capability Matching\n- **Architecture**: Auto-scaling worker pool with capability tags\n- **Worker States**: `initializing`, `idle`, `busy`, `draining`, `dead`\n- **Health Checks**: \n  - Heartbeat every 30 seconds with exponential timeout (3 missed = 90s)\n  - Active health probes for workers in `busy` state every 60 seconds\n- **Concurrency**: Per-worker semaphore with configurable capacity\n- **Graceful Shutdown**: `draining` state prevents new assignments while completing current tasks\n\n### 4. Task State Machine\n- **States**: `submitted` \u2192 `queued` \u2192 `assigned` \u2192 `executing` \u2192 [`completed` | `failed` | `cancelled` | `dead_letter`]\n- **Transitions**: \n  - All transitions recorded with timestamp and triggering event\n  - State machine enforced in application layer with PostgreSQL constraints\n  - Optimistic locking prevents race conditions\n\n### 5. Dead Letter Queue & Observability\n- **DLQ**: Separate Redis sorted set `queue:dlq` for tasks exceeding retry limits\n- **DLQ Processing**: Manual review interface + automated replay capability\n- **Distributed Tracing**: OpenTelemetry instrumentation for full task journey\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\",\n    \"required_capabilities\": [\"gpu\", \"high-memory\"]\n  },\n  \"status\": \"submitted|queued|assigned|executing|completed|failed|cancelled|dead_letter\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"queued_at\": \"timestamp|null\",\n  \"assigned_at\": \"timestamp|null\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker\": \"worker-id|null\",\n  \"result\": \"json-object|null\",\n  \"error\": {\n    \"message\": \"string\",\n    \"code\": \"string\",\n    \"retryable\": true,\n    \"details\": \"json-object\"\n  },\n  \"timeout_seconds\": 300,\n  \"user_id\": \"string\",\n  \"idempotency_key\": \"string|null\",\n  \"version\": 0,\n  \"trace_id\": \"string\",\n  \"estimated_duration_seconds\": 60,\n  \"visibility_timeout_until\": \"timestamp|null\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"initializing|idle|busy|draining|dead\",\n  \"current_tasks\": [\"task-id-1\", \"task-id-2\"],\n  \"last_heartbeat\": \"timestamp\",\n  \"consecutive_failures\": 0,\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"started_at\": \"timestamp\",\n  \"capabilities\": [\"gpu\", \"cpu-intensive\", \"high-memory\"],\n  \"version\": 0,\n  \"drain_initiated_at\": \"timestamp|null\"\n}\n```\n\n### Priority Configuration\n```json\n{\n  \"high\": {\n    \"weight\": 1000,\n    \"max_retry_delay_seconds\": 10,\n    \"max_retries\": 5,\n    \"timeout_multiplier\": 1.0\n  },\n  \"medium\": {\n    \"weight\": 100,\n    \"max_retry_delay_seconds\": 30,\n    \"max_retries\": 3,\n    \"timeout_multiplier\": 1.5\n  },\n  \"low\": {\n    \"weight\": 10,\n    \"max_retry_delay_seconds\": 60,\n    \"max_retries\": 2,\n    \"timeout_multiplier\": 2.0\n  },\n  \"starvation_prevention\": {\n    \"aging_factor\": 0.5,\n    \"max_wait_seconds\": 3600\n  }\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"required_capabilities\": [\"gpu\"],\n  \"timeout_seconds\": 600,\n  \"estimated_duration_seconds\": 120,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"queued\",\n  \"estimated_start_time\": \"timestamp\",\n  \"position_in_queue\": 5\n}\n```\n\n#### Task Status with History\n```\nGET /api/v1/tasks/{task_id}?include_history=true\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"executing\",\n  \"progress\": 45,\n  \"created_at\": \"...\",\n  \"assigned_worker\": \"worker-uuid\",\n  \"state_history\": [\n    {\"state\": \"submitted\", \"timestamp\": \"...\"},\n    {\"state\": \"queued\", \"timestamp\": \"...\"},\n    {\"state\": \"assigned\", \"timestamp\": \"...\", \"worker_id\": \"...\"},\n    {\"state\": \"executing\", \"timestamp\": \"...\"}\n  ]\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\",\n  \"cancellation_time\": \"timestamp\"\n}\n```\n\n#### Queue Statistics with Aging Metrics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"queue_depth\": {\n    \"high\": {\"pending\": 12, \"executing\": 3},\n    \"medium\": {\"pending\": 45, \"executing\": 10},\n    \"low\": {\"pending\": 123, \"executing\": 5}\n  },\n  \"aging_stats\": {\n    \"oldest_queued_task_age_seconds\": 450,\n    \"tasks_waiting_over_threshold\": 5\n  },\n  \"workers\": {\n    \"total\": 20,\n    \"idle\": 2,\n    \"busy\": 17,\n    \"draining\": 1,\n    \"dead\": 0\n  },\n  \"throughput\": {\n    \"completed_last_minute\": 120,\n    \"failed_last_minute\": 3\n  }\n}\n```\n\n#### Dead Letter Queue Management\n```\nGET /api/v1/dlq\nResponse: {\n  \"tasks\": [...],\n  \"total_count\": 15\n}\n\nPOST /api/v1/dlq/{task_id}/replay\nBody: {\n  \"reset_retry_count\": true,\n  \"new_priority\": \"high\"\n}\n```\n\n### Worker Interface (Internal)\n\n#### Worker Registration\n```\nPOST /internal/workers/register\nBody: {\n  \"capabilities\": [\"gpu\", \"high-memory\"],\n  \"capacity\": 5\n}\nResponse: {\n  \"worker_id\": \"uuid\",\n  \"poll_interval_seconds\": 5\n}\n```\n\n#### Poll for Tasks (with Capability Matching)\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"available_capacity\": 3\n}\nResponse: [\n  {\n    \"task_id\": \"uuid\",\n    \"task_type\": \"process_video\",\n    \"parameters\": {...},\n    \"timeout_seconds\": 600,\n    \"trace_id\": \"...\"\n  }\n]\n```\n\n#### Report Task Progress\n```\nPOST /internal/workers/{worker_id}/progress\nBody: {\n  \"task_id\": \"uuid\",\n  \"progress_percent\": 45,\n  \"status_message\": \"Processing frame 450/1000\"\n}\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n```\n\n#### Worker Heartbeat with Health\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"busy\",\n  \"health\": {\n    \"memory_usage_percent\": 65,\n    \"cpu_usage_percent\": 80\n  }\n}\nResponse: {\n  \"acknowledged\": true,\n  \"should_drain\": false\n}\n```\n\n## Enhanced Priority Scheduling Algorithm\n\n### Unified Scoring with Guaranteed Starvation Prevention\n\n**Mathematical Guarantee**:\n```\nGiven:\n- High priority weight: 1000\n- Low priority weight: 10\n- Aging factor: 0.5\n\nA low-priority task will overtake a high-priority task created simultaneously after:\n(1000 \u00d7 1000 - 10 \u00d7 1000) / 0.5 = 1,980,000 seconds\n\nHowever, high-priority tasks are typically short-lived. More realistically:\n- Low priority task created at t=0: score = 10000\n- High priority task created at t=1800 (30 min later): score = 1000000 - 900 = 999100\n- Low priority task at t=1800: score = 10000 - 900 = 9100\n\nThe low task needs to age enough to overcome the gap:\n(999100 - 9100) / 0.5 = 1,980,000 additional seconds\n\nThis is extreme, so we add an absolute maximum wait threshold.\n```\n\n**Starvation Prevention Rules**:\n1. **Dynamic Scoring**: Score decreases linearly with age for ALL tasks\n2. **Maximum Wait Time**: Any task waiting > 3600 seconds automatically gets boosted:\n   ```\n   if (current_time - created_at) > max_wait_seconds:\n       effective_priority = \"high\"\n       score = 1000000 - (age \u00d7 aging_factor)\n   ```\n3. **Priority Inversion Detection**: If a low-priority task has waited longer than the oldest high-priority task has existed, boost to medium\n4. **Fair Scheduling within Priority**: FIFO within the same effective priority band (\u00b1100 score points)\n\n### Task Assignment Algorithm\n\n```python\ndef assign_tasks_to_workers():\n    idle_workers = get_idle_workers_sorted_by_capacity()\n    \n    for worker in idle_workers:\n        available_slots = worker.capacity - worker.current_load\n        \n        # Get top N tasks that match worker capabilities\n        candidate_tasks = redis.zrevrange(\n            'queue:active', \n            0, \n            available_slots * 2  # Fetch extra for capability filtering\n        )\n        \n        matched_tasks = []\n        for task_id in candidate_tasks:\n            task = get_task_metadata(task_id)\n            \n            # Skip if task is visibility-locked\n            if task.visibility_timeout_until > now():\n                continue\n            \n            # Check capability match\n            if not worker.has_capabilities(task.required_capabilities):\n                continue\n            \n            # Check if task is within priority fairness window\n            if len(matched_tasks) > 0:\n                score_diff = get_score(matched_tasks[0]) - get_score(task)\n                if score_diff > fairness_threshold:\n                    break  # Don't assign drastically different priorities together\n            \n            matched_tasks.append(task)\n            if len(matched_tasks) >= available_slots:\n                break\n        \n        # Atomic assignment with optimistic locking\n        for task in matched_tasks:\n            assign_task_to_worker(task, worker)\n```\n\n## Comprehensive Error Handling\n\n### Worker Failure Detection & Recovery\n\n#### Three-Tier Health Monitoring\n1. **Heartbeat Monitoring** (Primary):\n   - Expected interval: 30 seconds\n   - Grace period: 3 missed heartbeats (90 seconds)\n   - Action: Mark worker as `dead`, trigger task recovery\n\n2. **Active Health Probes** (Secondary):\n   - For workers in `busy` state only\n   - HTTP health check every 60 seconds\n   - Independent of heartbeat\n   - Detects worker hangs or network partitions\n\n3. **Task Timeout Monitoring** (Tertiary):\n   - Each task has individual timeout\n   - Background job checks for tasks in `executing` state past timeout\n   - Action: Cancel task, mark for retry, investigate worker health\n\n#### Worker Failure Recovery Protocol\n```\nWhen worker fails (missed heartbeat or failed health probe):\n\n1. Immediately mark worker status as 'dead'\n2. Acquire distributed lock on worker_id\n3. Query all tasks assigned to worker in 'assigned' or 'executing' state\n4. For each task:\n   a. Check visibility timeout - if not expired, skip (another worker may have it)\n   b. Increment 'worker_failure_count' metric\n   c. Reset task state:\n      - If in 'assigned': return to 'queued'\n      - If in 'executing': \n          - If task duration < 10% of timeout: return to 'queued' (likely not started)\n          - If task duration >= 10%: mark as 'failed', increment retry_count\n   d. Boost priority by one level (low\u2192medium, medium\u2192high)\n   e. Add to queue with boosted score\n5. Release distributed lock\n6. Log worker failure event with task count\n7. Alert if worker failure rate > threshold\n```\n\n#### Preventing Thundering Herd on Worker Failure\n- **Rate Limiting**: Requeued tasks are staggered by 100ms each\n- **Backoff**: Tasks that failed on worker death get additional 5-second delay before becoming visible\n- **Load Shedding**: If queue depth exceeds 2\u00d7 normal capacity, new task submissions return 503\n\n### Enhanced Retry Strategy\n\n#### Intelligent Retry Decision\n```python\ndef should_retry(task, error):\n    # Check explicit retryability\n    if error.retryable == False:\n        return False\n    \n    # Check retry limits\n    if task.retry_count >= task.max_retries:\n        return False\n    \n    # Exponential backoff with jitter\n    base_delay = 2 ** task.retry_count\n    jitter = random.uniform(0, 0.3 * base_delay)\n    delay = min(base_delay + jitter, max_retry_delay_for_priority(task.priority))\n    \n    # Check error patterns\n    if error.code in ['RATE_LIMIT', 'TEMPORARY_UNAVAILABLE']:\n        delay *= 2  # Double delay for rate limits\n    \n    if error.code in ['WORKER_OOM', 'TIMEOUT']:\n        # Increase task timeout by 50% on retry\n        task.timeout_seconds *= 1.5\n    \n    # Schedule retry\n    task.visibility_timeout_until = now() + delay\n    task.retry_count += 1\n    redis.zadd('queue:active', task_id, calculate_score(task))\n    \n    return True\n```\n\n#### Retry Limits by Error Type\n- **Transient errors** (network, rate limit): 5 retries\n- **Resource errors** (OOM, timeout): 3 retries with increased resources\n- **Logic errors** (invalid input): 0 retries, immediate DLQ\n- **Worker failures**: 3 retries with priority boost\n\n### Data Consistency & Race Condition Prevention\n\n#### Optimistic Locking for State Transitions\n```python\ndef transition_task_state(task_id, expected_state, new_state):\n    # Atomic compare-and-swap\n    result = redis.eval(\"\"\"\n        local task_key = 'tasks:' .. ARGV[1]\n        local current_state = redis.call('HGET', task_key, 'status')\n        local version = redis.call('HGET', task_key, 'version')\n        \n        if current_state == ARGV[2] and version == ARGV[4] then\n            redis.call('HSET', task_key, 'status', ARGV[3])\n            redis.call('HINCRBY', task_key, 'version', 1)\n            redis.call('HSET', task_key, 'updated_at', ARGV[5])\n            return 1\n        else\n            return 0\n        end\n    \"\"\", 0, task_id, expected_state, new_state, current_version, timestamp)\n    \n    if result == 0:\n        raise ConcurrentModificationError()\n```\n\n#### Distributed Lock for Worker Assignment\n- Uses **Redlock algorithm** with 3 Redis instances\n- Lock TTL: 10 seconds (exceeding this indicates system issues)\n- Retry: 3 attempts with 200ms delay\n- Fallback: If redlock fails, use PostgreSQL advisory locks (slower but guaranteed)\n\n#### Idempotency Guarantees\n```python\ndef submit_task(payload, idempotency_key=None):\n    if idempotency_key:\n        # Check cache first (24-hour TTL)\n        existing_task_id = redis.get(f'idempotency:{idempotency_key}')\n        if existing_task_id:\n            return get_task(existing_task_id)\n        \n        # Acquire lock on idempotency key\n        with redis_lock(f'lock:idempotency:{idempotency_key}', timeout=5):\n            # Double-check after acquiring lock\n            existing_task_id = redis.get(f'idempotency:{idempotency_key}')\n            if existing_task_id:\n                return get_task(existing_task_id)\n            \n            # Create task\n            task = create_task_internal(payload)\n            \n            # Store idempotency mapping\n            redis.setex(\n                f'idempotency:{idempotency_key}',\n                86400,  # 24 hours\n                task.task_id\n            )\n            \n            return task\n    else:\n        return create_task_internal(payload)\n```\n\n### Cascading Failure Prevention\n\n#### Circuit Breaker Pattern\n```python\nclass WorkerCircuitBreaker:\n    def __init__(self, worker_id):\n        self.failure_threshold = 5\n        self.timeout = 60  # seconds\n        self.half_open_attempts = 3\n    \n    def record_failure(self):\n        failures = redis.incr(f'circuit:{worker_id}:failures')\n        redis.expire(f'circuit:{worker_id}:failures', self.timeout)\n        \n        if failures >= self.failure_threshold:\n            self.open_circuit()\n    \n    def open_circuit(self):\n        redis.setex(f'circuit:{worker_id}:state', self.timeout, 'open')\n        # Stop assigning tasks to this worker\n        \n    def attempt_half_open(self):\n        # After timeout, try limited assignments\n        pass\n```\n\n#### Queue Overflow Protection\n```\nRate Limiting per User:\n- Burst: 100 tasks/minute\n- Sustained: 1000 tasks/hour\n- Storage: Redis sliding window counter\n\nGlobal Backpressure:\n- If queue depth > 10,000: Return 429 with Retry-After header\n- If queue depth > 50,000: Reject all submissions except priority=high\n- If queue depth > 100,000: Emergency mode - only accept from allowlist\n```\n\n## Monitoring & Observability\n\n### Key Metrics (exported to Prometheus)\n\n**Queue Health**:\n- `queue_depth{priority=\"high|medium|low\"}` - Current tasks in queue\n- `queue_age_seconds{percentile=\"p50|p95|p99\"}` - Task wait time by percentile\n- `starvation_risk_count` - Tasks waiting > 80% of max_wait_seconds\n\n**Task Lifecycle**:\n- `task_duration_seconds{status=\"completed|failed\"}` - End-to-end duration\n- `task_retry_count{reason=\"worker_failure|timeout|error\"}` - Retry reasons\n- `task_state_transitions_total{from=\"X\",to=\"Y\"}` - State machine flow\n\n**Worker Health**:\n- `worker_count{status=\"idle|busy|draining|dead\"}` - Worker distribution\n- `worker_utilization_percent` - Average utilization across pool\n- `worker_failure_rate` - Failures per minute\n- `task_assignment_duration_seconds` - Time to assign task to worker\n\n**Error Rates**:\n- `task_dlq_total{reason=\"max_retries|unrecoverable_error\"}` - DLQ additions\n- `concurrent_modification_errors_total` - Lock contention\n- `circuit_breaker_state{worker_id}` - Worker circuit breaker states\n\n### Structured Logging\n```json\n{\n  \"timestamp\": \"2024-01-15T10:30:45.123Z\",\n  \"level\": \"INFO\",\n  \"trace_id\": \"abc123\",\n  \"span_id\": \"def456\",\n  \"event\": \"task_state_transition\",\n  \"task_id\": \"uuid\",\n  \"from_state\": \"queued\",\n  \"to_state\": \"assigned\",\n  \"worker_id\": \"worker-uuid\",\n  \"priority\": \"high\",\n  \"age_seconds\": 45,\n  \"retry_count\": 0,\n  \"user_id\": \"user-123\"\n}\n```\n\n### Alerting Rules\n\n**Critical**:\n- Worker failure rate > 20% over 5 minutes\n- Queue depth > 50,000 for any priority\n- DLQ growth rate > 10 tasks/minute\n- Task completion rate drops by 50% compared to baseline\n\n**Warning**:\n- Tasks waiting > max_wait_seconds (starvation)\n- Worker availability < 20% idle\n- Retry rate > 15% of completions\n- Circuit breakers open for > 5 workers\n\n**Info**:\n- Queue depth > 5,000\n- Worker pool auto-scaled\n- Idempotency key collision rate elevated\n\n## Deployment Considerations\n\n### Single-Region Deployment (Recommended)\n- Single Redis cluster (3 nodes, sentinel for HA)\n- PostgreSQL primary with read replicas\n- Worker pool in same VPC/region\n- Latency: <5ms for queue operations\n\n### Multi-Region (Advanced)\n- Redis cluster per region with async replication\n- Global PostgreSQL with region-specific read replicas\n- Task affinity to submission region\n- Cross-region failover with task reconciliation\n- Latency: <20ms for queue operations\n\n## Rationale\n\n### Why Unified Queue with Dynamic Scoring?\n**Better than multiple separate queues because**:\n- **Mathematical guarantee** against starvation (impossible with separate queues + boost)\n- Simpler reasoning about priority - no manual threshold tuning\n- No edge cases where boost timing affects fairness\n- Single atomic operation to get highest priority task\n- Aging is continuous and automatic, not periodic\n\n### Why Hybrid Redis + PostgreSQL?\n- **Redis** provides O(log N) priority queue operations with sub-millisecond latency\n- **PostgreSQL** provides durable storage, complex queries, and task history\n- Async replication keeps databases in sync without blocking queue operations\n- Redis failure: Can rebuild queue from PostgreSQL in <10 seconds\n- PostgreSQL failure: Queue continues operating, history queued for replay\n\n### Why Capability-Based Routing?\n- Prevents task assignment to incompatible workers (GPU task to CPU worker)\n- Enables heterogeneous worker pools with specialized hardware\n- Allows gradual worker pool composition changes\n- Foundation for auto-scaling based on capability demand\n\n### Why Optimistic Locking?\n- **10x better throughput** than pessimistic locks under normal load\n- Conflicts are rare (<1%) in this workload\n- Graceful retry mechanism\n- Fallback to PostgreSQL advisory locks for rare high-contention scenarios\n\n### Why Visibility Timeout?\n- Prevents duplicate processing if worker polls but crashes before heartbeat\n- Allows automatic recovery without explicit failure detection\n- Standard pattern from AWS SQS, battle-tested at scale\n- Simpler than distributed task ownership tracking\n\n## What I Improved\n\n1. **Starvation Prevention**: Replaced discrete priority queues with unified queue + dynamic scoring, providing **mathematical guarantee** against starvation rather than heuristic boosting.\n\n2. **Worker Failure Detection**: Added **three-tier health monitoring** (heartbeat, active probes, task timeout) to catch all failure modes including network partitions and hung workers.\n\n3. **Race Condition Prevention**: Added comprehensive **optimistic locking** for state transitions, **Redlock** for worker assignment, and **visibility timeouts** to prevent duplicate processing.\n\n4. **Task Recovery Protocol**: Detailed **step-by-step recovery procedure** for worker failures with priority boosting, thundering herd prevention, and intelligent retry decisions.\n\n5. **Capability-Based Routing**: Added worker capabilities and task requirements to prevent mismatched assignments and enable heterogeneous worker pools.\n\n6. **Circuit Breaker Pattern**: Protect against cascading failures by temporarily isolating consistently failing workers.\n\n7. **Enhanced State Machine**: More detailed states (`assigned` vs `executing`) to better track task lifecycle and enable smarter recovery decisions.\n\n8. **DLQ Management**: Added replay capability and manual review interface for dead letter queue, not just storage.\n\n9. **Monitoring**: Added specific metrics for starvation risk, aging statistics, and circuit breaker states beyond basic queue depth.\n\n## What I Kept\n\n1. **Core Architecture**: Three-layer design (API, Queue, Worker) is sound and well-separated.\n\n2. **Data Models**: Task and Worker entities are comprehensive with good metadata.\n\n3. **Idempotency Handling**: Redis-based idempotency key mapping with 24-hour TTL is appropriate.\n\n4. **Retry Strategy**: Exponential backoff with priority-based max delays is industry standard.\n\n5. **REST API Design**: Endpoints are clean and RESTful with appropriate HTTP semantics.\n\n6. **Pull Model for Workers**: Worker-controlled load is correct approach for backpressure.\n\n## Remaining Concerns\n\n1. **Redis Single Point of Failure**: While sentinel provides HA, there's still potential for split-brain scenarios. Consider Redis Cluster for true distributed operation at scale.\n\n2. **Task Payload Size**: No limits specified on `payload.parameters` size. Large payloads could impact Redis memory and network. Recommend 1MB limit with external storage (S3) for larger data.\n\n3. **Long-Running Tasks**: Tasks running for hours may hold worker capacity. Consider splitting into smaller chunks or separate \"batch job\" system.\n\n4. **Cost at Scale**: Redis memory for large queues can be expensive. May need cold storage tier for tasks in `submitted` state waiting for capacity.\n\n5. **Cross-Datacenter Latency**: Multi-region deployment has unresolved task reconciliation complexity if regions become partitioned.\n\n## Convergence Status\n**ITERATING**: Design significantly improved but would benefit from architect's review of dynamic scoring algorithm and feedback on complexity trade-offs.\n\n## Prompt for Collaborator",
        "prompt_for_other": "Please review the unified priority queue with dynamic scoring versus your separate queue approach. The math provides starvation guarantees, but adds complexity. Also evaluate whether the three-tier health monitoring for workers is over-engineered or necessary for production robustness. Consider if there are simplifications that maintain the guarantees while reducing implementation complexity.",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system implements a **priority-based task scheduling system** with strong guarantees against starvation and robust failure recovery. It uses a **hybrid push-pull model** with three main layers:\n\n1. **API Layer**: REST endpoints for task lifecycle management\n2. **Scheduling Layer**: Dynamic priority queue with aging-based starvation prevention\n3. **Execution Layer**: Worker pool with capability-based routing and failure detection\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels\n  - Maintain a unified priority queue with dynamic scoring\n  - Route tasks to capable workers based on requirements\n  - Orchestrate task lifecycle (submitted \u2192 queued \u2192 assigned \u2192 executing \u2192 terminal state)\n  - Implement circuit breakers for cascading failure prevention\n\n### 2. Unified Priority Queue Implementation\n- **Structure**: Single Redis sorted set with dynamic composite scores\n- **Score Calculation**: \n  ```\n  score = (priority_weight \u00d7 1000) - (age_seconds \u00d7 aging_factor)\n  \n  priority_weight: high=1000, medium=100, low=10\n  aging_factor: 0.5 (low priority tasks gain 2 points per second)\n  \n  Example after 600 seconds:\n  - High priority task: 1000000 - (600 \u00d7 0.5) = 999700\n  - Low priority task: 10000 - (600 \u00d7 0.5) = 9700\n  ```\n- **Storage**: \n  - Redis sorted set `queue:active` for fast priority-based polling\n  - Redis hash `tasks:{task_id}` for task metadata\n  - PostgreSQL for durable task history and analytics\n- **Guaranteed Starvation Prevention**: Mathematical guarantee that any task will eventually reach highest effective priority\n\n### 3. Worker Pool with Capability Matching\n- **Architecture**: Auto-scaling worker pool with capability tags\n- **Worker States**: `initializing`, `idle`, `busy`, `draining`, `dead`\n- **Health Checks**: \n  - Heartbeat every 30 seconds with exponential timeout (3 missed = 90s)\n  - Active health probes for workers in `busy` state every 60 seconds\n- **Concurrency**: Per-worker semaphore with configurable capacity\n- **Graceful Shutdown**: `draining` state prevents new assignments while completing current tasks\n\n### 4. Task State Machine\n- **States**: `submitted` \u2192 `queued` \u2192 `assigned` \u2192 `executing` \u2192 [`completed` | `failed` | `cancelled` | `dead_letter`]\n- **Transitions**: \n  - All transitions recorded with timestamp and triggering event\n  - State machine enforced in application layer with PostgreSQL constraints\n  - Optimistic locking prevents race conditions\n\n### 5. Dead Letter Queue & Observability\n- **DLQ**: Separate Redis sorted set `queue:dlq` for tasks exceeding retry limits\n- **DLQ Processing**: Manual review interface + automated replay capability\n- **Distributed Tracing**: OpenTelemetry instrumentation for full task journey\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\",\n    \"required_capabilities\": [\"gpu\", \"high-memory\"]\n  },\n  \"status\": \"submitted|queued|assigned|executing|completed|failed|cancelled|dead_letter\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"queued_at\": \"timestamp|null\",\n  \"assigned_at\": \"timestamp|null\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker\": \"worker-id|null\",\n  \"result\": \"json-object|null\",\n  \"error\": {\n    \"message\": \"string\",\n    \"code\": \"string\",\n    \"retryable\": true,\n    \"details\": \"json-object\"\n  },\n  \"timeout_seconds\": 300,\n  \"user_id\": \"string\",\n  \"idempotency_key\": \"string|null\",\n  \"version\": 0,\n  \"trace_id\": \"string\",\n  \"estimated_duration_seconds\": 60,\n  \"visibility_timeout_until\": \"timestamp|null\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"initializing|idle|busy|draining|dead\",\n  \"current_tasks\": [\"task-id-1\", \"task-id-2\"],\n  \"last_heartbeat\": \"timestamp\",\n  \"consecutive_failures\": 0,\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"started_at\": \"timestamp\",\n  \"capabilities\": [\"gpu\", \"cpu-intensive\", \"high-memory\"],\n  \"version\": 0,\n  \"drain_initiated_at\": \"timestamp|null\"\n}\n```\n\n### Priority Configuration\n```json\n{\n  \"high\": {\n    \"weight\": 1000,\n    \"max_retry_delay_seconds\": 10,\n    \"max_retries\": 5,\n    \"timeout_multiplier\": 1.0\n  },\n  \"medium\": {\n    \"weight\": 100,\n    \"max_retry_delay_seconds\": 30,\n    \"max_retries\": 3,\n    \"timeout_multiplier\": 1.5\n  },\n  \"low\": {\n    \"weight\": 10,\n    \"max_retry_delay_seconds\": 60,\n    \"max_retries\": 2,\n    \"timeout_multiplier\": 2.0\n  },\n  \"starvation_prevention\": {\n    \"aging_factor\": 0.5,\n    \"max_wait_seconds\": 3600\n  }\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"required_capabilities\": [\"gpu\"],\n  \"timeout_seconds\": 600,\n  \"estimated_duration_seconds\": 120,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"queued\",\n  \"estimated_start_time\": \"timestamp\",\n  \"position_in_queue\": 5\n}\n```\n\n#### Task Status with History\n```\nGET /api/v1/tasks/{task_id}?include_history=true\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"executing\",\n  \"progress\": 45,\n  \"created_at\": \"...\",\n  \"assigned_worker\": \"worker-uuid\",\n  \"state_history\": [\n    {\"state\": \"submitted\", \"timestamp\": \"...\"},\n    {\"state\": \"queued\", \"timestamp\": \"...\"},\n    {\"state\": \"assigned\", \"timestamp\": \"...\", \"worker_id\": \"...\"},\n    {\"state\": \"executing\", \"timestamp\": \"...\"}\n  ]\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\",\n  \"cancellation_time\": \"timestamp\"\n}\n```\n\n#### Queue Statistics with Aging Metrics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"queue_depth\": {\n    \"high\": {\"pending\": 12, \"executing\": 3},\n    \"medium\": {\"pending\": 45, \"executing\": 10},\n    \"low\": {\"pending\": 123, \"executing\": 5}\n  },\n  \"aging_stats\": {\n    \"oldest_queued_task_age_seconds\": 450,\n    \"tasks_waiting_over_threshold\": 5\n  },\n  \"workers\": {\n    \"total\": 20,\n    \"idle\": 2,\n    \"busy\": 17,\n    \"draining\": 1,\n    \"dead\": 0\n  },\n  \"throughput\": {\n    \"completed_last_minute\": 120,\n    \"failed_last_minute\": 3\n  }\n}\n```\n\n#### Dead Letter Queue Management\n```\nGET /api/v1/dlq\nResponse: {\n  \"tasks\": [...],\n  \"total_count\": 15\n}\n\nPOST /api/v1/dlq/{task_id}/replay\nBody: {\n  \"reset_retry_count\": true,\n  \"new_priority\": \"high\"\n}\n```\n\n### Worker Interface (Internal)\n\n#### Worker Registration\n```\nPOST /internal/workers/register\nBody: {\n  \"capabilities\": [\"gpu\", \"high-memory\"],\n  \"capacity\": 5\n}\nResponse: {\n  \"worker_id\": \"uuid\",\n  \"poll_interval_seconds\": 5\n}\n```\n\n#### Poll for Tasks (with Capability Matching)\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"available_capacity\": 3\n}\nResponse: [\n  {\n    \"task_id\": \"uuid\",\n    \"task_type\": \"process_video\",\n    \"parameters\": {...},\n    \"timeout_seconds\": 600,\n    \"trace_id\": \"...\"\n  }\n]\n```\n\n#### Report Task Progress\n```\nPOST /internal/workers/{worker_id}/progress\nBody: {\n  \"task_id\": \"uuid\",\n  \"progress_percent\": 45,\n  \"status_message\": \"Processing frame 450/1000\"\n}\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n```\n\n#### Worker Heartbeat with Health\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"busy\",\n  \"health\": {\n    \"memory_usage_percent\": 65,\n    \"cpu_usage_percent\": 80\n  }\n}\nResponse: {\n  \"acknowledged\": true,\n  \"should_drain\": false\n}\n```\n\n## Enhanced Priority Scheduling Algorithm\n\n### Unified Scoring with Guaranteed Starvation Prevention\n\n**Mathematical Guarantee**:\n```\nGiven:\n- High priority weight: 1000\n- Low priority weight: 10\n- Aging factor: 0.5\n\nA low-priority task will overtake a high-priority task created simultaneously after:\n(1000 \u00d7 1000 - 10 \u00d7 1000) / 0.5 = 1,980,000 seconds\n\nHowever, high-priority tasks are typically short-lived. More realistically:\n- Low priority task created at t=0: score = 10000\n- High priority task created at t=1800 (30 min later): score = 1000000 - 900 = 999100\n- Low priority task at t=1800: score = 10000 - 900 = 9100\n\nThe low task needs to age enough to overcome the gap:\n(999100 - 9100) / 0.5 = 1,980,000 additional seconds\n\nThis is extreme, so we add an absolute maximum wait threshold.\n```\n\n**Starvation Prevention Rules**:\n1. **Dynamic Scoring**: Score decreases linearly with age for ALL tasks\n2. **Maximum Wait Time**: Any task waiting > 3600 seconds automatically gets boosted:\n   ```\n   if (current_time - created_at) > max_wait_seconds:\n       effective_priority = \"high\"\n       score = 1000000 - (age \u00d7 aging_factor)\n   ```\n3. **Priority Inversion Detection**: If a low-priority task has waited longer than the oldest high-priority task has existed, boost to medium\n4. **Fair Scheduling within Priority**: FIFO within the same effective priority band (\u00b1100 score points)\n\n### Task Assignment Algorithm\n\n```python\ndef assign_tasks_to_workers():\n    idle_workers = get_idle_workers_sorted_by_capacity()\n    \n    for worker in idle_workers:\n        available_slots = worker.capacity - worker.current_load\n        \n        # Get top N tasks that match worker capabilities\n        candidate_tasks = redis.zrevrange(\n            'queue:active', \n            0, \n            available_slots * 2  # Fetch extra for capability filtering\n        )\n        \n        matched_tasks = []\n        for task_id in candidate_tasks:\n            task = get_task_metadata(task_id)\n            \n            # Skip if task is visibility-locked\n            if task.visibility_timeout_until > now():\n                continue\n            \n            # Check capability match\n            if not worker.has_capabilities(task.required_capabilities):\n                continue\n            \n            # Check if task is within priority fairness window\n            if len(matched_tasks) > 0:\n                score_diff = get_score(matched_tasks[0]) - get_score(task)\n                if score_diff > fairness_threshold:\n                    break  # Don't assign drastically different priorities together\n            \n            matched_tasks.append(task)\n            if len(matched_tasks) >= available_slots:\n                break\n        \n        # Atomic assignment with optimistic locking\n        for task in matched_tasks:\n            assign_task_to_worker(task, worker)\n```\n\n## Comprehensive Error Handling\n\n### Worker Failure Detection & Recovery\n\n#### Three-Tier Health Monitoring\n1. **Heartbeat Monitoring** (Primary):\n   - Expected interval: 30 seconds\n   - Grace period: 3 missed heartbeats (90 seconds)\n   - Action: Mark worker as `dead`, trigger task recovery\n\n2. **Active Health Probes** (Secondary):\n   - For workers in `busy` state only\n   - HTTP health check every 60 seconds\n   - Independent of heartbeat\n   - Detects worker hangs or network partitions\n\n3. **Task Timeout Monitoring** (Tertiary):\n   - Each task has individual timeout\n   - Background job checks for tasks in `executing` state past timeout\n   - Action: Cancel task, mark for retry, investigate worker health\n\n#### Worker Failure Recovery Protocol\n```\nWhen worker fails (missed heartbeat or failed health probe):\n\n1. Immediately mark worker status as 'dead'\n2. Acquire distributed lock on worker_id\n3. Query all tasks assigned to worker in 'assigned' or 'executing' state\n4. For each task:\n   a. Check visibility timeout - if not expired, skip (another worker may have it)\n   b. Increment 'worker_failure_count' metric\n   c. Reset task state:\n      - If in 'assigned': return to 'queued'\n      - If in 'executing': \n          - If task duration < 10% of timeout: return to 'queued' (likely not started)\n          - If task duration >= 10%: mark as 'failed', increment retry_count\n   d. Boost priority by one level (low\u2192medium, medium\u2192high)\n   e. Add to queue with boosted score\n5. Release distributed lock\n6. Log worker failure event with task count\n7. Alert if worker failure rate > threshold\n```\n\n#### Preventing Thundering Herd on Worker Failure\n- **Rate Limiting**: Requeued tasks are staggered by 100ms each\n- **Backoff**: Tasks that failed on worker death get additional 5-second delay before becoming visible\n- **Load Shedding**: If queue depth exceeds 2\u00d7 normal capacity, new task submissions return 503\n\n### Enhanced Retry Strategy\n\n#### Intelligent Retry Decision\n```python\ndef should_retry(task, error):\n    # Check explicit retryability\n    if error.retryable == False:\n        return False\n    \n    # Check retry limits\n    if task.retry_count >= task.max_retries:\n        return False\n    \n    # Exponential backoff with jitter\n    base_delay = 2 ** task.retry_count\n    jitter = random.uniform(0, 0.3 * base_delay)\n    delay = min(base_delay + jitter, max_retry_delay_for_priority(task.priority))\n    \n    # Check error patterns\n    if error.code in ['RATE_LIMIT', 'TEMPORARY_UNAVAILABLE']:\n        delay *= 2  # Double delay for rate limits\n    \n    if error.code in ['WORKER_OOM', 'TIMEOUT']:\n        # Increase task timeout by 50% on retry\n        task.timeout_seconds *= 1.5\n    \n    # Schedule retry\n    task.visibility_timeout_until = now() + delay\n    task.retry_count += 1\n    redis.zadd('queue:active', task_id, calculate_score(task))\n    \n    return True\n```\n\n#### Retry Limits by Error Type\n- **Transient errors** (network, rate limit): 5 retries\n- **Resource errors** (OOM, timeout): 3 retries with increased resources\n- **Logic errors** (invalid input): 0 retries, immediate DLQ\n- **Worker failures**: 3 retries with priority boost\n\n### Data Consistency & Race Condition Prevention\n\n#### Optimistic Locking for State Transitions\n```python\ndef transition_task_state(task_id, expected_state, new_state):\n    # Atomic compare-and-swap\n    result = redis.eval(\"\"\"\n        local task_key = 'tasks:' .. ARGV[1]\n        local current_state = redis.call('HGET', task_key, 'status')\n        local version = redis.call('HGET', task_key, 'version')\n        \n        if current_state == ARGV[2] and version == ARGV[4] then\n            redis.call('HSET', task_key, 'status', ARGV[3])\n            redis.call('HINCRBY', task_key, 'version', 1)\n            redis.call('HSET', task_key, 'updated_at', ARGV[5])\n            return 1\n        else\n            return 0\n        end\n    \"\"\", 0, task_id, expected_state, new_state, current_version, timestamp)\n    \n    if result == 0:\n        raise ConcurrentModificationError()\n```\n\n#### Distributed Lock for Worker Assignment\n- Uses **Redlock algorithm** with 3 Redis instances\n- Lock TTL: 10 seconds (exceeding this indicates system issues)\n- Retry: 3 attempts with 200ms delay\n- Fallback: If redlock fails, use PostgreSQL advisory locks (slower but guaranteed)\n\n#### Idempotency Guarantees\n```python\ndef submit_task(payload, idempotency_key=None):\n    if idempotency_key:\n        # Check cache first (24-hour TTL)\n        existing_task_id = redis.get(f'idempotency:{idempotency_key}')\n        if existing_task_id:\n            return get_task(existing_task_id)\n        \n        # Acquire lock on idempotency key\n        with redis_lock(f'lock:idempotency:{idempotency_key}', timeout=5):\n            # Double-check after acquiring lock\n            existing_task_id = redis.get(f'idempotency:{idempotency_key}')\n            if existing_task_id:\n                return get_task(existing_task_id)\n            \n            # Create task\n            task = create_task_internal(payload)\n            \n            # Store idempotency mapping\n            redis.setex(\n                f'idempotency:{idempotency_key}',\n                86400,  # 24 hours\n                task.task_id\n            )\n            \n            return task\n    else:\n        return create_task_internal(payload)\n```\n\n### Cascading Failure Prevention\n\n#### Circuit Breaker Pattern\n```python\nclass WorkerCircuitBreaker:\n    def __init__(self, worker_id):\n        self.failure_threshold = 5\n        self.timeout = 60  # seconds\n        self.half_open_attempts = 3\n    \n    def record_failure(self):\n        failures = redis.incr(f'circuit:{worker_id}:failures')\n        redis.expire(f'circuit:{worker_id}:failures', self.timeout)\n        \n        if failures >= self.failure_threshold:\n            self.open_circuit()\n    \n    def open_circuit(self):\n        redis.setex(f'circuit:{worker_id}:state', self.timeout, 'open')\n        # Stop assigning tasks to this worker\n        \n    def attempt_half_open(self):\n        # After timeout, try limited assignments\n        pass\n```\n\n#### Queue Overflow Protection\n```\nRate Limiting per User:\n- Burst: 100 tasks/minute\n- Sustained: 1000 tasks/hour\n- Storage: Redis sliding window counter\n\nGlobal Backpressure:\n- If queue depth > 10,000: Return 429 with Retry-After header\n- If queue depth > 50,000: Reject all submissions except priority=high\n- If queue depth > 100,000: Emergency mode - only accept from allowlist\n```\n\n## Monitoring & Observability\n\n### Key Metrics (exported to Prometheus)\n\n**Queue Health**:\n- `queue_depth{priority=\"high|medium|low\"}` - Current tasks in queue\n- `queue_age_seconds{percentile=\"p50|p95|p99\"}` - Task wait time by percentile\n- `starvation_risk_count` - Tasks waiting > 80% of max_wait_seconds\n\n**Task Lifecycle**:\n- `task_duration_seconds{status=\"completed|failed\"}` - End-to-end duration\n- `task_retry_count{reason=\"worker_failure|timeout|error\"}` - Retry reasons\n- `task_state_transitions_total{from=\"X\",to=\"Y\"}` - State machine flow\n\n**Worker Health**:\n- `worker_count{status=\"idle|busy|draining|dead\"}` - Worker distribution\n- `worker_utilization_percent` - Average utilization across pool\n- `worker_failure_rate` - Failures per minute\n- `task_assignment_duration_seconds` - Time to assign task to worker\n\n**Error Rates**:\n- `task_dlq_total{reason=\"max_retries|unrecoverable_error\"}` - DLQ additions\n- `concurrent_modification_errors_total` - Lock contention\n- `circuit_breaker_state{worker_id}` - Worker circuit breaker states\n\n### Structured Logging\n```json\n{\n  \"timestamp\": \"2024-01-15T10:30:45.123Z\",\n  \"level\": \"INFO\",\n  \"trace_id\": \"abc123\",\n  \"span_id\": \"def456\",\n  \"event\": \"task_state_transition\",\n  \"task_id\": \"uuid\",\n  \"from_state\": \"queued\",\n  \"to_state\": \"assigned\",\n  \"worker_id\": \"worker-uuid\",\n  \"priority\": \"high\",\n  \"age_seconds\": 45,\n  \"retry_count\": 0,\n  \"user_id\": \"user-123\"\n}\n```\n\n### Alerting Rules\n\n**Critical**:\n- Worker failure rate > 20% over 5 minutes\n- Queue depth > 50,000 for any priority\n- DLQ growth rate > 10 tasks/minute\n- Task completion rate drops by 50% compared to baseline\n\n**Warning**:\n- Tasks waiting > max_wait_seconds (starvation)\n- Worker availability < 20% idle\n- Retry rate > 15% of completions\n- Circuit breakers open for > 5 workers\n\n**Info**:\n- Queue depth > 5,000\n- Worker pool auto-scaled\n- Idempotency key collision rate elevated\n\n## Deployment Considerations\n\n### Single-Region Deployment (Recommended)\n- Single Redis cluster (3 nodes, sentinel for HA)\n- PostgreSQL primary with read replicas\n- Worker pool in same VPC/region\n- Latency: <5ms for queue operations\n\n### Multi-Region (Advanced)\n- Redis cluster per region with async replication\n- Global PostgreSQL with region-specific read replicas\n- Task affinity to submission region\n- Cross-region failover with task reconciliation\n- Latency: <20ms for queue operations\n\n## Rationale\n\n### Why Unified Queue with Dynamic Scoring?\n**Better than multiple separate queues because**:\n- **Mathematical guarantee** against starvation (impossible with separate queues + boost)\n- Simpler reasoning about priority - no manual threshold tuning\n- No edge cases where boost timing affects fairness\n- Single atomic operation to get highest priority task\n- Aging is continuous and automatic, not periodic\n\n### Why Hybrid Redis + PostgreSQL?\n- **Redis** provides O(log N) priority queue operations with sub-millisecond latency\n- **PostgreSQL** provides durable storage, complex queries, and task history\n- Async replication keeps databases in sync without blocking queue operations\n- Redis failure: Can rebuild queue from PostgreSQL in <10 seconds\n- PostgreSQL failure: Queue continues operating, history queued for replay\n\n### Why Capability-Based Routing?\n- Prevents task assignment to incompatible workers (GPU task to CPU worker)\n- Enables heterogeneous worker pools with specialized hardware\n- Allows gradual worker pool composition changes\n- Foundation for auto-scaling based on capability demand\n\n### Why Optimistic Locking?\n- **10x better throughput** than pessimistic locks under normal load\n- Conflicts are rare (<1%) in this workload\n- Graceful retry mechanism\n- Fallback to PostgreSQL advisory locks for rare high-contention scenarios\n\n### Why Visibility Timeout?\n- Prevents duplicate processing if worker polls but crashes before heartbeat\n- Allows automatic recovery without explicit failure detection\n- Standard pattern from AWS SQS, battle-tested at scale\n- Simpler than distributed task ownership tracking\n\n## What I Improved\n\n1. **Starvation Prevention**: Replaced discrete priority queues with unified queue + dynamic scoring, providing **mathematical guarantee** against starvation rather than heuristic boosting.\n\n2. **Worker Failure Detection**: Added **three-tier health monitoring** (heartbeat, active probes, task timeout) to catch all failure modes including network partitions and hung workers.\n\n3. **Race Condition Prevention**: Added comprehensive **optimistic locking** for state transitions, **Redlock** for worker assignment, and **visibility timeouts** to prevent duplicate processing.\n\n4. **Task Recovery Protocol**: Detailed **step-by-step recovery procedure** for worker failures with priority boosting, thundering herd prevention, and intelligent retry decisions.\n\n5. **Capability-Based Routing**: Added worker capabilities and task requirements to prevent mismatched assignments and enable heterogeneous worker pools.\n\n6. **Circuit Breaker Pattern**: Protect against cascading failures by temporarily isolating consistently failing workers.\n\n7. **Enhanced State Machine**: More detailed states (`assigned` vs `executing`) to better track task lifecycle and enable smarter recovery decisions.\n\n8. **DLQ Management**: Added replay capability and manual review interface for dead letter queue, not just storage.\n\n9. **Monitoring**: Added specific metrics for starvation risk, aging statistics, and circuit breaker states beyond basic queue depth.\n\n## What I Kept\n\n1. **Core Architecture**: Three-layer design (API, Queue, Worker) is sound and well-separated.\n\n2. **Data Models**: Task and Worker entities are comprehensive with good metadata.\n\n3. **Idempotency Handling**: Redis-based idempotency key mapping with 24-hour TTL is appropriate.\n\n4. **Retry Strategy**: Exponential backoff with priority-based max delays is industry standard.\n\n5. **REST API Design**: Endpoints are clean and RESTful with appropriate HTTP semantics.\n\n6. **Pull Model for Workers**: Worker-controlled load is correct approach for backpressure.\n\n## Remaining Concerns\n\n1. **Redis Single Point of Failure**: While sentinel provides HA, there's still potential for split-brain scenarios. Consider Redis Cluster for true distributed operation at scale.\n\n2. **Task Payload Size**: No limits specified on `payload.parameters` size. Large payloads could impact Redis memory and network. Recommend 1MB limit with external storage (S3) for larger data.\n\n3. **Long-Running Tasks**: Tasks running for hours may hold worker capacity. Consider splitting into smaller chunks or separate \"batch job\" system.\n\n4. **Cost at Scale**: Redis memory for large queues can be expensive. May need cold storage tier for tasks in `submitted` state waiting for capacity.\n\n5. **Cross-Datacenter Latency**: Multi-region deployment has unresolved task reconciliation complexity if regions become partitioned.\n\n## Convergence Status\n**ITERATING**: Design significantly improved but would benefit from architect's review of dynamic scoring algorithm and feedback on complexity trade-offs.\n\n## Prompt for Collaborator\nPROMPT_FOR_ARCHITECT:\nPlease review the unified priority queue with dynamic scoring versus your separate queue approach. The math provides starvation guarantees, but adds complexity. Also evaluate whether the three-tier health monitoring for workers is over-engineered or necessary for production robustness. Consider if there are simplifications that maintain the guarantees while reducing implementation complexity."
      }
    },
    {
      "round_number": 2,
      "agent_a": {
        "content": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system implements a **priority-based task scheduling system** with proven starvation prevention and pragmatic failure recovery. It uses a **worker-pull model** with three main layers:\n\n1. **API Layer**: REST endpoints for task lifecycle management\n2. **Scheduling Layer**: Multi-queue priority system with age-based promotion\n3. **Execution Layer**: Worker pool with health monitoring and graceful degradation\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels (high, medium, low)\n  - Maintain separate priority queues with periodic age-based promotion\n  - Route tasks to workers based on simple capability matching\n  - Orchestrate task lifecycle (submitted \u2192 queued \u2192 executing \u2192 terminal state)\n  - Handle task timeouts and retries\n\n### 2. Multi-Queue Priority System with Age-Based Promotion\n\n**Structure**: Three separate Redis sorted sets, scored by submission time\n```\nqueue:high    - High priority tasks (scored by created_at timestamp)\nqueue:medium  - Medium priority tasks (scored by created_at timestamp)\nqueue:low     - Low priority tasks (scored by created_at timestamp)\n```\n\n**Starvation Prevention**: Periodic promotion via background job\n```python\n# Runs every 60 seconds\ndef promote_aged_tasks():\n    now = time.time()\n    \n    # Promote low \u2192 medium if waiting > 10 minutes\n    aged_low = redis.zrangebyscore(\n        'queue:low', \n        0, \n        now - 600,  # 10 minutes\n        start=0, \n        num=100\n    )\n    for task_id in aged_low:\n        task = get_task(task_id)\n        task.effective_priority = 'medium'\n        redis.zrem('queue:low', task_id)\n        redis.zadd('queue:medium', {task_id: task.created_at})\n    \n    # Promote medium \u2192 high if waiting > 20 minutes\n    aged_medium = redis.zrangebyscore(\n        'queue:medium',\n        0,\n        now - 1200,  # 20 minutes\n        start=0,\n        num=50\n    )\n    for task_id in aged_medium:\n        task = get_task(task_id)\n        task.effective_priority = 'high'\n        redis.zrem('queue:medium', task_id)\n        redis.zadd('queue:high', {task_id: task.created_at})\n```\n\n**Guarantees**:\n- Low priority tasks promoted to medium after 10 minutes\n- Medium priority tasks promoted to high after 20 minutes\n- Maximum wait time: 30 minutes for any task\n- Within each queue: strict FIFO ordering\n\n**Rationale**: This approach is simpler to reason about, debug, and maintain than dynamic scoring. The discrete promotion thresholds are easy to explain to users (\"your low priority task will be treated as medium after 10 minutes\") and the behavior is predictable. The 60-second promotion interval is frequent enough for practical fairness while being cheap to execute.\n\n### 3. Worker Pool with Simple Health Monitoring\n\n**Architecture**: Fixed-size worker pool with capability tags\n- **Worker States**: `active`, `draining`, `dead`\n- **Health Checks**: \n  - **Heartbeat only**: Every 30 seconds, 3 missed heartbeats (90s) = worker dead\n  - **Task timeout**: Independent per-task timeout, triggers recovery on expiry\n- **Concurrency**: Per-worker capacity (default: 5 concurrent tasks)\n- **Graceful Shutdown**: `draining` state prevents new assignments\n\n**Rationale for Simplified Health Monitoring**: \n- Active health probes add complexity and create new failure modes (probe timeouts, probe service overload)\n- Task-level timeouts already detect hung workers effectively\n- Heartbeat + task timeout covers 99% of failure scenarios\n- Simpler system = fewer edge cases = more reliable\n\n### 4. Task State Machine\n\n**States**: `queued` \u2192 `executing` \u2192 [`completed` | `failed` | `cancelled` | `dead_letter`]\n\n**Simplified from 7 states to 5 states**: \n- Removed `submitted` (immediately becomes `queued`)\n- Removed `assigned` (worker atomically transitions `queued` \u2192 `executing` during poll)\n- Removed `initializing` worker state (workers register as `active`)\n\n**Rationale**: Each state adds complexity in recovery logic. The key insight is that if a worker has the task, the worker is responsible for it. We don't need fine-grained tracking of \"assigned but not started\" - the task timeout handles this case.\n\n### 5. Dead Letter Queue & Core Observability\n\n**DLQ**: Separate Redis sorted set `queue:dlq` for tasks exceeding retry limits\n- Score: timestamp when moved to DLQ\n- TTL: 30 days\n- Manual replay: Admin endpoint to requeue with reset retries\n\n**Observability**: \n- **Metrics**: Queue depth, task duration, worker utilization, error rates\n- **Logging**: Structured JSON logs for state transitions\n- **Tracing**: Trace ID propagated through task lifecycle\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"effective_priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\",\n    \"required_capabilities\": [\"gpu\"]\n  },\n  \"status\": \"queued|executing|completed|failed|cancelled|dead_letter\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker_id\": \"worker-id|null\",\n  \"result\": \"json-object|null\",\n  \"error\": {\n    \"message\": \"string\",\n    \"retriable\": true\n  },\n  \"timeout_seconds\": 300,\n  \"idempotency_key\": \"string|null\",\n  \"trace_id\": \"string\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"active|draining|dead\",\n  \"current_tasks\": [\"task-id-1\", \"task-id-2\"],\n  \"last_heartbeat\": \"timestamp\",\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"version\": \"string\"\n}\n```\n\n### Priority Configuration\n```json\n{\n  \"high\": {\n    \"max_retries\": 5,\n    \"timeout_seconds\": 300\n  },\n  \"medium\": {\n    \"max_retries\": 3,\n    \"timeout_seconds\": 600\n  },\n  \"low\": {\n    \"max_retries\": 2,\n    \"timeout_seconds\": 900\n  },\n  \"starvation_prevention\": {\n    \"low_to_medium_seconds\": 600,\n    \"medium_to_high_seconds\": 1200,\n    \"promotion_interval_seconds\": 60,\n    \"promotion_batch_size\": 100\n  }\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"required_capabilities\": [\"gpu\"],\n  \"timeout_seconds\": 600,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"queued\",\n  \"queue_position\": 5\n}\n```\n\n#### Task Status\n```\nGET /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"executing\",\n  \"priority\": \"high\",\n  \"effective_priority\": \"high\",\n  \"created_at\": \"...\",\n  \"assigned_worker_id\": \"worker-uuid\",\n  \"result\": null\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\"\n}\n```\n\n#### Queue Statistics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"queues\": {\n    \"high\": {\"depth\": 12, \"oldest_age_seconds\": 45},\n    \"medium\": {\"depth\": 45, \"oldest_age_seconds\": 320},\n    \"low\": {\"depth\": 123, \"oldest_age_seconds\": 580}\n  },\n  \"workers\": {\n    \"active\": 18,\n    \"draining\": 2,\n    \"dead\": 0,\n    \"total_capacity\": 100,\n    \"used_capacity\": 67\n  },\n  \"throughput\": {\n    \"completed_last_minute\": 120,\n    \"failed_last_minute\": 3\n  }\n}\n```\n\n### Worker Interface (Internal)\n\n#### Worker Registration\n```\nPOST /internal/workers/register\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"capacity\": 5\n}\nResponse: {\n  \"worker_id\": \"uuid\",\n  \"poll_interval_ms\": 1000\n}\n```\n\n#### Poll for Tasks\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"available_capacity\": 3\n}\nResponse: {\n  \"tasks\": [\n    {\n      \"task_id\": \"uuid\",\n      \"task_type\": \"process_video\",\n      \"parameters\": {...},\n      \"timeout_seconds\": 600,\n      \"trace_id\": \"...\"\n    }\n  ]\n}\n```\n\n**Poll Algorithm**:\n```python\ndef poll_tasks(worker_id, capabilities, available_capacity):\n    tasks = []\n    \n    # Try each queue in priority order\n    for queue_name in ['queue:high', 'queue:medium', 'queue:low']:\n        if len(tasks) >= available_capacity:\n            break\n        \n        # Get oldest tasks from this queue\n        candidate_ids = redis.zrange(queue_name, 0, available_capacity * 2)\n        \n        for task_id in candidate_ids:\n            if len(tasks) >= available_capacity:\n                break\n            \n            # Atomic claim: remove from queue and update task\n            success = try_claim_task(task_id, worker_id, queue_name)\n            if success:\n                task = get_task(task_id)\n                \n                # Check capabilities\n                if task.required_capabilities.issubset(capabilities):\n                    tasks.append(task)\n                else:\n                    # Return to queue if capabilities don't match\n                    return_task_to_queue(task_id, queue_name)\n    \n    return tasks\n\ndef try_claim_task(task_id, worker_id, queue_name):\n    # Atomic Lua script\n    result = redis.eval(\"\"\"\n        local task_id = ARGV[1]\n        local worker_id = ARGV[2]\n        local queue_name = ARGV[3]\n        \n        -- Remove from queue\n        local removed = redis.call('ZREM', queue_name, task_id)\n        if removed == 0 then\n            return 0  -- Already claimed\n        end\n        \n        -- Update task status\n        local task_key = 'tasks:' .. task_id\n        redis.call('HSET', task_key, 'status', 'executing')\n        redis.call('HSET', task_key, 'assigned_worker_id', worker_id)\n        redis.call('HSET', task_key, 'started_at', ARGV[4])\n        \n        return 1\n    \"\"\", 0, task_id, worker_id, queue_name, time.time())\n    \n    return result == 1\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n```\n\n#### Worker Heartbeat\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"active\"\n}\nResponse: {\n  \"acknowledged\": true\n}\n```\n\n## Error Handling & Recovery\n\n### Worker Failure Detection & Recovery\n\n**Detection**: Background job runs every 30 seconds\n```python\ndef detect_dead_workers():\n    now = time.time()\n    cutoff = now - 90  # 3 missed heartbeats\n    \n    all_workers = get_all_workers()\n    for worker in all_workers:\n        if worker.status == 'dead':\n            continue\n        \n        if worker.last_heartbeat < cutoff:\n            handle_worker_failure(worker)\n\ndef handle_worker_failure(worker):\n    # Mark worker as dead\n    worker.status = 'dead'\n    save_worker(worker)\n    \n    # Find all tasks assigned to this worker\n    tasks = find_tasks_by_worker(worker.worker_id)\n    \n    for task in tasks:\n        if task.status == 'executing':\n            # Requeue with incremented retry\n            requeue_task(task)\n```\n\n**Task Timeout Detection**: Separate background job runs every 30 seconds\n```python\ndef detect_timed_out_tasks():\n    now = time.time()\n    \n    # Find tasks in 'executing' state past their timeout\n    timed_out = query_tasks(\n        status='executing',\n        started_at__lt=now - max_task_timeout\n    )\n    \n    for task in timed_out:\n        # This handles hung workers AND legitimately long tasks\n        handle_task_timeout(task)\n```\n\n### Retry Strategy\n\n```python\ndef requeue_task(task):\n    task.retry_count += 1\n    \n    # Check retry limit\n    if task.retry_count > task.max_retries:\n        move_to_dlq(task, reason='max_retries_exceeded')\n        return\n    \n    # Reset state\n    task.status = 'queued'\n    task.assigned_worker_id = None\n    task.started_at = None\n    \n    # Increase timeout by 50% on retry (handle slow tasks)\n    task.timeout_seconds *= 1.5\n    \n    # Return to original priority queue (NOT effective_priority)\n    # This ensures retries don't skip ahead unfairly\n    queue_name = f'queue:{task.priority}'\n    redis.zadd(queue_name, {task.task_id: task.created_at})\n    \n    save_task(task)\n```\n\n### Idempotency\n\n```python\ndef submit_task(payload, idempotency_key=None):\n    if idempotency_key:\n        # Check if already exists (24-hour window)\n        cache_key = f'idempotent:{idempotency_key}'\n        existing_id = redis.get(cache_key)\n        \n        if existing_id:\n            return get_task(existing_id)\n    \n    # Create new task\n    task = create_task(payload)\n    \n    if idempotency_key:\n        # Store mapping with 24-hour TTL\n        redis.setex(cache_key, 86400, task.task_id)\n    \n    # Add to appropriate queue\n    queue_name = f'queue:{task.priority}'\n    redis.zadd(queue_name, {task.task_id: task.created_at})\n    \n    return task\n```\n\n## Monitoring & Observability\n\n### Key Metrics (Prometheus)\n\n**Queue Health**:\n- `queue_depth{priority=\"high|medium|low\"}` - Tasks waiting\n- `queue_age_seconds{priority,percentile=\"p50|p95|p99\"}` - Wait time distribution\n- `tasks_promoted_total{from,to}` - Starvation prevention activity\n\n**Task Lifecycle**:\n- `task_duration_seconds{priority,status}` - End-to-end time\n- `task_retries_total{priority,reason}` - Retry patterns\n- `task_state_total{status}` - Current task distribution\n\n**Worker Health**:\n- `workers_total{status}` - Worker pool state\n- `worker_capacity_used_ratio` - Utilization\n- `worker_heartbeat_failures_total` - Health issues\n\n**Error Rates**:\n- `tasks_failed_total{reason}` - Failure classification\n- `tasks_dlq_total` - Unrecoverable failures\n\n### Alerting Rules\n\n**Critical**:\n- No active workers for >2 minutes\n- High priority queue depth >1000\n- Task failure rate >25%\n- DLQ growth >10 tasks/minute\n\n**Warning**:\n- Worker utilization >90% for >10 minutes\n- Low priority tasks waiting >25 minutes (promotion not working)\n- Any queue depth >5000\n\n## Deployment Architecture\n\n**Single-Region (Recommended)**:\n- Redis Sentinel (3 nodes) for high availability\n- PostgreSQL for durable task history\n- Worker pool: 10-100 instances depending on load\n- All components in same VPC for <5ms latency\n\n**Scaling Characteristics**:\n- Queue operations: O(log N) where N = queue depth\n- Worker polling: O(1) - fixed batch size\n- Promotion job: O(P) where P = tasks to promote (bounded by batch_size)\n- Expected throughput: 1000+ tasks/second per Redis instance\n\n## Rationale\n\n### Why Three Separate Queues Instead of Unified Dynamic Scoring?\n\n**Simplicity wins**:\n1. **Easier to debug**: \"Task X is in the medium queue at position 47\" vs. \"Task X has score 99,234\"\n2. **Predictable behavior**: Users understand \"promoted after 10 minutes\" intuitively\n3. **Simpler code**: No complex score calculation, just FIFO within each queue\n4. **Cheaper operations**: ZRANGE with time score is faster than computed scores\n5. **Observable**: Can monitor each queue independently\n\n**Trade-off accepted**: Promotion happens in 60-second intervals rather than continuously. This is acceptable because:\n- 60 seconds is negligible compared to 10-minute promotion threshold\n- Reduces Redis load (promotion is a batch operation)\n- Makes testing and validation easier\n\n### Why Simplified State Machine?\n\nThe original design had 7 states: `submitted \u2192 queued \u2192 assigned \u2192 executing \u2192 completed|failed|cancelled|dead_letter`\n\n**Problems with `assigned` state**:\n- Adds recovery complexity: what if worker crashes after `assigned` but before `executing`?\n- Requires separate timeout monitoring for assignment phase\n- Creates ambiguity: who is responsible for the task?\n\n**Solution**: Workers atomically claim tasks during poll (`queued` \u2192 `executing`). If worker crashes before processing, task timeout handles recovery. This is simpler and handles all edge cases.\n\n### Why Heartbeat-Only Health Monitoring?\n\nThe collaborator's design had three-tier monitoring (heartbeat + active probes + task timeout). \n\n**Analysis**:\n- **Heartbeat**: Necessary and sufficient for worker liveness\n- **Active health probes**: Adds complexity, requires health probe infrastructure, creates new failure modes\n- **Task timeout**: Necessary and independent - catches hung workers AND legitimately slow tasks\n\n**Conclusion**: Heartbeat + task timeout covers all scenarios:\n- Worker crashes: Heartbeat fails\n- Worker hangs: Task timeout fires\n- Worker slow but progressing: Legitimate, no action needed\n- Network partition: Both heartbeat and task timeout will trigger\n\nAdding active probes doesn't add coverage but does add failure modes (probe service down, probe timeouts, etc.).\n\n### Why Fixed Worker Pool?\n\nSimpler than auto-scaling for initial version. Benefits:\n- Predictable capacity and cost\n- No scaling algorithm to tune\n- Easier to reason about system load\n- Can add auto-scaling later without changing core design\n\n### Why Redis + PostgreSQL?\n\n**Redis**: \n- Sub-millisecond queue operations\n- Atomic claim operations via Lua scripts\n- Native sorted set for FIFO ordering\n\n**PostgreSQL**:\n- Durable task history for analytics\n- Complex queries for monitoring\n- Reliable storage for task results\n\n**Separation of concerns**: Redis is ephemeral (can rebuild from PostgreSQL), PostgreSQL is source of truth.\n\n## What I Changed\n\n1. **Simplified Priority System**: Replaced complex dynamic scoring with discrete queues + periodic promotion. Easier to understand, debug, and maintain while still preventing starvation.\n\n2. **Reduced State Machine Complexity**: Eliminated `submitted` and `assigned` states. Workers atomically claim tasks from `queued` \u2192 `executing`.\n\n3. **Two-Tier Health Monitoring**: Removed active health probes. Heartbeat + task timeout covers all failure scenarios with less complexity.\n\n4. **Simplified Worker States**: Reduced from 5 states to 3 (`active`, `draining`, `dead`). Removed `initializing` and `busy` as they don't add actionable information.\n\n5. **Removed Optimistic Locking**: Used atomic Lua scripts for critical operations instead. Simpler and avoids retry loops.\n\n6. **Removed Visibility Timeouts**: Not needed with atomic claim operation. Worker either has the task or doesn't.\n\n7. **Fixed Worker Pool**: Removed auto-scaling complexity from initial design. Can add later if needed.\n\n8. **Streamlined Task Entity**: Removed fields that weren't used in core workflows (`estimated_duration_seconds`, `visibility_timeout_until`, user tracking).\n\n## What I Kept\n\n1. **Three-Layer Architecture**: API, Queue, Worker separation is clean and correct.\n\n2. **Worker-Pull Model**: Workers control their own load, providing natural backpressure.\n\n3. **Capability-Based Routing**: Essential for heterogeneous worker pools.\n\n4. **Idempotency Keys**: 24-hour Redis-based deduplication is industry standard.\n\n5. **Dead Letter Queue**: Necessary for handling unrecoverable failures.\n\n6. **Priority Levels**: High/Medium/Low is intuitive and sufficient.\n\n7. **Retry Strategy**: Exponential backoff with priority-based limits is appropriate.\n\n## Open Questions\n\n1. **Promotion Thresholds**: Are 10/20 minutes the right values? Should be configurable per deployment based on SLA requirements.\n\n2. **Task Payload Size**: Need to define max size (suggest 256KB). Larger payloads should use external storage with reference in task.\n\n3. **Long-Running Tasks**: Tasks running for hours may hold worker capacity. Consider separate \"batch job\" system or streaming progress updates.\n\n4. **Multi-Region**: Current design is single-region. Multi-region adds significant complexity around task ownership and queue consistency.\n\n5. **Worker Authentication**: How do workers authenticate to the system? Need to add auth tokens or mTLS.\n\n## Convergence Status\n**ITERATING** - This design is significantly simplified while maintaining core guarantees. However, it represents a major architectural shift (unified queue \u2192 separate queues) that warrants detailed review.\n\n## Prompt for Collaborator",
        "prompt_for_other": "Please focus your review on:\n\n1. **Starvation Prevention**: Does the discrete promotion approach (low\u2192medium at 10min, medium\u2192high at 20min) provide adequate fairness compared to continuous dynamic scoring? Are there edge cases where tasks could wait longer than 30 minutes?\n\n2. **Atomic Claim Operation**: Does the Lua script approach for claiming tasks (removing from queue + updating task state) adequately prevent race conditions? Are there scenarios where two workers could claim the same task?\n\n3. **Health Monitoring**: Is heartbeat + task timeout truly sufficient? Are there worker failure modes that would be missed without active health probes?\n\n4. **Complexity Reduction**: Does this design sacrifice important capabilities for simplicity? What are the practical operational impacts of the removed features (optimistic locking, visibility timeouts, fine-grained states)?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system implements a **priority-based task scheduling system** with proven starvation prevention and pragmatic failure recovery. It uses a **worker-pull model** with three main layers:\n\n1. **API Layer**: REST endpoints for task lifecycle management\n2. **Scheduling Layer**: Multi-queue priority system with age-based promotion\n3. **Execution Layer**: Worker pool with health monitoring and graceful degradation\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels (high, medium, low)\n  - Maintain separate priority queues with periodic age-based promotion\n  - Route tasks to workers based on simple capability matching\n  - Orchestrate task lifecycle (submitted \u2192 queued \u2192 executing \u2192 terminal state)\n  - Handle task timeouts and retries\n\n### 2. Multi-Queue Priority System with Age-Based Promotion\n\n**Structure**: Three separate Redis sorted sets, scored by submission time\n```\nqueue:high    - High priority tasks (scored by created_at timestamp)\nqueue:medium  - Medium priority tasks (scored by created_at timestamp)\nqueue:low     - Low priority tasks (scored by created_at timestamp)\n```\n\n**Starvation Prevention**: Periodic promotion via background job\n```python\n# Runs every 60 seconds\ndef promote_aged_tasks():\n    now = time.time()\n    \n    # Promote low \u2192 medium if waiting > 10 minutes\n    aged_low = redis.zrangebyscore(\n        'queue:low', \n        0, \n        now - 600,  # 10 minutes\n        start=0, \n        num=100\n    )\n    for task_id in aged_low:\n        task = get_task(task_id)\n        task.effective_priority = 'medium'\n        redis.zrem('queue:low', task_id)\n        redis.zadd('queue:medium', {task_id: task.created_at})\n    \n    # Promote medium \u2192 high if waiting > 20 minutes\n    aged_medium = redis.zrangebyscore(\n        'queue:medium',\n        0,\n        now - 1200,  # 20 minutes\n        start=0,\n        num=50\n    )\n    for task_id in aged_medium:\n        task = get_task(task_id)\n        task.effective_priority = 'high'\n        redis.zrem('queue:medium', task_id)\n        redis.zadd('queue:high', {task_id: task.created_at})\n```\n\n**Guarantees**:\n- Low priority tasks promoted to medium after 10 minutes\n- Medium priority tasks promoted to high after 20 minutes\n- Maximum wait time: 30 minutes for any task\n- Within each queue: strict FIFO ordering\n\n**Rationale**: This approach is simpler to reason about, debug, and maintain than dynamic scoring. The discrete promotion thresholds are easy to explain to users (\"your low priority task will be treated as medium after 10 minutes\") and the behavior is predictable. The 60-second promotion interval is frequent enough for practical fairness while being cheap to execute.\n\n### 3. Worker Pool with Simple Health Monitoring\n\n**Architecture**: Fixed-size worker pool with capability tags\n- **Worker States**: `active`, `draining`, `dead`\n- **Health Checks**: \n  - **Heartbeat only**: Every 30 seconds, 3 missed heartbeats (90s) = worker dead\n  - **Task timeout**: Independent per-task timeout, triggers recovery on expiry\n- **Concurrency**: Per-worker capacity (default: 5 concurrent tasks)\n- **Graceful Shutdown**: `draining` state prevents new assignments\n\n**Rationale for Simplified Health Monitoring**: \n- Active health probes add complexity and create new failure modes (probe timeouts, probe service overload)\n- Task-level timeouts already detect hung workers effectively\n- Heartbeat + task timeout covers 99% of failure scenarios\n- Simpler system = fewer edge cases = more reliable\n\n### 4. Task State Machine\n\n**States**: `queued` \u2192 `executing` \u2192 [`completed` | `failed` | `cancelled` | `dead_letter`]\n\n**Simplified from 7 states to 5 states**: \n- Removed `submitted` (immediately becomes `queued`)\n- Removed `assigned` (worker atomically transitions `queued` \u2192 `executing` during poll)\n- Removed `initializing` worker state (workers register as `active`)\n\n**Rationale**: Each state adds complexity in recovery logic. The key insight is that if a worker has the task, the worker is responsible for it. We don't need fine-grained tracking of \"assigned but not started\" - the task timeout handles this case.\n\n### 5. Dead Letter Queue & Core Observability\n\n**DLQ**: Separate Redis sorted set `queue:dlq` for tasks exceeding retry limits\n- Score: timestamp when moved to DLQ\n- TTL: 30 days\n- Manual replay: Admin endpoint to requeue with reset retries\n\n**Observability**: \n- **Metrics**: Queue depth, task duration, worker utilization, error rates\n- **Logging**: Structured JSON logs for state transitions\n- **Tracing**: Trace ID propagated through task lifecycle\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"effective_priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\",\n    \"required_capabilities\": [\"gpu\"]\n  },\n  \"status\": \"queued|executing|completed|failed|cancelled|dead_letter\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker_id\": \"worker-id|null\",\n  \"result\": \"json-object|null\",\n  \"error\": {\n    \"message\": \"string\",\n    \"retriable\": true\n  },\n  \"timeout_seconds\": 300,\n  \"idempotency_key\": \"string|null\",\n  \"trace_id\": \"string\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"active|draining|dead\",\n  \"current_tasks\": [\"task-id-1\", \"task-id-2\"],\n  \"last_heartbeat\": \"timestamp\",\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"version\": \"string\"\n}\n```\n\n### Priority Configuration\n```json\n{\n  \"high\": {\n    \"max_retries\": 5,\n    \"timeout_seconds\": 300\n  },\n  \"medium\": {\n    \"max_retries\": 3,\n    \"timeout_seconds\": 600\n  },\n  \"low\": {\n    \"max_retries\": 2,\n    \"timeout_seconds\": 900\n  },\n  \"starvation_prevention\": {\n    \"low_to_medium_seconds\": 600,\n    \"medium_to_high_seconds\": 1200,\n    \"promotion_interval_seconds\": 60,\n    \"promotion_batch_size\": 100\n  }\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"required_capabilities\": [\"gpu\"],\n  \"timeout_seconds\": 600,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"queued\",\n  \"queue_position\": 5\n}\n```\n\n#### Task Status\n```\nGET /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"executing\",\n  \"priority\": \"high\",\n  \"effective_priority\": \"high\",\n  \"created_at\": \"...\",\n  \"assigned_worker_id\": \"worker-uuid\",\n  \"result\": null\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\"\n}\n```\n\n#### Queue Statistics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"queues\": {\n    \"high\": {\"depth\": 12, \"oldest_age_seconds\": 45},\n    \"medium\": {\"depth\": 45, \"oldest_age_seconds\": 320},\n    \"low\": {\"depth\": 123, \"oldest_age_seconds\": 580}\n  },\n  \"workers\": {\n    \"active\": 18,\n    \"draining\": 2,\n    \"dead\": 0,\n    \"total_capacity\": 100,\n    \"used_capacity\": 67\n  },\n  \"throughput\": {\n    \"completed_last_minute\": 120,\n    \"failed_last_minute\": 3\n  }\n}\n```\n\n### Worker Interface (Internal)\n\n#### Worker Registration\n```\nPOST /internal/workers/register\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"capacity\": 5\n}\nResponse: {\n  \"worker_id\": \"uuid\",\n  \"poll_interval_ms\": 1000\n}\n```\n\n#### Poll for Tasks\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"available_capacity\": 3\n}\nResponse: {\n  \"tasks\": [\n    {\n      \"task_id\": \"uuid\",\n      \"task_type\": \"process_video\",\n      \"parameters\": {...},\n      \"timeout_seconds\": 600,\n      \"trace_id\": \"...\"\n    }\n  ]\n}\n```\n\n**Poll Algorithm**:\n```python\ndef poll_tasks(worker_id, capabilities, available_capacity):\n    tasks = []\n    \n    # Try each queue in priority order\n    for queue_name in ['queue:high', 'queue:medium', 'queue:low']:\n        if len(tasks) >= available_capacity:\n            break\n        \n        # Get oldest tasks from this queue\n        candidate_ids = redis.zrange(queue_name, 0, available_capacity * 2)\n        \n        for task_id in candidate_ids:\n            if len(tasks) >= available_capacity:\n                break\n            \n            # Atomic claim: remove from queue and update task\n            success = try_claim_task(task_id, worker_id, queue_name)\n            if success:\n                task = get_task(task_id)\n                \n                # Check capabilities\n                if task.required_capabilities.issubset(capabilities):\n                    tasks.append(task)\n                else:\n                    # Return to queue if capabilities don't match\n                    return_task_to_queue(task_id, queue_name)\n    \n    return tasks\n\ndef try_claim_task(task_id, worker_id, queue_name):\n    # Atomic Lua script\n    result = redis.eval(\"\"\"\n        local task_id = ARGV[1]\n        local worker_id = ARGV[2]\n        local queue_name = ARGV[3]\n        \n        -- Remove from queue\n        local removed = redis.call('ZREM', queue_name, task_id)\n        if removed == 0 then\n            return 0  -- Already claimed\n        end\n        \n        -- Update task status\n        local task_key = 'tasks:' .. task_id\n        redis.call('HSET', task_key, 'status', 'executing')\n        redis.call('HSET', task_key, 'assigned_worker_id', worker_id)\n        redis.call('HSET', task_key, 'started_at', ARGV[4])\n        \n        return 1\n    \"\"\", 0, task_id, worker_id, queue_name, time.time())\n    \n    return result == 1\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...}\n}\n```\n\n#### Worker Heartbeat\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"active\"\n}\nResponse: {\n  \"acknowledged\": true\n}\n```\n\n## Error Handling & Recovery\n\n### Worker Failure Detection & Recovery\n\n**Detection**: Background job runs every 30 seconds\n```python\ndef detect_dead_workers():\n    now = time.time()\n    cutoff = now - 90  # 3 missed heartbeats\n    \n    all_workers = get_all_workers()\n    for worker in all_workers:\n        if worker.status == 'dead':\n            continue\n        \n        if worker.last_heartbeat < cutoff:\n            handle_worker_failure(worker)\n\ndef handle_worker_failure(worker):\n    # Mark worker as dead\n    worker.status = 'dead'\n    save_worker(worker)\n    \n    # Find all tasks assigned to this worker\n    tasks = find_tasks_by_worker(worker.worker_id)\n    \n    for task in tasks:\n        if task.status == 'executing':\n            # Requeue with incremented retry\n            requeue_task(task)\n```\n\n**Task Timeout Detection**: Separate background job runs every 30 seconds\n```python\ndef detect_timed_out_tasks():\n    now = time.time()\n    \n    # Find tasks in 'executing' state past their timeout\n    timed_out = query_tasks(\n        status='executing',\n        started_at__lt=now - max_task_timeout\n    )\n    \n    for task in timed_out:\n        # This handles hung workers AND legitimately long tasks\n        handle_task_timeout(task)\n```\n\n### Retry Strategy\n\n```python\ndef requeue_task(task):\n    task.retry_count += 1\n    \n    # Check retry limit\n    if task.retry_count > task.max_retries:\n        move_to_dlq(task, reason='max_retries_exceeded')\n        return\n    \n    # Reset state\n    task.status = 'queued'\n    task.assigned_worker_id = None\n    task.started_at = None\n    \n    # Increase timeout by 50% on retry (handle slow tasks)\n    task.timeout_seconds *= 1.5\n    \n    # Return to original priority queue (NOT effective_priority)\n    # This ensures retries don't skip ahead unfairly\n    queue_name = f'queue:{task.priority}'\n    redis.zadd(queue_name, {task.task_id: task.created_at})\n    \n    save_task(task)\n```\n\n### Idempotency\n\n```python\ndef submit_task(payload, idempotency_key=None):\n    if idempotency_key:\n        # Check if already exists (24-hour window)\n        cache_key = f'idempotent:{idempotency_key}'\n        existing_id = redis.get(cache_key)\n        \n        if existing_id:\n            return get_task(existing_id)\n    \n    # Create new task\n    task = create_task(payload)\n    \n    if idempotency_key:\n        # Store mapping with 24-hour TTL\n        redis.setex(cache_key, 86400, task.task_id)\n    \n    # Add to appropriate queue\n    queue_name = f'queue:{task.priority}'\n    redis.zadd(queue_name, {task.task_id: task.created_at})\n    \n    return task\n```\n\n## Monitoring & Observability\n\n### Key Metrics (Prometheus)\n\n**Queue Health**:\n- `queue_depth{priority=\"high|medium|low\"}` - Tasks waiting\n- `queue_age_seconds{priority,percentile=\"p50|p95|p99\"}` - Wait time distribution\n- `tasks_promoted_total{from,to}` - Starvation prevention activity\n\n**Task Lifecycle**:\n- `task_duration_seconds{priority,status}` - End-to-end time\n- `task_retries_total{priority,reason}` - Retry patterns\n- `task_state_total{status}` - Current task distribution\n\n**Worker Health**:\n- `workers_total{status}` - Worker pool state\n- `worker_capacity_used_ratio` - Utilization\n- `worker_heartbeat_failures_total` - Health issues\n\n**Error Rates**:\n- `tasks_failed_total{reason}` - Failure classification\n- `tasks_dlq_total` - Unrecoverable failures\n\n### Alerting Rules\n\n**Critical**:\n- No active workers for >2 minutes\n- High priority queue depth >1000\n- Task failure rate >25%\n- DLQ growth >10 tasks/minute\n\n**Warning**:\n- Worker utilization >90% for >10 minutes\n- Low priority tasks waiting >25 minutes (promotion not working)\n- Any queue depth >5000\n\n## Deployment Architecture\n\n**Single-Region (Recommended)**:\n- Redis Sentinel (3 nodes) for high availability\n- PostgreSQL for durable task history\n- Worker pool: 10-100 instances depending on load\n- All components in same VPC for <5ms latency\n\n**Scaling Characteristics**:\n- Queue operations: O(log N) where N = queue depth\n- Worker polling: O(1) - fixed batch size\n- Promotion job: O(P) where P = tasks to promote (bounded by batch_size)\n- Expected throughput: 1000+ tasks/second per Redis instance\n\n## Rationale\n\n### Why Three Separate Queues Instead of Unified Dynamic Scoring?\n\n**Simplicity wins**:\n1. **Easier to debug**: \"Task X is in the medium queue at position 47\" vs. \"Task X has score 99,234\"\n2. **Predictable behavior**: Users understand \"promoted after 10 minutes\" intuitively\n3. **Simpler code**: No complex score calculation, just FIFO within each queue\n4. **Cheaper operations**: ZRANGE with time score is faster than computed scores\n5. **Observable**: Can monitor each queue independently\n\n**Trade-off accepted**: Promotion happens in 60-second intervals rather than continuously. This is acceptable because:\n- 60 seconds is negligible compared to 10-minute promotion threshold\n- Reduces Redis load (promotion is a batch operation)\n- Makes testing and validation easier\n\n### Why Simplified State Machine?\n\nThe original design had 7 states: `submitted \u2192 queued \u2192 assigned \u2192 executing \u2192 completed|failed|cancelled|dead_letter`\n\n**Problems with `assigned` state**:\n- Adds recovery complexity: what if worker crashes after `assigned` but before `executing`?\n- Requires separate timeout monitoring for assignment phase\n- Creates ambiguity: who is responsible for the task?\n\n**Solution**: Workers atomically claim tasks during poll (`queued` \u2192 `executing`). If worker crashes before processing, task timeout handles recovery. This is simpler and handles all edge cases.\n\n### Why Heartbeat-Only Health Monitoring?\n\nThe collaborator's design had three-tier monitoring (heartbeat + active probes + task timeout). \n\n**Analysis**:\n- **Heartbeat**: Necessary and sufficient for worker liveness\n- **Active health probes**: Adds complexity, requires health probe infrastructure, creates new failure modes\n- **Task timeout**: Necessary and independent - catches hung workers AND legitimately slow tasks\n\n**Conclusion**: Heartbeat + task timeout covers all scenarios:\n- Worker crashes: Heartbeat fails\n- Worker hangs: Task timeout fires\n- Worker slow but progressing: Legitimate, no action needed\n- Network partition: Both heartbeat and task timeout will trigger\n\nAdding active probes doesn't add coverage but does add failure modes (probe service down, probe timeouts, etc.).\n\n### Why Fixed Worker Pool?\n\nSimpler than auto-scaling for initial version. Benefits:\n- Predictable capacity and cost\n- No scaling algorithm to tune\n- Easier to reason about system load\n- Can add auto-scaling later without changing core design\n\n### Why Redis + PostgreSQL?\n\n**Redis**: \n- Sub-millisecond queue operations\n- Atomic claim operations via Lua scripts\n- Native sorted set for FIFO ordering\n\n**PostgreSQL**:\n- Durable task history for analytics\n- Complex queries for monitoring\n- Reliable storage for task results\n\n**Separation of concerns**: Redis is ephemeral (can rebuild from PostgreSQL), PostgreSQL is source of truth.\n\n## What I Changed\n\n1. **Simplified Priority System**: Replaced complex dynamic scoring with discrete queues + periodic promotion. Easier to understand, debug, and maintain while still preventing starvation.\n\n2. **Reduced State Machine Complexity**: Eliminated `submitted` and `assigned` states. Workers atomically claim tasks from `queued` \u2192 `executing`.\n\n3. **Two-Tier Health Monitoring**: Removed active health probes. Heartbeat + task timeout covers all failure scenarios with less complexity.\n\n4. **Simplified Worker States**: Reduced from 5 states to 3 (`active`, `draining`, `dead`). Removed `initializing` and `busy` as they don't add actionable information.\n\n5. **Removed Optimistic Locking**: Used atomic Lua scripts for critical operations instead. Simpler and avoids retry loops.\n\n6. **Removed Visibility Timeouts**: Not needed with atomic claim operation. Worker either has the task or doesn't.\n\n7. **Fixed Worker Pool**: Removed auto-scaling complexity from initial design. Can add later if needed.\n\n8. **Streamlined Task Entity**: Removed fields that weren't used in core workflows (`estimated_duration_seconds`, `visibility_timeout_until`, user tracking).\n\n## What I Kept\n\n1. **Three-Layer Architecture**: API, Queue, Worker separation is clean and correct.\n\n2. **Worker-Pull Model**: Workers control their own load, providing natural backpressure.\n\n3. **Capability-Based Routing**: Essential for heterogeneous worker pools.\n\n4. **Idempotency Keys**: 24-hour Redis-based deduplication is industry standard.\n\n5. **Dead Letter Queue**: Necessary for handling unrecoverable failures.\n\n6. **Priority Levels**: High/Medium/Low is intuitive and sufficient.\n\n7. **Retry Strategy**: Exponential backoff with priority-based limits is appropriate.\n\n## Open Questions\n\n1. **Promotion Thresholds**: Are 10/20 minutes the right values? Should be configurable per deployment based on SLA requirements.\n\n2. **Task Payload Size**: Need to define max size (suggest 256KB). Larger payloads should use external storage with reference in task.\n\n3. **Long-Running Tasks**: Tasks running for hours may hold worker capacity. Consider separate \"batch job\" system or streaming progress updates.\n\n4. **Multi-Region**: Current design is single-region. Multi-region adds significant complexity around task ownership and queue consistency.\n\n5. **Worker Authentication**: How do workers authenticate to the system? Need to add auth tokens or mTLS.\n\n## Convergence Status\n**ITERATING** - This design is significantly simplified while maintaining core guarantees. However, it represents a major architectural shift (unified queue \u2192 separate queues) that warrants detailed review.\n\n## Prompt for Collaborator\nPROMPT_FOR_CRITIC:\nPlease focus your review on:\n\n1. **Starvation Prevention**: Does the discrete promotion approach (low\u2192medium at 10min, medium\u2192high at 20min) provide adequate fairness compared to continuous dynamic scoring? Are there edge cases where tasks could wait longer than 30 minutes?\n\n2. **Atomic Claim Operation**: Does the Lua script approach for claiming tasks (removing from queue + updating task state) adequately prevent race conditions? Are there scenarios where two workers could claim the same task?\n\n3. **Health Monitoring**: Is heartbeat + task timeout truly sufficient? Are there worker failure modes that would be missed without active health probes?\n\n4. **Complexity Reduction**: Does this design sacrifice important capabilities for simplicity? What are the practical operational impacts of the removed features (optimistic locking, visibility timeouts, fine-grained states)?"
      },
      "agent_b": {
        "content": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system implements a **priority-based task scheduling system** with guaranteed starvation prevention and comprehensive failure recovery. It uses a **worker-pull model** with four main layers:\n\n1. **API Layer**: REST endpoints for task lifecycle management\n2. **Scheduling Layer**: Unified priority queue with continuous fair scheduling\n3. **Execution Layer**: Worker pool with dual-mode health monitoring\n4. **Persistence Layer**: Redis for hot path, PostgreSQL for durability\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels (high, medium, low)\n  - Maintain unified priority queue with age-aware scoring\n  - Route tasks to workers based on capability matching\n  - Orchestrate task lifecycle (queued \u2192 assigned \u2192 executing \u2192 terminal state)\n  - Handle task timeouts, retries, and cancellations\n\n### 2. Unified Priority Queue with Continuous Fair Scheduling\n\n**Structure**: Single Redis sorted set with composite score\n```\nqueue:tasks - All queued tasks (scored by priority + age)\n```\n\n**Scoring Function**: Continuous, monotonically increasing\n```python\ndef calculate_task_score(task):\n    now = time.time()\n    age_seconds = now - task.created_at\n    \n    # Base priority scores (lower = higher priority)\n    priority_base = {\n        'high': 0,\n        'medium': 100_000,\n        'low': 200_000\n    }\n    \n    # Age component: subtract age to move older tasks forward\n    # 1 second of age = 1 point reduction in score\n    # This ensures older tasks gradually overtake newer ones\n    age_score = -min(age_seconds, 100_000)  # Cap at 100k to prevent score inversion\n    \n    return priority_base[task.priority] + age_score\n```\n\n**Starvation Prevention Guarantees**:\n- High priority tasks execute immediately unless queue is full\n- Medium priority tasks will overtake newer high priority tasks after ~27.8 hours\n- Low priority tasks will overtake newer medium priority tasks after ~27.8 hours\n- Maximum theoretical wait: ~55.6 hours for a low priority task (in practice much shorter)\n\n**Rationale**: This provides **continuous fairness** without discrete jumps. The scoring is:\n- **Predictable**: Score calculation is deterministic and testable\n- **Observable**: Can query task score at any time\n- **Self-balancing**: No background jobs needed for promotion\n- **Fair**: Older tasks naturally move forward without manual intervention\n- **Debuggable**: Score explains exact queue position\n\n**Why Unified Queue Is Better**:\n1. **No promotion lag**: Tasks move forward continuously, not in 60-second batches\n2. **Atomic priority**: Single ZRANGE operation gets next task, no queue-hopping logic\n3. **Simpler recovery**: One queue to restore, not three\n4. **Better observability**: One metric for queue depth, clearer system state\n5. **Flexible tuning**: Can adjust age weight without changing queue structure\n\n### 3. Enhanced Task State Machine\n\n**States**: `queued` \u2192 `assigned` \u2192 `executing` \u2192 [`completed` | `failed` | `cancelled` | `dead_letter`]\n\n**Why Keep `assigned` State**:\nThe `assigned` state is **critical for accurate failure recovery**:\n\n```python\n# Without 'assigned' state (architect's design):\n# Worker polls and atomically moves task from queued \u2192 executing\n# Problem: If worker crashes AFTER poll but BEFORE starting task,\n# only the task timeout (5-15 minutes) will detect this failure\n\n# With 'assigned' state:\nqueued \u2192 assigned (worker acknowledged receipt)\n       \u2193 \n       executing (worker started processing)\n\n# Recovery windows:\n# - assigned \u2192 executing: 30 seconds (fast recovery)\n# - executing \u2192 timeout: 5-15 minutes (normal timeout)\n```\n\n**Assignment Timeout Detection**: Background job runs every 10 seconds\n```python\ndef detect_stalled_assignments():\n    now = time.time()\n    assignment_timeout = 30  # seconds\n    \n    # Find tasks stuck in 'assigned' state\n    stalled = query_tasks(\n        status='assigned',\n        assigned_at__lt=now - assignment_timeout\n    )\n    \n    for task in stalled:\n        # Worker crashed after claiming but before executing\n        logger.warning(f\"Task {task.task_id} stalled in assignment, requeueing\")\n        requeue_task(task, reason='assignment_timeout')\n```\n\n**Real-World Impact**:\n- Worker pod killed during rolling deployment: Recovers in 30s instead of 5-15min\n- Worker OOM during task initialization: Recovers in 30s instead of 5-15min\n- Network partition during claim operation: Recovers in 30s instead of 5-15min\n\n**State Transitions**:\n```\nqueued \u2500\u2500[worker poll]\u2500\u2500> assigned \u2500\u2500[worker start]\u2500\u2500> executing\n   \u2191                          \u2193                              \u2193\n   \u2502                     [30s timeout]                  [task timeout]\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            [requeue with retry++]\n```\n\n### 4. Worker Pool with Dual-Mode Health Monitoring\n\n**Architecture**: Dynamic worker pool with capability tags\n- **Worker States**: `active`, `draining`, `unhealthy`, `dead`\n- **Dual-Mode Health Checks**:\n  - **Passive (Heartbeat)**: Every 30 seconds, 3 missed = `dead` (90s)\n  - **Active (Health Probe)**: Every 60 seconds, 2 failures = `unhealthy` (120s)\n- **Concurrency**: Per-worker capacity (default: 5 concurrent tasks)\n- **Graceful Shutdown**: `draining` state prevents new assignments\n\n**Why Dual-Mode Monitoring**:\n\n| Failure Mode | Heartbeat Only | + Health Probe | Benefit |\n|--------------|----------------|----------------|---------|\n| Worker crash | \u2705 Detects (90s) | \u2705 Detects (90s) | No change |\n| Worker hang (thread deadlock) | \u2705 Detects via task timeout (5-15min) | \u2705 Detects (120s) | **3-13min faster** |\n| Worker resource exhaustion (OOM imminent) | \u274c Misses until OOM crashes | \u2705 Detects (120s) | **Proactive** |\n| Network partition (worker isolated) | \u2705 Detects (90s) | \u2705 Detects (120s) | No change |\n| Dependency failure (DB connection pool exhausted) | \u274c Misses until task timeout | \u2705 Detects (120s) | **3-13min faster** |\n\n**Health Probe Implementation**:\n```python\n# Worker exposes /health endpoint\n@app.get(\"/health\")\ndef health_check():\n    checks = {\n        \"memory\": check_memory_usage(),      # < 90% used\n        \"connections\": check_db_connections(), # pool not exhausted\n        \"cpu\": check_cpu_usage(),            # < 95% over 30s\n        \"threads\": check_thread_status()     # no deadlocks detected\n    }\n    \n    if all(checks.values()):\n        return {\"status\": \"healthy\", \"checks\": checks}\n    else:\n        return {\"status\": \"unhealthy\", \"checks\": checks}, 503\n\n# Queue manager probes workers\ndef probe_worker_health(worker):\n    try:\n        response = http_client.get(\n            f\"http://{worker.host}/health\",\n            timeout=5\n        )\n        return response.status_code == 200\n    except (Timeout, ConnectionError):\n        return False\n\ndef monitor_worker_health():\n    \"\"\"Runs every 60 seconds\"\"\"\n    for worker in get_active_workers():\n        if not probe_worker_health(worker):\n            worker.health_failures += 1\n            \n            if worker.health_failures >= 2:\n                # Mark unhealthy, stop new assignments\n                worker.status = 'unhealthy'\n                logger.warning(f\"Worker {worker.id} marked unhealthy\")\n                \n                # Tasks already assigned continue (may recover)\n                # No immediate requeue - wait for task timeout\n        else:\n            worker.health_failures = 0  # Reset on success\n```\n\n**Rationale for Active Probes**:\n- **Catches pre-failure states**: Resource exhaustion before crash\n- **Faster failure detection**: 120s vs 5-15min for hung workers\n- **Operational visibility**: Health dashboard shows degraded workers\n- **Graceful degradation**: Stop new assignments while investigating\n- **Cost is minimal**: 60-second interval, simple HTTP check, < 10ms overhead per worker\n\n**Complexity Trade-off**:\n- **Added complexity**: ~100 lines of code, one background job, health endpoint\n- **Value delivered**: 3-13 minute faster recovery for common failure modes\n- **Mitigation**: Health probe failures don't requeue tasks (only stop new assignments), so false positives are non-disruptive\n\n### 5. Atomic Task Claiming with Visibility Timeout\n\n**Why Visibility Timeout Matters**:\n\nThe architect's design uses atomic claim (remove from queue + update status). This has a **critical race condition window**:\n\n```python\n# Architect's approach:\ndef try_claim_task(task_id, worker_id):\n    # Lua script: ZREM + HSET (atomic)\n    redis.eval(\"\"\"\n        redis.call('ZREM', 'queue:tasks', task_id)  -- Remove from queue\n        redis.call('HSET', 'tasks:' .. task_id, 'status', 'assigned')\n    \"\"\")\n    \n# Problem: What if this worker crashes here, before starting the task?\n# The task is NOT in the queue anymore, but also not executing.\n# Only the 5-15 minute task timeout will rescue it.\n```\n\n**Solution: Visibility Timeout Pattern**\n```python\ndef try_claim_task(task_id, worker_id):\n    now = time.time()\n    visibility_timeout = 30  # seconds\n    \n    # Lua script: Update score to future time (becomes invisible)\n    result = redis.eval(\"\"\"\n        local task_id = ARGV[1]\n        local worker_id = ARGV[2]\n        local invisible_until = ARGV[3]\n        \n        -- Check current score (if missing, already claimed)\n        local current_score = redis.call('ZSCORE', 'queue:tasks', task_id)\n        if not current_score then\n            return 0  -- Already claimed\n        end\n        \n        -- Update score to future time (makes it invisible to ZRANGEBYSCORE)\n        redis.call('ZADD', 'queue:tasks', invisible_until, task_id)\n        \n        -- Update task metadata\n        redis.call('HSET', 'tasks:' .. task_id, 'status', 'assigned')\n        redis.call('HSET', 'tasks:' .. task_id, 'assigned_worker_id', worker_id)\n        redis.call('HSET', 'tasks:' .. task_id, 'assigned_at', ARGV[4])\n        redis.call('HSET', 'tasks:' .. task_id, 'visibility_timeout_until', invisible_until)\n        \n        return 1\n    \"\"\", 0, task_id, worker_id, now + visibility_timeout, now)\n    \n    return result == 1\n\n# When worker starts task execution:\ndef start_task_execution(task_id):\n    # Remove from queue permanently\n    redis.zrem('queue:tasks', task_id)\n    redis.hset(f'tasks:{task_id}', 'status', 'executing')\n    redis.hset(f'tasks:{task_id}', 'started_at', time.time())\n```\n\n**Recovery with Visibility Timeout**:\n```python\ndef recover_invisible_tasks():\n    \"\"\"Runs every 10 seconds\"\"\"\n    now = time.time()\n    \n    # Find tasks with visibility timeout expired but still in queue\n    # (Worker crashed after claim but before starting execution)\n    expired_tasks = redis.zrangebyscore(\n        'queue:tasks',\n        0,  # Old score (before claim)\n        now,  # Current time\n        withscores=True\n    )\n    \n    for task_id, score in expired_tasks:\n        task = get_task(task_id)\n        \n        # Check if task is in 'assigned' state with expired visibility\n        if task.status == 'assigned' and task.visibility_timeout_until <= now:\n            # Worker crashed, requeue task\n            requeue_task(task, reason='visibility_timeout_expired')\n```\n\n**Benefits**:\n- **Fast recovery**: 30 seconds instead of 5-15 minutes\n- **Task never lost**: Stays in queue with modified score until explicitly removed\n- **Idempotent**: Multiple recovery attempts are safe\n- **Observable**: Can monitor visibility timeout expirations as a health metric\n\n**Comparison**:\n| Approach | Worker Crash After Claim | Recovery Time | Queue Consistency |\n|----------|--------------------------|---------------|-------------------|\n| Atomic Remove | Task missing from queue, stuck in `assigned` | 5-15 minutes (task timeout) | \u274c Queue doesn't reflect reality |\n| Visibility Timeout | Task in queue with future score | 30 seconds | \u2705 Queue is source of truth |\n\n### 6. Dead Letter Queue & Enhanced Observability\n\n**DLQ Structure**:\n```python\n# Redis sorted set\nqueue:dlq - Scored by timestamp when moved to DLQ\n\n# PostgreSQL table for analysis\nCREATE TABLE dead_letter_tasks (\n    task_id UUID PRIMARY KEY,\n    original_task JSONB,\n    failure_reason TEXT,\n    retry_history JSONB,\n    moved_to_dlq_at TIMESTAMP,\n    last_error TEXT\n);\n```\n\n**DLQ Analytics**:\n```python\n# Automated pattern detection\ndef analyze_dlq_patterns():\n    \"\"\"Runs hourly\"\"\"\n    recent_dlq = query_dlq_tasks(last_hours=24)\n    \n    # Group by failure reason\n    failure_counts = Counter(t.failure_reason for t in recent_dlq)\n    \n    # Alert if new failure pattern emerges\n    if failure_counts['dependency_timeout'] > 50:\n        alert('High dependency timeout rate in DLQ')\n    \n    # Identify tasks safe to retry\n    retriable = [t for t in recent_dlq if t.error.retriable]\n    if len(retriable) > 100:\n        alert(f'{len(retriable)} retriable tasks in DLQ, consider bulk replay')\n```\n\n**Observability Enhancements**:\n\n**Metrics** (Prometheus):\n```python\n# Queue health\nqueue_depth_total\nqueue_score_p50, queue_score_p95  # Track score distribution\ntask_age_seconds{priority, percentile}\n\n# Task lifecycle (with assigned state)\ntask_duration_seconds{priority, status}\ntask_state_transitions_total{from, to}  # Track state machine flow\ntask_claim_latency_seconds  # Time to claim after queued\n\n# Worker health (dual-mode)\nworkers_total{status}  # active, draining, unhealthy, dead\nworker_heartbeat_failures_total\nworker_health_probe_failures_total  # New\nworker_capacity_used_ratio\n\n# Assignment phase (new visibility into fast recovery)\ntask_assignment_timeouts_total  # Tasks stuck in 'assigned'\ntask_visibility_timeouts_total  # Visibility timeout recoveries\n\n# Error patterns\ntasks_failed_total{reason, priority}\ntasks_dlq_total{reason}\ntask_retries_total{attempt}\n```\n\n**Tracing**:\n```python\n# Distributed tracing with OpenTelemetry\nspan = tracer.start_span('task.lifecycle', {\n    'task.id': task_id,\n    'task.priority': priority,\n    'task.type': task_type\n})\n\n# Child spans for each state transition\nwith tracer.start_span('task.queued', parent=span):\n    add_to_queue(task)\n\nwith tracer.start_span('task.assigned', parent=span):\n    assign_to_worker(task, worker_id)\n\nwith tracer.start_span('task.executing', parent=span):\n    execute_task(task)\n```\n\n**Dashboard Panels**:\n1. **Queue Health**: Depth, age distribution, score histogram\n2. **Worker Pool**: Status distribution, capacity utilization, health trends\n3. **Task Flow**: State transition rates, assignment\u2192execution latency\n4. **Error Analysis**: Failure reasons, retry patterns, DLQ growth\n5. **Performance**: Task duration by priority, throughput, p99 latency\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\",\n    \"required_capabilities\": [\"gpu\"]\n  },\n  \"status\": \"queued|assigned|executing|completed|failed|cancelled|dead_letter\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"assigned_at\": \"timestamp|null\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker_id\": \"worker-id|null\",\n  \"visibility_timeout_until\": \"timestamp|null\",\n  \"queue_score\": \"float\",\n  \"result\": \"json-object|null\",\n  \"error\": {\n    \"message\": \"string\",\n    \"retriable\": true,\n    \"failure_reason\": \"string\"\n  },\n  \"timeout_seconds\": 300,\n  \"idempotency_key\": \"string|null\",\n  \"trace_id\": \"string\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"active|draining|unhealthy|dead\",\n  \"current_tasks\": [\"task-id-1\", \"task-id-2\"],\n  \"last_heartbeat\": \"timestamp\",\n  \"health_failures\": 0,\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"version\": \"string\",\n  \"host\": \"worker-pod-123.default.svc.cluster.local\"\n}\n```\n\n### Priority Configuration\n```json\n{\n  \"high\": {\n    \"max_retries\": 5,\n    \"timeout_seconds\": 300,\n    \"base_score\": 0\n  },\n  \"medium\": {\n    \"max_retries\": 3,\n    \"timeout_seconds\": 600,\n    \"base_score\": 100000\n  },\n  \"low\": {\n    \"max_retries\": 2,\n    \"timeout_seconds\": 900,\n    \"base_score\": 200000\n  },\n  \"starvation_prevention\": {\n    \"age_weight\": 1.0,\n    \"max_age_bonus\": 100000\n  },\n  \"recovery\": {\n    \"assignment_timeout_seconds\": 30,\n    \"visibility_timeout_seconds\": 30,\n    \"heartbeat_interval_seconds\": 30,\n    \"heartbeat_timeout_multiplier\": 3,\n    \"health_probe_interval_seconds\": 60,\n    \"health_probe_failures_threshold\": 2\n  }\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"required_capabilities\": [\"gpu\"],\n  \"timeout_seconds\": 600,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"queued\",\n  \"estimated_wait_seconds\": 12,\n  \"queue_score\": 145.3\n}\n```\n\n#### Task Status\n```\nGET /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"executing\",\n  \"priority\": \"high\",\n  \"created_at\": \"...\",\n  \"assigned_at\": \"...\",\n  \"started_at\": \"...\",\n  \"assigned_worker_id\": \"worker-uuid\",\n  \"current_state_duration_seconds\": 45,\n  \"result\": null\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\",\n  \"was_executing\": true,\n  \"worker_notified\": true\n}\n```\n\n#### Queue Statistics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"queue\": {\n    \"total_depth\": 180,\n    \"by_priority\": {\n      \"high\": 12,\n      \"medium\": 45,\n      \"low\": 123\n    },\n    \"oldest_age_seconds\": 580,\n    \"p50_age_seconds\": 120,\n    \"p95_age_seconds\": 420\n  },\n  \"workers\": {\n    \"active\": 18,\n    \"draining\": 2,\n    \"unhealthy\": 1,\n    \"dead\": 0,\n    \"total_capacity\": 100,\n    \"used_capacity\": 67\n  },\n  \"throughput\": {\n    \"completed_last_minute\": 120,\n    \"failed_last_minute\": 3,\n    \"avg_task_duration_seconds\": 8.3\n  },\n  \"recovery_stats\": {\n    \"assignment_timeouts_last_hour\": 2,\n    \"visibility_timeouts_last_hour\": 1,\n    \"health_probe_failures_last_hour\": 3\n  }\n}\n```\n\n### Worker Interface (Internal)\n\n#### Worker Registration\n```\nPOST /internal/workers/register\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"capacity\": 5,\n  \"host\": \"worker-pod-123.default.svc.cluster.local\"\n}\nResponse: {\n  \"worker_id\": \"uuid\",\n  \"poll_interval_ms\": 1000,\n  \"heartbeat_interval_ms\": 30000\n}\n```\n\n#### Poll for Tasks\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"available_capacity\": 3\n}\nResponse: {\n  \"tasks\": [\n    {\n      \"task_id\": \"uuid\",\n      \"task_type\": \"process_video\",\n      \"parameters\": {...},\n      \"timeout_seconds\": 600,\n      \"trace_id\": \"...\",\n      \"visibility_timeout_until\": \"timestamp\"\n    }\n  ]\n}\n```\n\n**Poll Algorithm with Unified Queue**:\n```python\ndef poll_tasks(worker_id, capabilities, available_capacity):\n    now = time.time()\n    tasks = []\n    \n    # Get tasks with lowest scores (highest priority + oldest)\n    # Only fetch tasks that are currently visible (score <= now)\n    candidate_ids = redis.zrangebyscore(\n        'queue:tasks',\n        '-inf',\n        now,  # Only visible tasks\n        start=0,\n        num=available_capacity * 3  # Oversubscribe for capability filtering\n    )\n    \n    for task_id in candidate_ids:\n        if len(tasks) >= available_capacity:\n            break\n        \n        # Try to claim task with visibility timeout\n        success = try_claim_task(task_id, worker_id)\n        if not success:\n            continue  # Another worker claimed it\n        \n        task = get_task(task_id)\n        \n        # Check capabilities\n        if task.required_capabilities.issubset(capabilities):\n            tasks.append(task)\n        else:\n            # Return to queue if capabilities don't match\n            release_task_claim(task_id)\n    \n    return tasks\n```\n\n#### Start Task Execution\n```\nPOST /internal/workers/{worker_id}/start\nBody: {\n  \"task_id\": \"uuid\"\n}\nResponse: {\n  \"acknowledged\": true\n}\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...},\n  \"execution_duration_seconds\": 45.2\n}\n```\n\n#### Worker Heartbeat\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"active\"\n}\nResponse: {\n  \"acknowledged\": true,\n  \"should_drain\": false\n}\n```\n\n## Error Handling & Recovery\n\n### Worker Failure Detection & Recovery\n\n**Heartbeat Monitoring** (runs every 30 seconds):\n```python\ndef detect_dead_workers():\n    now = time.time()\n    cutoff = now - 90  # 3 missed heartbeats\n    \n    all_workers = get_active_workers()\n    for worker in all_workers:\n        if worker.status == 'dead':\n            continue\n        \n        if worker.last_heartbeat < cutoff:\n            handle_worker_failure(worker, reason='heartbeat_timeout')\n\ndef handle_worker_failure(worker, reason):\n    worker.status = 'dead'\n    save_worker(worker)\n    \n    # Find all tasks assigned to this worker\n    assigned_tasks = find_tasks_by_worker(worker.worker_id, status='assigned')\n    executing_tasks = find_tasks_by_worker(worker.worker_id, status='executing')\n    \n    # Requeue assigned tasks immediately (fast recovery)\n    for task in assigned_tasks:\n        requeue_task(task, reason=f'worker_failure_{reason}')\n    \n    # Requeue executing tasks with retry\n    for task in executing_tasks:\n        requeue_task(task, reason=f'worker_failure_{reason}')\n```\n\n**Health Probe Monitoring** (runs every 60 seconds):\n```python\ndef monitor_worker_health():\n    for worker in get_active_workers():\n        if not probe_worker_health(worker):\n            worker.health_failures += 1\n            \n            if worker.health_failures >= 2:\n                # Mark unhealthy, stop new assignments\n                worker.status = 'unhealthy'\n                emit_metric('worker_marked_unhealthy', worker_id=worker.id)\n                \n                # DO NOT requeue tasks yet - wait for task timeout\n                # This handles transient health issues gracefully\n        else:\n            worker.health_failures = 0\n```\n\n**Assignment Timeout Detection** (runs every 10 seconds):\n```python\ndef detect_stalled_assignments():\n    now = time.time()\n    assignment_timeout = 30\n    \n    stalled = query_tasks(\n        status='assigned',\n        assigned_at__lt=now - assignment_timeout\n    )\n    \n    for task in stalled:\n        logger.warning(f\"Task {task.task_id} stalled in assignment\")\n        requeue_task(task, reason='assignment_timeout')\n```\n\n**Visibility Timeout Recovery** (runs every 10 seconds):\n```python\ndef recover_invisible_tasks():\n    now = time.time()\n    \n    # Find tasks with expired visibility timeout still in queue\n    expired_tasks = redis.zrangebyscore('queue:tasks', 0, now)\n    \n    for task_id in expired_tasks:\n        task = get_task(task_id)\n        \n        if task.status == 'assigned' and task.visibility_timeout_until <= now:\n            # Worker crashed after claim, before execution\n            requeue_task(task, reason='visibility_timeout_expired')\n```\n\n**Task Timeout Detection** (runs every 30 seconds):\n```python\ndef detect_timed_out_tasks():\n    now = time.time()\n    \n    # Find executing tasks past their timeout\n    timed_out = query_tasks(status='executing')\n    \n    for task in timed_out:\n        elapsed = now - task.started_at\n        if elapsed > task.timeout_seconds:\n            handle_task_timeout(task)\n```\n\n### Retry Strategy\n\n```python\ndef requeue_task(task, reason):\n    task.retry_count += 1\n    \n    if task.retry_count > task.max_retries:\n        move_to_dlq(task, reason=f'max_retries_exceeded_{reason}')\n        return\n    \n    # Reset task state\n    task.status = 'queued'\n    task.assigned_worker_id = None\n    task.assigned_at = None\n    task.started_at = None\n    task.visibility_timeout_until = None\n    \n    # Exponential timeout increase\n    task.timeout_seconds = min(\n        task.timeout_seconds * 1.5,\n        3600  # Max 1 hour\n    )\n    \n    # Recalculate score and add back to queue\n    score = calculate_task_score(task)\n    task.queue_score = score\n    redis.zadd('queue:tasks', {task.task_id: score})\n    \n    save_task(task)\n    emit_metric('task_requeued', reason=reason, retry_count=task.retry_count)\n```\n\n### Idempotency\n\n```python\ndef submit_task(payload, idempotency_key=None):\n    if idempotency_key:\n        cache_key = f'idempotent:{idempotency_key}'\n        existing_id = redis.get(cache_key)\n        \n        if existing_id:\n            task = get_task(existing_id)\n            # Return existing task if still active\n            if task.status not in ['completed', 'failed', 'cancelled']:\n                return task\n            # Allow resubmission if terminal state\n    \n    # Create new task\n    task = create_task(payload)\n    score = calculate_task_score(task)\n    task.queue_score = score\n    \n    # Add to queue\n    redis.zadd('queue:tasks', {task.task_id: score})\n    \n    if idempotency_key:\n        redis.setex(cache_key, 86400, task.task_id)\n    \n    save_task(task)\n    return task\n```\n\n## Monitoring & Observability\n\n### Key Metrics (Prometheus)\n\n**Queue Health**:\n- `queue_depth_total{priority}` - Tasks by priority\n- `queue_age_seconds{priority, percentile}` - Wait time distribution\n- `queue_score_distribution` - Histogram of task scores\n\n**Task Lifecycle with Assignment Phase**:\n- `task_duration_seconds{priority, status}` - Full lifecycle time\n- `task_assignment_duration_seconds` - Time in `assigned` state\n- `task_execution_duration_seconds` - Time in `executing` state\n- `task_state_transitions_total{from, to}` - State flow\n\n**Worker Health (Dual-Mode)**:\n- `workers_total{status}` - active/draining/unhealthy/dead\n- `worker_heartbeat_failures_total{worker_id}` - Passive monitoring\n- `worker_health_probe_failures_total{worker_id}` - Active monitoring\n- `worker_capacity_used_ratio` - Utilization\n\n**Recovery Metrics (Enhanced)**:\n- `task_assignment_timeouts_total` - Stuck in assigned (30s recovery)\n- `task_visibility_timeouts_total` - Visibility timeout recovery\n- `task_execution_timeouts_total` - Task timeout (5-15min recovery)\n- `worker_failures_total{reason}` - heartbeat vs health probe\n\n**Error Patterns**:\n- `tasks_failed_total{reason, priority}` - Classified failures\n- `tasks_dlq_total{reason}` - DLQ additions\n- `task_retries_total{attempt, priority}` - Retry distribution\n\n### Alerting Rules\n\n**Critical**:\n- No active workers for >2 minutes\n- Queue depth >1000 AND growing for >5 minutes\n- Task failure rate >25% for >5 minutes\n- DLQ growth >10 tasks/minute\n\n**Warning**:\n- Worker utilization >90% for >10 minutes\n- High priority tasks waiting >60 seconds\n- Assignment timeout rate >5/minute (workers crashing during claim)\n- Health probe failure rate >10/minute (workers degraded)\n- Any queue age p95 >10 minutes\n\n**Info**:\n- Visibility timeout recovery >1/minute (investigate worker stability)\n- Unhealthy workers >10% of pool\n\n## Deployment Architecture\n\n**Single-Region**:\n- Redis Sentinel (3 nodes) for HA\n- PostgreSQL (primary + replica) for durability\n- Worker pool: Auto-scaling 10-100 instances\n- Queue manager: 2-3 instances behind load balancer\n\n**Scaling Characteristics**:\n- Queue operations: O(log N) for single queue\n- Worker polling: O(K log N) where K = available_capacity\n- Score calculation: O(1)\n- Expected throughput: 1000+ tasks/second\n\n## What I Improved\n\n1. **Unified Priority Queue with Continuous Fairness**: Replaced separate queues with single queue + dynamic scoring. Provides continuous (not discrete) starvation prevention without background promotion jobs. Simpler recovery and better observability.\n\n2. **Restored `assigned` State**: Critical for fast failure recovery. Enables 30-second detection of worker crashes during task initialization (vs 5-15 minute task timeout). Real-world benefit for rolling deployments and OOM scenarios.\n\n3. **Added Visibility Timeout Pattern**: Prevents task loss during worker crashes. Task stays in queue with future score until explicitly removed. Enables fast, safe recovery with queue as source of truth.\n\n4. **Dual-Mode Health Monitoring**: Added active health probes alongside heartbeat. Detects hung workers (3-13min faster), resource exhaustion (proactive), and dependency failures. Minimal cost (~100 LOC, 60s interval) for significant operational value.\n\n5. **Enhanced Observability**: Added metrics for assignment phase, visibility timeout recovery, health probe failures, and state transitions. Enables root cause analysis and proactive intervention.\n\n6. **DLQ Analytics**: Automated pattern detection and retriable task identification. Turns DLQ from \"where tasks go to die\" into operational intelligence.\n\n## What I Kept\n\n1. **Worker-Pull Model**: Core architecture is sound.\n\n2. **Capability-Based Routing**: Essential for heterogeneous workers.\n\n3. **Three Priority Levels**: High/medium/low is intuitive.\n\n4. **Idempotency Keys**: 24-hour deduplication pattern works well.\n\n5. **Dead Letter Queue**: Necessary for unrecoverable failures.\n\n6. **Retry Strategy**: Exponential backoff with configurable limits.\n\n7. **Redis + PostgreSQL Split**: Hot path vs durability separation is correct.\n\n## Remaining Concerns\n\n1. **Score Calculation Cost**: The unified queue requires score recalculation on requeue. With high retry rates, this could become a bottleneck. Mitigation: score calculation is O(1) and Redis sorted set updates are O(log N).\n\n2. **Capability Filtering After Claim**: Workers claim tasks then check capabilities. If mismatch, task is released. This wastes claim operations when workers are highly specialized. Mitigation: capability tags could be added to queue keys (e.g., `queue:tasks:gpu`).\n\n3. **Health Probe Thundering Herd**: If 100 workers all expose health endpoints, probing creates 100 HTTP requests every 60 seconds from queue manager. Mitigation: acceptable at 100 workers (1.67 req/s), but consider batching or sampling at scale.\n\n4. **Visibility Timeout vs Queue Depth Metrics**: Tasks with future scores appear in queue but aren't \"really\" queued. Queue depth metrics need to filter `score <= now`. Mitigation: added in queue stats endpoint.\n\n5. **Long-Running Task Fairness**: Tasks running for hours hold worker capacity while queue grows. Consider separating short-task and long-task queues, or streaming progress updates.\n\n## Convergence Status\n**ITERATING** - I've made significant improvements around recovery speed and observability, but the unified queue vs. separate queues is an architectural choice that warrants discussion. The continuous scoring adds elegance but may add operational complexity.\n\n## Prompt for Collaborator",
        "prompt_for_other": "Please focus your response on:\n\n1. **Unified Queue Trade-offs**: Does the continuous scoring approach provide enough practical benefit over separate queues to justify the added complexity? Consider:\n   - Score calculation on every requeue\n   - Observability (single depth metric vs three separate metrics)\n   - Debugging (task score vs queue name + position)\n   - Performance at scale (single sorted set vs three)\n\n2. **Assignment State Value**: I've argued the `assigned` state enables 30-second recovery vs 5-15 minute task timeout. Do you see scenarios where this is critical, or is the added state complexity not worth the faster recovery? Consider:\n   - Kubernetes pod evictions during deployment\n   - Worker OOM during task initialization\n   - Operator \"kill random worker pod\" chaos testing\n\n3. **Visibility Timeout Necessity**: Is the visibility timeout pattern essential, or does your atomic ZREM + HSET approach adequately handle worker crashes? Compare:\n   - Task lost from queue: How do we recover if worker crashes after ZREM?\n   - Complexity cost: Visibility timeout adds ~50 LOC, is it worth it?\n\n4. **Health Probe Benefit**: Does active health monitoring provide enough value (3-13min faster hung worker detection) to justify the added complexity (health endpoint, probe job, false positive handling)?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Task Queue System with Priority Support\n\n## Architecture Overview\n\nThe system implements a **priority-based task scheduling system** with guaranteed starvation prevention and comprehensive failure recovery. It uses a **worker-pull model** with four main layers:\n\n1. **API Layer**: REST endpoints for task lifecycle management\n2. **Scheduling Layer**: Unified priority queue with continuous fair scheduling\n3. **Execution Layer**: Worker pool with dual-mode health monitoring\n4. **Persistence Layer**: Redis for hot path, PostgreSQL for durability\n\n## Core Components\n\n### 1. Task Queue Manager\n- **Responsibilities**: \n  - Accept incoming tasks with priority levels (high, medium, low)\n  - Maintain unified priority queue with age-aware scoring\n  - Route tasks to workers based on capability matching\n  - Orchestrate task lifecycle (queued \u2192 assigned \u2192 executing \u2192 terminal state)\n  - Handle task timeouts, retries, and cancellations\n\n### 2. Unified Priority Queue with Continuous Fair Scheduling\n\n**Structure**: Single Redis sorted set with composite score\n```\nqueue:tasks - All queued tasks (scored by priority + age)\n```\n\n**Scoring Function**: Continuous, monotonically increasing\n```python\ndef calculate_task_score(task):\n    now = time.time()\n    age_seconds = now - task.created_at\n    \n    # Base priority scores (lower = higher priority)\n    priority_base = {\n        'high': 0,\n        'medium': 100_000,\n        'low': 200_000\n    }\n    \n    # Age component: subtract age to move older tasks forward\n    # 1 second of age = 1 point reduction in score\n    # This ensures older tasks gradually overtake newer ones\n    age_score = -min(age_seconds, 100_000)  # Cap at 100k to prevent score inversion\n    \n    return priority_base[task.priority] + age_score\n```\n\n**Starvation Prevention Guarantees**:\n- High priority tasks execute immediately unless queue is full\n- Medium priority tasks will overtake newer high priority tasks after ~27.8 hours\n- Low priority tasks will overtake newer medium priority tasks after ~27.8 hours\n- Maximum theoretical wait: ~55.6 hours for a low priority task (in practice much shorter)\n\n**Rationale**: This provides **continuous fairness** without discrete jumps. The scoring is:\n- **Predictable**: Score calculation is deterministic and testable\n- **Observable**: Can query task score at any time\n- **Self-balancing**: No background jobs needed for promotion\n- **Fair**: Older tasks naturally move forward without manual intervention\n- **Debuggable**: Score explains exact queue position\n\n**Why Unified Queue Is Better**:\n1. **No promotion lag**: Tasks move forward continuously, not in 60-second batches\n2. **Atomic priority**: Single ZRANGE operation gets next task, no queue-hopping logic\n3. **Simpler recovery**: One queue to restore, not three\n4. **Better observability**: One metric for queue depth, clearer system state\n5. **Flexible tuning**: Can adjust age weight without changing queue structure\n\n### 3. Enhanced Task State Machine\n\n**States**: `queued` \u2192 `assigned` \u2192 `executing` \u2192 [`completed` | `failed` | `cancelled` | `dead_letter`]\n\n**Why Keep `assigned` State**:\nThe `assigned` state is **critical for accurate failure recovery**:\n\n```python\n# Without 'assigned' state (architect's design):\n# Worker polls and atomically moves task from queued \u2192 executing\n# Problem: If worker crashes AFTER poll but BEFORE starting task,\n# only the task timeout (5-15 minutes) will detect this failure\n\n# With 'assigned' state:\nqueued \u2192 assigned (worker acknowledged receipt)\n       \u2193 \n       executing (worker started processing)\n\n# Recovery windows:\n# - assigned \u2192 executing: 30 seconds (fast recovery)\n# - executing \u2192 timeout: 5-15 minutes (normal timeout)\n```\n\n**Assignment Timeout Detection**: Background job runs every 10 seconds\n```python\ndef detect_stalled_assignments():\n    now = time.time()\n    assignment_timeout = 30  # seconds\n    \n    # Find tasks stuck in 'assigned' state\n    stalled = query_tasks(\n        status='assigned',\n        assigned_at__lt=now - assignment_timeout\n    )\n    \n    for task in stalled:\n        # Worker crashed after claiming but before executing\n        logger.warning(f\"Task {task.task_id} stalled in assignment, requeueing\")\n        requeue_task(task, reason='assignment_timeout')\n```\n\n**Real-World Impact**:\n- Worker pod killed during rolling deployment: Recovers in 30s instead of 5-15min\n- Worker OOM during task initialization: Recovers in 30s instead of 5-15min\n- Network partition during claim operation: Recovers in 30s instead of 5-15min\n\n**State Transitions**:\n```\nqueued \u2500\u2500[worker poll]\u2500\u2500> assigned \u2500\u2500[worker start]\u2500\u2500> executing\n   \u2191                          \u2193                              \u2193\n   \u2502                     [30s timeout]                  [task timeout]\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                            [requeue with retry++]\n```\n\n### 4. Worker Pool with Dual-Mode Health Monitoring\n\n**Architecture**: Dynamic worker pool with capability tags\n- **Worker States**: `active`, `draining`, `unhealthy`, `dead`\n- **Dual-Mode Health Checks**:\n  - **Passive (Heartbeat)**: Every 30 seconds, 3 missed = `dead` (90s)\n  - **Active (Health Probe)**: Every 60 seconds, 2 failures = `unhealthy` (120s)\n- **Concurrency**: Per-worker capacity (default: 5 concurrent tasks)\n- **Graceful Shutdown**: `draining` state prevents new assignments\n\n**Why Dual-Mode Monitoring**:\n\n| Failure Mode | Heartbeat Only | + Health Probe | Benefit |\n|--------------|----------------|----------------|---------|\n| Worker crash | \u2705 Detects (90s) | \u2705 Detects (90s) | No change |\n| Worker hang (thread deadlock) | \u2705 Detects via task timeout (5-15min) | \u2705 Detects (120s) | **3-13min faster** |\n| Worker resource exhaustion (OOM imminent) | \u274c Misses until OOM crashes | \u2705 Detects (120s) | **Proactive** |\n| Network partition (worker isolated) | \u2705 Detects (90s) | \u2705 Detects (120s) | No change |\n| Dependency failure (DB connection pool exhausted) | \u274c Misses until task timeout | \u2705 Detects (120s) | **3-13min faster** |\n\n**Health Probe Implementation**:\n```python\n# Worker exposes /health endpoint\n@app.get(\"/health\")\ndef health_check():\n    checks = {\n        \"memory\": check_memory_usage(),      # < 90% used\n        \"connections\": check_db_connections(), # pool not exhausted\n        \"cpu\": check_cpu_usage(),            # < 95% over 30s\n        \"threads\": check_thread_status()     # no deadlocks detected\n    }\n    \n    if all(checks.values()):\n        return {\"status\": \"healthy\", \"checks\": checks}\n    else:\n        return {\"status\": \"unhealthy\", \"checks\": checks}, 503\n\n# Queue manager probes workers\ndef probe_worker_health(worker):\n    try:\n        response = http_client.get(\n            f\"http://{worker.host}/health\",\n            timeout=5\n        )\n        return response.status_code == 200\n    except (Timeout, ConnectionError):\n        return False\n\ndef monitor_worker_health():\n    \"\"\"Runs every 60 seconds\"\"\"\n    for worker in get_active_workers():\n        if not probe_worker_health(worker):\n            worker.health_failures += 1\n            \n            if worker.health_failures >= 2:\n                # Mark unhealthy, stop new assignments\n                worker.status = 'unhealthy'\n                logger.warning(f\"Worker {worker.id} marked unhealthy\")\n                \n                # Tasks already assigned continue (may recover)\n                # No immediate requeue - wait for task timeout\n        else:\n            worker.health_failures = 0  # Reset on success\n```\n\n**Rationale for Active Probes**:\n- **Catches pre-failure states**: Resource exhaustion before crash\n- **Faster failure detection**: 120s vs 5-15min for hung workers\n- **Operational visibility**: Health dashboard shows degraded workers\n- **Graceful degradation**: Stop new assignments while investigating\n- **Cost is minimal**: 60-second interval, simple HTTP check, < 10ms overhead per worker\n\n**Complexity Trade-off**:\n- **Added complexity**: ~100 lines of code, one background job, health endpoint\n- **Value delivered**: 3-13 minute faster recovery for common failure modes\n- **Mitigation**: Health probe failures don't requeue tasks (only stop new assignments), so false positives are non-disruptive\n\n### 5. Atomic Task Claiming with Visibility Timeout\n\n**Why Visibility Timeout Matters**:\n\nThe architect's design uses atomic claim (remove from queue + update status). This has a **critical race condition window**:\n\n```python\n# Architect's approach:\ndef try_claim_task(task_id, worker_id):\n    # Lua script: ZREM + HSET (atomic)\n    redis.eval(\"\"\"\n        redis.call('ZREM', 'queue:tasks', task_id)  -- Remove from queue\n        redis.call('HSET', 'tasks:' .. task_id, 'status', 'assigned')\n    \"\"\")\n    \n# Problem: What if this worker crashes here, before starting the task?\n# The task is NOT in the queue anymore, but also not executing.\n# Only the 5-15 minute task timeout will rescue it.\n```\n\n**Solution: Visibility Timeout Pattern**\n```python\ndef try_claim_task(task_id, worker_id):\n    now = time.time()\n    visibility_timeout = 30  # seconds\n    \n    # Lua script: Update score to future time (becomes invisible)\n    result = redis.eval(\"\"\"\n        local task_id = ARGV[1]\n        local worker_id = ARGV[2]\n        local invisible_until = ARGV[3]\n        \n        -- Check current score (if missing, already claimed)\n        local current_score = redis.call('ZSCORE', 'queue:tasks', task_id)\n        if not current_score then\n            return 0  -- Already claimed\n        end\n        \n        -- Update score to future time (makes it invisible to ZRANGEBYSCORE)\n        redis.call('ZADD', 'queue:tasks', invisible_until, task_id)\n        \n        -- Update task metadata\n        redis.call('HSET', 'tasks:' .. task_id, 'status', 'assigned')\n        redis.call('HSET', 'tasks:' .. task_id, 'assigned_worker_id', worker_id)\n        redis.call('HSET', 'tasks:' .. task_id, 'assigned_at', ARGV[4])\n        redis.call('HSET', 'tasks:' .. task_id, 'visibility_timeout_until', invisible_until)\n        \n        return 1\n    \"\"\", 0, task_id, worker_id, now + visibility_timeout, now)\n    \n    return result == 1\n\n# When worker starts task execution:\ndef start_task_execution(task_id):\n    # Remove from queue permanently\n    redis.zrem('queue:tasks', task_id)\n    redis.hset(f'tasks:{task_id}', 'status', 'executing')\n    redis.hset(f'tasks:{task_id}', 'started_at', time.time())\n```\n\n**Recovery with Visibility Timeout**:\n```python\ndef recover_invisible_tasks():\n    \"\"\"Runs every 10 seconds\"\"\"\n    now = time.time()\n    \n    # Find tasks with visibility timeout expired but still in queue\n    # (Worker crashed after claim but before starting execution)\n    expired_tasks = redis.zrangebyscore(\n        'queue:tasks',\n        0,  # Old score (before claim)\n        now,  # Current time\n        withscores=True\n    )\n    \n    for task_id, score in expired_tasks:\n        task = get_task(task_id)\n        \n        # Check if task is in 'assigned' state with expired visibility\n        if task.status == 'assigned' and task.visibility_timeout_until <= now:\n            # Worker crashed, requeue task\n            requeue_task(task, reason='visibility_timeout_expired')\n```\n\n**Benefits**:\n- **Fast recovery**: 30 seconds instead of 5-15 minutes\n- **Task never lost**: Stays in queue with modified score until explicitly removed\n- **Idempotent**: Multiple recovery attempts are safe\n- **Observable**: Can monitor visibility timeout expirations as a health metric\n\n**Comparison**:\n| Approach | Worker Crash After Claim | Recovery Time | Queue Consistency |\n|----------|--------------------------|---------------|-------------------|\n| Atomic Remove | Task missing from queue, stuck in `assigned` | 5-15 minutes (task timeout) | \u274c Queue doesn't reflect reality |\n| Visibility Timeout | Task in queue with future score | 30 seconds | \u2705 Queue is source of truth |\n\n### 6. Dead Letter Queue & Enhanced Observability\n\n**DLQ Structure**:\n```python\n# Redis sorted set\nqueue:dlq - Scored by timestamp when moved to DLQ\n\n# PostgreSQL table for analysis\nCREATE TABLE dead_letter_tasks (\n    task_id UUID PRIMARY KEY,\n    original_task JSONB,\n    failure_reason TEXT,\n    retry_history JSONB,\n    moved_to_dlq_at TIMESTAMP,\n    last_error TEXT\n);\n```\n\n**DLQ Analytics**:\n```python\n# Automated pattern detection\ndef analyze_dlq_patterns():\n    \"\"\"Runs hourly\"\"\"\n    recent_dlq = query_dlq_tasks(last_hours=24)\n    \n    # Group by failure reason\n    failure_counts = Counter(t.failure_reason for t in recent_dlq)\n    \n    # Alert if new failure pattern emerges\n    if failure_counts['dependency_timeout'] > 50:\n        alert('High dependency timeout rate in DLQ')\n    \n    # Identify tasks safe to retry\n    retriable = [t for t in recent_dlq if t.error.retriable]\n    if len(retriable) > 100:\n        alert(f'{len(retriable)} retriable tasks in DLQ, consider bulk replay')\n```\n\n**Observability Enhancements**:\n\n**Metrics** (Prometheus):\n```python\n# Queue health\nqueue_depth_total\nqueue_score_p50, queue_score_p95  # Track score distribution\ntask_age_seconds{priority, percentile}\n\n# Task lifecycle (with assigned state)\ntask_duration_seconds{priority, status}\ntask_state_transitions_total{from, to}  # Track state machine flow\ntask_claim_latency_seconds  # Time to claim after queued\n\n# Worker health (dual-mode)\nworkers_total{status}  # active, draining, unhealthy, dead\nworker_heartbeat_failures_total\nworker_health_probe_failures_total  # New\nworker_capacity_used_ratio\n\n# Assignment phase (new visibility into fast recovery)\ntask_assignment_timeouts_total  # Tasks stuck in 'assigned'\ntask_visibility_timeouts_total  # Visibility timeout recoveries\n\n# Error patterns\ntasks_failed_total{reason, priority}\ntasks_dlq_total{reason}\ntask_retries_total{attempt}\n```\n\n**Tracing**:\n```python\n# Distributed tracing with OpenTelemetry\nspan = tracer.start_span('task.lifecycle', {\n    'task.id': task_id,\n    'task.priority': priority,\n    'task.type': task_type\n})\n\n# Child spans for each state transition\nwith tracer.start_span('task.queued', parent=span):\n    add_to_queue(task)\n\nwith tracer.start_span('task.assigned', parent=span):\n    assign_to_worker(task, worker_id)\n\nwith tracer.start_span('task.executing', parent=span):\n    execute_task(task)\n```\n\n**Dashboard Panels**:\n1. **Queue Health**: Depth, age distribution, score histogram\n2. **Worker Pool**: Status distribution, capacity utilization, health trends\n3. **Task Flow**: State transition rates, assignment\u2192execution latency\n4. **Error Analysis**: Failure reasons, retry patterns, DLQ growth\n5. **Performance**: Task duration by priority, throughput, p99 latency\n\n## Data Models\n\n### Task Entity\n```json\n{\n  \"task_id\": \"uuid-v4\",\n  \"priority\": \"high|medium|low\",\n  \"payload\": {\n    \"task_type\": \"string\",\n    \"parameters\": \"json-object\",\n    \"required_capabilities\": [\"gpu\"]\n  },\n  \"status\": \"queued|assigned|executing|completed|failed|cancelled|dead_letter\",\n  \"retry_count\": 0,\n  \"max_retries\": 3,\n  \"created_at\": \"timestamp\",\n  \"assigned_at\": \"timestamp|null\",\n  \"started_at\": \"timestamp|null\",\n  \"completed_at\": \"timestamp|null\",\n  \"assigned_worker_id\": \"worker-id|null\",\n  \"visibility_timeout_until\": \"timestamp|null\",\n  \"queue_score\": \"float\",\n  \"result\": \"json-object|null\",\n  \"error\": {\n    \"message\": \"string\",\n    \"retriable\": true,\n    \"failure_reason\": \"string\"\n  },\n  \"timeout_seconds\": 300,\n  \"idempotency_key\": \"string|null\",\n  \"trace_id\": \"string\"\n}\n```\n\n### Worker Entity\n```json\n{\n  \"worker_id\": \"uuid-v4\",\n  \"status\": \"active|draining|unhealthy|dead\",\n  \"current_tasks\": [\"task-id-1\", \"task-id-2\"],\n  \"last_heartbeat\": \"timestamp\",\n  \"health_failures\": 0,\n  \"capacity\": 5,\n  \"current_load\": 2,\n  \"capabilities\": [\"gpu\", \"cpu-intensive\"],\n  \"version\": \"string\",\n  \"host\": \"worker-pod-123.default.svc.cluster.local\"\n}\n```\n\n### Priority Configuration\n```json\n{\n  \"high\": {\n    \"max_retries\": 5,\n    \"timeout_seconds\": 300,\n    \"base_score\": 0\n  },\n  \"medium\": {\n    \"max_retries\": 3,\n    \"timeout_seconds\": 600,\n    \"base_score\": 100000\n  },\n  \"low\": {\n    \"max_retries\": 2,\n    \"timeout_seconds\": 900,\n    \"base_score\": 200000\n  },\n  \"starvation_prevention\": {\n    \"age_weight\": 1.0,\n    \"max_age_bonus\": 100000\n  },\n  \"recovery\": {\n    \"assignment_timeout_seconds\": 30,\n    \"visibility_timeout_seconds\": 30,\n    \"heartbeat_interval_seconds\": 30,\n    \"heartbeat_timeout_multiplier\": 3,\n    \"health_probe_interval_seconds\": 60,\n    \"health_probe_failures_threshold\": 2\n  }\n}\n```\n\n## Interfaces\n\n### REST API Endpoints\n\n#### Task Submission\n```\nPOST /api/v1/tasks\nBody: {\n  \"task_type\": \"process_video\",\n  \"priority\": \"high\",\n  \"parameters\": {...},\n  \"required_capabilities\": [\"gpu\"],\n  \"timeout_seconds\": 600,\n  \"idempotency_key\": \"optional-key\"\n}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"queued\",\n  \"estimated_wait_seconds\": 12,\n  \"queue_score\": 145.3\n}\n```\n\n#### Task Status\n```\nGET /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"executing\",\n  \"priority\": \"high\",\n  \"created_at\": \"...\",\n  \"assigned_at\": \"...\",\n  \"started_at\": \"...\",\n  \"assigned_worker_id\": \"worker-uuid\",\n  \"current_state_duration_seconds\": 45,\n  \"result\": null\n}\n```\n\n#### Task Cancellation\n```\nDELETE /api/v1/tasks/{task_id}\nResponse: {\n  \"task_id\": \"uuid\",\n  \"status\": \"cancelled\",\n  \"was_executing\": true,\n  \"worker_notified\": true\n}\n```\n\n#### Queue Statistics\n```\nGET /api/v1/queue/stats\nResponse: {\n  \"queue\": {\n    \"total_depth\": 180,\n    \"by_priority\": {\n      \"high\": 12,\n      \"medium\": 45,\n      \"low\": 123\n    },\n    \"oldest_age_seconds\": 580,\n    \"p50_age_seconds\": 120,\n    \"p95_age_seconds\": 420\n  },\n  \"workers\": {\n    \"active\": 18,\n    \"draining\": 2,\n    \"unhealthy\": 1,\n    \"dead\": 0,\n    \"total_capacity\": 100,\n    \"used_capacity\": 67\n  },\n  \"throughput\": {\n    \"completed_last_minute\": 120,\n    \"failed_last_minute\": 3,\n    \"avg_task_duration_seconds\": 8.3\n  },\n  \"recovery_stats\": {\n    \"assignment_timeouts_last_hour\": 2,\n    \"visibility_timeouts_last_hour\": 1,\n    \"health_probe_failures_last_hour\": 3\n  }\n}\n```\n\n### Worker Interface (Internal)\n\n#### Worker Registration\n```\nPOST /internal/workers/register\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"capacity\": 5,\n  \"host\": \"worker-pod-123.default.svc.cluster.local\"\n}\nResponse: {\n  \"worker_id\": \"uuid\",\n  \"poll_interval_ms\": 1000,\n  \"heartbeat_interval_ms\": 30000\n}\n```\n\n#### Poll for Tasks\n```\nPOST /internal/workers/{worker_id}/poll\nBody: {\n  \"capabilities\": [\"gpu\"],\n  \"available_capacity\": 3\n}\nResponse: {\n  \"tasks\": [\n    {\n      \"task_id\": \"uuid\",\n      \"task_type\": \"process_video\",\n      \"parameters\": {...},\n      \"timeout_seconds\": 600,\n      \"trace_id\": \"...\",\n      \"visibility_timeout_until\": \"timestamp\"\n    }\n  ]\n}\n```\n\n**Poll Algorithm with Unified Queue**:\n```python\ndef poll_tasks(worker_id, capabilities, available_capacity):\n    now = time.time()\n    tasks = []\n    \n    # Get tasks with lowest scores (highest priority + oldest)\n    # Only fetch tasks that are currently visible (score <= now)\n    candidate_ids = redis.zrangebyscore(\n        'queue:tasks',\n        '-inf',\n        now,  # Only visible tasks\n        start=0,\n        num=available_capacity * 3  # Oversubscribe for capability filtering\n    )\n    \n    for task_id in candidate_ids:\n        if len(tasks) >= available_capacity:\n            break\n        \n        # Try to claim task with visibility timeout\n        success = try_claim_task(task_id, worker_id)\n        if not success:\n            continue  # Another worker claimed it\n        \n        task = get_task(task_id)\n        \n        # Check capabilities\n        if task.required_capabilities.issubset(capabilities):\n            tasks.append(task)\n        else:\n            # Return to queue if capabilities don't match\n            release_task_claim(task_id)\n    \n    return tasks\n```\n\n#### Start Task Execution\n```\nPOST /internal/workers/{worker_id}/start\nBody: {\n  \"task_id\": \"uuid\"\n}\nResponse: {\n  \"acknowledged\": true\n}\n```\n\n#### Report Task Result\n```\nPOST /internal/workers/{worker_id}/result\nBody: {\n  \"task_id\": \"uuid\",\n  \"status\": \"completed\",\n  \"result\": {...},\n  \"execution_duration_seconds\": 45.2\n}\n```\n\n#### Worker Heartbeat\n```\nPOST /internal/workers/{worker_id}/heartbeat\nBody: {\n  \"current_load\": 3,\n  \"status\": \"active\"\n}\nResponse: {\n  \"acknowledged\": true,\n  \"should_drain\": false\n}\n```\n\n## Error Handling & Recovery\n\n### Worker Failure Detection & Recovery\n\n**Heartbeat Monitoring** (runs every 30 seconds):\n```python\ndef detect_dead_workers():\n    now = time.time()\n    cutoff = now - 90  # 3 missed heartbeats\n    \n    all_workers = get_active_workers()\n    for worker in all_workers:\n        if worker.status == 'dead':\n            continue\n        \n        if worker.last_heartbeat < cutoff:\n            handle_worker_failure(worker, reason='heartbeat_timeout')\n\ndef handle_worker_failure(worker, reason):\n    worker.status = 'dead'\n    save_worker(worker)\n    \n    # Find all tasks assigned to this worker\n    assigned_tasks = find_tasks_by_worker(worker.worker_id, status='assigned')\n    executing_tasks = find_tasks_by_worker(worker.worker_id, status='executing')\n    \n    # Requeue assigned tasks immediately (fast recovery)\n    for task in assigned_tasks:\n        requeue_task(task, reason=f'worker_failure_{reason}')\n    \n    # Requeue executing tasks with retry\n    for task in executing_tasks:\n        requeue_task(task, reason=f'worker_failure_{reason}')\n```\n\n**Health Probe Monitoring** (runs every 60 seconds):\n```python\ndef monitor_worker_health():\n    for worker in get_active_workers():\n        if not probe_worker_health(worker):\n            worker.health_failures += 1\n            \n            if worker.health_failures >= 2:\n                # Mark unhealthy, stop new assignments\n                worker.status = 'unhealthy'\n                emit_metric('worker_marked_unhealthy', worker_id=worker.id)\n                \n                # DO NOT requeue tasks yet - wait for task timeout\n                # This handles transient health issues gracefully\n        else:\n            worker.health_failures = 0\n```\n\n**Assignment Timeout Detection** (runs every 10 seconds):\n```python\ndef detect_stalled_assignments():\n    now = time.time()\n    assignment_timeout = 30\n    \n    stalled = query_tasks(\n        status='assigned',\n        assigned_at__lt=now - assignment_timeout\n    )\n    \n    for task in stalled:\n        logger.warning(f\"Task {task.task_id} stalled in assignment\")\n        requeue_task(task, reason='assignment_timeout')\n```\n\n**Visibility Timeout Recovery** (runs every 10 seconds):\n```python\ndef recover_invisible_tasks():\n    now = time.time()\n    \n    # Find tasks with expired visibility timeout still in queue\n    expired_tasks = redis.zrangebyscore('queue:tasks', 0, now)\n    \n    for task_id in expired_tasks:\n        task = get_task(task_id)\n        \n        if task.status == 'assigned' and task.visibility_timeout_until <= now:\n            # Worker crashed after claim, before execution\n            requeue_task(task, reason='visibility_timeout_expired')\n```\n\n**Task Timeout Detection** (runs every 30 seconds):\n```python\ndef detect_timed_out_tasks():\n    now = time.time()\n    \n    # Find executing tasks past their timeout\n    timed_out = query_tasks(status='executing')\n    \n    for task in timed_out:\n        elapsed = now - task.started_at\n        if elapsed > task.timeout_seconds:\n            handle_task_timeout(task)\n```\n\n### Retry Strategy\n\n```python\ndef requeue_task(task, reason):\n    task.retry_count += 1\n    \n    if task.retry_count > task.max_retries:\n        move_to_dlq(task, reason=f'max_retries_exceeded_{reason}')\n        return\n    \n    # Reset task state\n    task.status = 'queued'\n    task.assigned_worker_id = None\n    task.assigned_at = None\n    task.started_at = None\n    task.visibility_timeout_until = None\n    \n    # Exponential timeout increase\n    task.timeout_seconds = min(\n        task.timeout_seconds * 1.5,\n        3600  # Max 1 hour\n    )\n    \n    # Recalculate score and add back to queue\n    score = calculate_task_score(task)\n    task.queue_score = score\n    redis.zadd('queue:tasks', {task.task_id: score})\n    \n    save_task(task)\n    emit_metric('task_requeued', reason=reason, retry_count=task.retry_count)\n```\n\n### Idempotency\n\n```python\ndef submit_task(payload, idempotency_key=None):\n    if idempotency_key:\n        cache_key = f'idempotent:{idempotency_key}'\n        existing_id = redis.get(cache_key)\n        \n        if existing_id:\n            task = get_task(existing_id)\n            # Return existing task if still active\n            if task.status not in ['completed', 'failed', 'cancelled']:\n                return task\n            # Allow resubmission if terminal state\n    \n    # Create new task\n    task = create_task(payload)\n    score = calculate_task_score(task)\n    task.queue_score = score\n    \n    # Add to queue\n    redis.zadd('queue:tasks', {task.task_id: score})\n    \n    if idempotency_key:\n        redis.setex(cache_key, 86400, task.task_id)\n    \n    save_task(task)\n    return task\n```\n\n## Monitoring & Observability\n\n### Key Metrics (Prometheus)\n\n**Queue Health**:\n- `queue_depth_total{priority}` - Tasks by priority\n- `queue_age_seconds{priority, percentile}` - Wait time distribution\n- `queue_score_distribution` - Histogram of task scores\n\n**Task Lifecycle with Assignment Phase**:\n- `task_duration_seconds{priority, status}` - Full lifecycle time\n- `task_assignment_duration_seconds` - Time in `assigned` state\n- `task_execution_duration_seconds` - Time in `executing` state\n- `task_state_transitions_total{from, to}` - State flow\n\n**Worker Health (Dual-Mode)**:\n- `workers_total{status}` - active/draining/unhealthy/dead\n- `worker_heartbeat_failures_total{worker_id}` - Passive monitoring\n- `worker_health_probe_failures_total{worker_id}` - Active monitoring\n- `worker_capacity_used_ratio` - Utilization\n\n**Recovery Metrics (Enhanced)**:\n- `task_assignment_timeouts_total` - Stuck in assigned (30s recovery)\n- `task_visibility_timeouts_total` - Visibility timeout recovery\n- `task_execution_timeouts_total` - Task timeout (5-15min recovery)\n- `worker_failures_total{reason}` - heartbeat vs health probe\n\n**Error Patterns**:\n- `tasks_failed_total{reason, priority}` - Classified failures\n- `tasks_dlq_total{reason}` - DLQ additions\n- `task_retries_total{attempt, priority}` - Retry distribution\n\n### Alerting Rules\n\n**Critical**:\n- No active workers for >2 minutes\n- Queue depth >1000 AND growing for >5 minutes\n- Task failure rate >25% for >5 minutes\n- DLQ growth >10 tasks/minute\n\n**Warning**:\n- Worker utilization >90% for >10 minutes\n- High priority tasks waiting >60 seconds\n- Assignment timeout rate >5/minute (workers crashing during claim)\n- Health probe failure rate >10/minute (workers degraded)\n- Any queue age p95 >10 minutes\n\n**Info**:\n- Visibility timeout recovery >1/minute (investigate worker stability)\n- Unhealthy workers >10% of pool\n\n## Deployment Architecture\n\n**Single-Region**:\n- Redis Sentinel (3 nodes) for HA\n- PostgreSQL (primary + replica) for durability\n- Worker pool: Auto-scaling 10-100 instances\n- Queue manager: 2-3 instances behind load balancer\n\n**Scaling Characteristics**:\n- Queue operations: O(log N) for single queue\n- Worker polling: O(K log N) where K = available_capacity\n- Score calculation: O(1)\n- Expected throughput: 1000+ tasks/second\n\n## What I Improved\n\n1. **Unified Priority Queue with Continuous Fairness**: Replaced separate queues with single queue + dynamic scoring. Provides continuous (not discrete) starvation prevention without background promotion jobs. Simpler recovery and better observability.\n\n2. **Restored `assigned` State**: Critical for fast failure recovery. Enables 30-second detection of worker crashes during task initialization (vs 5-15 minute task timeout). Real-world benefit for rolling deployments and OOM scenarios.\n\n3. **Added Visibility Timeout Pattern**: Prevents task loss during worker crashes. Task stays in queue with future score until explicitly removed. Enables fast, safe recovery with queue as source of truth.\n\n4. **Dual-Mode Health Monitoring**: Added active health probes alongside heartbeat. Detects hung workers (3-13min faster), resource exhaustion (proactive), and dependency failures. Minimal cost (~100 LOC, 60s interval) for significant operational value.\n\n5. **Enhanced Observability**: Added metrics for assignment phase, visibility timeout recovery, health probe failures, and state transitions. Enables root cause analysis and proactive intervention.\n\n6. **DLQ Analytics**: Automated pattern detection and retriable task identification. Turns DLQ from \"where tasks go to die\" into operational intelligence.\n\n## What I Kept\n\n1. **Worker-Pull Model**: Core architecture is sound.\n\n2. **Capability-Based Routing**: Essential for heterogeneous workers.\n\n3. **Three Priority Levels**: High/medium/low is intuitive.\n\n4. **Idempotency Keys**: 24-hour deduplication pattern works well.\n\n5. **Dead Letter Queue**: Necessary for unrecoverable failures.\n\n6. **Retry Strategy**: Exponential backoff with configurable limits.\n\n7. **Redis + PostgreSQL Split**: Hot path vs durability separation is correct.\n\n## Remaining Concerns\n\n1. **Score Calculation Cost**: The unified queue requires score recalculation on requeue. With high retry rates, this could become a bottleneck. Mitigation: score calculation is O(1) and Redis sorted set updates are O(log N).\n\n2. **Capability Filtering After Claim**: Workers claim tasks then check capabilities. If mismatch, task is released. This wastes claim operations when workers are highly specialized. Mitigation: capability tags could be added to queue keys (e.g., `queue:tasks:gpu`).\n\n3. **Health Probe Thundering Herd**: If 100 workers all expose health endpoints, probing creates 100 HTTP requests every 60 seconds from queue manager. Mitigation: acceptable at 100 workers (1.67 req/s), but consider batching or sampling at scale.\n\n4. **Visibility Timeout vs Queue Depth Metrics**: Tasks with future scores appear in queue but aren't \"really\" queued. Queue depth metrics need to filter `score <= now`. Mitigation: added in queue stats endpoint.\n\n5. **Long-Running Task Fairness**: Tasks running for hours hold worker capacity while queue grows. Consider separating short-task and long-task queues, or streaming progress updates.\n\n## Convergence Status\n**ITERATING** - I've made significant improvements around recovery speed and observability, but the unified queue vs. separate queues is an architectural choice that warrants discussion. The continuous scoring adds elegance but may add operational complexity.\n\n## Prompt for Collaborator\nPROMPT_FOR_ARCHITECT:\n\nPlease focus your response on:\n\n1. **Unified Queue Trade-offs**: Does the continuous scoring approach provide enough practical benefit over separate queues to justify the added complexity? Consider:\n   - Score calculation on every requeue\n   - Observability (single depth metric vs three separate metrics)\n   - Debugging (task score vs queue name + position)\n   - Performance at scale (single sorted set vs three)\n\n2. **Assignment State Value**: I've argued the `assigned` state enables 30-second recovery vs 5-15 minute task timeout. Do you see scenarios where this is critical, or is the added state complexity not worth the faster recovery? Consider:\n   - Kubernetes pod evictions during deployment\n   - Worker OOM during task initialization\n   - Operator \"kill random worker pod\" chaos testing\n\n3. **Visibility Timeout Necessity**: Is the visibility timeout pattern essential, or does your atomic ZREM + HSET approach adequately handle worker crashes? Compare:\n   - Task lost from queue: How do we recover if worker crashes after ZREM?\n   - Complexity cost: Visibility timeout adds ~50 LOC, is it worth it?\n\n4. **Health Probe Benefit**: Does active health monitoring provide enough value (3-13min faster hung worker detection) to justify the added complexity (health endpoint, probe job, false positive handling)?"
      }
    }
  ],
  "final_design": "",
  "status": "debating",
  "started_at": "2026-01-17T09:20:59.057362",
  "ended_at": "",
  "max_rounds": 2,
  "working_dir": ".",
  "current_round": 2,
  "round_times": [
    147.76741194725037,
    239.99541997909546
  ]
}