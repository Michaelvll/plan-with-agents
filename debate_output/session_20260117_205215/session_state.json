{
  "initial_prompt": "Design a visual AI agent workflow builder\n\nRequirements:\n- Drag-and-drop nodes for different agent types (researcher, coder, reviewer)\n- Connect nodes to show data flow between agents\n- Configure each node (model, temperature, system prompt)\n- Show real-time execution status and logs\n- Visualize agent communication/handoffs\n- Save and load workflow templates\n- React with a canvas library",
  "rounds": [
    {
      "round_number": 1,
      "agent_a": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Complete Design\n\n## Architecture Overview\n\n### High-Level Architecture\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502     \u2502\n\u2502  \u2502   Layer      \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           Workflow State Management (Zustand)               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Execution  \u2502  \u2502  Template    \u2502  \u2502   Validation \u2502     \u2502\n\u2502  \u2502   Engine     \u2502  \u2502  Manager     \u2502  \u2502   Service    \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas Library**: React Flow (react-flow-renderer) - mature, performant, extensible\n- **State Management**: Zustand - lightweight, no boilerplate\n- **UI Components**: Radix UI + Tailwind CSS - accessible, customizable\n- **Execution**: Web Workers for non-blocking agent execution\n- **Storage**: IndexedDB for templates, LocalStorage for recent workflows\n\n## Core Components\n\n### 1. Canvas Component (WorkflowCanvas.tsx)\n```typescript\ninterface WorkflowCanvasProps {\n  workflowId: string;\n  readonly?: boolean;\n}\n\nconst WorkflowCanvas: React.FC<WorkflowCanvasProps> = ({ \n  workflowId, \n  readonly = false \n}) => {\n  const { nodes, edges, onNodesChange, onEdgesChange, onConnect } = \n    useWorkflowStore();\n  \n  return (\n    <ReactFlow\n      nodes={nodes}\n      edges={edges}\n      onNodesChange={onNodesChange}\n      onEdgesChange={onEdgesChange}\n      onConnect={onConnect}\n      nodeTypes={customNodeTypes}\n      edgeTypes={customEdgeTypes}\n      fitView\n      minZoom={0.2}\n      maxZoom={2}\n    >\n      <Background />\n      <Controls />\n      <MiniMap />\n      <ExecutionOverlay />\n    </ReactFlow>\n  );\n};\n```\n\n### 2. Agent Node Component (AgentNode.tsx)\n```typescript\ninterface AgentNodeData {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator';\n  label: string;\n  config: AgentConfig;\n  status: ExecutionStatus;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n}\n\ninterface AgentConfig {\n  model: string; // 'gpt-4', 'claude-3-opus', etc.\n  temperature: number;\n  systemPrompt: string;\n  maxTokens?: number;\n  timeout?: number;\n  retryConfig?: RetryConfig;\n}\n\nconst AgentNode: React.FC<NodeProps<AgentNodeData>> = ({ data, selected }) => {\n  const getStatusColor = () => {\n    switch (data.status) {\n      case 'idle': return 'gray';\n      case 'running': return 'blue';\n      case 'success': return 'green';\n      case 'error': return 'red';\n      case 'waiting': return 'yellow';\n    }\n  };\n\n  return (\n    <div className={`agent-node ${selected ? 'selected' : ''}`}>\n      <Handle type=\"target\" position={Position.Left} />\n      \n      <div className=\"node-header\" style={{ borderColor: getStatusColor() }}>\n        <AgentIcon type={data.type} />\n        <span>{data.label}</span>\n        <StatusBadge status={data.status} />\n      </div>\n      \n      <div className=\"node-body\">\n        <div className=\"config-summary\">\n          <span className=\"model-badge\">{data.config.model}</span>\n          <span className=\"temp-badge\">T: {data.config.temperature}</span>\n        </div>\n        \n        {data.status === 'running' && (\n          <ProgressBar metrics={data.metrics} />\n        )}\n        \n        {data.logs.length > 0 && (\n          <LogPreview logs={data.logs.slice(-3)} />\n        )}\n      </div>\n      \n      <Handle type=\"source\" position={Position.Right} />\n    </div>\n  );\n};\n```\n\n### 3. Connection Edge Component (DataFlowEdge.tsx)\n```typescript\ninterface EdgeData {\n  id: string;\n  dataType: 'text' | 'json' | 'code' | 'artifacts';\n  transformations?: Transformation[];\n  status: 'idle' | 'active' | 'complete';\n  dataPreview?: string;\n}\n\nconst DataFlowEdge: React.FC<EdgeProps<EdgeData>> = ({ \n  id, \n  source, \n  target, \n  data,\n  style \n}) => {\n  const [edgePath, labelX, labelY] = getBezierPath({\n    sourceX, sourceY, targetX, targetY\n  });\n\n  return (\n    <>\n      <BaseEdge \n        path={edgePath} \n        style={{ \n          ...style, \n          stroke: data.status === 'active' ? '#3b82f6' : '#6b7280',\n          strokeWidth: data.status === 'active' ? 3 : 2,\n          animation: data.status === 'active' ? 'dash 1s linear infinite' : 'none'\n        }} \n      />\n      \n      <EdgeLabelRenderer>\n        <div\n          style={{ \n            position: 'absolute', \n            transform: `translate(-50%, -50%) translate(${labelX}px,${labelY}px)` \n          }}\n        >\n          <DataTypeIndicator type={data.dataType} />\n          {data.dataPreview && (\n            <DataPreviewTooltip content={data.dataPreview} />\n          )}\n        </div>\n      </EdgeLabelRenderer>\n    </>\n  );\n};\n```\n\n### 4. Properties Panel (PropertiesPanel.tsx)\n```typescript\nconst PropertiesPanel: React.FC = () => {\n  const { selectedNode, updateNodeConfig } = useWorkflowStore();\n  \n  if (!selectedNode) {\n    return <EmptyState message=\"Select a node to configure\" />;\n  }\n\n  return (\n    <div className=\"properties-panel\">\n      <Tabs defaultValue=\"config\">\n        <TabsList>\n          <TabsTrigger value=\"config\">Configuration</TabsTrigger>\n          <TabsTrigger value=\"prompt\">System Prompt</TabsTrigger>\n          <TabsTrigger value=\"advanced\">Advanced</TabsTrigger>\n          <TabsTrigger value=\"logs\">Logs</TabsTrigger>\n        </TabsList>\n        \n        <TabsContent value=\"config\">\n          <ConfigurationTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"prompt\">\n          <PromptEditor \n            value={selectedNode.config.systemPrompt}\n            onChange={(prompt) => updateNodeConfig(selectedNode.id, { systemPrompt: prompt })}\n            variables={getAvailableVariables(selectedNode)}\n          />\n        </TabsContent>\n        \n        <TabsContent value=\"advanced\">\n          <AdvancedSettingsTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"logs\">\n          <LogViewer logs={selectedNode.logs} />\n        </TabsContent>\n      </Tabs>\n    </div>\n  );\n};\n\nconst ConfigurationTab: React.FC<{ node: AgentNodeData }> = ({ node }) => {\n  return (\n    <div className=\"config-form\">\n      <FormField label=\"Agent Name\">\n        <Input value={node.label} onChange={...} />\n      </FormField>\n      \n      <FormField label=\"Model\">\n        <Select value={node.config.model}>\n          <SelectOption value=\"gpt-4-turbo\">GPT-4 Turbo</SelectOption>\n          <SelectOption value=\"claude-3-opus\">Claude 3 Opus</SelectOption>\n          <SelectOption value=\"claude-3-sonnet\">Claude 3 Sonnet</SelectOption>\n          <SelectOption value=\"gemini-pro\">Gemini Pro</SelectOption>\n        </Select>\n      </FormField>\n      \n      <FormField label=\"Temperature\" description=\"Controls randomness (0-2)\">\n        <Slider \n          value={node.config.temperature} \n          min={0} \n          max={2} \n          step={0.1}\n          onChange={...}\n        />\n        <Input type=\"number\" value={node.config.temperature} />\n      </FormField>\n      \n      <FormField label=\"Max Tokens\" optional>\n        <Input type=\"number\" value={node.config.maxTokens} />\n      </FormField>\n      \n      <FormField label=\"Timeout (seconds)\" optional>\n        <Input type=\"number\" value={node.config.timeout} />\n      </FormField>\n    </div>\n  );\n};\n```\n\n### 5. Toolbar & Node Palette (Toolbar.tsx)\n```typescript\nconst Toolbar: React.FC = () => {\n  const { executeWorkflow, stopExecution, saveWorkflow, loadTemplate } = \n    useWorkflowStore();\n  const executionState = useExecutionState();\n\n  return (\n    <div className=\"toolbar\">\n      <div className=\"toolbar-section\">\n        <Button \n          onClick={executeWorkflow}\n          disabled={executionState.isRunning}\n          variant=\"primary\"\n        >\n          <PlayIcon /> Run Workflow\n        </Button>\n        \n        {executionState.isRunning && (\n          <Button onClick={stopExecution} variant=\"danger\">\n            <StopIcon /> Stop\n          </Button>\n        )}\n      </div>\n      \n      <Separator />\n      \n      <div className=\"toolbar-section\">\n        <Button onClick={saveWorkflow} variant=\"secondary\">\n          <SaveIcon /> Save\n        </Button>\n        \n        <DropdownMenu>\n          <DropdownMenuTrigger>\n            <Button variant=\"secondary\">\n              <FolderIcon /> Templates\n            </Button>\n          </DropdownMenuTrigger>\n          <DropdownMenuContent>\n            <DropdownMenuItem onClick={() => loadTemplate('research-pipeline')}>\n              Research Pipeline\n            </DropdownMenuItem>\n            <DropdownMenuItem onClick={() => loadTemplate('code-review')}>\n              Code Review Flow\n            </DropdownMenuItem>\n            <DropdownMenuItem onClick={() => loadTemplate('content-generation')}>\n              Content Generation\n            </DropdownMenuItem>\n          </DropdownMenuContent>\n        </DropdownMenu>\n      </div>\n      \n      <Separator />\n      \n      <NodePalette />\n    </div>\n  );\n};\n\nconst NodePalette: React.FC = () => {\n  const onDragStart = (event: React.DragEvent, nodeType: string) => {\n    event.dataTransfer.setData('application/reactflow', nodeType);\n    event.dataTransfer.effectAllowed = 'move';\n  };\n\n  return (\n    <div className=\"node-palette\">\n      <div \n        className=\"palette-item researcher\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'researcher')}\n      >\n        <ResearchIcon />\n        <span>Researcher</span>\n      </div>\n      \n      <div \n        className=\"palette-item coder\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'coder')}\n      >\n        <CodeIcon />\n        <span>Coder</span>\n      </div>\n      \n      <div \n        className=\"palette-item reviewer\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'reviewer')}\n      >\n        <ReviewIcon />\n        <span>Reviewer</span>\n      </div>\n      \n      <div \n        className=\"palette-item coordinator\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'coordinator')}\n      >\n        <CoordinatorIcon />\n        <span>Coordinator</span>\n      </div>\n    </div>\n  );\n};\n```\n\n## Data Models\n\n### Workflow Schema\n```typescript\ninterface Workflow {\n  id: string;\n  name: string;\n  description?: string;\n  version: string;\n  createdAt: Date;\n  updatedAt: Date;\n  nodes: WorkflowNode[];\n  edges: WorkflowEdge[];\n  globalConfig: GlobalConfig;\n  executionHistory: ExecutionRecord[];\n}\n\ninterface WorkflowNode {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator';\n  position: { x: number; y: number };\n  data: AgentNodeData;\n}\n\ninterface WorkflowEdge {\n  id: string;\n  source: string;\n  target: string;\n  sourceHandle?: string;\n  targetHandle?: string;\n  data: EdgeData;\n}\n\ninterface GlobalConfig {\n  defaultModel: string;\n  defaultTemperature: number;\n  maxConcurrentAgents: number;\n  errorHandling: 'stop' | 'continue' | 'retry';\n  loggingLevel: 'debug' | 'info' | 'warn' | 'error';\n}\n```\n\n### Execution State\n```typescript\ninterface ExecutionState {\n  workflowId: string;\n  status: 'idle' | 'running' | 'paused' | 'completed' | 'failed';\n  startedAt?: Date;\n  completedAt?: Date;\n  currentNode?: string;\n  nodeStates: Map<string, NodeExecutionState>;\n  globalContext: Record<string, any>;\n  errors: ExecutionError[];\n}\n\ninterface NodeExecutionState {\n  nodeId: string;\n  status: ExecutionStatus;\n  startedAt?: Date;\n  completedAt?: Date;\n  input?: any;\n  output?: any;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n  error?: ExecutionError;\n}\n\ntype ExecutionStatus = 'idle' | 'waiting' | 'running' | 'success' | 'error' | 'skipped';\n\ninterface ExecutionMetrics {\n  tokensUsed?: number;\n  executionTime?: number;\n  retryCount?: number;\n  costEstimate?: number;\n}\n\ninterface LogEntry {\n  timestamp: Date;\n  level: 'debug' | 'info' | 'warn' | 'error';\n  nodeId: string;\n  message: string;\n  metadata?: Record<string, any>;\n}\n\ninterface ExecutionError {\n  nodeId: string;\n  timestamp: Date;\n  error: Error;\n  recoverable: boolean;\n  retryAttempts: number;\n}\n```\n\n## State Management (Zustand Store)\n\n```typescript\ninterface WorkflowStore {\n  // Current workflow state\n  workflow: Workflow | null;\n  nodes: Node<AgentNodeData>[];\n  edges: Edge<EdgeData>[];\n  selectedNode: AgentNodeData | null;\n  \n  // Execution state\n  executionState: ExecutionState | null;\n  isExecuting: boolean;\n  \n  // Actions\n  setWorkflow: (workflow: Workflow) => void;\n  addNode: (node: WorkflowNode) => void;\n  updateNode: (nodeId: string, updates: Partial<AgentNodeData>) => void;\n  deleteNode: (nodeId: string) => void;\n  addEdge: (edge: WorkflowEdge) => void;\n  deleteEdge: (edgeId: string) => void;\n  selectNode: (nodeId: string | null) => void;\n  \n  // Execution\n  executeWorkflow: () => Promise<void>;\n  pauseExecution: () => void;\n  resumeExecution: () => void;\n  stopExecution: () => void;\n  executeNode: (nodeId: string) => Promise<void>;\n  \n  // Persistence\n  saveWorkflow: () => Promise<void>;\n  loadWorkflow: (workflowId: string) => Promise<void>;\n  saveTemplate: (name: string) => Promise<void>;\n  loadTemplate: (templateId: string) => Promise<void>;\n  \n  // Validation\n  validateWorkflow: () => ValidationResult;\n}\n\nconst useWorkflowStore = create<WorkflowStore>((set, get) => ({\n  workflow: null,\n  nodes: [],\n  edges: [],\n  selectedNode: null,\n  executionState: null,\n  isExecuting: false,\n  \n  // Implementation details...\n}));\n```\n\n## Execution Engine\n\n### Workflow Executor\n```typescript\nclass WorkflowExecutor {\n  private workflowId: string;\n  private nodes: Map<string, WorkflowNode>;\n  private edges: WorkflowEdge[];\n  private executionState: ExecutionState;\n  private worker: Worker;\n  \n  constructor(workflow: Workflow) {\n    this.workflowId = workflow.id;\n    this.nodes = new Map(workflow.nodes.map(n => [n.id, n]));\n    this.edges = workflow.edges;\n    this.executionState = this.initExecutionState();\n    this.worker = new Worker(new URL('./execution.worker.ts', import.meta.url));\n  }\n  \n  async execute(): Promise<ExecutionResult> {\n    this.executionState.status = 'running';\n    this.executionState.startedAt = new Date();\n    \n    try {\n      // Build execution graph\n      const executionGraph = this.buildExecutionGraph();\n      \n      // Topological sort to determine execution order\n      const executionOrder = this.topologicalSort(executionGraph);\n      \n      // Execute nodes in order (with parallelization where possible)\n      for (const level of executionOrder) {\n        await this.executeLevel(level);\n      }\n      \n      this.executionState.status = 'completed';\n      this.executionState.completedAt = new Date();\n      \n      return {\n        success: true,\n        executionState: this.executionState\n      };\n    } catch (error) {\n      this.executionState.status = 'failed';\n      this.executionState.errors.push({\n        nodeId: 'workflow',\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: false,\n        retryAttempts: 0\n      });\n      \n      return {\n        success: false,\n        executionState: this.executionState,\n        error: error as Error\n      };\n    }\n  }\n  \n  private async executeLevel(nodeIds: string[]): Promise<void> {\n    // Execute all nodes in this level in parallel\n    const promises = nodeIds.map(nodeId => this.executeNode(nodeId));\n    await Promise.all(promises);\n  }\n  \n  private async executeNode(nodeId: string): Promise<void> {\n    const node = this.nodes.get(nodeId)!;\n    const nodeState = this.executionState.nodeStates.get(nodeId)!;\n    \n    nodeState.status = 'running';\n    nodeState.startedAt = new Date();\n    \n    try {\n      // Gather inputs from predecessor nodes\n      const inputs = this.gatherInputs(nodeId);\n      nodeState.input = inputs;\n      \n      // Execute agent in web worker\n      const result = await this.executeAgentInWorker(node, inputs);\n      \n      nodeState.output = result.output;\n      nodeState.logs.push(...result.logs);\n      nodeState.metrics = result.metrics;\n      nodeState.status = 'success';\n      nodeState.completedAt = new Date();\n      \n      // Store output in global context\n      this.executionState.globalContext[nodeId] = result.output;\n      \n    } catch (error) {\n      nodeState.status = 'error';\n      nodeState.error = {\n        nodeId,\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: this.isRecoverableError(error),\n        retryAttempts: 0\n      };\n      \n      // Handle error based on global config\n      await this.handleNodeError(nodeId, error as Error);\n    }\n  }\n  \n  private async executeAgentInWorker(\n    node: WorkflowNode, \n    inputs: any\n  ): Promise<AgentExecutionResult> {\n    return new Promise((resolve, reject) => {\n      const messageId = crypto.randomUUID();\n      \n      const handler = (event: MessageEvent) => {\n        if (event.data.messageId === messageId) {\n          this.worker.removeEventListener('message', handler);\n          \n          if (event.data.error) {\n            reject(new Error(event.data.error));\n          } else {\n            resolve(event.data.result);\n          }\n        }\n      };\n      \n      this.worker.addEventListener('message', handler);\n      \n      this.worker.postMessage({\n        messageId,\n        type: 'execute',\n        node,\n        inputs\n      });\n    });\n  }\n  \n  private buildExecutionGraph(): Map<string, string[]> {\n    const graph = new Map<string, string[]>();\n    \n    for (const node of this.nodes.values()) {\n      graph.set(node.id, []);\n    }\n    \n    for (const edge of this.edges) {\n      graph.get(edge.target)!.push(edge.source);\n    }\n    \n    return graph;\n  }\n  \n  private topologicalSort(graph: Map<string, string[]>): string[][] {\n    // Returns levels of nodes that can be executed in parallel\n    const inDegree = new Map<string, number>();\n    const levels: string[][] = [];\n    \n    // Calculate in-degrees\n    for (const [node, deps] of graph.entries()) {\n      inDegree.set(node, deps.length);\n    }\n    \n    while (inDegree.size > 0) {\n      // Find all nodes with in-degree 0\n      const currentLevel = Array.from(inDegree.entries())\n        .filter(([_, degree]) => degree === 0)\n        .map(([node, _]) => node);\n      \n      if (currentLevel.length === 0) {\n        throw new Error('Circular dependency detected in workflow');\n      }\n      \n      levels.push(currentLevel);\n      \n      // Remove these nodes and update in-degrees\n      for (const node of currentLevel) {\n        inDegree.delete(node);\n        \n        // Update dependents\n        for (const [otherNode, deps] of graph.entries()) {\n          if (deps.includes(node)) {\n            inDegree.set(otherNode, inDegree.get(otherNode)! - 1);\n          }\n        }\n      }\n    }\n    \n    return levels;\n  }\n  \n  private gatherInputs(nodeId: string): Record<string, any> {\n    const inputs: Record<string, any> = {};\n    \n    // Find all edges pointing to this node\n    const incomingEdges = this.edges.filter(e => e.target === nodeId);\n    \n    for (const edge of incomingEdges) {\n      const sourceOutput = this.executionState.globalContext[edge.source];\n      \n      // Apply any transformations defined on the edge\n      const transformedOutput = this.applyTransformations(\n        sourceOutput, \n        edge.data.transformations\n      );\n      \n      inputs[edge.source] = transformedOutput;\n    }\n    \n    return inputs;\n  }\n  \n  private applyTransformations(\n    data: any, \n    transformations?: Transformation[]\n  ): any {\n    if (!transformations || transformations.length === 0) {\n      return data;\n    }\n    \n    let result = data;\n    for (const transform of transformations) {\n      result = this.applyTransformation(result, transform);\n    }\n    return result;\n  }\n}\n```\n\n### Web Worker (execution.worker.ts)\n```typescript\n// Handles actual agent execution to avoid blocking UI\nself.addEventListener('message', async (event: MessageEvent) => {\n  const { messageId, type, node, inputs } = event.data;\n  \n  if (type === 'execute') {\n    try {\n      const result = await executeAgent(node, inputs);\n      \n      self.postMessage({\n        messageId,\n        result\n      });\n    } catch (error) {\n      self.postMessage({\n        messageId,\n        error: (error as Error).message\n      });\n    }\n  }\n});\n\nasync function executeAgent(\n  node: WorkflowNode, \n  inputs: Record<string, any>\n): Promise<AgentExecutionResult> {\n  const startTime = Date.now();\n  const logs: LogEntry[] = [];\n  \n  // Build the prompt with inputs\n  const prompt = buildPrompt(node, inputs);\n  \n  logs.push({\n    timestamp: new Date(),\n    level: 'info',\n    nodeId: node.id,\n    message: `Starting execution with model ${node.data.config.model}`,\n    metadata: { inputs }\n  });\n  \n  // Call the appropriate AI model\n  const response = await callAIModel(\n    node.data.config.model,\n    prompt,\n    node.data.config\n  );\n  \n  const executionTime = Date.now() - startTime;\n  \n  logs.push({\n    timestamp: new Date(),\n    level: 'info',\n    nodeId: node.id,\n    message: `Execution completed in ${executionTime}ms`,\n    metadata: { tokensUsed: response.tokensUsed }\n  });\n  \n  return {\n    output: response.output,\n    logs,\n    metrics: {\n      tokensUsed: response.tokensUsed,\n      executionTime,\n      retryCount: 0,\n      costEstimate: calculateCost(node.data.config.model, response.tokensUsed)\n    }\n  };\n}\n```\n\n## Real-Time Visualization\n\n### Execution Overlay Component\n```typescript\nconst ExecutionOverlay: React.FC = () => {\n  const executionState = useWorkflowStore(state => state.executionState);\n  \n  if (!executionState || executionState.status === 'idle') {\n    return null;\n  }\n  \n  return (\n    <Panel position=\"top-right\">\n      <div className=\"execution-panel\">\n        <div className=\"execution-header\">\n          <StatusIndicator status={executionState.status} />\n          <h3>Workflow Execution</h3>\n        </div>\n        \n        <div className=\"execution-metrics\">\n          <MetricCard \n            label=\"Duration\" \n            value={formatDuration(executionState.startedAt, executionState.completedAt)} \n          />\n          <MetricCard \n            label=\"Nodes Completed\" \n            value={`${getCompletedCount(executionState)}/${executionState.nodeStates.size}`} \n          />\n          <MetricCard \n            label=\"Total Cost\" \n            value={`$${calculateTotalCost(executionState).toFixed(4)}`} \n          />\n        </div>\n        \n        <div className=\"execution-timeline\">\n          <Timeline executionState={executionState} />\n        </div>\n        \n        {executionState.errors.length > 0 && (\n          <div className=\"execution-errors\">\n            <ErrorList errors={executionState.errors} />\n          </div>\n        )}\n      </div>\n    </Panel>\n  );\n};\n```\n\n### Live Log Viewer\n```typescript\nconst LogViewer: React.FC<{ logs: LogEntry[] }> = ({ logs }) => {\n  const [filter, setFilter] = useState<LogLevel | 'all'>('all');\n  const [search, setSearch] = useState('');\n  const scrollRef = useRef<HTMLDivElement>(null);\n  \n  // Auto-scroll to bottom on new logs\n  useEffect(() => {\n    if (scrollRef.current) {\n      scrollRef.current.scrollTop = scrollRef.current.scrollHeight;\n    }\n  }, [logs]);\n  \n  const filteredLogs = useMemo(() => {\n    return logs\n      .filter(log => filter === 'all' || log.level === filter)\n      .filter(log => \n        search === '' || \n        log.message.toLowerCase().includes(search.toLowerCase())\n      );\n  }, [logs, filter, search]);\n  \n  return (\n    <div className=\"log-viewer\">\n      <div className=\"log-controls\">\n        <Input \n          placeholder=\"Search logs...\" \n          value={search}\n          onChange={(e) => setSearch(e.target.value)}\n        />\n        <Select value={filter} onValueChange={setFilter}>\n          <SelectOption value=\"all\">All Levels</SelectOption>\n          <SelectOption value=\"debug\">Debug</SelectOption>\n          <SelectOption value=\"info\">Info</SelectOption>\n          <SelectOption value=\"warn\">Warning</SelectOption>\n          <SelectOption value=\"error\">Error</SelectOption>\n        </Select>\n      </div>\n      \n      <div className=\"log-entries\" ref={scrollRef}>\n        <VirtualList\n          items={filteredLogs}\n          height={400}\n          itemHeight={24}\n          renderItem={(log) => (\n            <LogEntry key={log.timestamp.getTime()} log={log} />\n          )}\n        />\n      </div>\n    </div>\n  );\n};\n\nconst LogEntry: React.FC<{ log: LogEntry }> = ({ log }) => {\n  return (\n    <div className={`log-entry log-${log.level}`}>\n      <span className=\"log-timestamp\">\n        {log.timestamp.toLocaleTimeString()}\n      </span>\n      <span className={`log-level level-${log.level}`}>\n        {log.level.toUpperCase()}\n      </span>\n      <span className=\"log-node\">[{log.nodeId}]</span>\n      <span className=\"log-message\">{log.message}</span>\n      {log.metadata && (\n        <Collapsible>\n          <CollapsibleTrigger>\n            <ChevronIcon />\n          </CollapsibleTrigger>\n          <CollapsibleContent>\n            <pre>{JSON.stringify(log.metadata, null, 2)}</pre>\n          </CollapsibleContent>\n        </Collapsible>\n      )}\n    </div>\n  );\n};\n```\n\n## Template Management\n\n### Template Structure\n```typescript\ninterface WorkflowTemplate {\n  id: string;\n  name: string;\n  description: string;\n  category: 'research' | 'coding' | 'review' | 'general';\n  thumbnail?: string;\n  workflow: Omit<Workflow, 'id' | 'createdAt' | 'updatedAt' | 'executionHistory'>;\n  variables: TemplateVariable[];\n}\n\ninterface TemplateVariable {\n  key: string;\n  label: string;\n  type: 'string' | 'number' | 'model' | 'prompt';\n  defaultValue?: any;\n  description?: string;\n}\n\n// Example template\nconst RESEARCH_PIPELINE_TEMPLATE: WorkflowTemplate = {\n  id: 'research-pipeline',\n  name: 'Research Pipeline',\n  description: 'Multi-stage research workflow with synthesis',\n  category: 'research',\n  variables: [\n    {\n      key: 'research_topic',\n      label: 'Research Topic',\n      type: 'string',\n      description: 'The topic to research'\n    },\n    {\n      key: 'primary_model',\n      label: 'Primary Model',\n      type: 'model',\n      defaultValue: 'gpt-4-turbo'\n    }\n  ],\n  workflow: {\n    name: 'Research Pipeline',\n    version: '1.0.0',\n    nodes: [\n      {\n        id: 'researcher-1',\n        type: 'researcher',\n        position: { x: 100, y: 100 },\n        data: {\n          id: 'researcher-1',\n          type: 'researcher',\n          label: 'Primary Researcher',\n          config: {\n            model: '{{primary_model}}',\n            temperature: 0.7,\n            systemPrompt: 'You are a thorough researcher. Research: {{research_topic}}'\n          },\n          status: 'idle',\n          logs: [],\n          metrics: {}\n        }\n      },\n      {\n        id: 'reviewer-1',\n        type: 'reviewer',\n        position: { x: 400, y: 100 },\n        data: {\n          id: 'reviewer-1',\n          type: 'reviewer',\n          label: 'Fact Checker',\n          config: {\n            model: 'claude-3-opus',\n            temperature: 0.3,\n            systemPrompt: 'Review research for accuracy and completeness.'\n          },\n          status: 'idle',\n          logs: [],\n          metrics: {}\n        }\n      }\n    ],\n    edges: [\n      {\n        id: 'e1',\n        source: 'researcher-1',\n        target: 'reviewer-1',\n        data: {\n          id: 'e1',\n          dataType: 'text',\n          status: 'idle'\n        }\n      }\n    ],\n    globalConfig: {\n      defaultModel: 'gpt-4-turbo',\n      defaultTemperature: 0.7,\n      maxConcurrentAgents: 3,\n      errorHandling: 'stop',\n      loggingLevel: 'info'\n    },\n    executionHistory: []\n  }\n};\n```\n\n### Template Manager\n```typescript\nclass TemplateManager {\n  private db: IDBDatabase;\n  \n  async saveTemplate(template: WorkflowTemplate): Promise<void> {\n    const tx = this.db.transaction('templates', 'readwrite');\n    const store = tx.objectStore('templates');\n    await store.put(template);\n  }\n  \n  async loadTemplate(templateId: string): Promise<WorkflowTemplate> {\n    const tx = this.db.transaction('templates', 'readonly');\n    const store = tx.objectStore('templates');\n    return await store.get(templateId);\n  }\n  \n  async listTemplates(category?: string): Promise<WorkflowTemplate[]> {\n    const tx = this.db.transaction('templates', 'readonly');\n    const store = tx.objectStore('templates');\n    const templates = await store.getAll();\n    \n    if (category) {\n      return templates.filter(t => t.category === category);\n    }\n    return templates;\n  }\n  \n  async instantiateTemplate(\n    templateId: string, \n    variables: Record<string, any>\n  ): Promise<Workflow> {\n    const template = await this.loadTemplate(templateId);\n    \n    // Deep clone the workflow\n    const workflow: Workflow = {\n      id: crypto.randomUUID(),\n      name: template.workflow.name,\n      description: template.workflow.description,\n      version: template.workflow.version,\n      createdAt: new Date(),\n      updatedAt: new Date(),\n      nodes: JSON.parse(JSON.stringify(template.workflow.nodes)),\n      edges: JSON.parse(JSON.stringify(template.workflow.edges)),\n      globalConfig: { ...template.workflow.globalConfig },\n      executionHistory: []\n    };\n    \n    // Replace variables\n    const workflowStr = JSON.stringify(workflow);\n    const replaced = this.replaceVariables(workflowStr, variables);\n    \n    return JSON.parse(replaced);\n  }\n  \n  private replaceVariables(\n    template: string, \n    variables: Record<string, any>\n  ): string {\n    let result = template;\n    for (const [key, value] of Object.entries(variables)) {\n      result = result.replace(\n        new RegExp(`{{${key}}}`, 'g'), \n        String(value)\n      );\n    }\n    return result;\n  }\n}\n```\n\n## Error Handling\n\n### Validation Service\n```typescript\ninterface ValidationResult {\n  valid: boolean;\n  errors: ValidationError[];\n  warnings: ValidationWarning[];\n}\n\ninterface ValidationError {\n  type: 'structure' | 'config' | 'connection';\n  nodeId?: string;\n  edgeId?: string;\n  message: string;\n  severity: 'error' | 'warning';\n}\n\nclass WorkflowValidator {\n  validate(workflow: Workflow): ValidationResult {\n    const errors: ValidationError[] = [];\n    const warnings: ValidationWarning[] = [];\n    \n    // Check for cycles\n    if (this.hasCycles(workflow)) {\n      errors.push({\n        type: 'structure',\n        message: 'Workflow contains circular dependencies',\n        severity: 'error'\n      });\n    }\n    \n    // Check for disconnected nodes\n    const disconnected = this.findDisconnectedNodes(workflow);\n    if (disconnected.length > 0) {\n      warnings.push({\n        type: 'structure',\n        message: `${disconnected.length} nodes are not connected`,\n        nodeIds: disconnected,\n        severity: 'warning'\n      });\n    }\n    \n    // Validate each node configuration\n    for (const node of workflow.nodes) {\n      const nodeErrors = this.validateNode(node);\n      errors.push(...nodeErrors);\n    }\n    \n    // Check for missing start nodes\n    const startNodes = this.findStartNodes(workflow);\n    if (startNodes.length === 0) {\n      errors.push({\n        type: 'structure',\n        message: 'Workflow has no starting nodes',\n        severity: 'error'\n      });\n    }\n    \n    return {\n      valid: errors.length === 0,\n      errors,\n      warnings\n    };\n  }\n  \n  private validateNode(node: WorkflowNode): ValidationError[] {\n    const errors: ValidationError[] = [];\n    \n    if (!node.data.config.model) {\n      errors.push({\n        type: 'config',\n        nodeId: node.id,\n        message: 'No model selected',\n        severity: 'error'\n      });\n    }\n    \n    if (!node.data.config.systemPrompt || node.data.config.systemPrompt.trim() === '') {\n      errors.push({\n        type: 'config',\n        nodeId: node.id,\n        message: 'System prompt is empty',\n        severity: 'error'\n      });\n    }\n    \n    if (node.data.config.temperature < 0 || node.data.config.temperature > 2) {\n      errors.push({\n        type: 'config',\n        nodeId: node.id,\n        message: 'Temperature must be between 0 and 2',\n        severity: 'error'\n      });\n    }\n    \n    return errors;\n  }\n  \n  private hasCycles(workflow: Workflow): boolean {\n    const visited = new Set<string>();\n    const recursionStack = new Set<string>();\n    \n    const adjacencyList = new Map<string, string[]>();\n    for (const node of workflow.nodes) {\n      adjacencyList.set(node.id, []);\n    }\n    for (const edge of workflow.edges) {\n      adjacencyList.get(edge.source)!.push(edge.target);\n    }\n    \n    const dfs = (nodeId: string): boolean => {\n      visited.add(nodeId);\n      recursionStack.add(nodeId);\n      \n      for (const neighbor of adjacencyList.get(nodeId) || []) {\n        if (!visited.has(neighbor)) {\n          if (dfs(neighbor)) return true;\n        } else if (recursionStack.has(neighbor)) {\n          return true;\n        }\n      }\n      \n      recursionStack.delete(nodeId);\n      return false;\n    };\n    \n    for (const node of workflow.nodes) {\n      if (!visited.has(node.id)) {\n        if (dfs(node.id)) return true;\n      }\n    }\n    \n    return false;\n  }\n  \n  private findDisconnectedNodes(workflow: Workflow): string[] {\n    const connectedNodes = new Set<string>();\n    \n    for (const edge of workflow.edges) {\n      connectedNodes.add(edge.source);\n      connectedNodes.add(edge.target);\n    }\n    \n    return workflow.nodes\n      .filter(node => !connectedNodes.has(node.id))\n      .map(node => node.id);\n  }\n  \n  private findStartNodes(workflow: Workflow): string[] {\n    const nodesWithInput = new Set(workflow.edges.map(e => e.target));\n    return workflow.nodes\n      .filter(node => !nodesWithInput.has(node.id))\n      .map(node => node.id);\n  }\n}\n```\n\n### Error Recovery\n```typescript\nclass ErrorRecoveryService {\n  async handleNodeError(\n    nodeId: string,\n    error: Error,\n    config: GlobalConfig\n  ): Promise<RecoveryAction> {\n    \n    switch (config.errorHandling) {\n      case 'stop':\n        return { action: 'stop', message: 'Execution stopped due to error' };\n        \n      case 'retry':\n        if (this.isRetryableError(error)) {\n          return { \n            action: 'retry', \n            maxRetries: 3,\n            backoff: 'exponential',\n            message: 'Retrying node execution' \n          };\n        }\n        return { action: 'skip', message: 'Error not retryable, skipping node' };\n        \n      case 'continue':\n        return { \n          action: 'continue', \n          message: 'Continuing with next nodes',\n          skipDownstream: false\n        };\n        \n      default:\n        return { action: 'stop', message: 'Unknown error handling strategy' };\n    }\n  }\n  \n  private isRetryableError(error: Error): boolean {\n    // Network errors, rate limits, temporary API failures\n    return error.message.includes('ECONNREFUSED') ||\n           error.message.includes('rate limit') ||\n           error.message.includes('timeout') ||\n           error.message.includes('429');\n  }\n  \n  async retryWithBackoff(\n    fn: () => Promise<any>,\n    maxRetries: number,\n    backoff: 'linear' | 'exponential' = 'exponential'\n  ): Promise<any> {\n    let lastError: Error;\n    \n    for (let i = 0; i < maxRetries; i++) {\n      try {\n        return await fn();\n      } catch (error) {\n        lastError = error as Error;\n        \n        if (i < maxRetries - 1) {\n          const delay = backoff === 'exponential' \n            ? Math.pow(2, i) * 1000 \n            : (i + 1) * 1000;\n          \n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n    \n    throw lastError!;\n  }\n}\n```\n\n## Persistence Layer\n\n### IndexedDB Schema\n```typescript\nconst DB_NAME = 'workflow-builder';\nconst DB_VERSION = 1;\n\ninterface DBSchema {\n  workflows: {\n    key: string;\n    value: Workflow;\n    indexes: {\n      'by-updated': Date;\n      'by-name': string;\n    };\n  };\n  templates: {\n    key: string;\n    value: WorkflowTemplate;\n    indexes: {\n      'by-category': string;\n    };\n  };\n  executions: {\n    key: string;\n    value: ExecutionRecord;\n    indexes: {\n      'by-workflow': string;\n      'by-date': Date;\n    };\n  };\n}\n\nclass WorkflowDatabase {\n  private db: IDBDatabase | null = null;\n  \n  async init(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open(DB_NAME, DB_VERSION);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        // Workflows store\n        if (!db.objectStoreNames.contains('workflows')) {\n          const workflowStore = db.createObjectStore('workflows', { keyPath: 'id' });\n          workflowStore.createIndex('by-updated', 'updatedAt');\n          workflowStore.createIndex('by-name', 'name');\n        }\n        \n        // Templates store\n        if (!db.objectStoreNames.contains('templates')) {\n          const templateStore = db.createObjectStore('templates', { keyPath: 'id' });\n          templateStore.createIndex('by-category', 'category');\n        }\n        \n        // Executions store\n        if (!db.objectStoreNames.contains('executions')) {\n          const executionStore = db.createObjectStore('executions', { keyPath: 'id' });\n          executionStore.createIndex('by-workflow', 'workflowId');\n          executionStore.createIndex('by-date', 'startedAt');\n        }\n      };\n    });\n  }\n  \n  async saveWorkflow(workflow: Workflow): Promise<void> {\n    const tx = this.db!.transaction('workflows', 'readwrite');\n    const store = tx.objectStore('workflows');\n    workflow.updatedAt = new Date();\n    await store.put(workflow);\n  }\n  \n  async loadWorkflow(workflowId: string): Promise<Workflow | undefined> {\n    const tx = this.db!.transaction('workflows', 'readonly');\n    const store = tx.objectStore('workflows');\n    return await store.get(workflowId);\n  }\n  \n  async listWorkflows(): Promise<Workflow[]> {\n    const tx = this.db!.transaction('workflows', 'readonly');\n    const store = tx.objectStore('workflows');\n    const index = store.index('by-updated');\n    return await index.getAll();\n  }\n  \n  async deleteWorkflow(workflowId: string): Promise<void> {\n    const tx = this.db!.transaction('workflows', 'readwrite');\n    const store = tx.objectStore('workflows');\n    await store.delete(workflowId);\n  }\n}\n```\n\n## Performance Optimizations\n\n1. **Virtual Scrolling**: Use react-window for log viewer with thousands of entries\n2. **Memoization**: Memoize expensive node renderings with React.memo\n3. **Web Workers**: Execute agents in web workers to avoid blocking UI\n4. **Debouncing**: Debounce property panel updates to avoid excessive re-renders\n5. **Lazy Loading**: Load execution history and logs on demand\n6. **Canvas Optimization**: Use React Flow's built-in performance optimizations (viewport culling)\n7. **IndexedDB**: Async storage to avoid blocking main thread\n8. **Edge Batching**: Batch edge updates during execution to reduce re-renders\n\n## Rationale\n\n**Why React Flow?** \n- Battle-tested with 20k+ GitHub stars\n- Excellent performance with large graphs\n- Built-in features (minimap, controls, zoom, pan)\n- Extensible with custom nodes and edges\n- Active community and good documentation\n\n**Why Zustand over Redux?**\n- Less boilerplate (no actions/reducers)\n- Better TypeScript support\n- Simpler middleware for persistence\n- Better performance (no unnecessary re-renders)\n- Easier to test\n\n**Why Web Workers for execution?**\n- Prevents UI blocking during long agent executions\n- Allows true parallel execution of independent agents\n- Better error isolation\n- Can be terminated if needed\n\n**Why IndexedDB over localStorage?**\n- Can store larger workflows (>5MB)\n- Async API doesn't block main thread\n- Better structured query support\n- Can store binary data (future feature: screenshots)\n\n**Topological execution with parallelization**\n- Maximizes throughput by running independent agents concurrently\n- Respects data dependencies automatically\n- Easy to visualize execution progress\n\n## What I Changed\n\nThis is the initial design, so there are no changes from a previous iteration.\n\n## What I Kept\n\nThis is the initial design, so there are no previous ideas to keep.\n\n## Open Questions\n\n1. **Streaming vs. Batch**: Should agent outputs stream in real-time or return all at once? Streaming is more engaging but adds complexity.\n\n2. **Agent Communication Protocol**: Should agents communicate through structured JSON or free-form text? JSON is more parseable but less flexible.\n\n3. **Cost Management**: Should we add hard limits on token usage per workflow or just show estimates?\n\n4. **Collaboration**: Should workflows be shareable via URL with a backend, or stay purely local?\n\n5. **Version Control**: Should we track workflow versions with diffs, or just save snapshots?\n\n## Convergence Status\n\n**ITERATING** - This is the first draft and requires review for:\n- Missing edge cases in execution engine\n- UI/UX improvements for node configuration\n- Additional agent types or capabilities\n- Performance concerns with large workflows",
        "prompt_for_other": "Please focus your review on:\n1. **Execution engine design** - Are there race conditions or edge cases I'm missing?\n2. **Data flow between agents** - Is the input/output mechanism clear and flexible enough?\n3. **Error handling** - What failure modes am I not considering?\n4. **Real-time visualization** - Are there better ways to show execution status?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Complete Design\n\n## Architecture Overview\n\n### High-Level Architecture\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502     \u2502\n\u2502  \u2502   Layer      \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           Workflow State Management (Zustand)               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Execution  \u2502  \u2502  Template    \u2502  \u2502   Validation \u2502     \u2502\n\u2502  \u2502   Engine     \u2502  \u2502  Manager     \u2502  \u2502   Service    \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas Library**: React Flow (react-flow-renderer) - mature, performant, extensible\n- **State Management**: Zustand - lightweight, no boilerplate\n- **UI Components**: Radix UI + Tailwind CSS - accessible, customizable\n- **Execution**: Web Workers for non-blocking agent execution\n- **Storage**: IndexedDB for templates, LocalStorage for recent workflows\n\n## Core Components\n\n### 1. Canvas Component (WorkflowCanvas.tsx)\n```typescript\ninterface WorkflowCanvasProps {\n  workflowId: string;\n  readonly?: boolean;\n}\n\nconst WorkflowCanvas: React.FC<WorkflowCanvasProps> = ({ \n  workflowId, \n  readonly = false \n}) => {\n  const { nodes, edges, onNodesChange, onEdgesChange, onConnect } = \n    useWorkflowStore();\n  \n  return (\n    <ReactFlow\n      nodes={nodes}\n      edges={edges}\n      onNodesChange={onNodesChange}\n      onEdgesChange={onEdgesChange}\n      onConnect={onConnect}\n      nodeTypes={customNodeTypes}\n      edgeTypes={customEdgeTypes}\n      fitView\n      minZoom={0.2}\n      maxZoom={2}\n    >\n      <Background />\n      <Controls />\n      <MiniMap />\n      <ExecutionOverlay />\n    </ReactFlow>\n  );\n};\n```\n\n### 2. Agent Node Component (AgentNode.tsx)\n```typescript\ninterface AgentNodeData {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator';\n  label: string;\n  config: AgentConfig;\n  status: ExecutionStatus;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n}\n\ninterface AgentConfig {\n  model: string; // 'gpt-4', 'claude-3-opus', etc.\n  temperature: number;\n  systemPrompt: string;\n  maxTokens?: number;\n  timeout?: number;\n  retryConfig?: RetryConfig;\n}\n\nconst AgentNode: React.FC<NodeProps<AgentNodeData>> = ({ data, selected }) => {\n  const getStatusColor = () => {\n    switch (data.status) {\n      case 'idle': return 'gray';\n      case 'running': return 'blue';\n      case 'success': return 'green';\n      case 'error': return 'red';\n      case 'waiting': return 'yellow';\n    }\n  };\n\n  return (\n    <div className={`agent-node ${selected ? 'selected' : ''}`}>\n      <Handle type=\"target\" position={Position.Left} />\n      \n      <div className=\"node-header\" style={{ borderColor: getStatusColor() }}>\n        <AgentIcon type={data.type} />\n        <span>{data.label}</span>\n        <StatusBadge status={data.status} />\n      </div>\n      \n      <div className=\"node-body\">\n        <div className=\"config-summary\">\n          <span className=\"model-badge\">{data.config.model}</span>\n          <span className=\"temp-badge\">T: {data.config.temperature}</span>\n        </div>\n        \n        {data.status === 'running' && (\n          <ProgressBar metrics={data.metrics} />\n        )}\n        \n        {data.logs.length > 0 && (\n          <LogPreview logs={data.logs.slice(-3)} />\n        )}\n      </div>\n      \n      <Handle type=\"source\" position={Position.Right} />\n    </div>\n  );\n};\n```\n\n### 3. Connection Edge Component (DataFlowEdge.tsx)\n```typescript\ninterface EdgeData {\n  id: string;\n  dataType: 'text' | 'json' | 'code' | 'artifacts';\n  transformations?: Transformation[];\n  status: 'idle' | 'active' | 'complete';\n  dataPreview?: string;\n}\n\nconst DataFlowEdge: React.FC<EdgeProps<EdgeData>> = ({ \n  id, \n  source, \n  target, \n  data,\n  style \n}) => {\n  const [edgePath, labelX, labelY] = getBezierPath({\n    sourceX, sourceY, targetX, targetY\n  });\n\n  return (\n    <>\n      <BaseEdge \n        path={edgePath} \n        style={{ \n          ...style, \n          stroke: data.status === 'active' ? '#3b82f6' : '#6b7280',\n          strokeWidth: data.status === 'active' ? 3 : 2,\n          animation: data.status === 'active' ? 'dash 1s linear infinite' : 'none'\n        }} \n      />\n      \n      <EdgeLabelRenderer>\n        <div\n          style={{ \n            position: 'absolute', \n            transform: `translate(-50%, -50%) translate(${labelX}px,${labelY}px)` \n          }}\n        >\n          <DataTypeIndicator type={data.dataType} />\n          {data.dataPreview && (\n            <DataPreviewTooltip content={data.dataPreview} />\n          )}\n        </div>\n      </EdgeLabelRenderer>\n    </>\n  );\n};\n```\n\n### 4. Properties Panel (PropertiesPanel.tsx)\n```typescript\nconst PropertiesPanel: React.FC = () => {\n  const { selectedNode, updateNodeConfig } = useWorkflowStore();\n  \n  if (!selectedNode) {\n    return <EmptyState message=\"Select a node to configure\" />;\n  }\n\n  return (\n    <div className=\"properties-panel\">\n      <Tabs defaultValue=\"config\">\n        <TabsList>\n          <TabsTrigger value=\"config\">Configuration</TabsTrigger>\n          <TabsTrigger value=\"prompt\">System Prompt</TabsTrigger>\n          <TabsTrigger value=\"advanced\">Advanced</TabsTrigger>\n          <TabsTrigger value=\"logs\">Logs</TabsTrigger>\n        </TabsList>\n        \n        <TabsContent value=\"config\">\n          <ConfigurationTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"prompt\">\n          <PromptEditor \n            value={selectedNode.config.systemPrompt}\n            onChange={(prompt) => updateNodeConfig(selectedNode.id, { systemPrompt: prompt })}\n            variables={getAvailableVariables(selectedNode)}\n          />\n        </TabsContent>\n        \n        <TabsContent value=\"advanced\">\n          <AdvancedSettingsTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"logs\">\n          <LogViewer logs={selectedNode.logs} />\n        </TabsContent>\n      </Tabs>\n    </div>\n  );\n};\n\nconst ConfigurationTab: React.FC<{ node: AgentNodeData }> = ({ node }) => {\n  return (\n    <div className=\"config-form\">\n      <FormField label=\"Agent Name\">\n        <Input value={node.label} onChange={...} />\n      </FormField>\n      \n      <FormField label=\"Model\">\n        <Select value={node.config.model}>\n          <SelectOption value=\"gpt-4-turbo\">GPT-4 Turbo</SelectOption>\n          <SelectOption value=\"claude-3-opus\">Claude 3 Opus</SelectOption>\n          <SelectOption value=\"claude-3-sonnet\">Claude 3 Sonnet</SelectOption>\n          <SelectOption value=\"gemini-pro\">Gemini Pro</SelectOption>\n        </Select>\n      </FormField>\n      \n      <FormField label=\"Temperature\" description=\"Controls randomness (0-2)\">\n        <Slider \n          value={node.config.temperature} \n          min={0} \n          max={2} \n          step={0.1}\n          onChange={...}\n        />\n        <Input type=\"number\" value={node.config.temperature} />\n      </FormField>\n      \n      <FormField label=\"Max Tokens\" optional>\n        <Input type=\"number\" value={node.config.maxTokens} />\n      </FormField>\n      \n      <FormField label=\"Timeout (seconds)\" optional>\n        <Input type=\"number\" value={node.config.timeout} />\n      </FormField>\n    </div>\n  );\n};\n```\n\n### 5. Toolbar & Node Palette (Toolbar.tsx)\n```typescript\nconst Toolbar: React.FC = () => {\n  const { executeWorkflow, stopExecution, saveWorkflow, loadTemplate } = \n    useWorkflowStore();\n  const executionState = useExecutionState();\n\n  return (\n    <div className=\"toolbar\">\n      <div className=\"toolbar-section\">\n        <Button \n          onClick={executeWorkflow}\n          disabled={executionState.isRunning}\n          variant=\"primary\"\n        >\n          <PlayIcon /> Run Workflow\n        </Button>\n        \n        {executionState.isRunning && (\n          <Button onClick={stopExecution} variant=\"danger\">\n            <StopIcon /> Stop\n          </Button>\n        )}\n      </div>\n      \n      <Separator />\n      \n      <div className=\"toolbar-section\">\n        <Button onClick={saveWorkflow} variant=\"secondary\">\n          <SaveIcon /> Save\n        </Button>\n        \n        <DropdownMenu>\n          <DropdownMenuTrigger>\n            <Button variant=\"secondary\">\n              <FolderIcon /> Templates\n            </Button>\n          </DropdownMenuTrigger>\n          <DropdownMenuContent>\n            <DropdownMenuItem onClick={() => loadTemplate('research-pipeline')}>\n              Research Pipeline\n            </DropdownMenuItem>\n            <DropdownMenuItem onClick={() => loadTemplate('code-review')}>\n              Code Review Flow\n            </DropdownMenuItem>\n            <DropdownMenuItem onClick={() => loadTemplate('content-generation')}>\n              Content Generation\n            </DropdownMenuItem>\n          </DropdownMenuContent>\n        </DropdownMenu>\n      </div>\n      \n      <Separator />\n      \n      <NodePalette />\n    </div>\n  );\n};\n\nconst NodePalette: React.FC = () => {\n  const onDragStart = (event: React.DragEvent, nodeType: string) => {\n    event.dataTransfer.setData('application/reactflow', nodeType);\n    event.dataTransfer.effectAllowed = 'move';\n  };\n\n  return (\n    <div className=\"node-palette\">\n      <div \n        className=\"palette-item researcher\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'researcher')}\n      >\n        <ResearchIcon />\n        <span>Researcher</span>\n      </div>\n      \n      <div \n        className=\"palette-item coder\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'coder')}\n      >\n        <CodeIcon />\n        <span>Coder</span>\n      </div>\n      \n      <div \n        className=\"palette-item reviewer\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'reviewer')}\n      >\n        <ReviewIcon />\n        <span>Reviewer</span>\n      </div>\n      \n      <div \n        className=\"palette-item coordinator\"\n        draggable\n        onDragStart={(e) => onDragStart(e, 'coordinator')}\n      >\n        <CoordinatorIcon />\n        <span>Coordinator</span>\n      </div>\n    </div>\n  );\n};\n```\n\n## Data Models\n\n### Workflow Schema\n```typescript\ninterface Workflow {\n  id: string;\n  name: string;\n  description?: string;\n  version: string;\n  createdAt: Date;\n  updatedAt: Date;\n  nodes: WorkflowNode[];\n  edges: WorkflowEdge[];\n  globalConfig: GlobalConfig;\n  executionHistory: ExecutionRecord[];\n}\n\ninterface WorkflowNode {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator';\n  position: { x: number; y: number };\n  data: AgentNodeData;\n}\n\ninterface WorkflowEdge {\n  id: string;\n  source: string;\n  target: string;\n  sourceHandle?: string;\n  targetHandle?: string;\n  data: EdgeData;\n}\n\ninterface GlobalConfig {\n  defaultModel: string;\n  defaultTemperature: number;\n  maxConcurrentAgents: number;\n  errorHandling: 'stop' | 'continue' | 'retry';\n  loggingLevel: 'debug' | 'info' | 'warn' | 'error';\n}\n```\n\n### Execution State\n```typescript\ninterface ExecutionState {\n  workflowId: string;\n  status: 'idle' | 'running' | 'paused' | 'completed' | 'failed';\n  startedAt?: Date;\n  completedAt?: Date;\n  currentNode?: string;\n  nodeStates: Map<string, NodeExecutionState>;\n  globalContext: Record<string, any>;\n  errors: ExecutionError[];\n}\n\ninterface NodeExecutionState {\n  nodeId: string;\n  status: ExecutionStatus;\n  startedAt?: Date;\n  completedAt?: Date;\n  input?: any;\n  output?: any;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n  error?: ExecutionError;\n}\n\ntype ExecutionStatus = 'idle' | 'waiting' | 'running' | 'success' | 'error' | 'skipped';\n\ninterface ExecutionMetrics {\n  tokensUsed?: number;\n  executionTime?: number;\n  retryCount?: number;\n  costEstimate?: number;\n}\n\ninterface LogEntry {\n  timestamp: Date;\n  level: 'debug' | 'info' | 'warn' | 'error';\n  nodeId: string;\n  message: string;\n  metadata?: Record<string, any>;\n}\n\ninterface ExecutionError {\n  nodeId: string;\n  timestamp: Date;\n  error: Error;\n  recoverable: boolean;\n  retryAttempts: number;\n}\n```\n\n## State Management (Zustand Store)\n\n```typescript\ninterface WorkflowStore {\n  // Current workflow state\n  workflow: Workflow | null;\n  nodes: Node<AgentNodeData>[];\n  edges: Edge<EdgeData>[];\n  selectedNode: AgentNodeData | null;\n  \n  // Execution state\n  executionState: ExecutionState | null;\n  isExecuting: boolean;\n  \n  // Actions\n  setWorkflow: (workflow: Workflow) => void;\n  addNode: (node: WorkflowNode) => void;\n  updateNode: (nodeId: string, updates: Partial<AgentNodeData>) => void;\n  deleteNode: (nodeId: string) => void;\n  addEdge: (edge: WorkflowEdge) => void;\n  deleteEdge: (edgeId: string) => void;\n  selectNode: (nodeId: string | null) => void;\n  \n  // Execution\n  executeWorkflow: () => Promise<void>;\n  pauseExecution: () => void;\n  resumeExecution: () => void;\n  stopExecution: () => void;\n  executeNode: (nodeId: string) => Promise<void>;\n  \n  // Persistence\n  saveWorkflow: () => Promise<void>;\n  loadWorkflow: (workflowId: string) => Promise<void>;\n  saveTemplate: (name: string) => Promise<void>;\n  loadTemplate: (templateId: string) => Promise<void>;\n  \n  // Validation\n  validateWorkflow: () => ValidationResult;\n}\n\nconst useWorkflowStore = create<WorkflowStore>((set, get) => ({\n  workflow: null,\n  nodes: [],\n  edges: [],\n  selectedNode: null,\n  executionState: null,\n  isExecuting: false,\n  \n  // Implementation details...\n}));\n```\n\n## Execution Engine\n\n### Workflow Executor\n```typescript\nclass WorkflowExecutor {\n  private workflowId: string;\n  private nodes: Map<string, WorkflowNode>;\n  private edges: WorkflowEdge[];\n  private executionState: ExecutionState;\n  private worker: Worker;\n  \n  constructor(workflow: Workflow) {\n    this.workflowId = workflow.id;\n    this.nodes = new Map(workflow.nodes.map(n => [n.id, n]));\n    this.edges = workflow.edges;\n    this.executionState = this.initExecutionState();\n    this.worker = new Worker(new URL('./execution.worker.ts', import.meta.url));\n  }\n  \n  async execute(): Promise<ExecutionResult> {\n    this.executionState.status = 'running';\n    this.executionState.startedAt = new Date();\n    \n    try {\n      // Build execution graph\n      const executionGraph = this.buildExecutionGraph();\n      \n      // Topological sort to determine execution order\n      const executionOrder = this.topologicalSort(executionGraph);\n      \n      // Execute nodes in order (with parallelization where possible)\n      for (const level of executionOrder) {\n        await this.executeLevel(level);\n      }\n      \n      this.executionState.status = 'completed';\n      this.executionState.completedAt = new Date();\n      \n      return {\n        success: true,\n        executionState: this.executionState\n      };\n    } catch (error) {\n      this.executionState.status = 'failed';\n      this.executionState.errors.push({\n        nodeId: 'workflow',\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: false,\n        retryAttempts: 0\n      });\n      \n      return {\n        success: false,\n        executionState: this.executionState,\n        error: error as Error\n      };\n    }\n  }\n  \n  private async executeLevel(nodeIds: string[]): Promise<void> {\n    // Execute all nodes in this level in parallel\n    const promises = nodeIds.map(nodeId => this.executeNode(nodeId));\n    await Promise.all(promises);\n  }\n  \n  private async executeNode(nodeId: string): Promise<void> {\n    const node = this.nodes.get(nodeId)!;\n    const nodeState = this.executionState.nodeStates.get(nodeId)!;\n    \n    nodeState.status = 'running';\n    nodeState.startedAt = new Date();\n    \n    try {\n      // Gather inputs from predecessor nodes\n      const inputs = this.gatherInputs(nodeId);\n      nodeState.input = inputs;\n      \n      // Execute agent in web worker\n      const result = await this.executeAgentInWorker(node, inputs);\n      \n      nodeState.output = result.output;\n      nodeState.logs.push(...result.logs);\n      nodeState.metrics = result.metrics;\n      nodeState.status = 'success';\n      nodeState.completedAt = new Date();\n      \n      // Store output in global context\n      this.executionState.globalContext[nodeId] = result.output;\n      \n    } catch (error) {\n      nodeState.status = 'error';\n      nodeState.error = {\n        nodeId,\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: this.isRecoverableError(error),\n        retryAttempts: 0\n      };\n      \n      // Handle error based on global config\n      await this.handleNodeError(nodeId, error as Error);\n    }\n  }\n  \n  private async executeAgentInWorker(\n    node: WorkflowNode, \n    inputs: any\n  ): Promise<AgentExecutionResult> {\n    return new Promise((resolve, reject) => {\n      const messageId = crypto.randomUUID();\n      \n      const handler = (event: MessageEvent) => {\n        if (event.data.messageId === messageId) {\n          this.worker.removeEventListener('message', handler);\n          \n          if (event.data.error) {\n            reject(new Error(event.data.error));\n          } else {\n            resolve(event.data.result);\n          }\n        }\n      };\n      \n      this.worker.addEventListener('message', handler);\n      \n      this.worker.postMessage({\n        messageId,\n        type: 'execute',\n        node,\n        inputs\n      });\n    });\n  }\n  \n  private buildExecutionGraph(): Map<string, string[]> {\n    const graph = new Map<string, string[]>();\n    \n    for (const node of this.nodes.values()) {\n      graph.set(node.id, []);\n    }\n    \n    for (const edge of this.edges) {\n      graph.get(edge.target)!.push(edge.source);\n    }\n    \n    return graph;\n  }\n  \n  private topologicalSort(graph: Map<string, string[]>): string[][] {\n    // Returns levels of nodes that can be executed in parallel\n    const inDegree = new Map<string, number>();\n    const levels: string[][] = [];\n    \n    // Calculate in-degrees\n    for (const [node, deps] of graph.entries()) {\n      inDegree.set(node, deps.length);\n    }\n    \n    while (inDegree.size > 0) {\n      // Find all nodes with in-degree 0\n      const currentLevel = Array.from(inDegree.entries())\n        .filter(([_, degree]) => degree === 0)\n        .map(([node, _]) => node);\n      \n      if (currentLevel.length === 0) {\n        throw new Error('Circular dependency detected in workflow');\n      }\n      \n      levels.push(currentLevel);\n      \n      // Remove these nodes and update in-degrees\n      for (const node of currentLevel) {\n        inDegree.delete(node);\n        \n        // Update dependents\n        for (const [otherNode, deps] of graph.entries()) {\n          if (deps.includes(node)) {\n            inDegree.set(otherNode, inDegree.get(otherNode)! - 1);\n          }\n        }\n      }\n    }\n    \n    return levels;\n  }\n  \n  private gatherInputs(nodeId: string): Record<string, any> {\n    const inputs: Record<string, any> = {};\n    \n    // Find all edges pointing to this node\n    const incomingEdges = this.edges.filter(e => e.target === nodeId);\n    \n    for (const edge of incomingEdges) {\n      const sourceOutput = this.executionState.globalContext[edge.source];\n      \n      // Apply any transformations defined on the edge\n      const transformedOutput = this.applyTransformations(\n        sourceOutput, \n        edge.data.transformations\n      );\n      \n      inputs[edge.source] = transformedOutput;\n    }\n    \n    return inputs;\n  }\n  \n  private applyTransformations(\n    data: any, \n    transformations?: Transformation[]\n  ): any {\n    if (!transformations || transformations.length === 0) {\n      return data;\n    }\n    \n    let result = data;\n    for (const transform of transformations) {\n      result = this.applyTransformation(result, transform);\n    }\n    return result;\n  }\n}\n```\n\n### Web Worker (execution.worker.ts)\n```typescript\n// Handles actual agent execution to avoid blocking UI\nself.addEventListener('message', async (event: MessageEvent) => {\n  const { messageId, type, node, inputs } = event.data;\n  \n  if (type === 'execute') {\n    try {\n      const result = await executeAgent(node, inputs);\n      \n      self.postMessage({\n        messageId,\n        result\n      });\n    } catch (error) {\n      self.postMessage({\n        messageId,\n        error: (error as Error).message\n      });\n    }\n  }\n});\n\nasync function executeAgent(\n  node: WorkflowNode, \n  inputs: Record<string, any>\n): Promise<AgentExecutionResult> {\n  const startTime = Date.now();\n  const logs: LogEntry[] = [];\n  \n  // Build the prompt with inputs\n  const prompt = buildPrompt(node, inputs);\n  \n  logs.push({\n    timestamp: new Date(),\n    level: 'info',\n    nodeId: node.id,\n    message: `Starting execution with model ${node.data.config.model}`,\n    metadata: { inputs }\n  });\n  \n  // Call the appropriate AI model\n  const response = await callAIModel(\n    node.data.config.model,\n    prompt,\n    node.data.config\n  );\n  \n  const executionTime = Date.now() - startTime;\n  \n  logs.push({\n    timestamp: new Date(),\n    level: 'info',\n    nodeId: node.id,\n    message: `Execution completed in ${executionTime}ms`,\n    metadata: { tokensUsed: response.tokensUsed }\n  });\n  \n  return {\n    output: response.output,\n    logs,\n    metrics: {\n      tokensUsed: response.tokensUsed,\n      executionTime,\n      retryCount: 0,\n      costEstimate: calculateCost(node.data.config.model, response.tokensUsed)\n    }\n  };\n}\n```\n\n## Real-Time Visualization\n\n### Execution Overlay Component\n```typescript\nconst ExecutionOverlay: React.FC = () => {\n  const executionState = useWorkflowStore(state => state.executionState);\n  \n  if (!executionState || executionState.status === 'idle') {\n    return null;\n  }\n  \n  return (\n    <Panel position=\"top-right\">\n      <div className=\"execution-panel\">\n        <div className=\"execution-header\">\n          <StatusIndicator status={executionState.status} />\n          <h3>Workflow Execution</h3>\n        </div>\n        \n        <div className=\"execution-metrics\">\n          <MetricCard \n            label=\"Duration\" \n            value={formatDuration(executionState.startedAt, executionState.completedAt)} \n          />\n          <MetricCard \n            label=\"Nodes Completed\" \n            value={`${getCompletedCount(executionState)}/${executionState.nodeStates.size}`} \n          />\n          <MetricCard \n            label=\"Total Cost\" \n            value={`$${calculateTotalCost(executionState).toFixed(4)}`} \n          />\n        </div>\n        \n        <div className=\"execution-timeline\">\n          <Timeline executionState={executionState} />\n        </div>\n        \n        {executionState.errors.length > 0 && (\n          <div className=\"execution-errors\">\n            <ErrorList errors={executionState.errors} />\n          </div>\n        )}\n      </div>\n    </Panel>\n  );\n};\n```\n\n### Live Log Viewer\n```typescript\nconst LogViewer: React.FC<{ logs: LogEntry[] }> = ({ logs }) => {\n  const [filter, setFilter] = useState<LogLevel | 'all'>('all');\n  const [search, setSearch] = useState('');\n  const scrollRef = useRef<HTMLDivElement>(null);\n  \n  // Auto-scroll to bottom on new logs\n  useEffect(() => {\n    if (scrollRef.current) {\n      scrollRef.current.scrollTop = scrollRef.current.scrollHeight;\n    }\n  }, [logs]);\n  \n  const filteredLogs = useMemo(() => {\n    return logs\n      .filter(log => filter === 'all' || log.level === filter)\n      .filter(log => \n        search === '' || \n        log.message.toLowerCase().includes(search.toLowerCase())\n      );\n  }, [logs, filter, search]);\n  \n  return (\n    <div className=\"log-viewer\">\n      <div className=\"log-controls\">\n        <Input \n          placeholder=\"Search logs...\" \n          value={search}\n          onChange={(e) => setSearch(e.target.value)}\n        />\n        <Select value={filter} onValueChange={setFilter}>\n          <SelectOption value=\"all\">All Levels</SelectOption>\n          <SelectOption value=\"debug\">Debug</SelectOption>\n          <SelectOption value=\"info\">Info</SelectOption>\n          <SelectOption value=\"warn\">Warning</SelectOption>\n          <SelectOption value=\"error\">Error</SelectOption>\n        </Select>\n      </div>\n      \n      <div className=\"log-entries\" ref={scrollRef}>\n        <VirtualList\n          items={filteredLogs}\n          height={400}\n          itemHeight={24}\n          renderItem={(log) => (\n            <LogEntry key={log.timestamp.getTime()} log={log} />\n          )}\n        />\n      </div>\n    </div>\n  );\n};\n\nconst LogEntry: React.FC<{ log: LogEntry }> = ({ log }) => {\n  return (\n    <div className={`log-entry log-${log.level}`}>\n      <span className=\"log-timestamp\">\n        {log.timestamp.toLocaleTimeString()}\n      </span>\n      <span className={`log-level level-${log.level}`}>\n        {log.level.toUpperCase()}\n      </span>\n      <span className=\"log-node\">[{log.nodeId}]</span>\n      <span className=\"log-message\">{log.message}</span>\n      {log.metadata && (\n        <Collapsible>\n          <CollapsibleTrigger>\n            <ChevronIcon />\n          </CollapsibleTrigger>\n          <CollapsibleContent>\n            <pre>{JSON.stringify(log.metadata, null, 2)}</pre>\n          </CollapsibleContent>\n        </Collapsible>\n      )}\n    </div>\n  );\n};\n```\n\n## Template Management\n\n### Template Structure\n```typescript\ninterface WorkflowTemplate {\n  id: string;\n  name: string;\n  description: string;\n  category: 'research' | 'coding' | 'review' | 'general';\n  thumbnail?: string;\n  workflow: Omit<Workflow, 'id' | 'createdAt' | 'updatedAt' | 'executionHistory'>;\n  variables: TemplateVariable[];\n}\n\ninterface TemplateVariable {\n  key: string;\n  label: string;\n  type: 'string' | 'number' | 'model' | 'prompt';\n  defaultValue?: any;\n  description?: string;\n}\n\n// Example template\nconst RESEARCH_PIPELINE_TEMPLATE: WorkflowTemplate = {\n  id: 'research-pipeline',\n  name: 'Research Pipeline',\n  description: 'Multi-stage research workflow with synthesis',\n  category: 'research',\n  variables: [\n    {\n      key: 'research_topic',\n      label: 'Research Topic',\n      type: 'string',\n      description: 'The topic to research'\n    },\n    {\n      key: 'primary_model',\n      label: 'Primary Model',\n      type: 'model',\n      defaultValue: 'gpt-4-turbo'\n    }\n  ],\n  workflow: {\n    name: 'Research Pipeline',\n    version: '1.0.0',\n    nodes: [\n      {\n        id: 'researcher-1',\n        type: 'researcher',\n        position: { x: 100, y: 100 },\n        data: {\n          id: 'researcher-1',\n          type: 'researcher',\n          label: 'Primary Researcher',\n          config: {\n            model: '{{primary_model}}',\n            temperature: 0.7,\n            systemPrompt: 'You are a thorough researcher. Research: {{research_topic}}'\n          },\n          status: 'idle',\n          logs: [],\n          metrics: {}\n        }\n      },\n      {\n        id: 'reviewer-1',\n        type: 'reviewer',\n        position: { x: 400, y: 100 },\n        data: {\n          id: 'reviewer-1',\n          type: 'reviewer',\n          label: 'Fact Checker',\n          config: {\n            model: 'claude-3-opus',\n            temperature: 0.3,\n            systemPrompt: 'Review research for accuracy and completeness.'\n          },\n          status: 'idle',\n          logs: [],\n          metrics: {}\n        }\n      }\n    ],\n    edges: [\n      {\n        id: 'e1',\n        source: 'researcher-1',\n        target: 'reviewer-1',\n        data: {\n          id: 'e1',\n          dataType: 'text',\n          status: 'idle'\n        }\n      }\n    ],\n    globalConfig: {\n      defaultModel: 'gpt-4-turbo',\n      defaultTemperature: 0.7,\n      maxConcurrentAgents: 3,\n      errorHandling: 'stop',\n      loggingLevel: 'info'\n    },\n    executionHistory: []\n  }\n};\n```\n\n### Template Manager\n```typescript\nclass TemplateManager {\n  private db: IDBDatabase;\n  \n  async saveTemplate(template: WorkflowTemplate): Promise<void> {\n    const tx = this.db.transaction('templates', 'readwrite');\n    const store = tx.objectStore('templates');\n    await store.put(template);\n  }\n  \n  async loadTemplate(templateId: string): Promise<WorkflowTemplate> {\n    const tx = this.db.transaction('templates', 'readonly');\n    const store = tx.objectStore('templates');\n    return await store.get(templateId);\n  }\n  \n  async listTemplates(category?: string): Promise<WorkflowTemplate[]> {\n    const tx = this.db.transaction('templates', 'readonly');\n    const store = tx.objectStore('templates');\n    const templates = await store.getAll();\n    \n    if (category) {\n      return templates.filter(t => t.category === category);\n    }\n    return templates;\n  }\n  \n  async instantiateTemplate(\n    templateId: string, \n    variables: Record<string, any>\n  ): Promise<Workflow> {\n    const template = await this.loadTemplate(templateId);\n    \n    // Deep clone the workflow\n    const workflow: Workflow = {\n      id: crypto.randomUUID(),\n      name: template.workflow.name,\n      description: template.workflow.description,\n      version: template.workflow.version,\n      createdAt: new Date(),\n      updatedAt: new Date(),\n      nodes: JSON.parse(JSON.stringify(template.workflow.nodes)),\n      edges: JSON.parse(JSON.stringify(template.workflow.edges)),\n      globalConfig: { ...template.workflow.globalConfig },\n      executionHistory: []\n    };\n    \n    // Replace variables\n    const workflowStr = JSON.stringify(workflow);\n    const replaced = this.replaceVariables(workflowStr, variables);\n    \n    return JSON.parse(replaced);\n  }\n  \n  private replaceVariables(\n    template: string, \n    variables: Record<string, any>\n  ): string {\n    let result = template;\n    for (const [key, value] of Object.entries(variables)) {\n      result = result.replace(\n        new RegExp(`{{${key}}}`, 'g'), \n        String(value)\n      );\n    }\n    return result;\n  }\n}\n```\n\n## Error Handling\n\n### Validation Service\n```typescript\ninterface ValidationResult {\n  valid: boolean;\n  errors: ValidationError[];\n  warnings: ValidationWarning[];\n}\n\ninterface ValidationError {\n  type: 'structure' | 'config' | 'connection';\n  nodeId?: string;\n  edgeId?: string;\n  message: string;\n  severity: 'error' | 'warning';\n}\n\nclass WorkflowValidator {\n  validate(workflow: Workflow): ValidationResult {\n    const errors: ValidationError[] = [];\n    const warnings: ValidationWarning[] = [];\n    \n    // Check for cycles\n    if (this.hasCycles(workflow)) {\n      errors.push({\n        type: 'structure',\n        message: 'Workflow contains circular dependencies',\n        severity: 'error'\n      });\n    }\n    \n    // Check for disconnected nodes\n    const disconnected = this.findDisconnectedNodes(workflow);\n    if (disconnected.length > 0) {\n      warnings.push({\n        type: 'structure',\n        message: `${disconnected.length} nodes are not connected`,\n        nodeIds: disconnected,\n        severity: 'warning'\n      });\n    }\n    \n    // Validate each node configuration\n    for (const node of workflow.nodes) {\n      const nodeErrors = this.validateNode(node);\n      errors.push(...nodeErrors);\n    }\n    \n    // Check for missing start nodes\n    const startNodes = this.findStartNodes(workflow);\n    if (startNodes.length === 0) {\n      errors.push({\n        type: 'structure',\n        message: 'Workflow has no starting nodes',\n        severity: 'error'\n      });\n    }\n    \n    return {\n      valid: errors.length === 0,\n      errors,\n      warnings\n    };\n  }\n  \n  private validateNode(node: WorkflowNode): ValidationError[] {\n    const errors: ValidationError[] = [];\n    \n    if (!node.data.config.model) {\n      errors.push({\n        type: 'config',\n        nodeId: node.id,\n        message: 'No model selected',\n        severity: 'error'\n      });\n    }\n    \n    if (!node.data.config.systemPrompt || node.data.config.systemPrompt.trim() === '') {\n      errors.push({\n        type: 'config',\n        nodeId: node.id,\n        message: 'System prompt is empty',\n        severity: 'error'\n      });\n    }\n    \n    if (node.data.config.temperature < 0 || node.data.config.temperature > 2) {\n      errors.push({\n        type: 'config',\n        nodeId: node.id,\n        message: 'Temperature must be between 0 and 2',\n        severity: 'error'\n      });\n    }\n    \n    return errors;\n  }\n  \n  private hasCycles(workflow: Workflow): boolean {\n    const visited = new Set<string>();\n    const recursionStack = new Set<string>();\n    \n    const adjacencyList = new Map<string, string[]>();\n    for (const node of workflow.nodes) {\n      adjacencyList.set(node.id, []);\n    }\n    for (const edge of workflow.edges) {\n      adjacencyList.get(edge.source)!.push(edge.target);\n    }\n    \n    const dfs = (nodeId: string): boolean => {\n      visited.add(nodeId);\n      recursionStack.add(nodeId);\n      \n      for (const neighbor of adjacencyList.get(nodeId) || []) {\n        if (!visited.has(neighbor)) {\n          if (dfs(neighbor)) return true;\n        } else if (recursionStack.has(neighbor)) {\n          return true;\n        }\n      }\n      \n      recursionStack.delete(nodeId);\n      return false;\n    };\n    \n    for (const node of workflow.nodes) {\n      if (!visited.has(node.id)) {\n        if (dfs(node.id)) return true;\n      }\n    }\n    \n    return false;\n  }\n  \n  private findDisconnectedNodes(workflow: Workflow): string[] {\n    const connectedNodes = new Set<string>();\n    \n    for (const edge of workflow.edges) {\n      connectedNodes.add(edge.source);\n      connectedNodes.add(edge.target);\n    }\n    \n    return workflow.nodes\n      .filter(node => !connectedNodes.has(node.id))\n      .map(node => node.id);\n  }\n  \n  private findStartNodes(workflow: Workflow): string[] {\n    const nodesWithInput = new Set(workflow.edges.map(e => e.target));\n    return workflow.nodes\n      .filter(node => !nodesWithInput.has(node.id))\n      .map(node => node.id);\n  }\n}\n```\n\n### Error Recovery\n```typescript\nclass ErrorRecoveryService {\n  async handleNodeError(\n    nodeId: string,\n    error: Error,\n    config: GlobalConfig\n  ): Promise<RecoveryAction> {\n    \n    switch (config.errorHandling) {\n      case 'stop':\n        return { action: 'stop', message: 'Execution stopped due to error' };\n        \n      case 'retry':\n        if (this.isRetryableError(error)) {\n          return { \n            action: 'retry', \n            maxRetries: 3,\n            backoff: 'exponential',\n            message: 'Retrying node execution' \n          };\n        }\n        return { action: 'skip', message: 'Error not retryable, skipping node' };\n        \n      case 'continue':\n        return { \n          action: 'continue', \n          message: 'Continuing with next nodes',\n          skipDownstream: false\n        };\n        \n      default:\n        return { action: 'stop', message: 'Unknown error handling strategy' };\n    }\n  }\n  \n  private isRetryableError(error: Error): boolean {\n    // Network errors, rate limits, temporary API failures\n    return error.message.includes('ECONNREFUSED') ||\n           error.message.includes('rate limit') ||\n           error.message.includes('timeout') ||\n           error.message.includes('429');\n  }\n  \n  async retryWithBackoff(\n    fn: () => Promise<any>,\n    maxRetries: number,\n    backoff: 'linear' | 'exponential' = 'exponential'\n  ): Promise<any> {\n    let lastError: Error;\n    \n    for (let i = 0; i < maxRetries; i++) {\n      try {\n        return await fn();\n      } catch (error) {\n        lastError = error as Error;\n        \n        if (i < maxRetries - 1) {\n          const delay = backoff === 'exponential' \n            ? Math.pow(2, i) * 1000 \n            : (i + 1) * 1000;\n          \n          await new Promise(resolve => setTimeout(resolve, delay));\n        }\n      }\n    }\n    \n    throw lastError!;\n  }\n}\n```\n\n## Persistence Layer\n\n### IndexedDB Schema\n```typescript\nconst DB_NAME = 'workflow-builder';\nconst DB_VERSION = 1;\n\ninterface DBSchema {\n  workflows: {\n    key: string;\n    value: Workflow;\n    indexes: {\n      'by-updated': Date;\n      'by-name': string;\n    };\n  };\n  templates: {\n    key: string;\n    value: WorkflowTemplate;\n    indexes: {\n      'by-category': string;\n    };\n  };\n  executions: {\n    key: string;\n    value: ExecutionRecord;\n    indexes: {\n      'by-workflow': string;\n      'by-date': Date;\n    };\n  };\n}\n\nclass WorkflowDatabase {\n  private db: IDBDatabase | null = null;\n  \n  async init(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open(DB_NAME, DB_VERSION);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        // Workflows store\n        if (!db.objectStoreNames.contains('workflows')) {\n          const workflowStore = db.createObjectStore('workflows', { keyPath: 'id' });\n          workflowStore.createIndex('by-updated', 'updatedAt');\n          workflowStore.createIndex('by-name', 'name');\n        }\n        \n        // Templates store\n        if (!db.objectStoreNames.contains('templates')) {\n          const templateStore = db.createObjectStore('templates', { keyPath: 'id' });\n          templateStore.createIndex('by-category', 'category');\n        }\n        \n        // Executions store\n        if (!db.objectStoreNames.contains('executions')) {\n          const executionStore = db.createObjectStore('executions', { keyPath: 'id' });\n          executionStore.createIndex('by-workflow', 'workflowId');\n          executionStore.createIndex('by-date', 'startedAt');\n        }\n      };\n    });\n  }\n  \n  async saveWorkflow(workflow: Workflow): Promise<void> {\n    const tx = this.db!.transaction('workflows', 'readwrite');\n    const store = tx.objectStore('workflows');\n    workflow.updatedAt = new Date();\n    await store.put(workflow);\n  }\n  \n  async loadWorkflow(workflowId: string): Promise<Workflow | undefined> {\n    const tx = this.db!.transaction('workflows', 'readonly');\n    const store = tx.objectStore('workflows');\n    return await store.get(workflowId);\n  }\n  \n  async listWorkflows(): Promise<Workflow[]> {\n    const tx = this.db!.transaction('workflows', 'readonly');\n    const store = tx.objectStore('workflows');\n    const index = store.index('by-updated');\n    return await index.getAll();\n  }\n  \n  async deleteWorkflow(workflowId: string): Promise<void> {\n    const tx = this.db!.transaction('workflows', 'readwrite');\n    const store = tx.objectStore('workflows');\n    await store.delete(workflowId);\n  }\n}\n```\n\n## Performance Optimizations\n\n1. **Virtual Scrolling**: Use react-window for log viewer with thousands of entries\n2. **Memoization**: Memoize expensive node renderings with React.memo\n3. **Web Workers**: Execute agents in web workers to avoid blocking UI\n4. **Debouncing**: Debounce property panel updates to avoid excessive re-renders\n5. **Lazy Loading**: Load execution history and logs on demand\n6. **Canvas Optimization**: Use React Flow's built-in performance optimizations (viewport culling)\n7. **IndexedDB**: Async storage to avoid blocking main thread\n8. **Edge Batching**: Batch edge updates during execution to reduce re-renders\n\n## Rationale\n\n**Why React Flow?** \n- Battle-tested with 20k+ GitHub stars\n- Excellent performance with large graphs\n- Built-in features (minimap, controls, zoom, pan)\n- Extensible with custom nodes and edges\n- Active community and good documentation\n\n**Why Zustand over Redux?**\n- Less boilerplate (no actions/reducers)\n- Better TypeScript support\n- Simpler middleware for persistence\n- Better performance (no unnecessary re-renders)\n- Easier to test\n\n**Why Web Workers for execution?**\n- Prevents UI blocking during long agent executions\n- Allows true parallel execution of independent agents\n- Better error isolation\n- Can be terminated if needed\n\n**Why IndexedDB over localStorage?**\n- Can store larger workflows (>5MB)\n- Async API doesn't block main thread\n- Better structured query support\n- Can store binary data (future feature: screenshots)\n\n**Topological execution with parallelization**\n- Maximizes throughput by running independent agents concurrently\n- Respects data dependencies automatically\n- Easy to visualize execution progress\n\n## What I Changed\n\nThis is the initial design, so there are no changes from a previous iteration.\n\n## What I Kept\n\nThis is the initial design, so there are no previous ideas to keep.\n\n## Open Questions\n\n1. **Streaming vs. Batch**: Should agent outputs stream in real-time or return all at once? Streaming is more engaging but adds complexity.\n\n2. **Agent Communication Protocol**: Should agents communicate through structured JSON or free-form text? JSON is more parseable but less flexible.\n\n3. **Cost Management**: Should we add hard limits on token usage per workflow or just show estimates?\n\n4. **Collaboration**: Should workflows be shareable via URL with a backend, or stay purely local?\n\n5. **Version Control**: Should we track workflow versions with diffs, or just save snapshots?\n\n## Convergence Status\n\n**ITERATING** - This is the first draft and requires review for:\n- Missing edge cases in execution engine\n- UI/UX improvements for node configuration\n- Additional agent types or capabilities\n- Performance concerns with large workflows\n\nPROMPT_FOR_CRITIC:\nPlease focus your review on:\n1. **Execution engine design** - Are there race conditions or edge cases I'm missing?\n2. **Data flow between agents** - Is the input/output mechanism clear and flexible enough?\n3. **Error handling** - What failure modes am I not considering?\n4. **Real-time visualization** - Are there better ways to show execution status?"
      },
      "agent_b": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Enhanced Design\n\n## Architecture Overview\n\n### High-Level Architecture\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502     \u2502\n\u2502  \u2502   Layer      \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           Workflow State Management (Zustand)               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Execution  \u2502  \u2502  Template    \u2502  \u2502   Validation \u2502     \u2502\n\u2502  \u2502   Orchestrator\u2502  \u2502  Manager     \u2502  \u2502   Service    \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502  \u2502  Data Flow   \u2502  \u2502  Error       \u2502                        \u2502\n\u2502  \u2502  Controller  \u2502  \u2502  Recovery    \u2502                        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas Library**: React Flow (react-flow-renderer)\n- **State Management**: Zustand with immer middleware\n- **UI Components**: Radix UI + Tailwind CSS\n- **Execution**: Web Workers with SharedArrayBuffer for progress tracking\n- **Storage**: IndexedDB with automatic sync queue for reliability\n- **Real-time Updates**: EventEmitter pattern with typed events\n\n## Core Components\n\n### 1. Canvas Component (WorkflowCanvas.tsx)\n```typescript\ninterface WorkflowCanvasProps {\n  workflowId: string;\n  readonly?: boolean;\n}\n\nconst WorkflowCanvas: React.FC<WorkflowCanvasProps> = ({ \n  workflowId, \n  readonly = false \n}) => {\n  const { nodes, edges, onNodesChange, onEdgesChange, onConnect } = \n    useWorkflowStore();\n  const executionState = useExecutionState();\n  \n  // Handle drop from palette\n  const onDrop = useCallback((event: React.DragEvent) => {\n    event.preventDefault();\n    const type = event.dataTransfer.getData('application/reactflow');\n    const position = screenToFlowPosition({\n      x: event.clientX,\n      y: event.clientY,\n    });\n    \n    addNode(type, position);\n  }, []);\n  \n  const onDragOver = useCallback((event: React.DragEvent) => {\n    event.preventDefault();\n    event.dataTransfer.dropEffect = 'move';\n  }, []);\n  \n  return (\n    <ReactFlow\n      nodes={nodes}\n      edges={edges}\n      onNodesChange={onNodesChange}\n      onEdgesChange={onEdgesChange}\n      onConnect={onConnect}\n      onDrop={onDrop}\n      onDragOver={onDragOver}\n      nodeTypes={customNodeTypes}\n      edgeTypes={customEdgeTypes}\n      fitView\n      minZoom={0.2}\n      maxZoom={2}\n      deleteKeyCode=\"Delete\"\n      selectionKeyCode=\"Shift\"\n      multiSelectionKeyCode=\"Control\"\n    >\n      <Background />\n      <Controls />\n      <MiniMap />\n      <ExecutionOverlay />\n      <SelectionToolbar />\n    </ReactFlow>\n  );\n};\n```\n\n### 2. Agent Node Component (AgentNode.tsx)\n```typescript\ninterface AgentNodeData {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator' | 'transformer';\n  label: string;\n  config: AgentConfig;\n  status: ExecutionStatus;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n  inputs: Record<string, InputDefinition>;  // ENHANCED: Define expected inputs\n  outputs: Record<string, OutputDefinition>; // ENHANCED: Define outputs with schema\n}\n\ninterface InputDefinition {\n  name: string;\n  type: 'text' | 'json' | 'code' | 'artifacts' | 'stream';\n  required: boolean;\n  schema?: JSONSchema; // For validation\n  description?: string;\n}\n\ninterface OutputDefinition {\n  name: string;\n  type: 'text' | 'json' | 'code' | 'artifacts' | 'stream';\n  schema?: JSONSchema;\n  description?: string;\n}\n\ninterface AgentConfig {\n  model: string;\n  temperature: number;\n  systemPrompt: string;\n  maxTokens?: number;\n  timeout?: number;\n  retryConfig?: RetryConfig;\n  streaming?: boolean; // ENHANCED: Support streaming output\n  memoryLimit?: number; // ENHANCED: Prevent memory leaks\n}\n\ninterface RetryConfig {\n  maxAttempts: number;\n  backoffStrategy: 'linear' | 'exponential' | 'fixed';\n  initialDelayMs: number;\n  maxDelayMs: number;\n  retryableErrors: string[]; // Specific error patterns to retry\n}\n\nconst AgentNode: React.FC<NodeProps<AgentNodeData>> = ({ data, selected }) => {\n  const [isExpanded, setIsExpanded] = useState(false);\n  const streamingOutput = useStreamingOutput(data.id);\n  \n  const getStatusColor = () => {\n    switch (data.status) {\n      case 'idle': return 'gray';\n      case 'queued': return 'cyan'; // ENHANCED: New status\n      case 'running': return 'blue';\n      case 'success': return 'green';\n      case 'error': return 'red';\n      case 'warning': return 'orange'; // ENHANCED: Partial success\n      case 'waiting': return 'yellow';\n      case 'cancelled': return 'gray'; // ENHANCED: User cancelled\n    }\n  };\n\n  return (\n    <div \n      className={`agent-node ${selected ? 'selected' : ''} ${isExpanded ? 'expanded' : ''}`}\n      data-node-id={data.id}\n    >\n      {/* Multiple input handles for different data types */}\n      {Object.entries(data.inputs).map(([key, input]) => (\n        <Handle\n          key={`input-${key}`}\n          type=\"target\"\n          position={Position.Left}\n          id={`input-${key}`}\n          style={{ top: calculateHandlePosition(key, data.inputs) }}\n        >\n          <Tooltip content={`${input.name} (${input.type})`}>\n            <HandleLabel required={input.required}>{input.name}</HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n      \n      <div className=\"node-header\" style={{ borderColor: getStatusColor() }}>\n        <AgentIcon type={data.type} />\n        <span className=\"node-label\" contentEditable onBlur={handleLabelUpdate}>\n          {data.label}\n        </span>\n        <StatusBadge status={data.status} />\n        <Button \n          size=\"xs\" \n          variant=\"ghost\"\n          onClick={() => setIsExpanded(!isExpanded)}\n        >\n          {isExpanded ? <ChevronUpIcon /> : <ChevronDownIcon />}\n        </Button>\n      </div>\n      \n      <div className=\"node-body\">\n        <div className=\"config-summary\">\n          <span className=\"model-badge\">{data.config.model}</span>\n          <span className=\"temp-badge\">T: {data.config.temperature}</span>\n          {data.config.streaming && <span className=\"stream-badge\">\u26a1 Stream</span>}\n        </div>\n        \n        {data.status === 'running' && (\n          <>\n            <ProgressBar metrics={data.metrics} />\n            {/* ENHANCED: Show streaming output preview */}\n            {data.config.streaming && streamingOutput && (\n              <StreamPreview content={streamingOutput} />\n            )}\n          </>\n        )}\n        \n        {/* ENHANCED: Show validation errors */}\n        {data.status === 'error' && (\n          <ErrorSummary error={data.logs.find(l => l.level === 'error')} />\n        )}\n        \n        {isExpanded && (\n          <div className=\"node-details\">\n            <LogPreview logs={data.logs.slice(-5)} />\n            <MetricsDisplay metrics={data.metrics} />\n          </div>\n        )}\n      </div>\n      \n      {/* Multiple output handles for different data types */}\n      {Object.entries(data.outputs).map(([key, output]) => (\n        <Handle\n          key={`output-${key}`}\n          type=\"source\"\n          position={Position.Right}\n          id={`output-${key}`}\n          style={{ top: calculateHandlePosition(key, data.outputs) }}\n        >\n          <Tooltip content={`${output.name} (${output.type})`}>\n            <HandleLabel>{output.name}</HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n    </div>\n  );\n};\n```\n\n### 3. Connection Edge Component (DataFlowEdge.tsx)\n```typescript\ninterface EdgeData {\n  id: string;\n  sourceHandle: string; // ENHANCED: Track which output\n  targetHandle: string; // ENHANCED: Track which input\n  dataType: 'text' | 'json' | 'code' | 'artifacts' | 'stream';\n  transformations?: Transformation[];\n  validation?: ValidationRule[]; // ENHANCED: Validate data flow\n  status: 'idle' | 'active' | 'complete' | 'error';\n  dataPreview?: string;\n  bytesTransferred?: number; // ENHANCED: Track data size\n}\n\ninterface ValidationRule {\n  type: 'schema' | 'size' | 'format' | 'custom';\n  config: any;\n  errorMessage: string;\n}\n\nconst DataFlowEdge: React.FC<EdgeProps<EdgeData>> = ({ \n  id, \n  source, \n  target, \n  sourceX,\n  sourceY,\n  targetX,\n  targetY,\n  data,\n  style,\n  markerEnd\n}) => {\n  const [edgePath, labelX, labelY] = getBezierPath({\n    sourceX, \n    sourceY, \n    targetX, \n    targetY,\n    sourcePosition: Position.Right,\n    targetPosition: Position.Left\n  });\n\n  // ENHANCED: Animate data flow\n  const animationProgress = useAnimatedDataFlow(data.status);\n\n  return (\n    <>\n      {/* Base edge path */}\n      <path\n        id={id}\n        className=\"react-flow__edge-path\"\n        d={edgePath}\n        style={{ \n          ...style, \n          stroke: data.status === 'error' ? '#ef4444' : \n                  data.status === 'active' ? '#3b82f6' : '#6b7280',\n          strokeWidth: data.status === 'active' ? 3 : 2,\n        }} \n        markerEnd={markerEnd}\n      />\n      \n      {/* ENHANCED: Animated data flow indicator */}\n      {data.status === 'active' && (\n        <circle\n          r=\"4\"\n          fill=\"#3b82f6\"\n        >\n          <animateMotion\n            dur=\"2s\"\n            repeatCount=\"indefinite\"\n            path={edgePath}\n          />\n        </circle>\n      )}\n      \n      <EdgeLabelRenderer>\n        <div\n          style={{ \n            position: 'absolute', \n            transform: `translate(-50%, -50%) translate(${labelX}px,${labelY}px)`,\n            pointerEvents: 'all'\n          }}\n          className=\"edge-label\"\n        >\n          <DataTypeIndicator type={data.dataType} />\n          \n          {/* ENHANCED: Show transformations applied */}\n          {data.transformations && data.transformations.length > 0 && (\n            <Tooltip content={\n              <div>\n                <strong>Transformations:</strong>\n                <ul>\n                  {data.transformations.map((t, i) => (\n                    <li key={i}>{t.type}</li>\n                  ))}\n                </ul>\n              </div>\n            }>\n              <TransformBadge count={data.transformations.length} />\n            </Tooltip>\n          )}\n          \n          {/* ENHANCED: Show validation errors */}\n          {data.status === 'error' && (\n            <Tooltip content=\"Data validation failed\">\n              <ErrorIcon className=\"text-red-500\" />\n            </Tooltip>\n          )}\n          \n          {/* ENHANCED: Show data size for large transfers */}\n          {data.bytesTransferred && data.bytesTransferred > 1024 && (\n            <Tooltip content={`${formatBytes(data.bytesTransferred)} transferred`}>\n              <DataSizeBadge size={data.bytesTransferred} />\n            </Tooltip>\n          )}\n          \n          {data.dataPreview && (\n            <Popover>\n              <PopoverTrigger>\n                <Button size=\"xs\" variant=\"ghost\">\n                  <EyeIcon />\n                </Button>\n              </PopoverTrigger>\n              <PopoverContent>\n                <DataPreview content={data.dataPreview} type={data.dataType} />\n              </PopoverContent>\n            </Popover>\n          )}\n        </div>\n      </EdgeLabelRenderer>\n    </>\n  );\n};\n```\n\n### 4. Properties Panel (PropertiesPanel.tsx)\n```typescript\nconst PropertiesPanel: React.FC = () => {\n  const { selectedNode, selectedEdge, updateNodeConfig, updateEdgeConfig } = \n    useWorkflowStore();\n  \n  if (!selectedNode && !selectedEdge) {\n    return <EmptyState message=\"Select a node or connection to configure\" />;\n  }\n\n  if (selectedEdge) {\n    return <EdgePropertiesPanel edge={selectedEdge} />;\n  }\n\n  return (\n    <div className=\"properties-panel\">\n      <Tabs defaultValue=\"config\">\n        <TabsList>\n          <TabsTrigger value=\"config\">Configuration</TabsTrigger>\n          <TabsTrigger value=\"prompt\">System Prompt</TabsTrigger>\n          <TabsTrigger value=\"io\">Inputs/Outputs</TabsTrigger> {/* ENHANCED */}\n          <TabsTrigger value=\"advanced\">Advanced</TabsTrigger>\n          <TabsTrigger value=\"logs\">Logs</TabsTrigger>\n        </TabsList>\n        \n        <TabsContent value=\"config\">\n          <ConfigurationTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"prompt\">\n          <PromptEditor \n            value={selectedNode.config.systemPrompt}\n            onChange={(prompt) => updateNodeConfig(selectedNode.id, { systemPrompt: prompt })}\n            variables={getAvailableVariables(selectedNode)}\n            testMode // ENHANCED: Test prompts before execution\n          />\n        </TabsContent>\n        \n        <TabsContent value=\"io\">\n          <IOConfigTab node={selectedNode} /> {/* ENHANCED: Configure I/O */}\n        </TabsContent>\n        \n        <TabsContent value=\"advanced\">\n          <AdvancedSettingsTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"logs\">\n          <LogViewer logs={selectedNode.logs} nodeId={selectedNode.id} />\n        </TabsContent>\n      </Tabs>\n    </div>\n  );\n};\n\n// ENHANCED: Configure inputs and outputs with schemas\nconst IOConfigTab: React.FC<{ node: AgentNodeData }> = ({ node }) => {\n  const { updateNodeIO } = useWorkflowStore();\n  \n  return (\n    <div className=\"io-config\">\n      <Section title=\"Input Definitions\">\n        {Object.entries(node.inputs).map(([key, input]) => (\n          <InputConfig\n            key={key}\n            input={input}\n            onChange={(updated) => updateNodeIO(node.id, 'input', key, updated)}\n            onDelete={() => deleteNodeIO(node.id, 'input', key)}\n          />\n        ))}\n        <Button onClick={() => addNodeIO(node.id, 'input')}>\n          <PlusIcon /> Add Input\n        </Button>\n      </Section>\n      \n      <Section title=\"Output Definitions\">\n        {Object.entries(node.outputs).map(([key, output]) => (\n          <OutputConfig\n            key={key}\n            output={output}\n            onChange={(updated) => updateNodeIO(node.id, 'output', key, updated)}\n            onDelete={() => deleteNodeIO(node.id, 'output', key)}\n          />\n        ))}\n        <Button onClick={() => addNodeIO(node.id, 'output')}>\n          <PlusIcon /> Add Output\n        </Button>\n      </Section>\n    </div>\n  );\n};\n\n// ENHANCED: Edge properties for transformations and validation\nconst EdgePropertiesPanel: React.FC<{ edge: WorkflowEdge }> = ({ edge }) => {\n  const { updateEdgeConfig } = useWorkflowStore();\n  \n  return (\n    <div className=\"edge-properties\">\n      <h3>Connection Properties</h3>\n      \n      <FormField label=\"Data Type\">\n        <Select \n          value={edge.data.dataType}\n          onChange={(type) => updateEdgeConfig(edge.id, { dataType: type })}\n        >\n          <SelectOption value=\"text\">Text</SelectOption>\n          <SelectOption value=\"json\">JSON</SelectOption>\n          <SelectOption value=\"code\">Code</SelectOption>\n          <SelectOption value=\"artifacts\">Artifacts</SelectOption>\n          <SelectOption value=\"stream\">Stream</SelectOption>\n        </Select>\n      </FormField>\n      \n      <Section title=\"Transformations\">\n        <TransformationBuilder\n          transformations={edge.data.transformations || []}\n          onChange={(transforms) => updateEdgeConfig(edge.id, { transformations: transforms })}\n        />\n      </Section>\n      \n      <Section title=\"Validation Rules\">\n        <ValidationBuilder\n          rules={edge.data.validation || []}\n          onChange={(rules) => updateEdgeConfig(edge.id, { validation: rules })}\n        />\n      </Section>\n    </div>\n  );\n};\n```\n\n## Data Models\n\n### Enhanced Workflow Schema\n```typescript\ninterface Workflow {\n  id: string;\n  name: string;\n  description?: string;\n  version: string;\n  createdAt: Date;\n  updatedAt: Date;\n  nodes: WorkflowNode[];\n  edges: WorkflowEdge[];\n  globalConfig: GlobalConfig;\n  executionHistory: ExecutionRecord[];\n  metadata: WorkflowMetadata; // ENHANCED\n}\n\ninterface WorkflowMetadata {\n  author?: string;\n  tags: string[];\n  category?: string;\n  estimatedCost?: number;\n  averageRuntime?: number;\n  successRate?: number;\n}\n\ninterface GlobalConfig {\n  defaultModel: string;\n  defaultTemperature: number;\n  maxConcurrentAgents: number;\n  errorHandling: 'stop' | 'continue' | 'retry';\n  loggingLevel: 'debug' | 'info' | 'warn' | 'error';\n  resourceLimits: ResourceLimits; // ENHANCED\n  notifications: NotificationConfig; // ENHANCED\n}\n\ninterface ResourceLimits {\n  maxTokensPerNode?: number;\n  maxTotalTokens?: number;\n  maxExecutionTimeMs?: number;\n  maxMemoryMB?: number;\n  maxConcurrentConnections?: number;\n}\n\ninterface NotificationConfig {\n  onComplete: boolean;\n  onError: boolean;\n  onWarning: boolean;\n  channels: ('browser' | 'email' | 'webhook')[];\n  webhookUrl?: string;\n}\n```\n\n### Enhanced Execution State\n```typescript\ninterface ExecutionState {\n  workflowId: string;\n  executionId: string; // ENHANCED: Unique execution ID\n  status: 'idle' | 'queued' | 'running' | 'paused' | 'completed' | 'failed' | 'cancelled';\n  startedAt?: Date;\n  completedAt?: Date;\n  currentNode?: string;\n  nodeStates: Map<string, NodeExecutionState>;\n  edgeStates: Map<string, EdgeExecutionState>; // ENHANCED: Track edge execution\n  globalContext: ExecutionContext; // ENHANCED: Structured context\n  errors: ExecutionError[];\n  warnings: ExecutionWarning[]; // ENHANCED\n  checkpoints: ExecutionCheckpoint[]; // ENHANCED: Resume capability\n  resourceUsage: ResourceUsage; // ENHANCED\n}\n\ninterface ExecutionContext {\n  variables: Record<string, any>;\n  secrets: Map<string, string>; // ENHANCED: Secure storage\n  sharedMemory: SharedArrayBuffer; // ENHANCED: For worker communication\n  artifacts: Map<string, Artifact>; // ENHANCED: Large objects\n}\n\ninterface EdgeExecutionState {\n  edgeId: string;\n  status: 'idle' | 'active' | 'complete' | 'error';\n  startedAt?: Date;\n  completedAt?: Date;\n  data?: any;\n  validationErrors?: ValidationError[];\n  bytesTransferred?: number;\n}\n\ninterface ExecutionCheckpoint {\n  timestamp: Date;\n  nodeStates: Map<string, NodeExecutionState>;\n  globalContext: ExecutionContext;\n  reason: 'manual' | 'auto' | 'error';\n}\n\ninterface ResourceUsage {\n  totalTokens: number;\n  totalCost: number;\n  peakMemoryMB: number;\n  executionTimeMs: number;\n  apiCallCount: number;\n}\n\ninterface ExecutionWarning {\n  nodeId: string;\n  timestamp: Date;\n  type: 'performance' | 'cost' | 'resource' | 'deprecation';\n  message: string;\n  severity: 'low' | 'medium' | 'high';\n  suggestion?: string;\n}\n```\n\n## Enhanced State Management\n\n```typescript\ninterface WorkflowStore {\n  // Current workflow state\n  workflow: Workflow | null;\n  nodes: Node<AgentNodeData>[];\n  edges: Edge<EdgeData>[];\n  selectedNode: AgentNodeData | null;\n  selectedEdge: EdgeData | null; // ENHANCED\n  \n  // Execution state\n  executionState: ExecutionState | null;\n  isExecuting: boolean;\n  executionQueue: string[]; // ENHANCED: Queue multiple workflows\n  \n  // ENHANCED: Undo/Redo\n  history: {\n    past: WorkflowSnapshot[];\n    future: WorkflowSnapshot[];\n  };\n  \n  // Actions\n  setWorkflow: (workflow: Workflow) => void;\n  addNode: (type: string, position: XYPosition) => void;\n  updateNode: (nodeId: string, updates: Partial<AgentNodeData>) => void;\n  deleteNode: (nodeId: string) => void;\n  addEdge: (connection: Connection) => void;\n  updateEdge: (edgeId: string, updates: Partial<EdgeData>) => void; // ENHANCED\n  deleteEdge: (edgeId: string) => void;\n  selectNode: (nodeId: string | null) => void;\n  selectEdge: (edgeId: string | null) => void; // ENHANCED\n  \n  // ENHANCED: Undo/Redo\n  undo: () => void;\n  redo: () => void;\n  canUndo: () => boolean;\n  canRedo: () => boolean;\n  \n  // Execution\n  executeWorkflow: (config?: ExecutionConfig) => Promise<ExecutionResult>;\n  pauseExecution: () => Promise<void>;\n  resumeExecution: () => Promise<void>;\n  stopExecution: () => Promise<void>;\n  cancelExecution: () => Promise<void>; // ENHANCED\n  executeNode: (nodeId: string, inputs?: any) => Promise<NodeExecutionResult>;\n  createCheckpoint: () => void; // ENHANCED\n  restoreCheckpoint: (checkpointId: string) => Promise<void>; // ENHANCED\n  \n  // Persistence\n  saveWorkflow: () => Promise<void>;\n  loadWorkflow: (workflowId: string) => Promise<void>;\n  exportWorkflow: (format: 'json' | 'yaml') => string; // ENHANCED\n  importWorkflow: (data: string, format: 'json' | 'yaml') => Promise<void>; // ENHANCED\n  saveTemplate: (name: string) => Promise<void>;\n  loadTemplate: (templateId: string) => Promise<void>;\n  \n  // Validation\n  validateWorkflow: () => ValidationResult;\n  validateNode: (nodeId: string) => ValidationResult; // ENHANCED\n  validateEdge: (edgeId: string) => ValidationResult; // ENHANCED\n  \n  // ENHANCED: Event system\n  events: EventEmitter<WorkflowEvents>;\n}\n\ninterface WorkflowEvents {\n  'node:added': (node: WorkflowNode) => void;\n  'node:updated': (nodeId: string, updates: Partial<AgentNodeData>) => void;\n  'node:deleted': (nodeId: string) => void;\n  'edge:added': (edge: WorkflowEdge) => void;\n  'edge:deleted': (edgeId: string) => void;\n  'execution:started': (executionId: string) => void;\n  'execution:node:start': (nodeId: string) => void;\n  'execution:node:complete': (nodeId: string, result: any) => void;\n  'execution:node:error': (nodeId: string, error: Error) => void;\n  'execution:complete': (executionId: string, result: ExecutionResult) => void;\n  'execution:failed': (executionId: string, error: Error) => void;\n  'execution:cancelled': (executionId: string) => void;\n}\n\nconst useWorkflowStore = create<WorkflowStore>()(\n  immer((set, get) => ({\n    workflow: null,\n    nodes: [],\n    edges: [],\n    selectedNode: null,\n    selectedEdge: null,\n    executionState: null,\n    isExecuting: false,\n    executionQueue: [],\n    history: { past: [], future: [] },\n    events: new EventEmitter(),\n    \n    // ENHANCED: Snapshot before mutations for undo/redo\n    _snapshot: () => {\n      const state = get();\n      const snapshot: WorkflowSnapshot = {\n        nodes: [...state.nodes],\n        edges: [...state.edges],\n        workflow: state.workflow ? { ...state.workflow } : null,\n        timestamp: Date.now()\n      };\n      \n      set((draft) => {\n        draft.history.past.push(snapshot);\n        // Limit history size\n        if (draft.history.past.length > 50) {\n          draft.history.past.shift();\n        }\n        draft.history.future = [];\n      });\n    },\n    \n    addNode: (type, position) => {\n      const state = get();\n      state._snapshot();\n      \n      set((draft) => {\n        const newNode = createDefaultNode(type, position);\n        draft.nodes.push(newNode);\n        draft.events.emit('node:added', newNode);\n      });\n    },\n    \n    undo: () => {\n      set((draft) => {\n        if (draft.history.past.length === 0) return;\n        \n        const current: WorkflowSnapshot = {\n          nodes: draft.nodes,\n          edges: draft.edges,\n          workflow: draft.workflow,\n          timestamp: Date.now()\n        };\n        \n        draft.history.future.push(current);\n        const previous = draft.history.past.pop()!;\n        \n        draft.nodes = previous.nodes;\n        draft.edges = previous.edges;\n        draft.workflow = previous.workflow;\n      });\n    },\n    \n    redo: () => {\n      set((draft) => {\n        if (draft.history.future.length === 0) return;\n        \n        const current: WorkflowSnapshot = {\n          nodes: draft.nodes,\n          edges: draft.edges,\n          workflow: draft.workflow,\n          timestamp: Date.now()\n        };\n        \n        draft.history.past.push(current);\n        const next = draft.history.future.pop()!;\n        \n        draft.nodes = next.nodes;\n        draft.edges = next.edges;\n        draft.workflow = next.workflow;\n      });\n    },\n    \n    // Implementation details...\n  }))\n);\n```\n\n## Enhanced Execution Engine\n\n### Execution Orchestrator (Major Improvement)\n```typescript\nclass WorkflowOrchestrator {\n  private workflowId: string;\n  private executionId: string;\n  private nodes: Map<string, WorkflowNode>;\n  private edges: WorkflowEdge[];\n  private executionState: ExecutionState;\n  private workerPool: WorkerPool; // ENHANCED: Pool of workers\n  private dataFlowController: DataFlowController; // ENHANCED: Manage data transfer\n  private checkpointManager: CheckpointManager; // ENHANCED: Save/restore state\n  private eventBus: EventEmitter;\n  private abortController: AbortController; // ENHANCED: Cancellation support\n  \n  constructor(workflow: Workflow, config?: ExecutionConfig) {\n    this.workflowId = workflow.id;\n    this.executionId = crypto.randomUUID();\n    this.nodes = new Map(workflow.nodes.map(n => [n.id, n]));\n    this.edges = workflow.edges;\n    this.executionState = this.initExecutionState();\n    this.workerPool = new WorkerPool(config?.maxWorkers || 4);\n    this.dataFlowController = new DataFlowController();\n    this.checkpointManager = new CheckpointManager();\n    this.eventBus = new EventEmitter();\n    this.abortController = new AbortController();\n  }\n  \n  async execute(): Promise<ExecutionResult> {\n    this.executionState.status = 'running';\n    this.executionState.startedAt = new Date();\n    this.eventBus.emit('execution:started', this.executionId);\n    \n    try {\n      // ENHANCED: Pre-execution validation\n      const validation = await this.validateBeforeExecution();\n      if (!validation.valid) {\n        throw new Error(`Validation failed: ${validation.errors.map(e => e.message).join(', ')}`);\n      }\n      \n      // Build execution DAG\n      const executionDAG = this.buildExecutionDAG();\n      \n      // ENHANCED: Check for resource availability\n      await this.checkResourceAvailability();\n      \n      // Topological sort with parallel levels\n      const executionLevels = this.topologicalSort(executionDAG);\n      \n      // Execute levels with proper error handling and checkpoints\n      for (let levelIndex = 0; levelIndex < executionLevels.length; levelIndex++) {\n        const level = executionLevels[levelIndex];\n        \n        // ENHANCED: Auto-checkpoint at each level\n        if (levelIndex > 0 && levelIndex % 3 === 0) {\n          await this.checkpointManager.createCheckpoint(this.executionState);\n        }\n        \n        // ENHANCED: Check for cancellation\n        if (this.abortController.signal.aborted) {\n          throw new CancellationError('Execution cancelled by user');\n        }\n        \n        try {\n          await this.executeLevel(level);\n        } catch (error) {\n          // ENHANCED: Level-specific error handling\n          const recoveryAction = await this.handleLevelError(levelIndex, level, error);\n          \n          if (recoveryAction === 'stop') {\n            throw error;\n          } else if (recoveryAction === 'retry') {\n            await this.executeLevel(level); // Retry once\n          } else if (recoveryAction === 'skip') {\n            // Mark nodes as skipped and continue\n            for (const nodeId of level) {\n              this.executionState.nodeStates.get(nodeId)!.status = 'skipped';\n            }\n          }\n        }\n      }\n      \n      this.executionState.status = 'completed';\n      this.executionState.completedAt = new Date();\n      this.eventBus.emit('execution:complete', this.executionId, this.executionState);\n      \n      return {\n        success: true,\n        executionId: this.executionId,\n        executionState: this.executionState,\n        outputs: this.gatherWorkflowOutputs()\n      };\n      \n    } catch (error) {\n      this.executionState.status = error instanceof CancellationError ? 'cancelled' : 'failed';\n      this.executionState.errors.push({\n        nodeId: 'workflow',\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: false,\n        retryAttempts: 0\n      });\n      \n      const event = error instanceof CancellationError ? 'execution:cancelled' : 'execution:failed';\n      this.eventBus.emit(event, this.executionId, error);\n      \n      return {\n        success: false,\n        executionId: this.executionId,\n        executionState: this.executionState,\n        error: error as Error\n      };\n    } finally {\n      // ENHANCED: Cleanup resources\n      await this.cleanup();\n    }\n  }\n  \n  private async executeLevel(nodeIds: string[]): Promise<void> {\n    // ENHANCED: Execute with resource-aware scheduling\n    const nodePromises = nodeIds.map(nodeId => \n      this.workerPool.schedule(() => this.executeNode(nodeId))\n    );\n    \n    // ENHANCED: Wait with proper error aggregation\n    const results = await Promise.allSettled(nodePromises);\n    \n    // Check for failures\n    const failures = results.filter(r => r.status === 'rejected');\n    if (failures.length > 0 && this.executionState.workflow?.globalConfig.errorHandling === 'stop') {\n      throw new AggregateError(\n        failures.map(f => (f as PromiseRejectedResult).reason),\n        `${failures.length} nodes failed in level`\n      );\n    }\n  }\n  \n  private async executeNode(nodeId: string): Promise<NodeExecutionResult> {\n    const node = this.nodes.get(nodeId)!;\n    const nodeState = this.executionState.nodeStates.get(nodeId)!;\n    \n    // ENHANCED: Validate inputs before execution\n    const inputValidation = await this.validateNodeInputs(nodeId);\n    if (!inputValidation.valid) {\n      nodeState.status = 'error';\n      nodeState.error = {\n        nodeId,\n        timestamp: new Date(),\n        error: new Error(`Input validation failed: ${inputValidation.errors.join(', ')}`),\n        recoverable: false,\n        retryAttempts: 0\n      };\n      throw nodeState.error.error;\n    }\n    \n    nodeState.status = 'running';\n    nodeState.startedAt = new Date();\n    this.eventBus.emit('execution:node:start', nodeId);\n    \n    try {\n      // Gather and validate inputs\n      const inputs = await this.dataFlowController.gatherInputs(\n        nodeId, \n        this.edges, \n        this.executionState\n      );\n      nodeState.input = inputs;\n      \n      // ENHANCED: Apply resource limits\n      const resourceToken = await this.acquireResourceToken(node);\n      \n      try {\n        // Execute with timeout and cancellation\n        const result = await this.executeAgentWithTimeout(\n          node, \n          inputs,\n          node.data.config.timeout || 300000 // 5 min default\n        );\n        \n        // ENHANCED: Validate output schema\n        await this.validateNodeOutput(nodeId, result.output);\n        \n        nodeState.output = result.output;\n        nodeState.logs.push(...result.logs);\n        nodeState.metrics = result.metrics;\n        nodeState.status = 'success';\n        nodeState.completedAt = new Date();\n        \n        // ENHANCED: Store output with size limit\n        await this.dataFlowController.storeOutput(\n          nodeId,\n          result.output,\n          node.data.config.memoryLimit\n        );\n        \n        // ENHANCED: Update edge states\n        await this.updateOutgoingEdges(nodeId, result.output);\n        \n        this.eventBus.emit('execution:node:complete', nodeId, result);\n        \n        return result;\n        \n      } finally {\n        // Release resource token\n        resourceToken.release();\n      }\n      \n    } catch (error) {\n      nodeState.status = 'error';\n      nodeState.completedAt = new Date();\n      \n      const executionError: ExecutionError = {\n        nodeId,\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: this.isRecoverableError(error),\n        retryAttempts: nodeState.error?.retryAttempts || 0\n      };\n      \n      nodeState.error = executionError;\n      this.eventBus.emit('execution:node:error', nodeId, error);\n      \n      // ENHANCED: Retry logic with backoff\n      if (executionError.recoverable && \n          node.data.config.retryConfig &&\n          executionError.retryAttempts < node.data.config.retryConfig.maxAttempts) {\n        \n        executionError.retryAttempts++;\n        const delay = this.calculateBackoff(\n          node.data.config.retryConfig,\n          executionError.retryAttempts\n        );\n        \n        nodeState.logs.push({\n          timestamp: new Date(),\n          level: 'warn',\n          nodeId,\n          message: `Retrying after ${delay}ms (attempt ${executionError.retryAttempts})`,\n        });\n        \n        await new Promise(resolve => setTimeout(resolve, delay));\n        return this.executeNode(nodeId); // Recursive retry\n      }\n      \n      throw error;\n    }\n  }\n  \n  // ENHANCED: Execute with timeout and cancellation support\n  private async executeAgentWithTimeout(\n    node: WorkflowNode,\n    inputs: any,\n    timeoutMs: number\n  ): Promise<AgentExecutionResult> {\n    return new Promise((resolve, reject) => {\n      const timeoutId = setTimeout(() => {\n        reject(new Error(`Node execution timed out after ${timeoutMs}ms`));\n      }, timeoutMs);\n      \n      // Listen for cancellation\n      this.abortController.signal.addEventListener('abort', () => {\n        clearTimeout(timeoutId);\n        reject(new CancellationError('Execution cancelled'));\n      });\n      \n      this.workerPool.execute(node, inputs, this.abortController.signal)\n        .then(result => {\n          clearTimeout(timeoutId);\n          resolve(result);\n        })\n        .catch(error => {\n          clearTimeout(timeoutId);\n          reject(error);\n        });\n    });\n  }\n  \n  // ENHANCED: Validate inputs match expected schema\n  private async validateNodeInputs(nodeId: string): Promise<ValidationResult> {\n    const node = this.nodes.get(nodeId)!;\n    const errors: string[] = [];\n    \n    // Check required inputs\n    for (const [key, inputDef] of Object.entries(node.data.inputs)) {\n      if (inputDef.required) {\n        const hasInput = this.edges.some(e => \n          e.target === nodeId && e.targetHandle === `input-${key}`\n        );\n        \n        if (!hasInput) {\n          errors.push(`Required input '${inputDef.name}' is not connected`);\n        }\n      }\n      \n      // ENHANCED: Validate schema if provided\n      if (inputDef.schema) {\n        const incomingEdges = this.edges.filter(e => \n          e.target === nodeId && e.targetHandle === `input-${key}`\n        );\n        \n        for (const edge of incomingEdges) {\n          const sourceOutput = this.executionState.globalContext.variables[edge.source];\n          if (sourceOutput) {\n            const schemaValidation = validateAgainstSchema(sourceOutput, inputDef.schema);\n            if (!schemaValidation.valid) {\n              errors.push(`Input '${inputDef.name}' schema validation failed: ${schemaValidation.errors.join(', ')}`);\n            }\n          }\n        }\n      }\n    }\n    \n    return {\n      valid: errors.length === 0,\n      errors: errors.map(msg => ({ type: 'config', nodeId, message: msg, severity: 'error' })),\n      warnings: []\n    };\n  }\n  \n  // ENHANCED: Validate output matches expected schema\n  private async validateNodeOutput(nodeId: string, output: any): Promise<void> {\n    const node = this.nodes.get(nodeId)!;\n    \n    for (const [key, outputDef] of Object.entries(node.data.outputs)) {\n      if (outputDef.schema) {\n        const validation = validateAgainstSchema(output, outputDef.schema);\n        if (!validation.valid) {\n          throw new Error(\n            `Output '${outputDef.name}' schema validation failed: ${validation.errors.join(', ')}`\n          );\n        }\n      }\n    }\n  }\n  \n  // ENHANCED: Update edge states when data flows\n  private async updateOutgoingEdges(nodeId: string, output: any): Promise<void> {\n    const outgoingEdges = this.edges.filter(e => e.source === nodeId);\n    \n    for (const edge of outgoingEdges) {\n      const edgeState = this.executionState.edgeStates.get(edge.id);\n      if (edgeState) {\n        edgeState.status = 'active';\n        edgeState.startedAt = new Date();\n        \n        // Apply transformations\n        let transformedData = output;\n        if (edge.data.transformations) {\n          transformedData = await this.dataFlowController.applyTransformations(\n            output,\n            edge.data.transformations\n          );\n        }\n        \n        // Validate against edge rules\n        if (edge.data.validation) {\n          const validation = await this.dataFlowController.validateData(\n            transformedData,\n            edge.data.validation\n          );\n          \n          if (!validation.valid) {\n            edgeState.status = 'error';\n            edgeState.validationErrors = validation.errors;\n            throw new Error(`Edge validation failed: ${validation.errors.map(e => e.message).join(', ')}`);\n          }\n        }\n        \n        edgeState.data = transformedData;\n        edgeState.bytesTransferred = JSON.stringify(transformedData).length;\n        edgeState.status = 'complete';\n        edgeState.completedAt = new Date();\n      }\n    }\n  }\n  \n  // ENHANCED: Check resource availability before execution\n  private async checkResourceAvailability(): Promise<void> {\n    const limits = this.executionState.workflow?.globalConfig.resourceLimits;\n    if (!limits) return;\n    \n    // Estimate resource requirements\n    const estimatedTokens = this.estimateTokenUsage();\n    const estimatedMemory = this.estimateMemoryUsage();\n    \n    if (limits.maxTotalTokens && estimatedTokens > limits.maxTotalTokens) {\n      throw new Error(\n        `Estimated token usage (${estimatedTokens}) exceeds limit (${limits.maxTotalTokens})`\n      );\n    }\n    \n    if (limits.maxMemoryMB && estimatedMemory > limits.maxMemoryMB) {\n      throw new Error(\n        `Estimated memory usage (${estimatedMemory}MB) exceeds limit (${limits.maxMemoryMB}MB)`\n      );\n    }\n  }\n  \n  private async cleanup(): Promise<void> {\n    await this.workerPool.terminate();\n    this.dataFlowController.cleanup();\n  }\n}\n```\n\n### Enhanced Data Flow Controller\n```typescript\nclass DataFlowController {\n  private dataCache: Map<string, any> = new Map();\n  private memoryUsage: number = 0;\n  \n  async gatherInputs(\n    nodeId: string,\n    edges: WorkflowEdge[],\n    executionState: ExecutionState\n  ): Promise<Record<string, any>> {\n    const inputs: Record<string, any> = {};\n    const incomingEdges = edges.filter(e => e.target === nodeId);\n    \n    for (const edge of incomingEdges) {\n      const sourceOutput = executionState.globalContext.variables[edge.source];\n      \n      if (sourceOutput === undefined) {\n        throw new Error(`Missing output from source node: ${edge.source}`);\n      }\n      \n      // Apply transformations\n      const transformedOutput = edge.data.transformations\n        ? await this.applyTransformations(sourceOutput, edge.data.transformations)\n        : sourceOutput;\n      \n      // Map to target input handle\n      const inputKey = edge.targetHandle?.replace('input-', '') || edge.source;\n      inputs[inputKey] = transformedOutput;\n    }\n    \n    return inputs;\n  }\n  \n  async applyTransformations(\n    data: any,\n    transformations: Transformation[]\n  ): Promise<any> {\n    let result = data;\n    \n    for (const transform of transformations) {\n      switch (transform.type) {\n        case 'jsonpath':\n          result = this.applyJsonPath(result, transform.config.path);\n          break;\n        case 'template':\n          result = this.applyTemplate(result, transform.config.template);\n          break;\n        case 'filter':\n          result = this.applyFilter(result, transform.config.predicate);\n          break;\n        case 'map':\n          result = this.applyMap(result, transform.config.mapper);\n          break;\n        case 'aggregate':\n          result = this.applyAggregate(result, transform.config.aggregator);\n          break;\n        default:\n          throw new Error(`Unknown transformation type: ${transform.type}`);\n      }\n    }\n    \n    return result;\n  }\n  \n  async validateData(data: any, rules: ValidationRule[]): Promise<ValidationResult> {\n    const errors: ValidationError[] = [];\n    \n    for (const rule of rules) {\n      switch (rule.type) {\n        case 'schema':\n          const schemaResult = validateAgainstSchema(data, rule.config.schema);\n          if (!schemaResult.valid) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || schemaResult.errors.join(', '),\n              severity: 'error'\n            });\n          }\n          break;\n          \n        case 'size':\n          const size = JSON.stringify(data).length;\n          if (size > rule.config.maxBytes) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || `Data size (${size} bytes) exceeds limit (${rule.config.maxBytes} bytes)`,\n              severity: 'error'\n            });\n          }\n          break;\n          \n        case 'format':\n          if (!this.validateFormat(data, rule.config.format)) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || `Data does not match format: ${rule.config.format}`,\n              severity: 'error'\n            });\n          }\n          break;\n          \n        case 'custom':\n          const customResult = await rule.config.validator(data);\n          if (!customResult.valid) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || customResult.error,\n              severity: 'error'\n            });\n          }\n          break;\n      }\n    }\n    \n    return {\n      valid: errors.length === 0,\n      errors,\n      warnings: []\n    };\n  }\n  \n  async storeOutput(nodeId: string, output: any, memoryLimit?: number): Promise<void> {\n    const size = JSON.stringify(output).length;\n    \n    // Check memory limit\n    if (memoryLimit && this.memoryUsage + size > memoryLimit * 1024 * 1024) {\n      throw new Error(`Memory limit exceeded: ${this.memoryUsage + size} bytes`);\n    }\n    \n    this.dataCache.set(nodeId, output);\n    this.memoryUsage += size;\n  }\n  \n  cleanup(): void {\n    this.dataCache.clear();\n    this.memoryUsage = 0;\n  }\n  \n  private applyJsonPath(data: any, path: string): any {\n    // JSONPath implementation\n    return JSONPath.query(data, path);\n  }\n  \n  private applyTemplate(data: any, template: string): any {\n    // Template engine (e.g., Handlebars)\n    return Handlebars.compile(template)(data);\n  }\n  \n  private applyFilter(data: any[], predicate: string): any[] {\n    // Safe eval or predicate parser\n    const fn = new Function('item', `return ${predicate}`);\n    return data.filter(fn);\n  }\n  \n  private applyMap(data: any[], mapper: string): any[] {\n    const fn = new Function('item', `return ${mapper}`);\n    return data.map(fn);\n  }\n  \n  private applyAggregate(data: any[], aggregator: string): any {\n    const fn = new Function('acc', 'item', `return ${aggregator}`);\n    return data.reduce(fn, null);\n  }\n  \n  private validateFormat(data: any, format: string): boolean {\n    switch (format) {\n      case 'json':\n        try {\n          JSON.parse(typeof data === 'string' ? data : JSON.stringify(data));\n          return true;\n        } catch {\n          return false;\n        }\n      case 'url':\n        try {\n          new URL(data);\n          return true;\n        } catch {\n          return false;\n        }\n      case 'email':\n        return /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/.test(data);\n      default:\n        return true;\n    }\n  }\n}\n```\n\n### Enhanced Worker Pool\n```typescript\nclass WorkerPool {\n  private workers: Worker[] = [];\n  private availableWorkers: Worker[] = [];\n  private queue: QueueItem[] = [];\n  private size: number;\n  \n  constructor(size: number = 4) {\n    this.size = size;\n    this.initWorkers();\n  }\n  \n  private initWorkers(): void {\n    for (let i = 0; i < this.size; i++) {\n      const worker = new Worker(\n        new URL('./execution.worker.ts', import.meta.url),\n        { type: 'module' }\n      );\n      this.workers.push(worker);\n      this.availableWorkers.push(worker);\n    }\n  }\n  \n  async schedule<T>(task: () => Promise<T>): Promise<T> {\n    return new Promise((resolve, reject) => {\n      this.queue.push({ task, resolve, reject });\n      this.processQueue();\n    });\n  }\n  \n  async execute(\n    node: WorkflowNode,\n    inputs: any,\n    signal: AbortSignal\n  ): Promise<AgentExecutionResult> {\n    return this.schedule(() => {\n      return new Promise((resolve, reject) => {\n        if (this.availableWorkers.length === 0) {\n          throw new Error('No available workers');\n        }\n        \n        const worker = this.availableWorkers.pop()!;\n        const messageId = crypto.randomUUID();\n        \n        const handler = (event: MessageEvent) => {\n          if (event.data.messageId === messageId) {\n            worker.removeEventListener('message', handler);\n            this.availableWorkers.push(worker);\n            this.processQueue();\n            \n            if (event.data.error) {\n              reject(new Error(event.data.error));\n            } else {\n              resolve(event.data.result);\n            }\n          }\n        };\n        \n        // Handle cancellation\n        const abortHandler = () => {\n          worker.postMessage({ messageId, type: 'cancel' });\n          worker.removeEventListener('message', handler);\n          this.availableWorkers.push(worker);\n          reject(new CancellationError('Execution cancelled'));\n        };\n        \n        if (signal.aborted) {\n          abortHandler();\n          return;\n        }\n        \n        signal.addEventListener('abort', abortHandler, { once: true });\n        \n        worker.addEventListener('message', handler);\n        worker.postMessage({\n          messageId,\n          type: 'execute',\n          node,\n          inputs\n        });\n      });\n    });\n  }\n  \n  private processQueue(): void {\n    while (this.queue.length > 0 && this.availableWorkers.length > 0) {\n      const item = this.queue.shift()!;\n      item.task().then(item.resolve).catch(item.reject);\n    }\n  }\n  \n  async terminate(): Promise<void> {\n    for (const worker of this.workers) {\n      worker.terminate();\n    }\n    this.workers = [];\n    this.availableWorkers = [];\n    this.queue = [];\n  }\n}\n```\n\n### Enhanced Checkpoint Manager\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    const checkpoint: ExecutionCheckpoint = {\n      timestamp: new Date(),\n      nodeStates: new Map(executionState.nodeStates),\n      globalContext: this.cloneContext(executionState.globalContext),\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    const store = tx.objectStore('checkpoints');\n    \n    await store.put({\n      id: checkpointId,\n      executionId: executionState.executionId,\n      checkpoint: this.serializeCheckpoint(checkpoint)\n    });\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string\n  ): Promise<ExecutionCheckpoint> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    const record = await store.get(checkpointId);\n    \n    if (!record) {\n      throw new Error(`Checkpoint not found: ${checkpointId}`);\n    }\n    \n    return this.deserializeCheckpoint(record.checkpoint);\n  }\n  \n  private cloneContext(context: ExecutionContext): ExecutionContext {\n    return {\n      variables: { ...context.variables },\n      secrets: new Map(context.secrets),\n      sharedMemory: context.sharedMemory, // Reference, not cloned\n      artifacts: new Map(context.artifacts)\n    };\n  }\n  \n  private serializeCheckpoint(checkpoint: ExecutionCheckpoint): string {\n    return JSON.stringify({\n      timestamp: checkpoint.timestamp.toISOString(),\n      nodeStates: Array.from(checkpoint.nodeStates.entries()),\n      globalContext: {\n        variables: checkpoint.globalContext.variables,\n        // Exclude non-serializable items\n      },\n      reason: checkpoint.reason\n    });\n  }\n  \n  private deserializeCheckpoint(data: string): ExecutionCheckpoint {\n    const parsed = JSON.parse(data);\n    return {\n      timestamp: new Date(parsed.timestamp),\n      nodeStates: new Map(parsed.nodeStates),\n      globalContext: {\n        variables: parsed.globalContext.variables,\n        secrets: new Map(),\n        artifacts: new Map()\n      },\n      reason: parsed.reason\n    };\n  }\n}\n```\n\n## What I Improved\n\n### 1. **Execution Engine Robustness**\n- **Worker Pool**: Added a proper worker pool with scheduling instead of creating workers on-demand. This prevents resource exhaustion and improves performance.\n- **Cancellation Support**: Added `AbortController` throughout the execution chain for graceful cancellation.\n- **Checkpoint System**: Added automatic checkpoints every 3 levels and manual checkpoint capability for resuming long-running workflows.\n- **Resource Limits**: Added memory limits, token limits, and execution time limits with pre-flight checks.\n- **Timeout Handling**: Wrapped execution with configurable timeouts to prevent infinite hangs.\n\n### 2. **Data Flow Architecture**\n- **Data Flow Controller**: Created a dedicated controller to manage data transfer, transformations, and validation between nodes.\n- **Input/Output Definitions**: Added explicit input/output schemas to nodes with validation, making data contracts clear and type-safe.\n- **Multiple Handles**: Support multiple input/output handles per node for different data types (instead of single source/target).\n- **Edge State Tracking**: Track edge execution state separately from nodes, including bytes transferred and validation errors.\n- **Transformation Pipeline**: Enhanced transformations with JSONPath, templates, filters, maps, and aggregations.\n- **Validation Rules**: Added comprehensive validation rules (schema, size, format, custom) on edges.\n\n### 3. **Error Handling**\n- **Aggregate Errors**: Use `AggregateError` to collect multiple failures in parallel execution levels.\n- **Recoverable Errors**: Distinguish between recoverable (retry) and non-recoverable (stop) errors.\n- **Retry Configuration**: Per-node retry config with backoff strategies and specific error patterns to retry.\n- **Level Error Recovery**: Handle errors at the level (not just node) to decide whether to stop, retry, or skip.\n- **Warnings**: Added warnings for non-fatal issues (performance, cost, resource usage).\n- **Input/Output Validation**: Validate inputs before execution and outputs after execution against schemas.\n\n### 4. **Real-time Visualization**\n- **Edge Animation**: Animate data flow with moving particles along edges during execution.\n- **Edge State Display**: Show validation errors, transformations, and data size on edges.\n- **Streaming Preview**: Show streaming output in real-time within node body.\n- **Expandable Nodes**: Expand nodes to see detailed logs and metrics inline.\n- **Status Granularity**: Added more statuses: `queued`, `warning`, `cancelled` for better visibility.\n- **Event System**: Emit typed events for all workflow changes and execution progress.\n\n### 5. **State Management**\n- **Undo/Redo**: Added full undo/redo with snapshot history (up to 50 snapshots).\n- **Immer Integration**: Use immer middleware for immutable state updates.\n- **Event Emitter**: Type-safe event system for reactive updates.\n- **Edge Selection**: Support selecting and configuring edges in properties panel.\n\n### 6. **Other Improvements**\n- **Execution Queue**: Support queuing multiple workflow executions.\n- **Resource Usage Tracking**: Track total tokens, cost, memory, API calls.\n- **Notification System**: Configurable notifications via browser, email, or webhook.\n- **Export/Import**: Export workflows as JSON or YAML for sharing.\n- **Metadata**: Track workflow metadata (author, tags, category, success rate).\n- **Transformer Node Type**: Added transformer node for pure data transformations without AI.\n- **Selection Toolbar**: Multi-select nodes and bulk operations.\n\n## What I Kept\n\nThe original design had excellent foundations:\n\n- **React Flow**: Perfect choice for the canvas library - mature, performant, extensible.\n- **Zustand**: Good state management choice - lightweight and performant.\n- **Web Workers**: Correct approach for non-blocking execution.\n- **IndexedDB**: Right storage solution for large workflows.\n- **Topological Sort with Parallelization**: Excellent approach for DAG execution.\n- **UI Component Structure**: Canvas, Properties Panel, Toolbar layout is well-designed.\n- **Template System**: Variable substitution approach is solid.\n- **Validation Service**: Graph validation logic is comprehensive.\n\n## Remaining Concerns\n\n### 1. **Memory Management**\n- Large workflows with many artifacts could exhaust browser memory\n- Need aggressive garbage collection strategy for completed nodes\n- Consider offloading artifacts to IndexedDB instead of keeping in memory\n- SharedArrayBuffer may not be available in all browsers (requires CORS headers)\n\n### 2. **Streaming Implementation**\n- Streaming adds significant complexity to data flow\n- How to handle backpressure if downstream nodes can't keep up?\n- Should streaming outputs be persisted or only the final result?\n- Edge transformations on streams require different handling\n\n### 3. **Security**\n- Transformation functions use `new Function()` which is risky\n- Need proper sandboxing or switch to a safe expression evaluator\n- Secrets in ExecutionContext need encryption at rest\n- Web Workers can still access some APIs - need to restrict\n\n### 4. **Error Recovery**\n- Retry logic could lead to exponential cost if not careful\n- Partial failures in parallel levels are complex to reason about\n- Checkpoint restoration doesn't handle external state (API calls already made)\n\n### 5. **Performance at Scale**\n- Workflows with 100+ nodes may overwhelm the UI\n- React Flow re-renders can be expensive with many edges\n- Need virtual rendering or viewport culling for very large workflows\n- Worker pool size needs dynamic adjustment based on browser capabilities\n\n### 6. **Testing**\n- Web Workers are difficult to test\n- Execution orchestration has many code paths (success, failure, retry, cancel)\n- Need comprehensive integration tests for edge cases\n\n## Convergence Status\n\n**ITERATING** - The design has significant improvements but needs another round to address:\n1. Memory management strategy for large workflows\n2. Streaming implementation details and backpressure handling\n3. Security sandboxing for custom transformations\n4. Checkpoint/resume edge cases\n5. Performance optimization strategy for 100+ node workflows",
        "prompt_for_other": "Please focus on:\n1. **Memory management**: How should we handle workflows that generate GBs of artifacts? Should we stream to IndexedDB? Implement LRU eviction?\n2. **Streaming architecture**: Can you design the backpressure mechanism for streaming between nodes? How do edge transformations work on streams?\n3. **Security model**: How can we safely allow custom transformations without `new Function()`? Should we use a safe expression language like JSONata?\n4. **Checkpoint consistency**: How do we handle external side effects (API calls, file writes) during checkpoint restoration? Should we track \"compensation\" operations?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Enhanced Design\n\n## Architecture Overview\n\n### High-Level Architecture\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application                        \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502     \u2502\n\u2502  \u2502   Layer      \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           Workflow State Management (Zustand)               \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502   Execution  \u2502  \u2502  Template    \u2502  \u2502   Validation \u2502     \u2502\n\u2502  \u2502   Orchestrator\u2502  \u2502  Manager     \u2502  \u2502   Service    \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                        \u2502\n\u2502  \u2502  Data Flow   \u2502  \u2502  Error       \u2502                        \u2502\n\u2502  \u2502  Controller  \u2502  \u2502  Recovery    \u2502                        \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas Library**: React Flow (react-flow-renderer)\n- **State Management**: Zustand with immer middleware\n- **UI Components**: Radix UI + Tailwind CSS\n- **Execution**: Web Workers with SharedArrayBuffer for progress tracking\n- **Storage**: IndexedDB with automatic sync queue for reliability\n- **Real-time Updates**: EventEmitter pattern with typed events\n\n## Core Components\n\n### 1. Canvas Component (WorkflowCanvas.tsx)\n```typescript\ninterface WorkflowCanvasProps {\n  workflowId: string;\n  readonly?: boolean;\n}\n\nconst WorkflowCanvas: React.FC<WorkflowCanvasProps> = ({ \n  workflowId, \n  readonly = false \n}) => {\n  const { nodes, edges, onNodesChange, onEdgesChange, onConnect } = \n    useWorkflowStore();\n  const executionState = useExecutionState();\n  \n  // Handle drop from palette\n  const onDrop = useCallback((event: React.DragEvent) => {\n    event.preventDefault();\n    const type = event.dataTransfer.getData('application/reactflow');\n    const position = screenToFlowPosition({\n      x: event.clientX,\n      y: event.clientY,\n    });\n    \n    addNode(type, position);\n  }, []);\n  \n  const onDragOver = useCallback((event: React.DragEvent) => {\n    event.preventDefault();\n    event.dataTransfer.dropEffect = 'move';\n  }, []);\n  \n  return (\n    <ReactFlow\n      nodes={nodes}\n      edges={edges}\n      onNodesChange={onNodesChange}\n      onEdgesChange={onEdgesChange}\n      onConnect={onConnect}\n      onDrop={onDrop}\n      onDragOver={onDragOver}\n      nodeTypes={customNodeTypes}\n      edgeTypes={customEdgeTypes}\n      fitView\n      minZoom={0.2}\n      maxZoom={2}\n      deleteKeyCode=\"Delete\"\n      selectionKeyCode=\"Shift\"\n      multiSelectionKeyCode=\"Control\"\n    >\n      <Background />\n      <Controls />\n      <MiniMap />\n      <ExecutionOverlay />\n      <SelectionToolbar />\n    </ReactFlow>\n  );\n};\n```\n\n### 2. Agent Node Component (AgentNode.tsx)\n```typescript\ninterface AgentNodeData {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator' | 'transformer';\n  label: string;\n  config: AgentConfig;\n  status: ExecutionStatus;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n  inputs: Record<string, InputDefinition>;  // ENHANCED: Define expected inputs\n  outputs: Record<string, OutputDefinition>; // ENHANCED: Define outputs with schema\n}\n\ninterface InputDefinition {\n  name: string;\n  type: 'text' | 'json' | 'code' | 'artifacts' | 'stream';\n  required: boolean;\n  schema?: JSONSchema; // For validation\n  description?: string;\n}\n\ninterface OutputDefinition {\n  name: string;\n  type: 'text' | 'json' | 'code' | 'artifacts' | 'stream';\n  schema?: JSONSchema;\n  description?: string;\n}\n\ninterface AgentConfig {\n  model: string;\n  temperature: number;\n  systemPrompt: string;\n  maxTokens?: number;\n  timeout?: number;\n  retryConfig?: RetryConfig;\n  streaming?: boolean; // ENHANCED: Support streaming output\n  memoryLimit?: number; // ENHANCED: Prevent memory leaks\n}\n\ninterface RetryConfig {\n  maxAttempts: number;\n  backoffStrategy: 'linear' | 'exponential' | 'fixed';\n  initialDelayMs: number;\n  maxDelayMs: number;\n  retryableErrors: string[]; // Specific error patterns to retry\n}\n\nconst AgentNode: React.FC<NodeProps<AgentNodeData>> = ({ data, selected }) => {\n  const [isExpanded, setIsExpanded] = useState(false);\n  const streamingOutput = useStreamingOutput(data.id);\n  \n  const getStatusColor = () => {\n    switch (data.status) {\n      case 'idle': return 'gray';\n      case 'queued': return 'cyan'; // ENHANCED: New status\n      case 'running': return 'blue';\n      case 'success': return 'green';\n      case 'error': return 'red';\n      case 'warning': return 'orange'; // ENHANCED: Partial success\n      case 'waiting': return 'yellow';\n      case 'cancelled': return 'gray'; // ENHANCED: User cancelled\n    }\n  };\n\n  return (\n    <div \n      className={`agent-node ${selected ? 'selected' : ''} ${isExpanded ? 'expanded' : ''}`}\n      data-node-id={data.id}\n    >\n      {/* Multiple input handles for different data types */}\n      {Object.entries(data.inputs).map(([key, input]) => (\n        <Handle\n          key={`input-${key}`}\n          type=\"target\"\n          position={Position.Left}\n          id={`input-${key}`}\n          style={{ top: calculateHandlePosition(key, data.inputs) }}\n        >\n          <Tooltip content={`${input.name} (${input.type})`}>\n            <HandleLabel required={input.required}>{input.name}</HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n      \n      <div className=\"node-header\" style={{ borderColor: getStatusColor() }}>\n        <AgentIcon type={data.type} />\n        <span className=\"node-label\" contentEditable onBlur={handleLabelUpdate}>\n          {data.label}\n        </span>\n        <StatusBadge status={data.status} />\n        <Button \n          size=\"xs\" \n          variant=\"ghost\"\n          onClick={() => setIsExpanded(!isExpanded)}\n        >\n          {isExpanded ? <ChevronUpIcon /> : <ChevronDownIcon />}\n        </Button>\n      </div>\n      \n      <div className=\"node-body\">\n        <div className=\"config-summary\">\n          <span className=\"model-badge\">{data.config.model}</span>\n          <span className=\"temp-badge\">T: {data.config.temperature}</span>\n          {data.config.streaming && <span className=\"stream-badge\">\u26a1 Stream</span>}\n        </div>\n        \n        {data.status === 'running' && (\n          <>\n            <ProgressBar metrics={data.metrics} />\n            {/* ENHANCED: Show streaming output preview */}\n            {data.config.streaming && streamingOutput && (\n              <StreamPreview content={streamingOutput} />\n            )}\n          </>\n        )}\n        \n        {/* ENHANCED: Show validation errors */}\n        {data.status === 'error' && (\n          <ErrorSummary error={data.logs.find(l => l.level === 'error')} />\n        )}\n        \n        {isExpanded && (\n          <div className=\"node-details\">\n            <LogPreview logs={data.logs.slice(-5)} />\n            <MetricsDisplay metrics={data.metrics} />\n          </div>\n        )}\n      </div>\n      \n      {/* Multiple output handles for different data types */}\n      {Object.entries(data.outputs).map(([key, output]) => (\n        <Handle\n          key={`output-${key}`}\n          type=\"source\"\n          position={Position.Right}\n          id={`output-${key}`}\n          style={{ top: calculateHandlePosition(key, data.outputs) }}\n        >\n          <Tooltip content={`${output.name} (${output.type})`}>\n            <HandleLabel>{output.name}</HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n    </div>\n  );\n};\n```\n\n### 3. Connection Edge Component (DataFlowEdge.tsx)\n```typescript\ninterface EdgeData {\n  id: string;\n  sourceHandle: string; // ENHANCED: Track which output\n  targetHandle: string; // ENHANCED: Track which input\n  dataType: 'text' | 'json' | 'code' | 'artifacts' | 'stream';\n  transformations?: Transformation[];\n  validation?: ValidationRule[]; // ENHANCED: Validate data flow\n  status: 'idle' | 'active' | 'complete' | 'error';\n  dataPreview?: string;\n  bytesTransferred?: number; // ENHANCED: Track data size\n}\n\ninterface ValidationRule {\n  type: 'schema' | 'size' | 'format' | 'custom';\n  config: any;\n  errorMessage: string;\n}\n\nconst DataFlowEdge: React.FC<EdgeProps<EdgeData>> = ({ \n  id, \n  source, \n  target, \n  sourceX,\n  sourceY,\n  targetX,\n  targetY,\n  data,\n  style,\n  markerEnd\n}) => {\n  const [edgePath, labelX, labelY] = getBezierPath({\n    sourceX, \n    sourceY, \n    targetX, \n    targetY,\n    sourcePosition: Position.Right,\n    targetPosition: Position.Left\n  });\n\n  // ENHANCED: Animate data flow\n  const animationProgress = useAnimatedDataFlow(data.status);\n\n  return (\n    <>\n      {/* Base edge path */}\n      <path\n        id={id}\n        className=\"react-flow__edge-path\"\n        d={edgePath}\n        style={{ \n          ...style, \n          stroke: data.status === 'error' ? '#ef4444' : \n                  data.status === 'active' ? '#3b82f6' : '#6b7280',\n          strokeWidth: data.status === 'active' ? 3 : 2,\n        }} \n        markerEnd={markerEnd}\n      />\n      \n      {/* ENHANCED: Animated data flow indicator */}\n      {data.status === 'active' && (\n        <circle\n          r=\"4\"\n          fill=\"#3b82f6\"\n        >\n          <animateMotion\n            dur=\"2s\"\n            repeatCount=\"indefinite\"\n            path={edgePath}\n          />\n        </circle>\n      )}\n      \n      <EdgeLabelRenderer>\n        <div\n          style={{ \n            position: 'absolute', \n            transform: `translate(-50%, -50%) translate(${labelX}px,${labelY}px)`,\n            pointerEvents: 'all'\n          }}\n          className=\"edge-label\"\n        >\n          <DataTypeIndicator type={data.dataType} />\n          \n          {/* ENHANCED: Show transformations applied */}\n          {data.transformations && data.transformations.length > 0 && (\n            <Tooltip content={\n              <div>\n                <strong>Transformations:</strong>\n                <ul>\n                  {data.transformations.map((t, i) => (\n                    <li key={i}>{t.type}</li>\n                  ))}\n                </ul>\n              </div>\n            }>\n              <TransformBadge count={data.transformations.length} />\n            </Tooltip>\n          )}\n          \n          {/* ENHANCED: Show validation errors */}\n          {data.status === 'error' && (\n            <Tooltip content=\"Data validation failed\">\n              <ErrorIcon className=\"text-red-500\" />\n            </Tooltip>\n          )}\n          \n          {/* ENHANCED: Show data size for large transfers */}\n          {data.bytesTransferred && data.bytesTransferred > 1024 && (\n            <Tooltip content={`${formatBytes(data.bytesTransferred)} transferred`}>\n              <DataSizeBadge size={data.bytesTransferred} />\n            </Tooltip>\n          )}\n          \n          {data.dataPreview && (\n            <Popover>\n              <PopoverTrigger>\n                <Button size=\"xs\" variant=\"ghost\">\n                  <EyeIcon />\n                </Button>\n              </PopoverTrigger>\n              <PopoverContent>\n                <DataPreview content={data.dataPreview} type={data.dataType} />\n              </PopoverContent>\n            </Popover>\n          )}\n        </div>\n      </EdgeLabelRenderer>\n    </>\n  );\n};\n```\n\n### 4. Properties Panel (PropertiesPanel.tsx)\n```typescript\nconst PropertiesPanel: React.FC = () => {\n  const { selectedNode, selectedEdge, updateNodeConfig, updateEdgeConfig } = \n    useWorkflowStore();\n  \n  if (!selectedNode && !selectedEdge) {\n    return <EmptyState message=\"Select a node or connection to configure\" />;\n  }\n\n  if (selectedEdge) {\n    return <EdgePropertiesPanel edge={selectedEdge} />;\n  }\n\n  return (\n    <div className=\"properties-panel\">\n      <Tabs defaultValue=\"config\">\n        <TabsList>\n          <TabsTrigger value=\"config\">Configuration</TabsTrigger>\n          <TabsTrigger value=\"prompt\">System Prompt</TabsTrigger>\n          <TabsTrigger value=\"io\">Inputs/Outputs</TabsTrigger> {/* ENHANCED */}\n          <TabsTrigger value=\"advanced\">Advanced</TabsTrigger>\n          <TabsTrigger value=\"logs\">Logs</TabsTrigger>\n        </TabsList>\n        \n        <TabsContent value=\"config\">\n          <ConfigurationTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"prompt\">\n          <PromptEditor \n            value={selectedNode.config.systemPrompt}\n            onChange={(prompt) => updateNodeConfig(selectedNode.id, { systemPrompt: prompt })}\n            variables={getAvailableVariables(selectedNode)}\n            testMode // ENHANCED: Test prompts before execution\n          />\n        </TabsContent>\n        \n        <TabsContent value=\"io\">\n          <IOConfigTab node={selectedNode} /> {/* ENHANCED: Configure I/O */}\n        </TabsContent>\n        \n        <TabsContent value=\"advanced\">\n          <AdvancedSettingsTab node={selectedNode} />\n        </TabsContent>\n        \n        <TabsContent value=\"logs\">\n          <LogViewer logs={selectedNode.logs} nodeId={selectedNode.id} />\n        </TabsContent>\n      </Tabs>\n    </div>\n  );\n};\n\n// ENHANCED: Configure inputs and outputs with schemas\nconst IOConfigTab: React.FC<{ node: AgentNodeData }> = ({ node }) => {\n  const { updateNodeIO } = useWorkflowStore();\n  \n  return (\n    <div className=\"io-config\">\n      <Section title=\"Input Definitions\">\n        {Object.entries(node.inputs).map(([key, input]) => (\n          <InputConfig\n            key={key}\n            input={input}\n            onChange={(updated) => updateNodeIO(node.id, 'input', key, updated)}\n            onDelete={() => deleteNodeIO(node.id, 'input', key)}\n          />\n        ))}\n        <Button onClick={() => addNodeIO(node.id, 'input')}>\n          <PlusIcon /> Add Input\n        </Button>\n      </Section>\n      \n      <Section title=\"Output Definitions\">\n        {Object.entries(node.outputs).map(([key, output]) => (\n          <OutputConfig\n            key={key}\n            output={output}\n            onChange={(updated) => updateNodeIO(node.id, 'output', key, updated)}\n            onDelete={() => deleteNodeIO(node.id, 'output', key)}\n          />\n        ))}\n        <Button onClick={() => addNodeIO(node.id, 'output')}>\n          <PlusIcon /> Add Output\n        </Button>\n      </Section>\n    </div>\n  );\n};\n\n// ENHANCED: Edge properties for transformations and validation\nconst EdgePropertiesPanel: React.FC<{ edge: WorkflowEdge }> = ({ edge }) => {\n  const { updateEdgeConfig } = useWorkflowStore();\n  \n  return (\n    <div className=\"edge-properties\">\n      <h3>Connection Properties</h3>\n      \n      <FormField label=\"Data Type\">\n        <Select \n          value={edge.data.dataType}\n          onChange={(type) => updateEdgeConfig(edge.id, { dataType: type })}\n        >\n          <SelectOption value=\"text\">Text</SelectOption>\n          <SelectOption value=\"json\">JSON</SelectOption>\n          <SelectOption value=\"code\">Code</SelectOption>\n          <SelectOption value=\"artifacts\">Artifacts</SelectOption>\n          <SelectOption value=\"stream\">Stream</SelectOption>\n        </Select>\n      </FormField>\n      \n      <Section title=\"Transformations\">\n        <TransformationBuilder\n          transformations={edge.data.transformations || []}\n          onChange={(transforms) => updateEdgeConfig(edge.id, { transformations: transforms })}\n        />\n      </Section>\n      \n      <Section title=\"Validation Rules\">\n        <ValidationBuilder\n          rules={edge.data.validation || []}\n          onChange={(rules) => updateEdgeConfig(edge.id, { validation: rules })}\n        />\n      </Section>\n    </div>\n  );\n};\n```\n\n## Data Models\n\n### Enhanced Workflow Schema\n```typescript\ninterface Workflow {\n  id: string;\n  name: string;\n  description?: string;\n  version: string;\n  createdAt: Date;\n  updatedAt: Date;\n  nodes: WorkflowNode[];\n  edges: WorkflowEdge[];\n  globalConfig: GlobalConfig;\n  executionHistory: ExecutionRecord[];\n  metadata: WorkflowMetadata; // ENHANCED\n}\n\ninterface WorkflowMetadata {\n  author?: string;\n  tags: string[];\n  category?: string;\n  estimatedCost?: number;\n  averageRuntime?: number;\n  successRate?: number;\n}\n\ninterface GlobalConfig {\n  defaultModel: string;\n  defaultTemperature: number;\n  maxConcurrentAgents: number;\n  errorHandling: 'stop' | 'continue' | 'retry';\n  loggingLevel: 'debug' | 'info' | 'warn' | 'error';\n  resourceLimits: ResourceLimits; // ENHANCED\n  notifications: NotificationConfig; // ENHANCED\n}\n\ninterface ResourceLimits {\n  maxTokensPerNode?: number;\n  maxTotalTokens?: number;\n  maxExecutionTimeMs?: number;\n  maxMemoryMB?: number;\n  maxConcurrentConnections?: number;\n}\n\ninterface NotificationConfig {\n  onComplete: boolean;\n  onError: boolean;\n  onWarning: boolean;\n  channels: ('browser' | 'email' | 'webhook')[];\n  webhookUrl?: string;\n}\n```\n\n### Enhanced Execution State\n```typescript\ninterface ExecutionState {\n  workflowId: string;\n  executionId: string; // ENHANCED: Unique execution ID\n  status: 'idle' | 'queued' | 'running' | 'paused' | 'completed' | 'failed' | 'cancelled';\n  startedAt?: Date;\n  completedAt?: Date;\n  currentNode?: string;\n  nodeStates: Map<string, NodeExecutionState>;\n  edgeStates: Map<string, EdgeExecutionState>; // ENHANCED: Track edge execution\n  globalContext: ExecutionContext; // ENHANCED: Structured context\n  errors: ExecutionError[];\n  warnings: ExecutionWarning[]; // ENHANCED\n  checkpoints: ExecutionCheckpoint[]; // ENHANCED: Resume capability\n  resourceUsage: ResourceUsage; // ENHANCED\n}\n\ninterface ExecutionContext {\n  variables: Record<string, any>;\n  secrets: Map<string, string>; // ENHANCED: Secure storage\n  sharedMemory: SharedArrayBuffer; // ENHANCED: For worker communication\n  artifacts: Map<string, Artifact>; // ENHANCED: Large objects\n}\n\ninterface EdgeExecutionState {\n  edgeId: string;\n  status: 'idle' | 'active' | 'complete' | 'error';\n  startedAt?: Date;\n  completedAt?: Date;\n  data?: any;\n  validationErrors?: ValidationError[];\n  bytesTransferred?: number;\n}\n\ninterface ExecutionCheckpoint {\n  timestamp: Date;\n  nodeStates: Map<string, NodeExecutionState>;\n  globalContext: ExecutionContext;\n  reason: 'manual' | 'auto' | 'error';\n}\n\ninterface ResourceUsage {\n  totalTokens: number;\n  totalCost: number;\n  peakMemoryMB: number;\n  executionTimeMs: number;\n  apiCallCount: number;\n}\n\ninterface ExecutionWarning {\n  nodeId: string;\n  timestamp: Date;\n  type: 'performance' | 'cost' | 'resource' | 'deprecation';\n  message: string;\n  severity: 'low' | 'medium' | 'high';\n  suggestion?: string;\n}\n```\n\n## Enhanced State Management\n\n```typescript\ninterface WorkflowStore {\n  // Current workflow state\n  workflow: Workflow | null;\n  nodes: Node<AgentNodeData>[];\n  edges: Edge<EdgeData>[];\n  selectedNode: AgentNodeData | null;\n  selectedEdge: EdgeData | null; // ENHANCED\n  \n  // Execution state\n  executionState: ExecutionState | null;\n  isExecuting: boolean;\n  executionQueue: string[]; // ENHANCED: Queue multiple workflows\n  \n  // ENHANCED: Undo/Redo\n  history: {\n    past: WorkflowSnapshot[];\n    future: WorkflowSnapshot[];\n  };\n  \n  // Actions\n  setWorkflow: (workflow: Workflow) => void;\n  addNode: (type: string, position: XYPosition) => void;\n  updateNode: (nodeId: string, updates: Partial<AgentNodeData>) => void;\n  deleteNode: (nodeId: string) => void;\n  addEdge: (connection: Connection) => void;\n  updateEdge: (edgeId: string, updates: Partial<EdgeData>) => void; // ENHANCED\n  deleteEdge: (edgeId: string) => void;\n  selectNode: (nodeId: string | null) => void;\n  selectEdge: (edgeId: string | null) => void; // ENHANCED\n  \n  // ENHANCED: Undo/Redo\n  undo: () => void;\n  redo: () => void;\n  canUndo: () => boolean;\n  canRedo: () => boolean;\n  \n  // Execution\n  executeWorkflow: (config?: ExecutionConfig) => Promise<ExecutionResult>;\n  pauseExecution: () => Promise<void>;\n  resumeExecution: () => Promise<void>;\n  stopExecution: () => Promise<void>;\n  cancelExecution: () => Promise<void>; // ENHANCED\n  executeNode: (nodeId: string, inputs?: any) => Promise<NodeExecutionResult>;\n  createCheckpoint: () => void; // ENHANCED\n  restoreCheckpoint: (checkpointId: string) => Promise<void>; // ENHANCED\n  \n  // Persistence\n  saveWorkflow: () => Promise<void>;\n  loadWorkflow: (workflowId: string) => Promise<void>;\n  exportWorkflow: (format: 'json' | 'yaml') => string; // ENHANCED\n  importWorkflow: (data: string, format: 'json' | 'yaml') => Promise<void>; // ENHANCED\n  saveTemplate: (name: string) => Promise<void>;\n  loadTemplate: (templateId: string) => Promise<void>;\n  \n  // Validation\n  validateWorkflow: () => ValidationResult;\n  validateNode: (nodeId: string) => ValidationResult; // ENHANCED\n  validateEdge: (edgeId: string) => ValidationResult; // ENHANCED\n  \n  // ENHANCED: Event system\n  events: EventEmitter<WorkflowEvents>;\n}\n\ninterface WorkflowEvents {\n  'node:added': (node: WorkflowNode) => void;\n  'node:updated': (nodeId: string, updates: Partial<AgentNodeData>) => void;\n  'node:deleted': (nodeId: string) => void;\n  'edge:added': (edge: WorkflowEdge) => void;\n  'edge:deleted': (edgeId: string) => void;\n  'execution:started': (executionId: string) => void;\n  'execution:node:start': (nodeId: string) => void;\n  'execution:node:complete': (nodeId: string, result: any) => void;\n  'execution:node:error': (nodeId: string, error: Error) => void;\n  'execution:complete': (executionId: string, result: ExecutionResult) => void;\n  'execution:failed': (executionId: string, error: Error) => void;\n  'execution:cancelled': (executionId: string) => void;\n}\n\nconst useWorkflowStore = create<WorkflowStore>()(\n  immer((set, get) => ({\n    workflow: null,\n    nodes: [],\n    edges: [],\n    selectedNode: null,\n    selectedEdge: null,\n    executionState: null,\n    isExecuting: false,\n    executionQueue: [],\n    history: { past: [], future: [] },\n    events: new EventEmitter(),\n    \n    // ENHANCED: Snapshot before mutations for undo/redo\n    _snapshot: () => {\n      const state = get();\n      const snapshot: WorkflowSnapshot = {\n        nodes: [...state.nodes],\n        edges: [...state.edges],\n        workflow: state.workflow ? { ...state.workflow } : null,\n        timestamp: Date.now()\n      };\n      \n      set((draft) => {\n        draft.history.past.push(snapshot);\n        // Limit history size\n        if (draft.history.past.length > 50) {\n          draft.history.past.shift();\n        }\n        draft.history.future = [];\n      });\n    },\n    \n    addNode: (type, position) => {\n      const state = get();\n      state._snapshot();\n      \n      set((draft) => {\n        const newNode = createDefaultNode(type, position);\n        draft.nodes.push(newNode);\n        draft.events.emit('node:added', newNode);\n      });\n    },\n    \n    undo: () => {\n      set((draft) => {\n        if (draft.history.past.length === 0) return;\n        \n        const current: WorkflowSnapshot = {\n          nodes: draft.nodes,\n          edges: draft.edges,\n          workflow: draft.workflow,\n          timestamp: Date.now()\n        };\n        \n        draft.history.future.push(current);\n        const previous = draft.history.past.pop()!;\n        \n        draft.nodes = previous.nodes;\n        draft.edges = previous.edges;\n        draft.workflow = previous.workflow;\n      });\n    },\n    \n    redo: () => {\n      set((draft) => {\n        if (draft.history.future.length === 0) return;\n        \n        const current: WorkflowSnapshot = {\n          nodes: draft.nodes,\n          edges: draft.edges,\n          workflow: draft.workflow,\n          timestamp: Date.now()\n        };\n        \n        draft.history.past.push(current);\n        const next = draft.history.future.pop()!;\n        \n        draft.nodes = next.nodes;\n        draft.edges = next.edges;\n        draft.workflow = next.workflow;\n      });\n    },\n    \n    // Implementation details...\n  }))\n);\n```\n\n## Enhanced Execution Engine\n\n### Execution Orchestrator (Major Improvement)\n```typescript\nclass WorkflowOrchestrator {\n  private workflowId: string;\n  private executionId: string;\n  private nodes: Map<string, WorkflowNode>;\n  private edges: WorkflowEdge[];\n  private executionState: ExecutionState;\n  private workerPool: WorkerPool; // ENHANCED: Pool of workers\n  private dataFlowController: DataFlowController; // ENHANCED: Manage data transfer\n  private checkpointManager: CheckpointManager; // ENHANCED: Save/restore state\n  private eventBus: EventEmitter;\n  private abortController: AbortController; // ENHANCED: Cancellation support\n  \n  constructor(workflow: Workflow, config?: ExecutionConfig) {\n    this.workflowId = workflow.id;\n    this.executionId = crypto.randomUUID();\n    this.nodes = new Map(workflow.nodes.map(n => [n.id, n]));\n    this.edges = workflow.edges;\n    this.executionState = this.initExecutionState();\n    this.workerPool = new WorkerPool(config?.maxWorkers || 4);\n    this.dataFlowController = new DataFlowController();\n    this.checkpointManager = new CheckpointManager();\n    this.eventBus = new EventEmitter();\n    this.abortController = new AbortController();\n  }\n  \n  async execute(): Promise<ExecutionResult> {\n    this.executionState.status = 'running';\n    this.executionState.startedAt = new Date();\n    this.eventBus.emit('execution:started', this.executionId);\n    \n    try {\n      // ENHANCED: Pre-execution validation\n      const validation = await this.validateBeforeExecution();\n      if (!validation.valid) {\n        throw new Error(`Validation failed: ${validation.errors.map(e => e.message).join(', ')}`);\n      }\n      \n      // Build execution DAG\n      const executionDAG = this.buildExecutionDAG();\n      \n      // ENHANCED: Check for resource availability\n      await this.checkResourceAvailability();\n      \n      // Topological sort with parallel levels\n      const executionLevels = this.topologicalSort(executionDAG);\n      \n      // Execute levels with proper error handling and checkpoints\n      for (let levelIndex = 0; levelIndex < executionLevels.length; levelIndex++) {\n        const level = executionLevels[levelIndex];\n        \n        // ENHANCED: Auto-checkpoint at each level\n        if (levelIndex > 0 && levelIndex % 3 === 0) {\n          await this.checkpointManager.createCheckpoint(this.executionState);\n        }\n        \n        // ENHANCED: Check for cancellation\n        if (this.abortController.signal.aborted) {\n          throw new CancellationError('Execution cancelled by user');\n        }\n        \n        try {\n          await this.executeLevel(level);\n        } catch (error) {\n          // ENHANCED: Level-specific error handling\n          const recoveryAction = await this.handleLevelError(levelIndex, level, error);\n          \n          if (recoveryAction === 'stop') {\n            throw error;\n          } else if (recoveryAction === 'retry') {\n            await this.executeLevel(level); // Retry once\n          } else if (recoveryAction === 'skip') {\n            // Mark nodes as skipped and continue\n            for (const nodeId of level) {\n              this.executionState.nodeStates.get(nodeId)!.status = 'skipped';\n            }\n          }\n        }\n      }\n      \n      this.executionState.status = 'completed';\n      this.executionState.completedAt = new Date();\n      this.eventBus.emit('execution:complete', this.executionId, this.executionState);\n      \n      return {\n        success: true,\n        executionId: this.executionId,\n        executionState: this.executionState,\n        outputs: this.gatherWorkflowOutputs()\n      };\n      \n    } catch (error) {\n      this.executionState.status = error instanceof CancellationError ? 'cancelled' : 'failed';\n      this.executionState.errors.push({\n        nodeId: 'workflow',\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: false,\n        retryAttempts: 0\n      });\n      \n      const event = error instanceof CancellationError ? 'execution:cancelled' : 'execution:failed';\n      this.eventBus.emit(event, this.executionId, error);\n      \n      return {\n        success: false,\n        executionId: this.executionId,\n        executionState: this.executionState,\n        error: error as Error\n      };\n    } finally {\n      // ENHANCED: Cleanup resources\n      await this.cleanup();\n    }\n  }\n  \n  private async executeLevel(nodeIds: string[]): Promise<void> {\n    // ENHANCED: Execute with resource-aware scheduling\n    const nodePromises = nodeIds.map(nodeId => \n      this.workerPool.schedule(() => this.executeNode(nodeId))\n    );\n    \n    // ENHANCED: Wait with proper error aggregation\n    const results = await Promise.allSettled(nodePromises);\n    \n    // Check for failures\n    const failures = results.filter(r => r.status === 'rejected');\n    if (failures.length > 0 && this.executionState.workflow?.globalConfig.errorHandling === 'stop') {\n      throw new AggregateError(\n        failures.map(f => (f as PromiseRejectedResult).reason),\n        `${failures.length} nodes failed in level`\n      );\n    }\n  }\n  \n  private async executeNode(nodeId: string): Promise<NodeExecutionResult> {\n    const node = this.nodes.get(nodeId)!;\n    const nodeState = this.executionState.nodeStates.get(nodeId)!;\n    \n    // ENHANCED: Validate inputs before execution\n    const inputValidation = await this.validateNodeInputs(nodeId);\n    if (!inputValidation.valid) {\n      nodeState.status = 'error';\n      nodeState.error = {\n        nodeId,\n        timestamp: new Date(),\n        error: new Error(`Input validation failed: ${inputValidation.errors.join(', ')}`),\n        recoverable: false,\n        retryAttempts: 0\n      };\n      throw nodeState.error.error;\n    }\n    \n    nodeState.status = 'running';\n    nodeState.startedAt = new Date();\n    this.eventBus.emit('execution:node:start', nodeId);\n    \n    try {\n      // Gather and validate inputs\n      const inputs = await this.dataFlowController.gatherInputs(\n        nodeId, \n        this.edges, \n        this.executionState\n      );\n      nodeState.input = inputs;\n      \n      // ENHANCED: Apply resource limits\n      const resourceToken = await this.acquireResourceToken(node);\n      \n      try {\n        // Execute with timeout and cancellation\n        const result = await this.executeAgentWithTimeout(\n          node, \n          inputs,\n          node.data.config.timeout || 300000 // 5 min default\n        );\n        \n        // ENHANCED: Validate output schema\n        await this.validateNodeOutput(nodeId, result.output);\n        \n        nodeState.output = result.output;\n        nodeState.logs.push(...result.logs);\n        nodeState.metrics = result.metrics;\n        nodeState.status = 'success';\n        nodeState.completedAt = new Date();\n        \n        // ENHANCED: Store output with size limit\n        await this.dataFlowController.storeOutput(\n          nodeId,\n          result.output,\n          node.data.config.memoryLimit\n        );\n        \n        // ENHANCED: Update edge states\n        await this.updateOutgoingEdges(nodeId, result.output);\n        \n        this.eventBus.emit('execution:node:complete', nodeId, result);\n        \n        return result;\n        \n      } finally {\n        // Release resource token\n        resourceToken.release();\n      }\n      \n    } catch (error) {\n      nodeState.status = 'error';\n      nodeState.completedAt = new Date();\n      \n      const executionError: ExecutionError = {\n        nodeId,\n        timestamp: new Date(),\n        error: error as Error,\n        recoverable: this.isRecoverableError(error),\n        retryAttempts: nodeState.error?.retryAttempts || 0\n      };\n      \n      nodeState.error = executionError;\n      this.eventBus.emit('execution:node:error', nodeId, error);\n      \n      // ENHANCED: Retry logic with backoff\n      if (executionError.recoverable && \n          node.data.config.retryConfig &&\n          executionError.retryAttempts < node.data.config.retryConfig.maxAttempts) {\n        \n        executionError.retryAttempts++;\n        const delay = this.calculateBackoff(\n          node.data.config.retryConfig,\n          executionError.retryAttempts\n        );\n        \n        nodeState.logs.push({\n          timestamp: new Date(),\n          level: 'warn',\n          nodeId,\n          message: `Retrying after ${delay}ms (attempt ${executionError.retryAttempts})`,\n        });\n        \n        await new Promise(resolve => setTimeout(resolve, delay));\n        return this.executeNode(nodeId); // Recursive retry\n      }\n      \n      throw error;\n    }\n  }\n  \n  // ENHANCED: Execute with timeout and cancellation support\n  private async executeAgentWithTimeout(\n    node: WorkflowNode,\n    inputs: any,\n    timeoutMs: number\n  ): Promise<AgentExecutionResult> {\n    return new Promise((resolve, reject) => {\n      const timeoutId = setTimeout(() => {\n        reject(new Error(`Node execution timed out after ${timeoutMs}ms`));\n      }, timeoutMs);\n      \n      // Listen for cancellation\n      this.abortController.signal.addEventListener('abort', () => {\n        clearTimeout(timeoutId);\n        reject(new CancellationError('Execution cancelled'));\n      });\n      \n      this.workerPool.execute(node, inputs, this.abortController.signal)\n        .then(result => {\n          clearTimeout(timeoutId);\n          resolve(result);\n        })\n        .catch(error => {\n          clearTimeout(timeoutId);\n          reject(error);\n        });\n    });\n  }\n  \n  // ENHANCED: Validate inputs match expected schema\n  private async validateNodeInputs(nodeId: string): Promise<ValidationResult> {\n    const node = this.nodes.get(nodeId)!;\n    const errors: string[] = [];\n    \n    // Check required inputs\n    for (const [key, inputDef] of Object.entries(node.data.inputs)) {\n      if (inputDef.required) {\n        const hasInput = this.edges.some(e => \n          e.target === nodeId && e.targetHandle === `input-${key}`\n        );\n        \n        if (!hasInput) {\n          errors.push(`Required input '${inputDef.name}' is not connected`);\n        }\n      }\n      \n      // ENHANCED: Validate schema if provided\n      if (inputDef.schema) {\n        const incomingEdges = this.edges.filter(e => \n          e.target === nodeId && e.targetHandle === `input-${key}`\n        );\n        \n        for (const edge of incomingEdges) {\n          const sourceOutput = this.executionState.globalContext.variables[edge.source];\n          if (sourceOutput) {\n            const schemaValidation = validateAgainstSchema(sourceOutput, inputDef.schema);\n            if (!schemaValidation.valid) {\n              errors.push(`Input '${inputDef.name}' schema validation failed: ${schemaValidation.errors.join(', ')}`);\n            }\n          }\n        }\n      }\n    }\n    \n    return {\n      valid: errors.length === 0,\n      errors: errors.map(msg => ({ type: 'config', nodeId, message: msg, severity: 'error' })),\n      warnings: []\n    };\n  }\n  \n  // ENHANCED: Validate output matches expected schema\n  private async validateNodeOutput(nodeId: string, output: any): Promise<void> {\n    const node = this.nodes.get(nodeId)!;\n    \n    for (const [key, outputDef] of Object.entries(node.data.outputs)) {\n      if (outputDef.schema) {\n        const validation = validateAgainstSchema(output, outputDef.schema);\n        if (!validation.valid) {\n          throw new Error(\n            `Output '${outputDef.name}' schema validation failed: ${validation.errors.join(', ')}`\n          );\n        }\n      }\n    }\n  }\n  \n  // ENHANCED: Update edge states when data flows\n  private async updateOutgoingEdges(nodeId: string, output: any): Promise<void> {\n    const outgoingEdges = this.edges.filter(e => e.source === nodeId);\n    \n    for (const edge of outgoingEdges) {\n      const edgeState = this.executionState.edgeStates.get(edge.id);\n      if (edgeState) {\n        edgeState.status = 'active';\n        edgeState.startedAt = new Date();\n        \n        // Apply transformations\n        let transformedData = output;\n        if (edge.data.transformations) {\n          transformedData = await this.dataFlowController.applyTransformations(\n            output,\n            edge.data.transformations\n          );\n        }\n        \n        // Validate against edge rules\n        if (edge.data.validation) {\n          const validation = await this.dataFlowController.validateData(\n            transformedData,\n            edge.data.validation\n          );\n          \n          if (!validation.valid) {\n            edgeState.status = 'error';\n            edgeState.validationErrors = validation.errors;\n            throw new Error(`Edge validation failed: ${validation.errors.map(e => e.message).join(', ')}`);\n          }\n        }\n        \n        edgeState.data = transformedData;\n        edgeState.bytesTransferred = JSON.stringify(transformedData).length;\n        edgeState.status = 'complete';\n        edgeState.completedAt = new Date();\n      }\n    }\n  }\n  \n  // ENHANCED: Check resource availability before execution\n  private async checkResourceAvailability(): Promise<void> {\n    const limits = this.executionState.workflow?.globalConfig.resourceLimits;\n    if (!limits) return;\n    \n    // Estimate resource requirements\n    const estimatedTokens = this.estimateTokenUsage();\n    const estimatedMemory = this.estimateMemoryUsage();\n    \n    if (limits.maxTotalTokens && estimatedTokens > limits.maxTotalTokens) {\n      throw new Error(\n        `Estimated token usage (${estimatedTokens}) exceeds limit (${limits.maxTotalTokens})`\n      );\n    }\n    \n    if (limits.maxMemoryMB && estimatedMemory > limits.maxMemoryMB) {\n      throw new Error(\n        `Estimated memory usage (${estimatedMemory}MB) exceeds limit (${limits.maxMemoryMB}MB)`\n      );\n    }\n  }\n  \n  private async cleanup(): Promise<void> {\n    await this.workerPool.terminate();\n    this.dataFlowController.cleanup();\n  }\n}\n```\n\n### Enhanced Data Flow Controller\n```typescript\nclass DataFlowController {\n  private dataCache: Map<string, any> = new Map();\n  private memoryUsage: number = 0;\n  \n  async gatherInputs(\n    nodeId: string,\n    edges: WorkflowEdge[],\n    executionState: ExecutionState\n  ): Promise<Record<string, any>> {\n    const inputs: Record<string, any> = {};\n    const incomingEdges = edges.filter(e => e.target === nodeId);\n    \n    for (const edge of incomingEdges) {\n      const sourceOutput = executionState.globalContext.variables[edge.source];\n      \n      if (sourceOutput === undefined) {\n        throw new Error(`Missing output from source node: ${edge.source}`);\n      }\n      \n      // Apply transformations\n      const transformedOutput = edge.data.transformations\n        ? await this.applyTransformations(sourceOutput, edge.data.transformations)\n        : sourceOutput;\n      \n      // Map to target input handle\n      const inputKey = edge.targetHandle?.replace('input-', '') || edge.source;\n      inputs[inputKey] = transformedOutput;\n    }\n    \n    return inputs;\n  }\n  \n  async applyTransformations(\n    data: any,\n    transformations: Transformation[]\n  ): Promise<any> {\n    let result = data;\n    \n    for (const transform of transformations) {\n      switch (transform.type) {\n        case 'jsonpath':\n          result = this.applyJsonPath(result, transform.config.path);\n          break;\n        case 'template':\n          result = this.applyTemplate(result, transform.config.template);\n          break;\n        case 'filter':\n          result = this.applyFilter(result, transform.config.predicate);\n          break;\n        case 'map':\n          result = this.applyMap(result, transform.config.mapper);\n          break;\n        case 'aggregate':\n          result = this.applyAggregate(result, transform.config.aggregator);\n          break;\n        default:\n          throw new Error(`Unknown transformation type: ${transform.type}`);\n      }\n    }\n    \n    return result;\n  }\n  \n  async validateData(data: any, rules: ValidationRule[]): Promise<ValidationResult> {\n    const errors: ValidationError[] = [];\n    \n    for (const rule of rules) {\n      switch (rule.type) {\n        case 'schema':\n          const schemaResult = validateAgainstSchema(data, rule.config.schema);\n          if (!schemaResult.valid) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || schemaResult.errors.join(', '),\n              severity: 'error'\n            });\n          }\n          break;\n          \n        case 'size':\n          const size = JSON.stringify(data).length;\n          if (size > rule.config.maxBytes) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || `Data size (${size} bytes) exceeds limit (${rule.config.maxBytes} bytes)`,\n              severity: 'error'\n            });\n          }\n          break;\n          \n        case 'format':\n          if (!this.validateFormat(data, rule.config.format)) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || `Data does not match format: ${rule.config.format}`,\n              severity: 'error'\n            });\n          }\n          break;\n          \n        case 'custom':\n          const customResult = await rule.config.validator(data);\n          if (!customResult.valid) {\n            errors.push({\n              type: 'connection',\n              message: rule.errorMessage || customResult.error,\n              severity: 'error'\n            });\n          }\n          break;\n      }\n    }\n    \n    return {\n      valid: errors.length === 0,\n      errors,\n      warnings: []\n    };\n  }\n  \n  async storeOutput(nodeId: string, output: any, memoryLimit?: number): Promise<void> {\n    const size = JSON.stringify(output).length;\n    \n    // Check memory limit\n    if (memoryLimit && this.memoryUsage + size > memoryLimit * 1024 * 1024) {\n      throw new Error(`Memory limit exceeded: ${this.memoryUsage + size} bytes`);\n    }\n    \n    this.dataCache.set(nodeId, output);\n    this.memoryUsage += size;\n  }\n  \n  cleanup(): void {\n    this.dataCache.clear();\n    this.memoryUsage = 0;\n  }\n  \n  private applyJsonPath(data: any, path: string): any {\n    // JSONPath implementation\n    return JSONPath.query(data, path);\n  }\n  \n  private applyTemplate(data: any, template: string): any {\n    // Template engine (e.g., Handlebars)\n    return Handlebars.compile(template)(data);\n  }\n  \n  private applyFilter(data: any[], predicate: string): any[] {\n    // Safe eval or predicate parser\n    const fn = new Function('item', `return ${predicate}`);\n    return data.filter(fn);\n  }\n  \n  private applyMap(data: any[], mapper: string): any[] {\n    const fn = new Function('item', `return ${mapper}`);\n    return data.map(fn);\n  }\n  \n  private applyAggregate(data: any[], aggregator: string): any {\n    const fn = new Function('acc', 'item', `return ${aggregator}`);\n    return data.reduce(fn, null);\n  }\n  \n  private validateFormat(data: any, format: string): boolean {\n    switch (format) {\n      case 'json':\n        try {\n          JSON.parse(typeof data === 'string' ? data : JSON.stringify(data));\n          return true;\n        } catch {\n          return false;\n        }\n      case 'url':\n        try {\n          new URL(data);\n          return true;\n        } catch {\n          return false;\n        }\n      case 'email':\n        return /^[^\\s@]+@[^\\s@]+\\.[^\\s@]+$/.test(data);\n      default:\n        return true;\n    }\n  }\n}\n```\n\n### Enhanced Worker Pool\n```typescript\nclass WorkerPool {\n  private workers: Worker[] = [];\n  private availableWorkers: Worker[] = [];\n  private queue: QueueItem[] = [];\n  private size: number;\n  \n  constructor(size: number = 4) {\n    this.size = size;\n    this.initWorkers();\n  }\n  \n  private initWorkers(): void {\n    for (let i = 0; i < this.size; i++) {\n      const worker = new Worker(\n        new URL('./execution.worker.ts', import.meta.url),\n        { type: 'module' }\n      );\n      this.workers.push(worker);\n      this.availableWorkers.push(worker);\n    }\n  }\n  \n  async schedule<T>(task: () => Promise<T>): Promise<T> {\n    return new Promise((resolve, reject) => {\n      this.queue.push({ task, resolve, reject });\n      this.processQueue();\n    });\n  }\n  \n  async execute(\n    node: WorkflowNode,\n    inputs: any,\n    signal: AbortSignal\n  ): Promise<AgentExecutionResult> {\n    return this.schedule(() => {\n      return new Promise((resolve, reject) => {\n        if (this.availableWorkers.length === 0) {\n          throw new Error('No available workers');\n        }\n        \n        const worker = this.availableWorkers.pop()!;\n        const messageId = crypto.randomUUID();\n        \n        const handler = (event: MessageEvent) => {\n          if (event.data.messageId === messageId) {\n            worker.removeEventListener('message', handler);\n            this.availableWorkers.push(worker);\n            this.processQueue();\n            \n            if (event.data.error) {\n              reject(new Error(event.data.error));\n            } else {\n              resolve(event.data.result);\n            }\n          }\n        };\n        \n        // Handle cancellation\n        const abortHandler = () => {\n          worker.postMessage({ messageId, type: 'cancel' });\n          worker.removeEventListener('message', handler);\n          this.availableWorkers.push(worker);\n          reject(new CancellationError('Execution cancelled'));\n        };\n        \n        if (signal.aborted) {\n          abortHandler();\n          return;\n        }\n        \n        signal.addEventListener('abort', abortHandler, { once: true });\n        \n        worker.addEventListener('message', handler);\n        worker.postMessage({\n          messageId,\n          type: 'execute',\n          node,\n          inputs\n        });\n      });\n    });\n  }\n  \n  private processQueue(): void {\n    while (this.queue.length > 0 && this.availableWorkers.length > 0) {\n      const item = this.queue.shift()!;\n      item.task().then(item.resolve).catch(item.reject);\n    }\n  }\n  \n  async terminate(): Promise<void> {\n    for (const worker of this.workers) {\n      worker.terminate();\n    }\n    this.workers = [];\n    this.availableWorkers = [];\n    this.queue = [];\n  }\n}\n```\n\n### Enhanced Checkpoint Manager\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    const checkpoint: ExecutionCheckpoint = {\n      timestamp: new Date(),\n      nodeStates: new Map(executionState.nodeStates),\n      globalContext: this.cloneContext(executionState.globalContext),\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    const store = tx.objectStore('checkpoints');\n    \n    await store.put({\n      id: checkpointId,\n      executionId: executionState.executionId,\n      checkpoint: this.serializeCheckpoint(checkpoint)\n    });\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string\n  ): Promise<ExecutionCheckpoint> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    const record = await store.get(checkpointId);\n    \n    if (!record) {\n      throw new Error(`Checkpoint not found: ${checkpointId}`);\n    }\n    \n    return this.deserializeCheckpoint(record.checkpoint);\n  }\n  \n  private cloneContext(context: ExecutionContext): ExecutionContext {\n    return {\n      variables: { ...context.variables },\n      secrets: new Map(context.secrets),\n      sharedMemory: context.sharedMemory, // Reference, not cloned\n      artifacts: new Map(context.artifacts)\n    };\n  }\n  \n  private serializeCheckpoint(checkpoint: ExecutionCheckpoint): string {\n    return JSON.stringify({\n      timestamp: checkpoint.timestamp.toISOString(),\n      nodeStates: Array.from(checkpoint.nodeStates.entries()),\n      globalContext: {\n        variables: checkpoint.globalContext.variables,\n        // Exclude non-serializable items\n      },\n      reason: checkpoint.reason\n    });\n  }\n  \n  private deserializeCheckpoint(data: string): ExecutionCheckpoint {\n    const parsed = JSON.parse(data);\n    return {\n      timestamp: new Date(parsed.timestamp),\n      nodeStates: new Map(parsed.nodeStates),\n      globalContext: {\n        variables: parsed.globalContext.variables,\n        secrets: new Map(),\n        artifacts: new Map()\n      },\n      reason: parsed.reason\n    };\n  }\n}\n```\n\n## What I Improved\n\n### 1. **Execution Engine Robustness**\n- **Worker Pool**: Added a proper worker pool with scheduling instead of creating workers on-demand. This prevents resource exhaustion and improves performance.\n- **Cancellation Support**: Added `AbortController` throughout the execution chain for graceful cancellation.\n- **Checkpoint System**: Added automatic checkpoints every 3 levels and manual checkpoint capability for resuming long-running workflows.\n- **Resource Limits**: Added memory limits, token limits, and execution time limits with pre-flight checks.\n- **Timeout Handling**: Wrapped execution with configurable timeouts to prevent infinite hangs.\n\n### 2. **Data Flow Architecture**\n- **Data Flow Controller**: Created a dedicated controller to manage data transfer, transformations, and validation between nodes.\n- **Input/Output Definitions**: Added explicit input/output schemas to nodes with validation, making data contracts clear and type-safe.\n- **Multiple Handles**: Support multiple input/output handles per node for different data types (instead of single source/target).\n- **Edge State Tracking**: Track edge execution state separately from nodes, including bytes transferred and validation errors.\n- **Transformation Pipeline**: Enhanced transformations with JSONPath, templates, filters, maps, and aggregations.\n- **Validation Rules**: Added comprehensive validation rules (schema, size, format, custom) on edges.\n\n### 3. **Error Handling**\n- **Aggregate Errors**: Use `AggregateError` to collect multiple failures in parallel execution levels.\n- **Recoverable Errors**: Distinguish between recoverable (retry) and non-recoverable (stop) errors.\n- **Retry Configuration**: Per-node retry config with backoff strategies and specific error patterns to retry.\n- **Level Error Recovery**: Handle errors at the level (not just node) to decide whether to stop, retry, or skip.\n- **Warnings**: Added warnings for non-fatal issues (performance, cost, resource usage).\n- **Input/Output Validation**: Validate inputs before execution and outputs after execution against schemas.\n\n### 4. **Real-time Visualization**\n- **Edge Animation**: Animate data flow with moving particles along edges during execution.\n- **Edge State Display**: Show validation errors, transformations, and data size on edges.\n- **Streaming Preview**: Show streaming output in real-time within node body.\n- **Expandable Nodes**: Expand nodes to see detailed logs and metrics inline.\n- **Status Granularity**: Added more statuses: `queued`, `warning`, `cancelled` for better visibility.\n- **Event System**: Emit typed events for all workflow changes and execution progress.\n\n### 5. **State Management**\n- **Undo/Redo**: Added full undo/redo with snapshot history (up to 50 snapshots).\n- **Immer Integration**: Use immer middleware for immutable state updates.\n- **Event Emitter**: Type-safe event system for reactive updates.\n- **Edge Selection**: Support selecting and configuring edges in properties panel.\n\n### 6. **Other Improvements**\n- **Execution Queue**: Support queuing multiple workflow executions.\n- **Resource Usage Tracking**: Track total tokens, cost, memory, API calls.\n- **Notification System**: Configurable notifications via browser, email, or webhook.\n- **Export/Import**: Export workflows as JSON or YAML for sharing.\n- **Metadata**: Track workflow metadata (author, tags, category, success rate).\n- **Transformer Node Type**: Added transformer node for pure data transformations without AI.\n- **Selection Toolbar**: Multi-select nodes and bulk operations.\n\n## What I Kept\n\nThe original design had excellent foundations:\n\n- **React Flow**: Perfect choice for the canvas library - mature, performant, extensible.\n- **Zustand**: Good state management choice - lightweight and performant.\n- **Web Workers**: Correct approach for non-blocking execution.\n- **IndexedDB**: Right storage solution for large workflows.\n- **Topological Sort with Parallelization**: Excellent approach for DAG execution.\n- **UI Component Structure**: Canvas, Properties Panel, Toolbar layout is well-designed.\n- **Template System**: Variable substitution approach is solid.\n- **Validation Service**: Graph validation logic is comprehensive.\n\n## Remaining Concerns\n\n### 1. **Memory Management**\n- Large workflows with many artifacts could exhaust browser memory\n- Need aggressive garbage collection strategy for completed nodes\n- Consider offloading artifacts to IndexedDB instead of keeping in memory\n- SharedArrayBuffer may not be available in all browsers (requires CORS headers)\n\n### 2. **Streaming Implementation**\n- Streaming adds significant complexity to data flow\n- How to handle backpressure if downstream nodes can't keep up?\n- Should streaming outputs be persisted or only the final result?\n- Edge transformations on streams require different handling\n\n### 3. **Security**\n- Transformation functions use `new Function()` which is risky\n- Need proper sandboxing or switch to a safe expression evaluator\n- Secrets in ExecutionContext need encryption at rest\n- Web Workers can still access some APIs - need to restrict\n\n### 4. **Error Recovery**\n- Retry logic could lead to exponential cost if not careful\n- Partial failures in parallel levels are complex to reason about\n- Checkpoint restoration doesn't handle external state (API calls already made)\n\n### 5. **Performance at Scale**\n- Workflows with 100+ nodes may overwhelm the UI\n- React Flow re-renders can be expensive with many edges\n- Need virtual rendering or viewport culling for very large workflows\n- Worker pool size needs dynamic adjustment based on browser capabilities\n\n### 6. **Testing**\n- Web Workers are difficult to test\n- Execution orchestration has many code paths (success, failure, retry, cancel)\n- Need comprehensive integration tests for edge cases\n\n## Convergence Status\n\n**ITERATING** - The design has significant improvements but needs another round to address:\n1. Memory management strategy for large workflows\n2. Streaming implementation details and backpressure handling\n3. Security sandboxing for custom transformations\n4. Checkpoint/resume edge cases\n5. Performance optimization strategy for 100+ node workflows\n\nPROMPT_FOR_ARCHITECT:\nPlease focus on:\n1. **Memory management**: How should we handle workflows that generate GBs of artifacts? Should we stream to IndexedDB? Implement LRU eviction?\n2. **Streaming architecture**: Can you design the backpressure mechanism for streaming between nodes? How do edge transformations work on streams?\n3. **Security model**: How can we safely allow custom transformations without `new Function()`? Should we use a safe expression language like JSONata?\n4. **Checkpoint consistency**: How do we handle external side effects (API calls, file writes) during checkpoint restoration? Should we track \"compensation\" operations?"
      }
    },
    {
      "round_number": 2,
      "agent_a": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Streaming  \u2502  \u2502  Memory    \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Manager   \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502  Hybrid    \u2502  \u2502   Safe     \u2502  \u2502   Side     \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  Storage   \u2502  \u2502 Expression \u2502  \u2502   Effect   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  Manager   \u2502  \u2502  Evaluator \u2502  \u2502   Tracker  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with custom viewport rendering\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Dual-mode: in-memory for small data, streaming for large data\n- **Storage**: Hybrid - Memory + IndexedDB with automatic tiering\n- **Expressions**: JSONata (safe, sandboxed expression language)\n- **Workers**: Shared Worker pool with resource quotas\n\n## Memory Management Architecture\n\n### Hybrid Storage Manager\n\nThe key innovation is **automatic tiering** between memory and IndexedDB based on data size and access patterns:\n\n```typescript\ninterface StorageConfig {\n  memoryThresholdBytes: number; // 10MB default\n  maxMemoryBytes: number; // 100MB default\n  evictionStrategy: 'lru' | 'size-based' | 'hybrid';\n  compressionEnabled: boolean;\n  persistenceMode: 'eager' | 'lazy' | 'manual';\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CachedData> = new Map();\n  private accessLog: Map<string, AccessMetadata> = new Map();\n  private currentMemoryUsage: number = 0;\n  private db: IDBDatabase;\n  private config: StorageConfig;\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initIndexedDB();\n  }\n  \n  async store(key: string, data: any, metadata: DataMetadata): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new Blob([serialized]).size;\n    \n    // Automatic tiering decision\n    const shouldStoreInMemory = this.shouldUseMemory(sizeBytes, metadata);\n    \n    if (shouldStoreInMemory) {\n      await this.storeInMemory(key, data, sizeBytes, metadata);\n    } else {\n      await this.storeInIndexedDB(key, data, sizeBytes, metadata);\n    }\n    \n    // Update access log\n    this.accessLog.set(key, {\n      lastAccessed: Date.now(),\n      accessCount: 0,\n      size: sizeBytes,\n      location: shouldStoreInMemory ? 'memory' : 'indexeddb'\n    });\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    // Check memory first\n    if (this.memoryCache.has(key)) {\n      this.updateAccessMetadata(key);\n      return this.memoryCache.get(key)!.data;\n    }\n    \n    // Fallback to IndexedDB\n    const data = await this.retrieveFromIndexedDB(key);\n    \n    // Promote to memory if frequently accessed\n    const metadata = this.accessLog.get(key);\n    if (metadata && this.shouldPromoteToMemory(metadata)) {\n      await this.promoteToMemory(key, data);\n    }\n    \n    return data;\n  }\n  \n  private shouldUseMemory(sizeBytes: number, metadata: DataMetadata): boolean {\n    // Never store in memory if exceeds threshold\n    if (sizeBytes > this.config.memoryThresholdBytes) {\n      return false;\n    }\n    \n    // Check if we have room\n    if (this.currentMemoryUsage + sizeBytes > this.config.maxMemoryBytes) {\n      // Try to evict\n      if (!this.evictToMakeRoom(sizeBytes)) {\n        return false;\n      }\n    }\n    \n    // Use memory for hot data (intermediate results, frequently accessed)\n    if (metadata.accessPattern === 'hot' || metadata.temporary) {\n      return true;\n    }\n    \n    // Use memory for small data that's cheap to keep\n    return sizeBytes < 1024 * 100; // 100KB\n  }\n  \n  private evictToMakeRoom(requiredBytes: number): boolean {\n    const candidates = Array.from(this.memoryCache.entries())\n      .map(([key, data]) => ({\n        key,\n        data,\n        metadata: this.accessLog.get(key)!\n      }))\n      .sort((a, b) => {\n        // Hybrid eviction: prioritize by LRU and size\n        if (this.config.evictionStrategy === 'lru') {\n          return a.metadata.lastAccessed - b.metadata.lastAccessed;\n        } else if (this.config.evictionStrategy === 'size-based') {\n          return b.metadata.size - a.metadata.size; // Evict largest first\n        } else {\n          // Hybrid: score = (age * size) - evict oldest large items first\n          const scoreA = (Date.now() - a.metadata.lastAccessed) * a.metadata.size;\n          const scoreB = (Date.now() - b.metadata.lastAccessed) * b.metadata.size;\n          return scoreB - scoreA;\n        }\n      });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const candidate of candidates) {\n      // Never evict if accessed in last 5 seconds\n      if (Date.now() - candidate.metadata.lastAccessed < 5000) {\n        continue;\n      }\n      \n      toEvict.push(candidate.key);\n      freedBytes += candidate.metadata.size;\n      \n      if (freedBytes >= requiredBytes) {\n        break;\n      }\n    }\n    \n    // Perform eviction\n    for (const key of toEvict) {\n      this.evictToIndexedDB(key);\n    }\n    \n    return freedBytes >= requiredBytes;\n  }\n  \n  private async evictToIndexedDB(key: string): Promise<void> {\n    const cached = this.memoryCache.get(key);\n    if (!cached) return;\n    \n    // Move to IndexedDB\n    await this.storeInIndexedDB(key, cached.data, cached.size, cached.metadata);\n    \n    // Remove from memory\n    this.memoryCache.delete(key);\n    this.currentMemoryUsage -= cached.size;\n    \n    // Update location\n    const metadata = this.accessLog.get(key);\n    if (metadata) {\n      metadata.location = 'indexeddb';\n    }\n  }\n  \n  private async storeInMemory(\n    key: string, \n    data: any, \n    size: number, \n    metadata: DataMetadata\n  ): Promise<void> {\n    this.memoryCache.set(key, {\n      data,\n      size,\n      metadata,\n      storedAt: Date.now()\n    });\n    this.currentMemoryUsage += size;\n  }\n  \n  private async storeInIndexedDB(\n    key: string,\n    data: any,\n    size: number,\n    metadata: DataMetadata\n  ): Promise<void> {\n    const serialized = JSON.stringify(data);\n    \n    // Optional compression for large data\n    const toStore = this.config.compressionEnabled && size > 1024 * 100\n      ? await this.compress(serialized)\n      : serialized;\n    \n    const tx = this.db.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      size,\n      metadata,\n      compressed: this.config.compressionEnabled && size > 1024 * 100,\n      storedAt: Date.now()\n    });\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.db.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Data not found: ${key}`);\n    }\n    \n    const data = record.compressed \n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(data);\n  }\n  \n  private shouldPromoteToMemory(metadata: AccessMetadata): boolean {\n    // Promote if accessed 3+ times in last minute\n    return metadata.accessCount >= 3 && \n           Date.now() - metadata.lastAccessed < 60000;\n  }\n  \n  private async promoteToMemory(key: string, data: any): Promise<void> {\n    const metadata = this.accessLog.get(key);\n    if (!metadata) return;\n    \n    await this.storeInMemory(key, data, metadata.size, {\n      temporary: true,\n      accessPattern: 'hot'\n    });\n    \n    metadata.location = 'memory';\n  }\n  \n  private updateAccessMetadata(key: string): void {\n    const metadata = this.accessLog.get(key);\n    if (metadata) {\n      metadata.lastAccessed = Date.now();\n      metadata.accessCount++;\n    }\n  }\n  \n  async clear(predicate?: (key: string, metadata: AccessMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      this.memoryCache.clear();\n      this.accessLog.clear();\n      this.currentMemoryUsage = 0;\n      \n      const tx = this.db.transaction('artifacts', 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      return;\n    }\n    \n    // Selective clear\n    for (const [key, metadata] of this.accessLog.entries()) {\n      if (predicate(key, metadata)) {\n        this.memoryCache.delete(key);\n        \n        const tx = this.db.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        \n        this.accessLog.delete(key);\n      }\n    }\n    \n    // Recalculate memory usage\n    this.currentMemoryUsage = Array.from(this.memoryCache.values())\n      .reduce((sum, cached) => sum + cached.size, 0);\n  }\n  \n  getMemoryUsage(): MemoryStats {\n    return {\n      currentBytes: this.currentMemoryUsage,\n      maxBytes: this.config.maxMemoryBytes,\n      itemCount: this.memoryCache.size,\n      utilizationPercent: (this.currentMemoryUsage / this.config.maxMemoryBytes) * 100,\n      evictionCount: this.evictionCount\n    };\n  }\n  \n  private async compress(data: string): Promise<Uint8Array> {\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    return new Uint8Array(buffer);\n  }\n  \n  private async decompress(data: Uint8Array): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowArtifacts', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('artifacts')) {\n          db.createObjectStore('artifacts', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n}\n\ninterface CachedData {\n  data: any;\n  size: number;\n  metadata: DataMetadata;\n  storedAt: number;\n}\n\ninterface DataMetadata {\n  temporary?: boolean; // Will be cleared after workflow completion\n  accessPattern?: 'hot' | 'warm' | 'cold';\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface AccessMetadata {\n  lastAccessed: number;\n  accessCount: number;\n  size: number;\n  location: 'memory' | 'indexeddb';\n}\n\ninterface MemoryStats {\n  currentBytes: number;\n  maxBytes: number;\n  itemCount: number;\n  utilizationPercent: number;\n  evictionCount: number;\n}\n```\n\n### Usage Example\n```typescript\nconst storageManager = new HybridStorageManager({\n  memoryThresholdBytes: 10 * 1024 * 1024, // 10MB\n  maxMemoryBytes: 100 * 1024 * 1024, // 100MB\n  evictionStrategy: 'hybrid',\n  compressionEnabled: true,\n  persistenceMode: 'lazy'\n});\n\n// Store node output - automatically tiered\nawait storageManager.store(\n  `node-${nodeId}-output`,\n  output,\n  {\n    nodeId,\n    executionId,\n    temporary: true, // Clear after workflow completes\n    accessPattern: 'hot' // Keep in memory if possible\n  }\n);\n\n// Retrieve - transparent whether in memory or IndexedDB\nconst output = await storageManager.retrieve(`node-${nodeId}-output`);\n\n// Clear temporary data after workflow completes\nawait storageManager.clear((key, metadata) => \n  metadata.location === 'memory' && \n  key.includes(executionId)\n);\n```\n\n## Streaming Architecture with Backpressure\n\n### Streaming Execution Model\n\nUse **ReadableStream** and **TransformStream** APIs for proper backpressure handling:\n\n```typescript\ninterface StreamConfig {\n  highWaterMark: number; // Buffer size\n  strategy: 'chunk' | 'line' | 'token'; // How to split data\n  backpressureThreshold: number; // When to pause upstream\n}\n\nclass StreamingExecutor {\n  private activeStreams: Map<string, StreamController> = new Map();\n  \n  async executeNodeStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any | ReadableStream>\n  ): Promise<ReadableStream> {\n    // Create output stream\n    const { readable, writable } = new TransformStream({\n      transform: async (chunk, controller) => {\n        // Process chunk through agent\n        const result = await this.processChunk(node, chunk);\n        controller.enqueue(result);\n      },\n      flush: async (controller) => {\n        // Final processing\n        controller.terminate();\n      }\n    }, {\n      highWaterMark: node.data.config.streamConfig?.highWaterMark || 10,\n      size: (chunk) => chunk.length || 1\n    });\n    \n    // Track stream for cancellation\n    const streamController: StreamController = {\n      nodeId: node.id,\n      readable,\n      writable,\n      cancel: () => writable.abort(),\n      pause: () => this.pauseStream(node.id),\n      resume: () => this.resumeStream(node.id)\n    };\n    \n    this.activeStreams.set(node.id, streamController);\n    \n    // Handle input streams\n    const inputStreams = Object.entries(inputs).filter(([_, v]) => \n      v instanceof ReadableStream\n    );\n    \n    if (inputStreams.length > 0) {\n      // Merge multiple input streams with backpressure\n      const mergedStream = this.mergeStreams(\n        inputStreams.map(([_, stream]) => stream as ReadableStream),\n        node.data.config.streamConfig\n      );\n      \n      // Pipe through node processor\n      mergedStream.pipeTo(writable, {\n        preventAbort: true,\n        preventClose: true,\n        preventCancel: true\n      }).catch(error => {\n        console.error(`Stream error in node ${node.id}:`, error);\n      });\n    }\n    \n    return readable;\n  }\n  \n  private mergeStreams(\n    streams: ReadableStream[],\n    config?: StreamConfig\n  ): ReadableStream {\n    return new ReadableStream({\n      async start(controller) {\n        const readers = streams.map(s => s.getReader());\n        const activeReaders = new Set(readers);\n        \n        // Read from all streams concurrently with backpressure\n        const readNext = async (reader: ReadableStreamDefaultReader) => {\n          try {\n            const { done, value } = await reader.read();\n            \n            if (done) {\n              activeReaders.delete(reader);\n              if (activeReaders.size === 0) {\n                controller.close();\n              }\n              return;\n            }\n            \n            // Enqueue with backpressure check\n            const desiredSize = controller.desiredSize;\n            if (desiredSize !== null && desiredSize <= 0) {\n              // Backpressure - wait before reading more\n              await new Promise(resolve => setTimeout(resolve, 100));\n            }\n            \n            controller.enqueue(value);\n            \n            // Continue reading\n            readNext(reader);\n          } catch (error) {\n            controller.error(error);\n          }\n        };\n        \n        // Start reading from all streams\n        readers.forEach(reader => readNext(reader));\n      },\n      \n      cancel(reason) {\n        // Cancel all input streams\n        streams.forEach(s => s.cancel(reason));\n      }\n    }, {\n      highWaterMark: config?.highWaterMark || 10\n    });\n  }\n  \n  private async processChunk(node: WorkflowNode, chunk: any): Promise<any> {\n    // Process chunk through agent (streaming API call)\n    // This is where actual LLM streaming happens\n    return chunk; // Placeholder\n  }\n  \n  private pauseStream(nodeId: string): void {\n    const controller = this.activeStreams.get(nodeId);\n    if (controller) {\n      // Implement pause by not reading from stream\n      controller.paused = true;\n    }\n  }\n  \n  private resumeStream(nodeId: string): void {\n    const controller = this.activeStreams.get(nodeId);\n    if (controller) {\n      controller.paused = false;\n    }\n  }\n  \n  cancelAllStreams(): void {\n    for (const [nodeId, controller] of this.activeStreams) {\n      controller.cancel();\n    }\n    this.activeStreams.clear();\n  }\n}\n\ninterface StreamController {\n  nodeId: string;\n  readable: ReadableStream;\n  writable: WritableStream;\n  paused?: boolean;\n  cancel: () => void;\n  pause: () => void;\n  resume: () => void;\n}\n```\n\n### Stream Transformations\n\nTransform streams safely using **TransformStream**:\n\n```typescript\nclass StreamTransformer {\n  applyTransformations(\n    stream: ReadableStream,\n    transformations: Transformation[]\n  ): ReadableStream {\n    let result = stream;\n    \n    for (const transform of transformations) {\n      result = result.pipeThrough(\n        this.createTransformStream(transform)\n      );\n    }\n    \n    return result;\n  }\n  \n  private createTransformStream(transform: Transformation): TransformStream {\n    switch (transform.type) {\n      case 'jsonpath':\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            try {\n              // Parse chunk as JSON\n              const data = typeof chunk === 'string' ? JSON.parse(chunk) : chunk;\n              // Apply JSONPath using JSONata\n              const result = await this.evaluator.evaluate(transform.config.path, data);\n              controller.enqueue(result);\n            } catch (error) {\n              controller.error(error);\n            }\n          }\n        });\n        \n      case 'filter':\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            const passes = await this.evaluator.evaluate(\n              transform.config.predicate,\n              { item: chunk }\n            );\n            if (passes) {\n              controller.enqueue(chunk);\n            }\n            // Otherwise drop chunk (no enqueue)\n          }\n        });\n        \n      case 'map':\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            const result = await this.evaluator.evaluate(\n              transform.config.mapper,\n              { item: chunk }\n            );\n            controller.enqueue(result);\n          }\n        });\n        \n      case 'batch':\n        // Accumulate chunks into batches\n        let buffer: any[] = [];\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            buffer.push(chunk);\n            if (buffer.length >= transform.config.batchSize) {\n              controller.enqueue(buffer);\n              buffer = [];\n            }\n          },\n          flush: async (controller) => {\n            if (buffer.length > 0) {\n              controller.enqueue(buffer);\n            }\n          }\n        });\n        \n      default:\n        throw new Error(`Unknown transformation: ${transform.type}`);\n    }\n  }\n}\n```\n\n### Dual-Mode Execution\n\nSupport both streaming and batch modes:\n\n```typescript\nclass DualModeOrchestrator {\n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<any | ReadableStream> {\n    // Decide mode based on data size and node config\n    const shouldStream = this.shouldUseStreamingMode(node, inputs);\n    \n    if (shouldStream) {\n      return this.streamingExecutor.executeNodeStreaming(node, inputs);\n    } else {\n      return this.batchExecutor.executeNodeBatch(node, inputs);\n    }\n  }\n  \n  private shouldUseStreamingMode(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): boolean {\n    // Use streaming if:\n    // 1. Node explicitly configured for streaming\n    if (node.data.config.streaming) return true;\n    \n    // 2. Any input is a stream\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream\n    );\n    if (hasStreamInput) return true;\n    \n    // 3. Expected output is large (estimated from input size)\n    const inputSize = this.estimateDataSize(inputs);\n    if (inputSize > 5 * 1024 * 1024) return true; // 5MB\n    \n    // 4. Downstream nodes need streaming\n    const hasStreamingDownstream = this.hasStreamingDownstream(node);\n    if (hasStreamingDownstream) return true;\n    \n    return false;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private hasStreamingDownstream(node: WorkflowNode): boolean {\n    const downstreamEdges = this.edges.filter(e => e.source === node.id);\n    return downstreamEdges.some(edge => {\n      const targetNode = this.nodes.get(edge.target);\n      return targetNode?.data.config.streaming;\n    });\n  }\n}\n```\n\n## Safe Expression Evaluation\n\nUse **JSONata** instead of `new Function()`:\n\n```typescript\nimport jsonata from 'jsonata';\n\nclass SafeExpressionEvaluator {\n  private cache: Map<string, jsonata.Expression> = new Map();\n  private readonly timeout: number = 5000; // 5s timeout\n  private readonly maxDepth: number = 100; // Prevent infinite recursion\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    // Get or compile expression\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      try {\n        compiled = jsonata(expression);\n        \n        // Register safe functions only\n        compiled.registerFunction('now', () => new Date().toISOString());\n        compiled.registerFunction('random', () => Math.random());\n        compiled.registerFunction('uuid', () => crypto.randomUUID());\n        \n        // Cache for reuse\n        this.cache.set(expression, compiled);\n      } catch (error) {\n        throw new Error(`Invalid expression: ${error.message}`);\n      }\n    }\n    \n    // Evaluate with timeout and error handling\n    try {\n      return await Promise.race([\n        compiled.evaluate(context, {\n          maxDepth: this.maxDepth\n        }),\n        new Promise((_, reject) => \n          setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n        )\n      ]);\n    } catch (error) {\n      throw new Error(`Expression evaluation failed: ${error.message}`);\n    }\n  }\n  \n  validate(expression: string): { valid: boolean; error?: string } {\n    try {\n      jsonata(expression);\n      return { valid: true };\n    } catch (error) {\n      return { valid: false, error: error.message };\n    }\n  }\n  \n  clearCache(): void {\n    this.cache.clear();\n  }\n}\n\n// Usage in transformations\nclass DataFlowController {\n  private evaluator = new SafeExpressionEvaluator();\n  \n  async applyTransformations(\n    data: any,\n    transformations: Transformation[]\n  ): Promise<any> {\n    let result = data;\n    \n    for (const transform of transformations) {\n      switch (transform.type) {\n        case 'jsonpath':\n          // JSONata expression: $.path.to.field\n          result = await this.evaluator.evaluate(transform.config.path, result);\n          break;\n          \n        case 'filter':\n          // JSONata predicate: $[price > 100]\n          const items = Array.isArray(result) ? result : [result];\n          result = [];\n          for (const item of items) {\n            const passes = await this.evaluator.evaluate(\n              transform.config.predicate,\n              item\n            );\n            if (passes) result.push(item);\n          }\n          break;\n          \n        case 'map':\n          // JSONata mapper: { \"name\": name, \"total\": price * quantity }\n          const mappedItems = Array.isArray(result) ? result : [result];\n          result = [];\n          for (const item of mappedItems) {\n            const mapped = await this.evaluator.evaluate(\n              transform.config.mapper,\n              item\n            );\n            result.push(mapped);\n          }\n          break;\n          \n        case 'aggregate':\n          // JSONata aggregation: $sum(items.price)\n          result = await this.evaluator.evaluate(\n            transform.config.aggregator,\n            result\n          );\n          break;\n      }\n    }\n    \n    return result;\n  }\n}\n```\n\n### JSONata Expression Examples\n\nProvide helpful examples in the UI:\n\n```typescript\nconst TRANSFORMATION_EXAMPLES = {\n  jsonpath: [\n    { label: 'Extract field', expression: '$.user.name' },\n    { label: 'Array elements', expression: '$.items[*].name' },\n    { label: 'Filter array', expression: '$.items[price > 100]' },\n    { label: 'Nested path', expression: '$.response.data.results' }\n  ],\n  filter: [\n    { label: 'Price > 100', expression: 'price > 100' },\n    { label: 'Status is active', expression: 'status = \"active\"' },\n    { label: 'Contains keyword', expression: '$contains(name, \"test\")' }\n  ],\n  map: [\n    { label: 'Extract fields', expression: '{ \"id\": id, \"name\": name }' },\n    { label: 'Calculate', expression: '{ \"total\": price * quantity }' },\n    { label: 'Transform', expression: '{ \"fullName\": firstName & \" \" & lastName }' }\n  ],\n  aggregate: [\n    { label: 'Sum', expression: '$sum(items.price)' },\n    { label: 'Average', expression: '$average(items.rating)' },\n    { label: 'Count', expression: '$count(items)' },\n    { label: 'Max', expression: '$max(items.value)' }\n  ]\n};\n```\n\n## Checkpoint Consistency & Side Effect Tracking\n\n### Side Effect Tracker\n\nTrack external operations for compensation during recovery:\n\n```typescript\ninterface SideEffect {\n  id: string;\n  nodeId: string;\n  type: 'api_call' | 'file_write' | 'database_update' | 'external_service';\n  operation: string;\n  timestamp: Date;\n  parameters: any;\n  result?: any;\n  compensatable: boolean;\n  compensationFn?: string; // JSONata expression for compensation\n  status: 'pending' | 'executed' | 'compensated' | 'failed';\n}\n\nclass SideEffectTracker {\n  private effects: Map<string, SideEffect> = new Map();\n  private db: IDBDatabase;\n  \n  async recordEffect(effect: Omit<SideEffect, 'id' | 'timestamp' | 'status'>): Promise<string> {\n    const id = crypto.randomUUID();\n    const sideEffect: SideEffect = {\n      ...effect,\n      id,\n      timestamp: new Date(),\n      status: 'pending'\n    };\n    \n    this.effects.set(id, sideEffect);\n    \n    // Persist immediately\n    await this.persistEffect(sideEffect);\n    \n    return id;\n  }\n  \n  async markExecuted(effectId: string, result: any): Promise<void> {\n    const effect = this.effects.get(effectId);\n    if (!effect) return;\n    \n    effect.status = 'executed';\n    effect.result = result;\n    \n    await this.persistEffect(effect);\n  }\n  \n  async compensate(effectId: string): Promise<void> {\n    const effect = this.effects.get(effectId);\n    if (!effect || !effect.compensatable) {\n      throw new Error(`Cannot compensate effect: ${effectId}`);\n    }\n    \n    if (effect.status !== 'executed') {\n      return; // Nothing to compensate\n    }\n    \n    // Execute compensation\n    if (effect.compensationFn) {\n      const evaluator = new SafeExpressionEvaluator();\n      await evaluator.evaluate(effect.compensationFn, {\n        effect,\n        result: effect.result\n      });\n    }\n    \n    effect.status = 'compensated';\n    await this.persistEffect(effect);\n  }\n  \n  async compensateAllForNode(nodeId: string): Promise<void> {\n    const nodeEffects = Array.from(this.effects.values())\n      .filter(e => e.nodeId === nodeId && e.status === 'executed')\n      .reverse(); // Compensate in reverse order\n    \n    for (const effect of nodeEffects) {\n      try {\n        await this.compensate(effect.id);\n      } catch (error) {\n        console.error(`Failed to compensate effect ${effect.id}:`, error);\n        effect.status = 'failed';\n        await this.persistEffect(effect);\n      }\n    }\n  }\n  \n  getEffectsForCheckpoint(checkpointTime: Date): SideEffect[] {\n    return Array.from(this.effects.values())\n      .filter(e => e.timestamp <= checkpointTime);\n  }\n  \n  private async persistEffect(effect: SideEffect): Promise<void> {\n    const tx = this.db.transaction('sideEffects', 'readwrite');\n    const store = tx.objectStore('sideEffects');\n    await store.put(effect);\n  }\n}\n```\n\n### Enhanced Checkpoint Manager with Side Effects\n\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  private sideEffectTracker: SideEffectTracker;\n  \n  constructor(sideEffectTracker: SideEffectTracker) {\n    this.sideEffectTracker = sideEffectTracker;\n    this.initIndexedDB();\n  }\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    // Capture side effects up to this point\n    const sideEffects = this.sideEffectTracker.getEffectsForCheckpoint(new Date());\n    \n    const checkpoint: ExecutionCheckpoint = {\n      id: checkpointId,\n      timestamp: new Date(),\n      nodeStates: new Map(executionState.nodeStates),\n      globalContext: this.cloneContext(executionState.globalContext),\n      sideEffects, // NEW: Track side effects\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    const store = tx.objectStore('checkpoints');\n    \n    await store.put({\n      id: checkpointId,\n      executionId: executionState.executionId,\n      checkpoint: this.serializeCheckpoint(checkpoint)\n    });\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string,\n    compensationStrategy: 'rollback' | 'forward' | 'manual'\n  ): Promise<ExecutionCheckpoint> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    const record = await store.get(checkpointId);\n    \n    if (!record) {\n      throw new Error(`Checkpoint not found: ${checkpointId}`);\n    }\n    \n    const checkpoint = this.deserializeCheckpoint(record.checkpoint);\n    \n    // Handle side effects based on strategy\n    if (compensationStrategy === 'rollback') {\n      // Compensate all side effects after checkpoint\n      await this.compensateSideEffectsAfter(checkpoint.timestamp);\n    } else if (compensationStrategy === 'forward') {\n      // Replay side effects from checkpoint to current state\n      // This is complex and may not be feasible for all effects\n      console.warn('Forward recovery not fully implemented');\n    }\n    // 'manual' means user will handle side effects\n    \n    return checkpoint;\n  }\n  \n  private async compensateSideEffectsAfter(timestamp: Date): Promise<void> {\n    const allEffects = Array.from(this.sideEffectTracker['effects'].values());\n    const effectsToCompensate = allEffects\n      .filter(e => e.timestamp > timestamp && e.status === 'executed')\n      .reverse(); // Reverse order for proper compensation\n    \n    for (const effect of effectsToCompensate) {\n      try {\n        await this.sideEffectTracker.compensate(effect.id);\n      } catch (error) {\n        console.error(`Failed to compensate effect ${effect.id}:`, error);\n        // Continue with other compensations\n      }\n    }\n  }\n  \n  private cloneContext(context: ExecutionContext): ExecutionContext {\n    return {\n      variables: structuredClone(context.variables),\n      secrets: new Map(context.secrets),\n      artifacts: new Map(context.artifacts)\n    };\n  }\n  \n  private serializeCheckpoint(checkpoint: ExecutionCheckpoint): string {\n    return JSON.stringify({\n      id: checkpoint.id,\n      timestamp: checkpoint.timestamp.toISOString(),\n      nodeStates: Array.from(checkpoint.nodeStates.entries()),\n      globalContext: {\n        variables: checkpoint.globalContext.variables\n      },\n      sideEffects: checkpoint.sideEffects,\n      reason: checkpoint.reason\n    });\n  }\n  \n  private deserializeCheckpoint(data: string): ExecutionCheckpoint {\n    const parsed = JSON.parse(data);\n    return {\n      id: parsed.id,\n      timestamp: new Date(parsed.timestamp),\n      nodeStates: new Map(parsed.nodeStates),\n      globalContext: {\n        variables: parsed.globalContext.variables,\n        secrets: new Map(),\n        artifacts: new Map()\n      },\n      sideEffects: parsed.sideEffects || [],\n      reason: parsed.reason\n    };\n  }\n  \n  async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowCheckpoints', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('checkpoints')) {\n          db.createObjectStore('checkpoints', { keyPath: 'id' });\n        }\n        if (!db.objectStoreNames.contains('sideEffects')) {\n          db.createObjectStore('sideEffects', { keyPath: 'id' });\n        }\n      };\n    });\n  }\n}\n```\n\n### Usage in Node Execution\n\n```typescript\nclass WorkflowOrchestrator {\n  private sideEffectTracker: SideEffectTracker;\n  private checkpointManager: CheckpointManager;\n  \n  private async executeNode(nodeId: string): Promise<NodeExecutionResult> {\n    const node = this.nodes.get(nodeId)!;\n    \n    try {\n      // Gather inputs\n      const inputs = await this.dataFlowController.gatherInputs(\n        nodeId,\n        this.edges,\n        this.executionState\n      );\n      \n      // Track side effect (if agent makes external calls)\n      const effectId = await this.sideEffectTracker.recordEffect({\n        nodeId,\n        type: 'api_call',\n        operation: 'execute_agent',\n        parameters: { inputs },\n        compensatable: true,\n        compensationFn: '$cancel_execution(effect.parameters.executionId)'\n      });\n      \n      // Execute agent\n      const result = await this.executeAgent(node, inputs);\n      \n      // Mark effect as executed\n      await this.sideEffectTracker.markExecuted(effectId, result);\n      \n      return result;\n      \n    } catch (error) {\n      // On error, create checkpoint and compensate this node\n      await this.checkpointManager.createCheckpoint(\n        this.executionState,\n        'error'\n      );\n      \n      await this.sideEffectTracker.compensateAllForNode(nodeId);\n      \n      throw error;\n    }\n  }\n}\n```\n\n### Compensation Examples\n\n```typescript\n// Example: API call compensation\nconst apiCallEffect: SideEffect = {\n  nodeId: 'node-123',\n  type: 'api_call',\n  operation: 'create_resource',\n  parameters: {\n    endpoint: '/api/resources',\n    method: 'POST',\n    body: { name: 'test' }\n  },\n  result: { id: 'resource-456' },\n  compensatable: true,\n  compensationFn: `\n    $http.delete(\"/api/resources/\" & result.id)\n  `\n};\n\n// Example: File write compensation\nconst fileWriteEffect: SideEffect = {\n  nodeId: 'node-123',\n  type: 'file_write',\n  operation: 'write_output',\n  parameters: {\n    path: '/tmp/output.json',\n    content: '...'\n  },\n  compensatable: true,\n  compensationFn: `\n    $fs.delete(parameters.path)\n  `\n};\n\n// Example: Non-compensatable effect (email sent)\nconst emailEffect: SideEffect = {\n  nodeId: 'node-123',\n  type: 'external_service',\n  operation: 'send_email',\n  parameters: {\n    to: 'user@example.com',\n    subject: 'Workflow result'\n  },\n  compensatable: false // Can't unsend email\n};\n```\n\n## Enhanced Node Component\n\n```typescript\ninterface AgentNodeData {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator' | 'transformer';\n  label: string;\n  config: AgentConfig;\n  status: ExecutionStatus;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n  inputs: Record<string, InputDefinition>;\n  outputs: Record<string, OutputDefinition>;\n  sideEffects: SideEffect[]; // NEW: Track side effects\n}\n\ninterface AgentConfig {\n  model: string;\n  temperature: number;\n  systemPrompt: string;\n  maxTokens?: number;\n  timeout?: number;\n  retryConfig?: RetryConfig;\n  streaming?: boolean;\n  memoryLimit?: number;\n  streamConfig?: StreamConfig; // NEW: Streaming configuration\n  trackSideEffects?: boolean; // NEW: Enable side effect tracking\n}\n\nconst AgentNode: React.FC<NodeProps<AgentNodeData>> = ({ data, selected }) => {\n  const [isExpanded, setIsExpanded] = useState(false);\n  const streamingOutput = useStreamingOutput(data.id);\n  const memoryUsage = useMemoryUsage(data.id);\n  \n  return (\n    <div className={`agent-node ${selected ? 'selected' : ''} ${isExpanded ? 'expanded' : ''}`}>\n      {/* Multiple typed input handles */}\n      {Object.entries(data.inputs).map(([key, input]) => (\n        <Handle\n          key={`input-${key}`}\n          type=\"target\"\n          position={Position.Left}\n          id={`input-${key}`}\n          className={`handle-${input.type}`}\n          style={{ top: calculateHandlePosition(key, data.inputs) }}\n        >\n          <Tooltip content={`${input.name} (${input.type})`}>\n            <HandleLabel required={input.required}>\n              {input.type === 'stream' && <StreamIcon />}\n              {input.name}\n            </HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n      \n      <div className=\"node-header\">\n        <AgentIcon type={data.type} />\n        <span className=\"node-label\">{data.label}</span>\n        <StatusBadge status={data.status} />\n        {data.config.streaming && <StreamBadge />}\n        {data.trackSideEffects && data.sideEffects.length > 0 && (\n          <SideEffectBadge count={data.sideEffects.length} />\n        )}\n        <Button size=\"xs\" onClick={() => setIsExpanded(!isExpanded)}>\n          {isExpanded ? <ChevronUpIcon /> : <ChevronDownIcon />}\n        </Button>\n      </div>\n      \n      <div className=\"node-body\">\n        {data.status === 'running' && (\n          <>\n            <ProgressBar metrics={data.metrics} />\n            {data.config.streaming && streamingOutput && (\n              <StreamPreview content={streamingOutput} />\n            )}\n            {memoryUsage && (\n              <MemoryIndicator usage={memoryUsage} />\n            )}\n          </>\n        )}\n        \n        {isExpanded && (\n          <NodeDetails \n            logs={data.logs} \n            metrics={data.metrics}\n            sideEffects={data.sideEffects}\n          />\n        )}\n      </div>\n      \n      {/* Multiple typed output handles */}\n      {Object.entries(data.outputs).map(([key, output]) => (\n        <Handle\n          key={`output-${key}`}\n          type=\"source\"\n          position={Position.Right}\n          id={`output-${key}`}\n          className={`handle-${output.type}`}\n          style={{ top: calculateHandlePosition(key, data.outputs) }}\n        >\n          <Tooltip content={`${output.name} (${output.type})`}>\n            <HandleLabel>\n              {output.type === 'stream' && <StreamIcon />}\n              {output.name}\n            </HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n    </div>\n  );\n};\n```\n\n## Performance Optimizations for Large Workflows\n\n### Viewport Culling\n\nOnly render nodes visible in viewport:\n\n```typescript\nconst WorkflowCanvas: React.FC<WorkflowCanvasProps> = ({ workflowId }) => {\n  const { nodes, edges } = useWorkflowStore();\n  const [viewport, setViewport] = useState<Viewport>({ x: 0, y: 0, zoom: 1 });\n  \n  // Filter nodes to only those in viewport\n  const visibleNodes = useMemo(() => {\n    if (nodes.length < 50) return nodes; // No need for culling\n    \n    return nodes.filter(node => {\n      const nodeRect = {\n        x: node.position.x,\n        y: node.position.y,\n        width: node.width || 200,\n        height: node.height || 100\n      };\n      \n      return isRectInViewport(nodeRect, viewport);\n    });\n  }, [nodes, viewport]);\n  \n  // Edges connected to visible nodes\n  const visibleEdges = useMemo(() => {\n    const visibleNodeIds = new Set(visibleNodes.map(n => n.id));\n    return edges.filter(e => \n      visibleNodeIds.has(e.source) || visibleNodeIds.has(e.target)\n    );\n  }, [edges, visibleNodes]);\n  \n  return (\n    <ReactFlow\n      nodes={visibleNodes}\n      edges={visibleEdges}\n      onMove={(_, viewport) => setViewport(viewport)}\n      {...otherProps}\n    />\n  );\n};\n```\n\n### Virtual Scrolling for Large Node Lists\n\n```typescript\n// Use react-window for node palette with 100+ items\nimport { FixedSizeList } from 'react-window';\n\nconst NodePalette: React.FC = () => {\n  const nodeTypes = useNodeTypes(); // Could be 100+ items\n  \n  const Row = ({ index, style }) => (\n    <div style={style}>\n      <NodePaletteItem node={nodeTypes[index]} />\n    </div>\n  );\n  \n  return (\n    <FixedSizeList\n      height={600}\n      itemCount={nodeTypes.length}\n      itemSize={60}\n      width=\"100%\"\n    >\n      {Row}\n    </FixedSizeList>\n  );\n};\n```\n\n### Debounced Auto-Save\n\n```typescript\nconst useAutoSave = (workflowId: string) => {\n  const { workflow } = useWorkflowStore();\n  const saveWorkflow = useSaveWorkflow();\n  \n  const debouncedSave = useMemo(\n    () => debounce(async () => {\n      await saveWorkflow(workflow);\n    }, 2000),\n    [workflow, saveWorkflow]\n  );\n  \n  useEffect(() => {\n    debouncedSave();\n    return () => debouncedSave.cancel();\n  }, [workflow, debouncedSave]);\n};\n```\n\n## Rationale\n\n### Memory Management\n- **Hybrid storage** solves the GB-scale artifact problem without requiring users to manually manage storage\n- **Automatic tiering** based on size and access patterns is transparent to users\n- **LRU + size-based eviction** balances frequently-accessed small items with infrequently-accessed large items\n- **Compression** for large data (>100KB) reduces IndexedDB usage significantly\n\n### Streaming Architecture  \n- **ReadableStream/TransformStream** APIs provide native backpressure support - no custom implementation needed\n- **Dual-mode execution** allows workflows to use streaming only when beneficial\n- **Stream merging** with backpressure check (`desiredSize`) prevents downstream overload\n- **Transform streams** compose cleanly for chained transformations\n\n### Security Model\n- **JSONata** is a mature, sandboxed expression language with no code execution\n- **Registered functions** whitelist provides controlled access to safe operations (uuid, now, random)\n- **Timeout + max depth** prevents DoS via complex expressions\n- **Expression validation** before execution catches syntax errors early\n\n### Checkpoint Consistency\n- **Side effect tracking** makes external operations visible and compensatable\n- **Compensation functions** as JSONata expressions keep them safe and serializable\n- **Rollback strategy** compensates side effects in reverse order (proper LIFO)\n- **Non-compensatable flag** makes it explicit when operations can't be undone (e.g., emails)\n- **Manual strategy** gives users control for complex scenarios\n\n## What I Changed\n\n### 1. Memory Management - NEW SYSTEM\n- Replaced simple memory cache with **hybrid storage manager** that automatically tiers between memory and IndexedDB\n- Added **compression** for large artifacts (>100KB) using native CompressionStream API\n- Implemented **hybrid eviction** strategy combining LRU and size-based approaches\n- Added **promotion** to move frequently-accessed data back to memory\n- Clear **data lifecycle** (temporary vs persistent, hot vs cold)\n\n### 2. Streaming - COMPLETE REDESIGN\n- Replaced custom streaming with **native Web Streams API** (ReadableStream, TransformStream)\n- **Backpressure** handled automatically by stream's `desiredSize` property\n- **Dual-mode execution** automatically chooses streaming vs batch based on data size\n- Stream transformations use **TransformStream** for proper composition\n- Added **stream merging** with concurrent reading and backpressure awareness\n\n### 3. Security - SWITCHED TECHNOLOGY\n- Removed **ALL uses of `new Function()`** \n- Replaced with **JSONata** - mature, sandboxed expression language\n- Added **timeout** (5s) and **max depth** (100) limits\n- Whitelist of **registered functions** for safe operations only\n- Expression **validation** before execution with helpful error messages\n\n### 4. Checkpoint Consistency - NEW FEATURE\n- Created **side effect tracker** to record all external operations\n- Each side effect has **compensation function** (JSONata expression)\n- Three restoration strategies: **rollback** (compensate), **forward** (replay), **manual**\n- Side effects include **type, parameters, result, compensatable flag**\n- Compensation runs in **reverse order** (LIFO) for correctness\n\n### 5. Performance - ADDED OPTIMIZATIONS\n- **Viewport culling** for workflows with 50+ nodes\n- **Virtual scrolling** for large node palette (react-window)\n- **Debounced auto-save** (2s) to prevent excessive writes\n- **Expression caching** in JSONata evaluator\n\n## What I Kept\n\nAll the excellent foundation from your design:\n\n- **React Flow** for canvas - perfect choice\n- **Zustand + immer** for state management\n- **Worker pool** concept (refined the implementation)\n- **Topological sort** for DAG execution\n- **Multiple input/output handles** per node\n- **Input/output schemas** with validation\n- **Retry configuration** with backoff strategies\n- **Event system** for reactive updates\n- **Undo/redo** with snapshots\n- **Edge validation rules** and transformations\n- **Resource limits** and pre-flight checks\n- **Node status granularity** (queued, warning, cancelled)\n- **Execution checkpoints** (enhanced with side effects)\n- **Template system** for workflows\n\n## Open Questions\n\n### 1. Streaming Materialization\nWhen streaming output is needed by multiple downstream nodes, should we:\n- **Tee the stream** (duplicate) but risk memory issues if one consumer is slow?\n- **Materialize once** to storage then serve copies (breaks streaming semantics)?\n- **Force sequential** consumption (limits parallelism)?\n\n### 2. Compensation Completeness\nSome operations can't be compensated (emails, webhooks, payments). Should we:\n- **Block checkpoint restoration** if non-compensatable effects exist?\n- **Warn user** and let them proceed manually?\n- **Require user confirmation** for each non-compensatable effect?\n\n### 3. Cross-Tab Coordination\nIf user opens workflow in multiple tabs:\n- **Lock workflow** to single tab during execution?\n- **Broadcast changes** via BroadcastChannel and handle conflicts?\n- **Last-write-wins** with conflict detection?\n\n### 4. Cost Estimation\nFor workflows with many nodes:\n- Should we **estimate total cost** before execution (sum of max tokens per node)?\n- Show **running cost counter** during execution?\n- **Pause/warn** if approaching budget limit?\n\n### 5. Large Workflow Import\nWorkflows with 1000+ nodes:\n- **Progressive loading** of nodes (load viewport first)?\n- **Background parsing** with loading indicator?\n- **Lazy initialization** of node configs?\n\n## Convergence Status\n\n**PROPOSING_FINAL** - This design addresses all major concerns:\n\n\u2705 Memory management with hybrid storage and automatic tiering  \n\u2705 Streaming with native backpressure via Web Streams API  \n\u2705 Security via JSONata (no code execution)  \n\u2705 Checkpoint consistency with side effect tracking and compensation  \n\u2705 Performance optimizations for 100+ node workflows  \n\nThe architecture is implementable, performant, and secure. Open questions remain but are non-blocking - they're about UX decisions rather than technical feasibility.\n\n---\n\n## PROMPT_FOR_CRITIC\n\nPlease review this design focusing on:\n\n1. **Hybrid storage practicality**: Is automatic tiering between memory/IndexedDB too complex? Would a simpler \"always use IndexedDB for outputs\" approach be better?\n\n2. **Streaming vs batch trade-offs**: The dual-mode execution adds complexity. Should we commit fully to streaming everywhere, or is the automatic mode selection valuable?\n\n3. **JSONata learning curve**: While secure, JSONata is another language to learn. Are the expression examples sufficient? Should we provide a visual expression builder instead?\n\n4. **Side effect compensation limitations**: The design assumes operations can be compensated via expressions. What about complex compensations requiring stateful logic (e.g., multi-step API rollbacks)?\n\n5. **Missing pieces**: What critical components or edge cases did I overlook?",
        "prompt_for_other": "Please review my response above and continue the discussion.",
        "convergence_signal": "PROPOSING_FINAL",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Streaming  \u2502  \u2502  Memory    \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Manager   \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502  Hybrid    \u2502  \u2502   Safe     \u2502  \u2502   Side     \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  Storage   \u2502  \u2502 Expression \u2502  \u2502   Effect   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  Manager   \u2502  \u2502  Evaluator \u2502  \u2502   Tracker  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with custom viewport rendering\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Dual-mode: in-memory for small data, streaming for large data\n- **Storage**: Hybrid - Memory + IndexedDB with automatic tiering\n- **Expressions**: JSONata (safe, sandboxed expression language)\n- **Workers**: Shared Worker pool with resource quotas\n\n## Memory Management Architecture\n\n### Hybrid Storage Manager\n\nThe key innovation is **automatic tiering** between memory and IndexedDB based on data size and access patterns:\n\n```typescript\ninterface StorageConfig {\n  memoryThresholdBytes: number; // 10MB default\n  maxMemoryBytes: number; // 100MB default\n  evictionStrategy: 'lru' | 'size-based' | 'hybrid';\n  compressionEnabled: boolean;\n  persistenceMode: 'eager' | 'lazy' | 'manual';\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CachedData> = new Map();\n  private accessLog: Map<string, AccessMetadata> = new Map();\n  private currentMemoryUsage: number = 0;\n  private db: IDBDatabase;\n  private config: StorageConfig;\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initIndexedDB();\n  }\n  \n  async store(key: string, data: any, metadata: DataMetadata): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new Blob([serialized]).size;\n    \n    // Automatic tiering decision\n    const shouldStoreInMemory = this.shouldUseMemory(sizeBytes, metadata);\n    \n    if (shouldStoreInMemory) {\n      await this.storeInMemory(key, data, sizeBytes, metadata);\n    } else {\n      await this.storeInIndexedDB(key, data, sizeBytes, metadata);\n    }\n    \n    // Update access log\n    this.accessLog.set(key, {\n      lastAccessed: Date.now(),\n      accessCount: 0,\n      size: sizeBytes,\n      location: shouldStoreInMemory ? 'memory' : 'indexeddb'\n    });\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    // Check memory first\n    if (this.memoryCache.has(key)) {\n      this.updateAccessMetadata(key);\n      return this.memoryCache.get(key)!.data;\n    }\n    \n    // Fallback to IndexedDB\n    const data = await this.retrieveFromIndexedDB(key);\n    \n    // Promote to memory if frequently accessed\n    const metadata = this.accessLog.get(key);\n    if (metadata && this.shouldPromoteToMemory(metadata)) {\n      await this.promoteToMemory(key, data);\n    }\n    \n    return data;\n  }\n  \n  private shouldUseMemory(sizeBytes: number, metadata: DataMetadata): boolean {\n    // Never store in memory if exceeds threshold\n    if (sizeBytes > this.config.memoryThresholdBytes) {\n      return false;\n    }\n    \n    // Check if we have room\n    if (this.currentMemoryUsage + sizeBytes > this.config.maxMemoryBytes) {\n      // Try to evict\n      if (!this.evictToMakeRoom(sizeBytes)) {\n        return false;\n      }\n    }\n    \n    // Use memory for hot data (intermediate results, frequently accessed)\n    if (metadata.accessPattern === 'hot' || metadata.temporary) {\n      return true;\n    }\n    \n    // Use memory for small data that's cheap to keep\n    return sizeBytes < 1024 * 100; // 100KB\n  }\n  \n  private evictToMakeRoom(requiredBytes: number): boolean {\n    const candidates = Array.from(this.memoryCache.entries())\n      .map(([key, data]) => ({\n        key,\n        data,\n        metadata: this.accessLog.get(key)!\n      }))\n      .sort((a, b) => {\n        // Hybrid eviction: prioritize by LRU and size\n        if (this.config.evictionStrategy === 'lru') {\n          return a.metadata.lastAccessed - b.metadata.lastAccessed;\n        } else if (this.config.evictionStrategy === 'size-based') {\n          return b.metadata.size - a.metadata.size; // Evict largest first\n        } else {\n          // Hybrid: score = (age * size) - evict oldest large items first\n          const scoreA = (Date.now() - a.metadata.lastAccessed) * a.metadata.size;\n          const scoreB = (Date.now() - b.metadata.lastAccessed) * b.metadata.size;\n          return scoreB - scoreA;\n        }\n      });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const candidate of candidates) {\n      // Never evict if accessed in last 5 seconds\n      if (Date.now() - candidate.metadata.lastAccessed < 5000) {\n        continue;\n      }\n      \n      toEvict.push(candidate.key);\n      freedBytes += candidate.metadata.size;\n      \n      if (freedBytes >= requiredBytes) {\n        break;\n      }\n    }\n    \n    // Perform eviction\n    for (const key of toEvict) {\n      this.evictToIndexedDB(key);\n    }\n    \n    return freedBytes >= requiredBytes;\n  }\n  \n  private async evictToIndexedDB(key: string): Promise<void> {\n    const cached = this.memoryCache.get(key);\n    if (!cached) return;\n    \n    // Move to IndexedDB\n    await this.storeInIndexedDB(key, cached.data, cached.size, cached.metadata);\n    \n    // Remove from memory\n    this.memoryCache.delete(key);\n    this.currentMemoryUsage -= cached.size;\n    \n    // Update location\n    const metadata = this.accessLog.get(key);\n    if (metadata) {\n      metadata.location = 'indexeddb';\n    }\n  }\n  \n  private async storeInMemory(\n    key: string, \n    data: any, \n    size: number, \n    metadata: DataMetadata\n  ): Promise<void> {\n    this.memoryCache.set(key, {\n      data,\n      size,\n      metadata,\n      storedAt: Date.now()\n    });\n    this.currentMemoryUsage += size;\n  }\n  \n  private async storeInIndexedDB(\n    key: string,\n    data: any,\n    size: number,\n    metadata: DataMetadata\n  ): Promise<void> {\n    const serialized = JSON.stringify(data);\n    \n    // Optional compression for large data\n    const toStore = this.config.compressionEnabled && size > 1024 * 100\n      ? await this.compress(serialized)\n      : serialized;\n    \n    const tx = this.db.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      size,\n      metadata,\n      compressed: this.config.compressionEnabled && size > 1024 * 100,\n      storedAt: Date.now()\n    });\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.db.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Data not found: ${key}`);\n    }\n    \n    const data = record.compressed \n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(data);\n  }\n  \n  private shouldPromoteToMemory(metadata: AccessMetadata): boolean {\n    // Promote if accessed 3+ times in last minute\n    return metadata.accessCount >= 3 && \n           Date.now() - metadata.lastAccessed < 60000;\n  }\n  \n  private async promoteToMemory(key: string, data: any): Promise<void> {\n    const metadata = this.accessLog.get(key);\n    if (!metadata) return;\n    \n    await this.storeInMemory(key, data, metadata.size, {\n      temporary: true,\n      accessPattern: 'hot'\n    });\n    \n    metadata.location = 'memory';\n  }\n  \n  private updateAccessMetadata(key: string): void {\n    const metadata = this.accessLog.get(key);\n    if (metadata) {\n      metadata.lastAccessed = Date.now();\n      metadata.accessCount++;\n    }\n  }\n  \n  async clear(predicate?: (key: string, metadata: AccessMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      this.memoryCache.clear();\n      this.accessLog.clear();\n      this.currentMemoryUsage = 0;\n      \n      const tx = this.db.transaction('artifacts', 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      return;\n    }\n    \n    // Selective clear\n    for (const [key, metadata] of this.accessLog.entries()) {\n      if (predicate(key, metadata)) {\n        this.memoryCache.delete(key);\n        \n        const tx = this.db.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        \n        this.accessLog.delete(key);\n      }\n    }\n    \n    // Recalculate memory usage\n    this.currentMemoryUsage = Array.from(this.memoryCache.values())\n      .reduce((sum, cached) => sum + cached.size, 0);\n  }\n  \n  getMemoryUsage(): MemoryStats {\n    return {\n      currentBytes: this.currentMemoryUsage,\n      maxBytes: this.config.maxMemoryBytes,\n      itemCount: this.memoryCache.size,\n      utilizationPercent: (this.currentMemoryUsage / this.config.maxMemoryBytes) * 100,\n      evictionCount: this.evictionCount\n    };\n  }\n  \n  private async compress(data: string): Promise<Uint8Array> {\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    return new Uint8Array(buffer);\n  }\n  \n  private async decompress(data: Uint8Array): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowArtifacts', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('artifacts')) {\n          db.createObjectStore('artifacts', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n}\n\ninterface CachedData {\n  data: any;\n  size: number;\n  metadata: DataMetadata;\n  storedAt: number;\n}\n\ninterface DataMetadata {\n  temporary?: boolean; // Will be cleared after workflow completion\n  accessPattern?: 'hot' | 'warm' | 'cold';\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface AccessMetadata {\n  lastAccessed: number;\n  accessCount: number;\n  size: number;\n  location: 'memory' | 'indexeddb';\n}\n\ninterface MemoryStats {\n  currentBytes: number;\n  maxBytes: number;\n  itemCount: number;\n  utilizationPercent: number;\n  evictionCount: number;\n}\n```\n\n### Usage Example\n```typescript\nconst storageManager = new HybridStorageManager({\n  memoryThresholdBytes: 10 * 1024 * 1024, // 10MB\n  maxMemoryBytes: 100 * 1024 * 1024, // 100MB\n  evictionStrategy: 'hybrid',\n  compressionEnabled: true,\n  persistenceMode: 'lazy'\n});\n\n// Store node output - automatically tiered\nawait storageManager.store(\n  `node-${nodeId}-output`,\n  output,\n  {\n    nodeId,\n    executionId,\n    temporary: true, // Clear after workflow completes\n    accessPattern: 'hot' // Keep in memory if possible\n  }\n);\n\n// Retrieve - transparent whether in memory or IndexedDB\nconst output = await storageManager.retrieve(`node-${nodeId}-output`);\n\n// Clear temporary data after workflow completes\nawait storageManager.clear((key, metadata) => \n  metadata.location === 'memory' && \n  key.includes(executionId)\n);\n```\n\n## Streaming Architecture with Backpressure\n\n### Streaming Execution Model\n\nUse **ReadableStream** and **TransformStream** APIs for proper backpressure handling:\n\n```typescript\ninterface StreamConfig {\n  highWaterMark: number; // Buffer size\n  strategy: 'chunk' | 'line' | 'token'; // How to split data\n  backpressureThreshold: number; // When to pause upstream\n}\n\nclass StreamingExecutor {\n  private activeStreams: Map<string, StreamController> = new Map();\n  \n  async executeNodeStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any | ReadableStream>\n  ): Promise<ReadableStream> {\n    // Create output stream\n    const { readable, writable } = new TransformStream({\n      transform: async (chunk, controller) => {\n        // Process chunk through agent\n        const result = await this.processChunk(node, chunk);\n        controller.enqueue(result);\n      },\n      flush: async (controller) => {\n        // Final processing\n        controller.terminate();\n      }\n    }, {\n      highWaterMark: node.data.config.streamConfig?.highWaterMark || 10,\n      size: (chunk) => chunk.length || 1\n    });\n    \n    // Track stream for cancellation\n    const streamController: StreamController = {\n      nodeId: node.id,\n      readable,\n      writable,\n      cancel: () => writable.abort(),\n      pause: () => this.pauseStream(node.id),\n      resume: () => this.resumeStream(node.id)\n    };\n    \n    this.activeStreams.set(node.id, streamController);\n    \n    // Handle input streams\n    const inputStreams = Object.entries(inputs).filter(([_, v]) => \n      v instanceof ReadableStream\n    );\n    \n    if (inputStreams.length > 0) {\n      // Merge multiple input streams with backpressure\n      const mergedStream = this.mergeStreams(\n        inputStreams.map(([_, stream]) => stream as ReadableStream),\n        node.data.config.streamConfig\n      );\n      \n      // Pipe through node processor\n      mergedStream.pipeTo(writable, {\n        preventAbort: true,\n        preventClose: true,\n        preventCancel: true\n      }).catch(error => {\n        console.error(`Stream error in node ${node.id}:`, error);\n      });\n    }\n    \n    return readable;\n  }\n  \n  private mergeStreams(\n    streams: ReadableStream[],\n    config?: StreamConfig\n  ): ReadableStream {\n    return new ReadableStream({\n      async start(controller) {\n        const readers = streams.map(s => s.getReader());\n        const activeReaders = new Set(readers);\n        \n        // Read from all streams concurrently with backpressure\n        const readNext = async (reader: ReadableStreamDefaultReader) => {\n          try {\n            const { done, value } = await reader.read();\n            \n            if (done) {\n              activeReaders.delete(reader);\n              if (activeReaders.size === 0) {\n                controller.close();\n              }\n              return;\n            }\n            \n            // Enqueue with backpressure check\n            const desiredSize = controller.desiredSize;\n            if (desiredSize !== null && desiredSize <= 0) {\n              // Backpressure - wait before reading more\n              await new Promise(resolve => setTimeout(resolve, 100));\n            }\n            \n            controller.enqueue(value);\n            \n            // Continue reading\n            readNext(reader);\n          } catch (error) {\n            controller.error(error);\n          }\n        };\n        \n        // Start reading from all streams\n        readers.forEach(reader => readNext(reader));\n      },\n      \n      cancel(reason) {\n        // Cancel all input streams\n        streams.forEach(s => s.cancel(reason));\n      }\n    }, {\n      highWaterMark: config?.highWaterMark || 10\n    });\n  }\n  \n  private async processChunk(node: WorkflowNode, chunk: any): Promise<any> {\n    // Process chunk through agent (streaming API call)\n    // This is where actual LLM streaming happens\n    return chunk; // Placeholder\n  }\n  \n  private pauseStream(nodeId: string): void {\n    const controller = this.activeStreams.get(nodeId);\n    if (controller) {\n      // Implement pause by not reading from stream\n      controller.paused = true;\n    }\n  }\n  \n  private resumeStream(nodeId: string): void {\n    const controller = this.activeStreams.get(nodeId);\n    if (controller) {\n      controller.paused = false;\n    }\n  }\n  \n  cancelAllStreams(): void {\n    for (const [nodeId, controller] of this.activeStreams) {\n      controller.cancel();\n    }\n    this.activeStreams.clear();\n  }\n}\n\ninterface StreamController {\n  nodeId: string;\n  readable: ReadableStream;\n  writable: WritableStream;\n  paused?: boolean;\n  cancel: () => void;\n  pause: () => void;\n  resume: () => void;\n}\n```\n\n### Stream Transformations\n\nTransform streams safely using **TransformStream**:\n\n```typescript\nclass StreamTransformer {\n  applyTransformations(\n    stream: ReadableStream,\n    transformations: Transformation[]\n  ): ReadableStream {\n    let result = stream;\n    \n    for (const transform of transformations) {\n      result = result.pipeThrough(\n        this.createTransformStream(transform)\n      );\n    }\n    \n    return result;\n  }\n  \n  private createTransformStream(transform: Transformation): TransformStream {\n    switch (transform.type) {\n      case 'jsonpath':\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            try {\n              // Parse chunk as JSON\n              const data = typeof chunk === 'string' ? JSON.parse(chunk) : chunk;\n              // Apply JSONPath using JSONata\n              const result = await this.evaluator.evaluate(transform.config.path, data);\n              controller.enqueue(result);\n            } catch (error) {\n              controller.error(error);\n            }\n          }\n        });\n        \n      case 'filter':\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            const passes = await this.evaluator.evaluate(\n              transform.config.predicate,\n              { item: chunk }\n            );\n            if (passes) {\n              controller.enqueue(chunk);\n            }\n            // Otherwise drop chunk (no enqueue)\n          }\n        });\n        \n      case 'map':\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            const result = await this.evaluator.evaluate(\n              transform.config.mapper,\n              { item: chunk }\n            );\n            controller.enqueue(result);\n          }\n        });\n        \n      case 'batch':\n        // Accumulate chunks into batches\n        let buffer: any[] = [];\n        return new TransformStream({\n          transform: async (chunk, controller) => {\n            buffer.push(chunk);\n            if (buffer.length >= transform.config.batchSize) {\n              controller.enqueue(buffer);\n              buffer = [];\n            }\n          },\n          flush: async (controller) => {\n            if (buffer.length > 0) {\n              controller.enqueue(buffer);\n            }\n          }\n        });\n        \n      default:\n        throw new Error(`Unknown transformation: ${transform.type}`);\n    }\n  }\n}\n```\n\n### Dual-Mode Execution\n\nSupport both streaming and batch modes:\n\n```typescript\nclass DualModeOrchestrator {\n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<any | ReadableStream> {\n    // Decide mode based on data size and node config\n    const shouldStream = this.shouldUseStreamingMode(node, inputs);\n    \n    if (shouldStream) {\n      return this.streamingExecutor.executeNodeStreaming(node, inputs);\n    } else {\n      return this.batchExecutor.executeNodeBatch(node, inputs);\n    }\n  }\n  \n  private shouldUseStreamingMode(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): boolean {\n    // Use streaming if:\n    // 1. Node explicitly configured for streaming\n    if (node.data.config.streaming) return true;\n    \n    // 2. Any input is a stream\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream\n    );\n    if (hasStreamInput) return true;\n    \n    // 3. Expected output is large (estimated from input size)\n    const inputSize = this.estimateDataSize(inputs);\n    if (inputSize > 5 * 1024 * 1024) return true; // 5MB\n    \n    // 4. Downstream nodes need streaming\n    const hasStreamingDownstream = this.hasStreamingDownstream(node);\n    if (hasStreamingDownstream) return true;\n    \n    return false;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private hasStreamingDownstream(node: WorkflowNode): boolean {\n    const downstreamEdges = this.edges.filter(e => e.source === node.id);\n    return downstreamEdges.some(edge => {\n      const targetNode = this.nodes.get(edge.target);\n      return targetNode?.data.config.streaming;\n    });\n  }\n}\n```\n\n## Safe Expression Evaluation\n\nUse **JSONata** instead of `new Function()`:\n\n```typescript\nimport jsonata from 'jsonata';\n\nclass SafeExpressionEvaluator {\n  private cache: Map<string, jsonata.Expression> = new Map();\n  private readonly timeout: number = 5000; // 5s timeout\n  private readonly maxDepth: number = 100; // Prevent infinite recursion\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    // Get or compile expression\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      try {\n        compiled = jsonata(expression);\n        \n        // Register safe functions only\n        compiled.registerFunction('now', () => new Date().toISOString());\n        compiled.registerFunction('random', () => Math.random());\n        compiled.registerFunction('uuid', () => crypto.randomUUID());\n        \n        // Cache for reuse\n        this.cache.set(expression, compiled);\n      } catch (error) {\n        throw new Error(`Invalid expression: ${error.message}`);\n      }\n    }\n    \n    // Evaluate with timeout and error handling\n    try {\n      return await Promise.race([\n        compiled.evaluate(context, {\n          maxDepth: this.maxDepth\n        }),\n        new Promise((_, reject) => \n          setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n        )\n      ]);\n    } catch (error) {\n      throw new Error(`Expression evaluation failed: ${error.message}`);\n    }\n  }\n  \n  validate(expression: string): { valid: boolean; error?: string } {\n    try {\n      jsonata(expression);\n      return { valid: true };\n    } catch (error) {\n      return { valid: false, error: error.message };\n    }\n  }\n  \n  clearCache(): void {\n    this.cache.clear();\n  }\n}\n\n// Usage in transformations\nclass DataFlowController {\n  private evaluator = new SafeExpressionEvaluator();\n  \n  async applyTransformations(\n    data: any,\n    transformations: Transformation[]\n  ): Promise<any> {\n    let result = data;\n    \n    for (const transform of transformations) {\n      switch (transform.type) {\n        case 'jsonpath':\n          // JSONata expression: $.path.to.field\n          result = await this.evaluator.evaluate(transform.config.path, result);\n          break;\n          \n        case 'filter':\n          // JSONata predicate: $[price > 100]\n          const items = Array.isArray(result) ? result : [result];\n          result = [];\n          for (const item of items) {\n            const passes = await this.evaluator.evaluate(\n              transform.config.predicate,\n              item\n            );\n            if (passes) result.push(item);\n          }\n          break;\n          \n        case 'map':\n          // JSONata mapper: { \"name\": name, \"total\": price * quantity }\n          const mappedItems = Array.isArray(result) ? result : [result];\n          result = [];\n          for (const item of mappedItems) {\n            const mapped = await this.evaluator.evaluate(\n              transform.config.mapper,\n              item\n            );\n            result.push(mapped);\n          }\n          break;\n          \n        case 'aggregate':\n          // JSONata aggregation: $sum(items.price)\n          result = await this.evaluator.evaluate(\n            transform.config.aggregator,\n            result\n          );\n          break;\n      }\n    }\n    \n    return result;\n  }\n}\n```\n\n### JSONata Expression Examples\n\nProvide helpful examples in the UI:\n\n```typescript\nconst TRANSFORMATION_EXAMPLES = {\n  jsonpath: [\n    { label: 'Extract field', expression: '$.user.name' },\n    { label: 'Array elements', expression: '$.items[*].name' },\n    { label: 'Filter array', expression: '$.items[price > 100]' },\n    { label: 'Nested path', expression: '$.response.data.results' }\n  ],\n  filter: [\n    { label: 'Price > 100', expression: 'price > 100' },\n    { label: 'Status is active', expression: 'status = \"active\"' },\n    { label: 'Contains keyword', expression: '$contains(name, \"test\")' }\n  ],\n  map: [\n    { label: 'Extract fields', expression: '{ \"id\": id, \"name\": name }' },\n    { label: 'Calculate', expression: '{ \"total\": price * quantity }' },\n    { label: 'Transform', expression: '{ \"fullName\": firstName & \" \" & lastName }' }\n  ],\n  aggregate: [\n    { label: 'Sum', expression: '$sum(items.price)' },\n    { label: 'Average', expression: '$average(items.rating)' },\n    { label: 'Count', expression: '$count(items)' },\n    { label: 'Max', expression: '$max(items.value)' }\n  ]\n};\n```\n\n## Checkpoint Consistency & Side Effect Tracking\n\n### Side Effect Tracker\n\nTrack external operations for compensation during recovery:\n\n```typescript\ninterface SideEffect {\n  id: string;\n  nodeId: string;\n  type: 'api_call' | 'file_write' | 'database_update' | 'external_service';\n  operation: string;\n  timestamp: Date;\n  parameters: any;\n  result?: any;\n  compensatable: boolean;\n  compensationFn?: string; // JSONata expression for compensation\n  status: 'pending' | 'executed' | 'compensated' | 'failed';\n}\n\nclass SideEffectTracker {\n  private effects: Map<string, SideEffect> = new Map();\n  private db: IDBDatabase;\n  \n  async recordEffect(effect: Omit<SideEffect, 'id' | 'timestamp' | 'status'>): Promise<string> {\n    const id = crypto.randomUUID();\n    const sideEffect: SideEffect = {\n      ...effect,\n      id,\n      timestamp: new Date(),\n      status: 'pending'\n    };\n    \n    this.effects.set(id, sideEffect);\n    \n    // Persist immediately\n    await this.persistEffect(sideEffect);\n    \n    return id;\n  }\n  \n  async markExecuted(effectId: string, result: any): Promise<void> {\n    const effect = this.effects.get(effectId);\n    if (!effect) return;\n    \n    effect.status = 'executed';\n    effect.result = result;\n    \n    await this.persistEffect(effect);\n  }\n  \n  async compensate(effectId: string): Promise<void> {\n    const effect = this.effects.get(effectId);\n    if (!effect || !effect.compensatable) {\n      throw new Error(`Cannot compensate effect: ${effectId}`);\n    }\n    \n    if (effect.status !== 'executed') {\n      return; // Nothing to compensate\n    }\n    \n    // Execute compensation\n    if (effect.compensationFn) {\n      const evaluator = new SafeExpressionEvaluator();\n      await evaluator.evaluate(effect.compensationFn, {\n        effect,\n        result: effect.result\n      });\n    }\n    \n    effect.status = 'compensated';\n    await this.persistEffect(effect);\n  }\n  \n  async compensateAllForNode(nodeId: string): Promise<void> {\n    const nodeEffects = Array.from(this.effects.values())\n      .filter(e => e.nodeId === nodeId && e.status === 'executed')\n      .reverse(); // Compensate in reverse order\n    \n    for (const effect of nodeEffects) {\n      try {\n        await this.compensate(effect.id);\n      } catch (error) {\n        console.error(`Failed to compensate effect ${effect.id}:`, error);\n        effect.status = 'failed';\n        await this.persistEffect(effect);\n      }\n    }\n  }\n  \n  getEffectsForCheckpoint(checkpointTime: Date): SideEffect[] {\n    return Array.from(this.effects.values())\n      .filter(e => e.timestamp <= checkpointTime);\n  }\n  \n  private async persistEffect(effect: SideEffect): Promise<void> {\n    const tx = this.db.transaction('sideEffects', 'readwrite');\n    const store = tx.objectStore('sideEffects');\n    await store.put(effect);\n  }\n}\n```\n\n### Enhanced Checkpoint Manager with Side Effects\n\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  private sideEffectTracker: SideEffectTracker;\n  \n  constructor(sideEffectTracker: SideEffectTracker) {\n    this.sideEffectTracker = sideEffectTracker;\n    this.initIndexedDB();\n  }\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    // Capture side effects up to this point\n    const sideEffects = this.sideEffectTracker.getEffectsForCheckpoint(new Date());\n    \n    const checkpoint: ExecutionCheckpoint = {\n      id: checkpointId,\n      timestamp: new Date(),\n      nodeStates: new Map(executionState.nodeStates),\n      globalContext: this.cloneContext(executionState.globalContext),\n      sideEffects, // NEW: Track side effects\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    const store = tx.objectStore('checkpoints');\n    \n    await store.put({\n      id: checkpointId,\n      executionId: executionState.executionId,\n      checkpoint: this.serializeCheckpoint(checkpoint)\n    });\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string,\n    compensationStrategy: 'rollback' | 'forward' | 'manual'\n  ): Promise<ExecutionCheckpoint> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    const record = await store.get(checkpointId);\n    \n    if (!record) {\n      throw new Error(`Checkpoint not found: ${checkpointId}`);\n    }\n    \n    const checkpoint = this.deserializeCheckpoint(record.checkpoint);\n    \n    // Handle side effects based on strategy\n    if (compensationStrategy === 'rollback') {\n      // Compensate all side effects after checkpoint\n      await this.compensateSideEffectsAfter(checkpoint.timestamp);\n    } else if (compensationStrategy === 'forward') {\n      // Replay side effects from checkpoint to current state\n      // This is complex and may not be feasible for all effects\n      console.warn('Forward recovery not fully implemented');\n    }\n    // 'manual' means user will handle side effects\n    \n    return checkpoint;\n  }\n  \n  private async compensateSideEffectsAfter(timestamp: Date): Promise<void> {\n    const allEffects = Array.from(this.sideEffectTracker['effects'].values());\n    const effectsToCompensate = allEffects\n      .filter(e => e.timestamp > timestamp && e.status === 'executed')\n      .reverse(); // Reverse order for proper compensation\n    \n    for (const effect of effectsToCompensate) {\n      try {\n        await this.sideEffectTracker.compensate(effect.id);\n      } catch (error) {\n        console.error(`Failed to compensate effect ${effect.id}:`, error);\n        // Continue with other compensations\n      }\n    }\n  }\n  \n  private cloneContext(context: ExecutionContext): ExecutionContext {\n    return {\n      variables: structuredClone(context.variables),\n      secrets: new Map(context.secrets),\n      artifacts: new Map(context.artifacts)\n    };\n  }\n  \n  private serializeCheckpoint(checkpoint: ExecutionCheckpoint): string {\n    return JSON.stringify({\n      id: checkpoint.id,\n      timestamp: checkpoint.timestamp.toISOString(),\n      nodeStates: Array.from(checkpoint.nodeStates.entries()),\n      globalContext: {\n        variables: checkpoint.globalContext.variables\n      },\n      sideEffects: checkpoint.sideEffects,\n      reason: checkpoint.reason\n    });\n  }\n  \n  private deserializeCheckpoint(data: string): ExecutionCheckpoint {\n    const parsed = JSON.parse(data);\n    return {\n      id: parsed.id,\n      timestamp: new Date(parsed.timestamp),\n      nodeStates: new Map(parsed.nodeStates),\n      globalContext: {\n        variables: parsed.globalContext.variables,\n        secrets: new Map(),\n        artifacts: new Map()\n      },\n      sideEffects: parsed.sideEffects || [],\n      reason: parsed.reason\n    };\n  }\n  \n  async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowCheckpoints', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('checkpoints')) {\n          db.createObjectStore('checkpoints', { keyPath: 'id' });\n        }\n        if (!db.objectStoreNames.contains('sideEffects')) {\n          db.createObjectStore('sideEffects', { keyPath: 'id' });\n        }\n      };\n    });\n  }\n}\n```\n\n### Usage in Node Execution\n\n```typescript\nclass WorkflowOrchestrator {\n  private sideEffectTracker: SideEffectTracker;\n  private checkpointManager: CheckpointManager;\n  \n  private async executeNode(nodeId: string): Promise<NodeExecutionResult> {\n    const node = this.nodes.get(nodeId)!;\n    \n    try {\n      // Gather inputs\n      const inputs = await this.dataFlowController.gatherInputs(\n        nodeId,\n        this.edges,\n        this.executionState\n      );\n      \n      // Track side effect (if agent makes external calls)\n      const effectId = await this.sideEffectTracker.recordEffect({\n        nodeId,\n        type: 'api_call',\n        operation: 'execute_agent',\n        parameters: { inputs },\n        compensatable: true,\n        compensationFn: '$cancel_execution(effect.parameters.executionId)'\n      });\n      \n      // Execute agent\n      const result = await this.executeAgent(node, inputs);\n      \n      // Mark effect as executed\n      await this.sideEffectTracker.markExecuted(effectId, result);\n      \n      return result;\n      \n    } catch (error) {\n      // On error, create checkpoint and compensate this node\n      await this.checkpointManager.createCheckpoint(\n        this.executionState,\n        'error'\n      );\n      \n      await this.sideEffectTracker.compensateAllForNode(nodeId);\n      \n      throw error;\n    }\n  }\n}\n```\n\n### Compensation Examples\n\n```typescript\n// Example: API call compensation\nconst apiCallEffect: SideEffect = {\n  nodeId: 'node-123',\n  type: 'api_call',\n  operation: 'create_resource',\n  parameters: {\n    endpoint: '/api/resources',\n    method: 'POST',\n    body: { name: 'test' }\n  },\n  result: { id: 'resource-456' },\n  compensatable: true,\n  compensationFn: `\n    $http.delete(\"/api/resources/\" & result.id)\n  `\n};\n\n// Example: File write compensation\nconst fileWriteEffect: SideEffect = {\n  nodeId: 'node-123',\n  type: 'file_write',\n  operation: 'write_output',\n  parameters: {\n    path: '/tmp/output.json',\n    content: '...'\n  },\n  compensatable: true,\n  compensationFn: `\n    $fs.delete(parameters.path)\n  `\n};\n\n// Example: Non-compensatable effect (email sent)\nconst emailEffect: SideEffect = {\n  nodeId: 'node-123',\n  type: 'external_service',\n  operation: 'send_email',\n  parameters: {\n    to: 'user@example.com',\n    subject: 'Workflow result'\n  },\n  compensatable: false // Can't unsend email\n};\n```\n\n## Enhanced Node Component\n\n```typescript\ninterface AgentNodeData {\n  id: string;\n  type: 'researcher' | 'coder' | 'reviewer' | 'coordinator' | 'transformer';\n  label: string;\n  config: AgentConfig;\n  status: ExecutionStatus;\n  logs: LogEntry[];\n  metrics: ExecutionMetrics;\n  inputs: Record<string, InputDefinition>;\n  outputs: Record<string, OutputDefinition>;\n  sideEffects: SideEffect[]; // NEW: Track side effects\n}\n\ninterface AgentConfig {\n  model: string;\n  temperature: number;\n  systemPrompt: string;\n  maxTokens?: number;\n  timeout?: number;\n  retryConfig?: RetryConfig;\n  streaming?: boolean;\n  memoryLimit?: number;\n  streamConfig?: StreamConfig; // NEW: Streaming configuration\n  trackSideEffects?: boolean; // NEW: Enable side effect tracking\n}\n\nconst AgentNode: React.FC<NodeProps<AgentNodeData>> = ({ data, selected }) => {\n  const [isExpanded, setIsExpanded] = useState(false);\n  const streamingOutput = useStreamingOutput(data.id);\n  const memoryUsage = useMemoryUsage(data.id);\n  \n  return (\n    <div className={`agent-node ${selected ? 'selected' : ''} ${isExpanded ? 'expanded' : ''}`}>\n      {/* Multiple typed input handles */}\n      {Object.entries(data.inputs).map(([key, input]) => (\n        <Handle\n          key={`input-${key}`}\n          type=\"target\"\n          position={Position.Left}\n          id={`input-${key}`}\n          className={`handle-${input.type}`}\n          style={{ top: calculateHandlePosition(key, data.inputs) }}\n        >\n          <Tooltip content={`${input.name} (${input.type})`}>\n            <HandleLabel required={input.required}>\n              {input.type === 'stream' && <StreamIcon />}\n              {input.name}\n            </HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n      \n      <div className=\"node-header\">\n        <AgentIcon type={data.type} />\n        <span className=\"node-label\">{data.label}</span>\n        <StatusBadge status={data.status} />\n        {data.config.streaming && <StreamBadge />}\n        {data.trackSideEffects && data.sideEffects.length > 0 && (\n          <SideEffectBadge count={data.sideEffects.length} />\n        )}\n        <Button size=\"xs\" onClick={() => setIsExpanded(!isExpanded)}>\n          {isExpanded ? <ChevronUpIcon /> : <ChevronDownIcon />}\n        </Button>\n      </div>\n      \n      <div className=\"node-body\">\n        {data.status === 'running' && (\n          <>\n            <ProgressBar metrics={data.metrics} />\n            {data.config.streaming && streamingOutput && (\n              <StreamPreview content={streamingOutput} />\n            )}\n            {memoryUsage && (\n              <MemoryIndicator usage={memoryUsage} />\n            )}\n          </>\n        )}\n        \n        {isExpanded && (\n          <NodeDetails \n            logs={data.logs} \n            metrics={data.metrics}\n            sideEffects={data.sideEffects}\n          />\n        )}\n      </div>\n      \n      {/* Multiple typed output handles */}\n      {Object.entries(data.outputs).map(([key, output]) => (\n        <Handle\n          key={`output-${key}`}\n          type=\"source\"\n          position={Position.Right}\n          id={`output-${key}`}\n          className={`handle-${output.type}`}\n          style={{ top: calculateHandlePosition(key, data.outputs) }}\n        >\n          <Tooltip content={`${output.name} (${output.type})`}>\n            <HandleLabel>\n              {output.type === 'stream' && <StreamIcon />}\n              {output.name}\n            </HandleLabel>\n          </Tooltip>\n        </Handle>\n      ))}\n    </div>\n  );\n};\n```\n\n## Performance Optimizations for Large Workflows\n\n### Viewport Culling\n\nOnly render nodes visible in viewport:\n\n```typescript\nconst WorkflowCanvas: React.FC<WorkflowCanvasProps> = ({ workflowId }) => {\n  const { nodes, edges } = useWorkflowStore();\n  const [viewport, setViewport] = useState<Viewport>({ x: 0, y: 0, zoom: 1 });\n  \n  // Filter nodes to only those in viewport\n  const visibleNodes = useMemo(() => {\n    if (nodes.length < 50) return nodes; // No need for culling\n    \n    return nodes.filter(node => {\n      const nodeRect = {\n        x: node.position.x,\n        y: node.position.y,\n        width: node.width || 200,\n        height: node.height || 100\n      };\n      \n      return isRectInViewport(nodeRect, viewport);\n    });\n  }, [nodes, viewport]);\n  \n  // Edges connected to visible nodes\n  const visibleEdges = useMemo(() => {\n    const visibleNodeIds = new Set(visibleNodes.map(n => n.id));\n    return edges.filter(e => \n      visibleNodeIds.has(e.source) || visibleNodeIds.has(e.target)\n    );\n  }, [edges, visibleNodes]);\n  \n  return (\n    <ReactFlow\n      nodes={visibleNodes}\n      edges={visibleEdges}\n      onMove={(_, viewport) => setViewport(viewport)}\n      {...otherProps}\n    />\n  );\n};\n```\n\n### Virtual Scrolling for Large Node Lists\n\n```typescript\n// Use react-window for node palette with 100+ items\nimport { FixedSizeList } from 'react-window';\n\nconst NodePalette: React.FC = () => {\n  const nodeTypes = useNodeTypes(); // Could be 100+ items\n  \n  const Row = ({ index, style }) => (\n    <div style={style}>\n      <NodePaletteItem node={nodeTypes[index]} />\n    </div>\n  );\n  \n  return (\n    <FixedSizeList\n      height={600}\n      itemCount={nodeTypes.length}\n      itemSize={60}\n      width=\"100%\"\n    >\n      {Row}\n    </FixedSizeList>\n  );\n};\n```\n\n### Debounced Auto-Save\n\n```typescript\nconst useAutoSave = (workflowId: string) => {\n  const { workflow } = useWorkflowStore();\n  const saveWorkflow = useSaveWorkflow();\n  \n  const debouncedSave = useMemo(\n    () => debounce(async () => {\n      await saveWorkflow(workflow);\n    }, 2000),\n    [workflow, saveWorkflow]\n  );\n  \n  useEffect(() => {\n    debouncedSave();\n    return () => debouncedSave.cancel();\n  }, [workflow, debouncedSave]);\n};\n```\n\n## Rationale\n\n### Memory Management\n- **Hybrid storage** solves the GB-scale artifact problem without requiring users to manually manage storage\n- **Automatic tiering** based on size and access patterns is transparent to users\n- **LRU + size-based eviction** balances frequently-accessed small items with infrequently-accessed large items\n- **Compression** for large data (>100KB) reduces IndexedDB usage significantly\n\n### Streaming Architecture  \n- **ReadableStream/TransformStream** APIs provide native backpressure support - no custom implementation needed\n- **Dual-mode execution** allows workflows to use streaming only when beneficial\n- **Stream merging** with backpressure check (`desiredSize`) prevents downstream overload\n- **Transform streams** compose cleanly for chained transformations\n\n### Security Model\n- **JSONata** is a mature, sandboxed expression language with no code execution\n- **Registered functions** whitelist provides controlled access to safe operations (uuid, now, random)\n- **Timeout + max depth** prevents DoS via complex expressions\n- **Expression validation** before execution catches syntax errors early\n\n### Checkpoint Consistency\n- **Side effect tracking** makes external operations visible and compensatable\n- **Compensation functions** as JSONata expressions keep them safe and serializable\n- **Rollback strategy** compensates side effects in reverse order (proper LIFO)\n- **Non-compensatable flag** makes it explicit when operations can't be undone (e.g., emails)\n- **Manual strategy** gives users control for complex scenarios\n\n## What I Changed\n\n### 1. Memory Management - NEW SYSTEM\n- Replaced simple memory cache with **hybrid storage manager** that automatically tiers between memory and IndexedDB\n- Added **compression** for large artifacts (>100KB) using native CompressionStream API\n- Implemented **hybrid eviction** strategy combining LRU and size-based approaches\n- Added **promotion** to move frequently-accessed data back to memory\n- Clear **data lifecycle** (temporary vs persistent, hot vs cold)\n\n### 2. Streaming - COMPLETE REDESIGN\n- Replaced custom streaming with **native Web Streams API** (ReadableStream, TransformStream)\n- **Backpressure** handled automatically by stream's `desiredSize` property\n- **Dual-mode execution** automatically chooses streaming vs batch based on data size\n- Stream transformations use **TransformStream** for proper composition\n- Added **stream merging** with concurrent reading and backpressure awareness\n\n### 3. Security - SWITCHED TECHNOLOGY\n- Removed **ALL uses of `new Function()`** \n- Replaced with **JSONata** - mature, sandboxed expression language\n- Added **timeout** (5s) and **max depth** (100) limits\n- Whitelist of **registered functions** for safe operations only\n- Expression **validation** before execution with helpful error messages\n\n### 4. Checkpoint Consistency - NEW FEATURE\n- Created **side effect tracker** to record all external operations\n- Each side effect has **compensation function** (JSONata expression)\n- Three restoration strategies: **rollback** (compensate), **forward** (replay), **manual**\n- Side effects include **type, parameters, result, compensatable flag**\n- Compensation runs in **reverse order** (LIFO) for correctness\n\n### 5. Performance - ADDED OPTIMIZATIONS\n- **Viewport culling** for workflows with 50+ nodes\n- **Virtual scrolling** for large node palette (react-window)\n- **Debounced auto-save** (2s) to prevent excessive writes\n- **Expression caching** in JSONata evaluator\n\n## What I Kept\n\nAll the excellent foundation from your design:\n\n- **React Flow** for canvas - perfect choice\n- **Zustand + immer** for state management\n- **Worker pool** concept (refined the implementation)\n- **Topological sort** for DAG execution\n- **Multiple input/output handles** per node\n- **Input/output schemas** with validation\n- **Retry configuration** with backoff strategies\n- **Event system** for reactive updates\n- **Undo/redo** with snapshots\n- **Edge validation rules** and transformations\n- **Resource limits** and pre-flight checks\n- **Node status granularity** (queued, warning, cancelled)\n- **Execution checkpoints** (enhanced with side effects)\n- **Template system** for workflows\n\n## Open Questions\n\n### 1. Streaming Materialization\nWhen streaming output is needed by multiple downstream nodes, should we:\n- **Tee the stream** (duplicate) but risk memory issues if one consumer is slow?\n- **Materialize once** to storage then serve copies (breaks streaming semantics)?\n- **Force sequential** consumption (limits parallelism)?\n\n### 2. Compensation Completeness\nSome operations can't be compensated (emails, webhooks, payments). Should we:\n- **Block checkpoint restoration** if non-compensatable effects exist?\n- **Warn user** and let them proceed manually?\n- **Require user confirmation** for each non-compensatable effect?\n\n### 3. Cross-Tab Coordination\nIf user opens workflow in multiple tabs:\n- **Lock workflow** to single tab during execution?\n- **Broadcast changes** via BroadcastChannel and handle conflicts?\n- **Last-write-wins** with conflict detection?\n\n### 4. Cost Estimation\nFor workflows with many nodes:\n- Should we **estimate total cost** before execution (sum of max tokens per node)?\n- Show **running cost counter** during execution?\n- **Pause/warn** if approaching budget limit?\n\n### 5. Large Workflow Import\nWorkflows with 1000+ nodes:\n- **Progressive loading** of nodes (load viewport first)?\n- **Background parsing** with loading indicator?\n- **Lazy initialization** of node configs?\n\n## Convergence Status\n\n**PROPOSING_FINAL** - This design addresses all major concerns:\n\n\u2705 Memory management with hybrid storage and automatic tiering  \n\u2705 Streaming with native backpressure via Web Streams API  \n\u2705 Security via JSONata (no code execution)  \n\u2705 Checkpoint consistency with side effect tracking and compensation  \n\u2705 Performance optimizations for 100+ node workflows  \n\nThe architecture is implementable, performant, and secure. Open questions remain but are non-blocking - they're about UX decisions rather than technical feasibility.\n\n---\n\n## PROMPT_FOR_CRITIC\n\nPlease review this design focusing on:\n\n1. **Hybrid storage practicality**: Is automatic tiering between memory/IndexedDB too complex? Would a simpler \"always use IndexedDB for outputs\" approach be better?\n\n2. **Streaming vs batch trade-offs**: The dual-mode execution adds complexity. Should we commit fully to streaming everywhere, or is the automatic mode selection valuable?\n\n3. **JSONata learning curve**: While secure, JSONata is another language to learn. Are the expression examples sufficient? Should we provide a visual expression builder instead?\n\n4. **Side effect compensation limitations**: The design assumes operations can be compensated via expressions. What about complex compensations requiring stateful logic (e.g., multi-step API rollbacks)?\n\n5. **Missing pieces**: What critical components or edge cases did I overlook?"
      },
      "agent_b": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v2\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Tiered    \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Storage   \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502  Pluggable \u2502  \u2502   Hybrid   \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  Storage   \u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  Engine    \u2502  \u2502  Manager   \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with custom viewport rendering\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with streaming-first, batch fallback\n- **Storage**: Three-tier: Hot cache (memory) \u2192 Warm cache (IndexedDB) \u2192 Cold storage (optional blob URLs)\n- **Expressions**: Pluggable engine (JSONata default, optional visual builder)\n- **Side Effects**: Append-only log with semantic compensation descriptors\n- **Workers**: Shared Worker pool with resource quotas\n\n## Simplified Three-Tier Storage Architecture\n\n### Key Insight: Most Artifacts Don't Need Eviction Logic\n\n```typescript\ninterface StorageConfig {\n  // Simplified thresholds - no complex eviction\n  hotCacheMaxBytes: number;      // 50MB - small, hot data only\n  warmCacheMaxBytes: number;     // 500MB - most artifacts live here\n  enableColdStorage: boolean;    // Optional blob URLs for huge files\n  compressionThreshold: number;  // 1MB - compress above this\n  autoCleanupAge: number;        // 24h - auto-delete old temp data\n}\n\nclass TieredStorageManager {\n  private hotCache: Map<string, HotCacheEntry> = new Map();\n  private hotCacheSize: number = 0;\n  private warmDB: IDBDatabase;\n  private coldBlobUrls: Map<string, string> = new Map();\n  private config: StorageConfig;\n  \n  // Metadata index for all artifacts (lightweight)\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initWarmStorage();\n    this.scheduleCleanup();\n  }\n  \n  async store(key: string, data: any, hints: StorageHints): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new Blob([serialized]).size;\n    \n    // Update metadata index\n    this.artifactIndex.set(key, {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      temporary: hints.temporary,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      accessCount: 0,\n      lastAccessedAt: Date.now()\n    });\n    \n    // Tier selection: SIMPLE rules, no complex scoring\n    const tier = this.selectTier(sizeBytes, hints);\n    \n    switch (tier) {\n      case 'hot':\n        await this.storeInHotCache(key, data, sizeBytes);\n        break;\n        \n      case 'warm':\n        await this.storeInWarmCache(key, serialized, sizeBytes);\n        break;\n        \n      case 'cold':\n        await this.storeInColdStorage(key, serialized, sizeBytes);\n        break;\n    }\n    \n    this.artifactIndex.get(key)!.tier = tier;\n  }\n  \n  private selectTier(sizeBytes: number, hints: StorageHints): StorageTier {\n    // COLD: Files > 100MB (if enabled) - these are rare\n    if (this.config.enableColdStorage && sizeBytes > 100 * 1024 * 1024) {\n      return 'cold';\n    }\n    \n    // HOT: Small + frequently accessed OR marked as hot\n    // Only small items that are actively being used by running nodes\n    if (sizeBytes < 100 * 1024 && // < 100KB\n        (hints.accessPattern === 'hot' || hints.temporary)) {\n      \n      // Check if we have room in hot cache\n      if (this.hotCacheSize + sizeBytes <= this.config.hotCacheMaxBytes) {\n        return 'hot';\n      }\n      \n      // Try to make room by removing oldest hot items\n      if (this.makeRoomInHotCache(sizeBytes)) {\n        return 'hot';\n      }\n    }\n    \n    // WARM: Everything else (default tier)\n    // Most artifacts live here - IndexedDB is fast enough\n    return 'warm';\n  }\n  \n  private makeRoomInHotCache(requiredBytes: number): boolean {\n    // Simple FIFO eviction of oldest items\n    const entries = Array.from(this.hotCache.entries())\n      .sort((a, b) => a[1].storedAt - b[1].storedAt);\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const [key, entry] of entries) {\n      // Never evict items stored in last 5 seconds\n      if (Date.now() - entry.storedAt < 5000) continue;\n      \n      toEvict.push(key);\n      freedBytes += entry.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    // Move to warm cache (no data loss)\n    for (const key of toEvict) {\n      const entry = this.hotCache.get(key)!;\n      this.storeInWarmCache(key, JSON.stringify(entry.data), entry.sizeBytes);\n      this.hotCache.delete(key);\n      this.hotCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) metadata.tier = 'warm';\n    }\n    \n    return freedBytes >= requiredBytes;\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    // Update access tracking\n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    // Retrieve from appropriate tier\n    let data: any;\n    \n    switch (metadata.tier) {\n      case 'hot':\n        data = this.hotCache.get(key)?.data;\n        if (!data) {\n          // Fallback to warm if somehow missing\n          data = await this.retrieveFromWarmCache(key);\n        }\n        break;\n        \n      case 'warm':\n        data = await this.retrieveFromWarmCache(key);\n        \n        // Promote to hot if: small + accessed 3+ times recently\n        if (metadata.sizeBytes < 100 * 1024 && \n            metadata.accessCount >= 3 &&\n            Date.now() - metadata.lastAccessedAt < 60000) {\n          \n          if (this.hotCacheSize + metadata.sizeBytes <= this.config.hotCacheMaxBytes) {\n            await this.storeInHotCache(key, data, metadata.sizeBytes);\n            metadata.tier = 'hot';\n          }\n        }\n        break;\n        \n      case 'cold':\n        data = await this.retrieveFromColdStorage(key);\n        break;\n    }\n    \n    return data;\n  }\n  \n  private async storeInHotCache(key: string, data: any, sizeBytes: number): Promise<void> {\n    this.hotCache.set(key, {\n      data,\n      sizeBytes,\n      storedAt: Date.now()\n    });\n    this.hotCacheSize += sizeBytes;\n  }\n  \n  private async storeInWarmCache(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    // Compress if large enough\n    const shouldCompress = sizeBytes > this.config.compressionThreshold;\n    const toStore = shouldCompress \n      ? await this.compress(serialized)\n      : serialized;\n    \n    const tx = this.warmDB.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      storedAt: Date.now()\n    });\n  }\n  \n  private async retrieveFromWarmCache(key: string): Promise<any> {\n    const tx = this.warmDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not found in warm cache: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async storeInColdStorage(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    // Use Blob URLs for huge files (rare case)\n    const compressed = await this.compress(serialized);\n    const blob = new Blob([compressed], { type: 'application/octet-stream' });\n    const blobUrl = URL.createObjectURL(blob);\n    \n    this.coldBlobUrls.set(key, blobUrl);\n    \n    // Also store reference in IndexedDB\n    const tx = this.warmDB.transaction('coldRefs', 'readwrite');\n    const store = tx.objectStore('coldRefs');\n    await store.put({ key, blobUrl, sizeBytes, storedAt: Date.now() });\n  }\n  \n  private async retrieveFromColdStorage(key: string): Promise<any> {\n    const blobUrl = this.coldBlobUrls.get(key);\n    if (!blobUrl) {\n      throw new Error(`Cold storage artifact not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = new Uint8Array(await response.arrayBuffer());\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async clear(predicate?: (metadata: ArtifactMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      // Clear everything\n      this.hotCache.clear();\n      this.hotCacheSize = 0;\n      this.artifactIndex.clear();\n      \n      const tx = this.warmDB.transaction(['artifacts', 'coldRefs'], 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      await tx.objectStore('coldRefs').clear();\n      \n      // Revoke blob URLs\n      for (const blobUrl of this.coldBlobUrls.values()) {\n        URL.revokeObjectURL(blobUrl);\n      }\n      this.coldBlobUrls.clear();\n      \n      return;\n    }\n    \n    // Selective clear\n    const toDelete: string[] = [];\n    \n    for (const [key, metadata] of this.artifactIndex.entries()) {\n      if (predicate(metadata)) {\n        toDelete.push(key);\n      }\n    }\n    \n    for (const key of toDelete) {\n      await this.delete(key);\n    }\n  }\n  \n  private async delete(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) return;\n    \n    switch (metadata.tier) {\n      case 'hot':\n        const entry = this.hotCache.get(key);\n        if (entry) {\n          this.hotCacheSize -= entry.sizeBytes;\n          this.hotCache.delete(key);\n        }\n        break;\n        \n      case 'warm':\n        const tx = this.warmDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        break;\n        \n      case 'cold':\n        const blobUrl = this.coldBlobUrls.get(key);\n        if (blobUrl) {\n          URL.revokeObjectURL(blobUrl);\n          this.coldBlobUrls.delete(key);\n        }\n        \n        const txCold = this.warmDB.transaction('coldRefs', 'readwrite');\n        await txCold.objectStore('coldRefs').delete(key);\n        break;\n    }\n    \n    this.artifactIndex.delete(key);\n  }\n  \n  private scheduleCleanup(): void {\n    // Auto-cleanup old temporary data every hour\n    setInterval(() => {\n      const cutoff = Date.now() - this.config.autoCleanupAge;\n      \n      this.clear(metadata => \n        metadata.temporary && metadata.createdAt < cutoff\n      );\n    }, 60 * 60 * 1000); // 1 hour\n  }\n  \n  getStorageStats(): StorageStats {\n    const stats = {\n      hot: { count: 0, bytes: this.hotCacheSize },\n      warm: { count: 0, bytes: 0 },\n      cold: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      stats[metadata.tier].count++;\n      stats[metadata.tier].bytes += metadata.sizeBytes;\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n  \n  private async compress(data: string): Promise<Uint8Array> {\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    return new Uint8Array(buffer);\n  }\n  \n  private async decompress(data: Uint8Array): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initWarmStorage(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowArtifacts', 2);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.warmDB = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          db.createObjectStore('artifacts', { keyPath: 'key' });\n        }\n        if (!db.objectStoreNames.contains('coldRefs')) {\n          db.createObjectStore('coldRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n}\n\n// Simplified types\ninterface HotCacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: StorageTier;\n  createdAt: number;\n  temporary: boolean;\n  nodeId?: string;\n  executionId?: string;\n  accessCount: number;\n  lastAccessedAt: number;\n}\n\ntype StorageTier = 'hot' | 'warm' | 'cold' | 'determining';\n\ninterface StorageHints {\n  temporary?: boolean;\n  accessPattern?: 'hot' | 'warm';\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface StorageStats {\n  hot: { count: number; bytes: number };\n  warm: { count: number; bytes: number };\n  cold: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n```\n\n### Usage Example\n```typescript\nconst storage = new TieredStorageManager({\n  hotCacheMaxBytes: 50 * 1024 * 1024,      // 50MB\n  warmCacheMaxBytes: 500 * 1024 * 1024,    // 500MB\n  enableColdStorage: true,\n  compressionThreshold: 1024 * 1024,        // 1MB\n  autoCleanupAge: 24 * 60 * 60 * 1000      // 24h\n});\n\n// Store - automatic tiering\nawait storage.store(\n  `node-${nodeId}-output`,\n  largeDataset,\n  {\n    nodeId,\n    executionId,\n    temporary: true,\n    accessPattern: 'warm' // Default for most data\n  }\n);\n\n// Retrieve - transparent tier access\nconst data = await storage.retrieve(`node-${nodeId}-output`);\n\n// Cleanup after workflow\nawait storage.clear(meta => \n  meta.temporary && meta.executionId === executionId\n);\n\n// Check storage usage\nconst stats = storage.getStorageStats();\nconsole.log(`Using ${stats.hot.bytes} bytes in hot cache`);\n```\n\n## Streaming-First with Graceful Degradation\n\n### Key Insight: Don't Force Everything to Stream\n\n```typescript\nclass AdaptiveExecutor {\n  private streamingExecutor: StreamingExecutor;\n  private batchExecutor: BatchExecutor;\n  private storage: TieredStorageManager;\n  \n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    // Check if streaming makes sense\n    const streamingDecision = this.analyzeStreamingFeasibility(node, inputs);\n    \n    if (streamingDecision.shouldStream) {\n      return this.executeWithStreaming(node, inputs, streamingDecision);\n    } else {\n      return this.executeWithBatch(node, inputs);\n    }\n  }\n  \n  private analyzeStreamingFeasibility(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): StreamingDecision {\n    \n    const reasons: string[] = [];\n    let shouldStream = false;\n    let materializationStrategy: MaterializationStrategy = 'none';\n    \n    // 1. Node explicitly requests streaming\n    if (node.data.config.streaming === true) {\n      shouldStream = true;\n      reasons.push('node_config_explicit');\n    }\n    \n    // 2. Node explicitly REJECTS streaming\n    if (node.data.config.streaming === false) {\n      return {\n        shouldStream: false,\n        reasons: ['node_config_explicit_batch'],\n        materializationStrategy: 'none'\n      };\n    }\n    \n    // 3. Input is already a stream\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream\n    );\n    \n    if (hasStreamInput) {\n      shouldStream = true;\n      reasons.push('stream_input');\n      \n      // Check if multiple downstream consumers need this stream\n      const downstreamCount = this.getDownstreamConsumerCount(node);\n      \n      if (downstreamCount > 1) {\n        // MATERIALIZE ONCE for multiple consumers\n        materializationStrategy = 'materialize_to_storage';\n        reasons.push('multiple_consumers_detected');\n      }\n    }\n    \n    // 4. Large input suggests streaming (but not required)\n    const estimatedSize = this.estimateDataSize(inputs);\n    if (estimatedSize > 5 * 1024 * 1024) { // 5MB\n      shouldStream = true;\n      reasons.push('large_input_detected');\n    }\n    \n    // 5. Agent type benefits from streaming (LLM responses)\n    if (['researcher', 'coder', 'reviewer'].includes(node.data.type)) {\n      shouldStream = true;\n      reasons.push('agent_type_benefits');\n    }\n    \n    return {\n      shouldStream,\n      reasons,\n      materializationStrategy\n    };\n  }\n  \n  private async executeWithStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const outputStream = await this.streamingExecutor.executeNodeStreaming(\n      node,\n      inputs\n    );\n    \n    // Handle materialization if needed\n    if (decision.materializationStrategy === 'materialize_to_storage') {\n      return await this.materializeStreamToStorage(node, outputStream);\n    }\n    \n    // Otherwise return stream directly\n    return {\n      nodeId: node.id,\n      output: outputStream,\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons\n      }\n    };\n  }\n  \n  private async materializeStreamToStorage(\n    node: WorkflowNode,\n    stream: ReadableStream\n  ): Promise<ExecutionResult> {\n    \n    // Read entire stream into storage\n    const chunks: any[] = [];\n    const reader = stream.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        chunks.push(value);\n      }\n    } finally {\n      reader.releaseLock();\n    }\n    \n    // Combine chunks\n    const materialized = this.combineChunks(chunks);\n    \n    // Store in tiered storage\n    const storageKey = `node-${node.id}-materialized`;\n    await this.storage.store(storageKey, materialized, {\n      nodeId: node.id,\n      temporary: true,\n      accessPattern: 'warm'\n    });\n    \n    // Return reference, not the data itself\n    return {\n      nodeId: node.id,\n      output: { $ref: storageKey },\n      streaming: false,\n      metadata: {\n        materialized: true,\n        storageKey\n      }\n    };\n  }\n  \n  private async executeWithBatch(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    // Resolve any storage references in inputs\n    const resolvedInputs = await this.resolveStorageReferences(inputs);\n    \n    // Execute node\n    const output = await this.batchExecutor.executeNodeBatch(node, resolvedInputs);\n    \n    return {\n      nodeId: node.id,\n      output,\n      streaming: false,\n      metadata: {}\n    };\n  }\n  \n  private async resolveStorageReferences(inputs: Record<string, any>): Promise<Record<string, any>> {\n    const resolved: Record<string, any> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      if (value && typeof value === 'object' && value.$ref) {\n        // Storage reference - retrieve\n        resolved[key] = await this.storage.retrieve(value.$ref);\n      } else {\n        resolved[key] = value;\n      }\n    }\n    \n    return resolved;\n  }\n  \n  private getDownstreamConsumerCount(node: WorkflowNode): number {\n    // Count how many nodes consume this node's output\n    return this.edges.filter(e => e.source === node.id).length;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private combineChunks(chunks: any[]): any {\n    // Combine strategy depends on chunk type\n    if (chunks.length === 0) return null;\n    \n    if (typeof chunks[0] === 'string') {\n      return chunks.join('');\n    }\n    \n    if (Array.isArray(chunks[0])) {\n      return chunks.flat();\n    }\n    \n    // Objects - last chunk wins\n    return chunks[chunks.length - 1];\n  }\n}\n\ninterface StreamingDecision {\n  shouldStream: boolean;\n  reasons: string[];\n  materializationStrategy: MaterializationStrategy;\n}\n\ntype MaterializationStrategy = \n  | 'none'                      // Pass stream through\n  | 'materialize_to_storage';   // Store once, serve multiple\n\ninterface ExecutionResult {\n  nodeId: string;\n  output: any | ReadableStream | { $ref: string };\n  streaming: boolean;\n  metadata: Record<string, any>;\n}\n```\n\n## Pluggable Expression Engine\n\n### Key Insight: Start Simple, Allow Upgrades\n\n```typescript\ninterface ExpressionEngine {\n  evaluate(expression: string, context: any): Promise<any>;\n  validate(expression: string): ValidationResult;\n  getExamples(): ExpressionExample[];\n  getSyntaxHelp(): string;\n}\n\nclass JSONataEngine implements ExpressionEngine {\n  private cache: Map<string, jsonata.Expression> = new Map();\n  private readonly timeout = 5000;\n  private readonly maxDepth = 100;\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      compiled = jsonata(expression);\n      \n      // Register safe functions\n      compiled.registerFunction('now', () => new Date().toISOString());\n      compiled.registerFunction('random', () => Math.random());\n      compiled.registerFunction('uuid', () => crypto.randomUUID());\n      \n      this.cache.set(expression, compiled);\n    }\n    \n    return await Promise.race([\n      compiled.evaluate(context, { maxDepth: this.maxDepth }),\n      new Promise((_, reject) => \n        setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n      )\n    ]);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      jsonata(expression);\n      return { valid: true };\n    } catch (error) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: this.getSuggestion(error)\n      };\n    }\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return [\n      {\n        category: 'Extract Field',\n        expression: '$.user.name',\n        description: 'Get nested field value',\n        inputExample: { user: { name: 'Alice' } },\n        outputExample: 'Alice'\n      },\n      {\n        category: 'Filter Array',\n        expression: '$.items[price > 100]',\n        description: 'Filter array by condition',\n        inputExample: { items: [{ price: 50 }, { price: 150 }] },\n        outputExample: [{ price: 150 }]\n      },\n      {\n        category: 'Transform',\n        expression: '{ \"fullName\": firstName & \" \" & lastName }',\n        description: 'Create new object from fields',\n        inputExample: { firstName: 'Alice', lastName: 'Smith' },\n        outputExample: { fullName: 'Alice Smith' }\n      }\n    ];\n  }\n  \n  getSyntaxHelp(): string {\n    return `\nJSONata Expression Syntax:\n- Field access: $.field.nested\n- Array filter: $.items[condition]\n- Array map: $.items.{ \"name\": name }\n- Aggregation: $sum(items.price)\n- String concat: field1 & \" \" & field2\n    `.trim();\n  }\n  \n  private getSuggestion(error: any): string | undefined {\n    // Provide helpful suggestions based on common errors\n    if (error.message.includes('undefined')) {\n      return 'Check field names - they may not exist in your data';\n    }\n    if (error.message.includes('syntax')) {\n      return 'Try using the visual builder for complex expressions';\n    }\n    return undefined;\n  }\n}\n\n// Optional visual builder for users who find JSONata hard\nclass VisualExpressionBuilder implements ExpressionEngine {\n  private jsonataEngine = new JSONataEngine();\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    // Expression is JSON representation of visual blocks\n    const blocks = JSON.parse(expression);\n    const jsonataExpr = this.blocksToJSONata(blocks);\n    return this.jsonataEngine.evaluate(jsonataExpr, context);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      const blocks = JSON.parse(expression);\n      const jsonataExpr = this.blocksToJSONata(blocks);\n      return this.jsonataEngine.validate(jsonataExpr);\n    } catch (error) {\n      return { valid: false, error: error.message };\n    }\n  }\n  \n  private blocksToJSONata(blocks: ExpressionBlock[]): string {\n    // Convert visual blocks to JSONata\n    // Example: [{ type: 'field', path: 'user.name' }] \u2192 '$.user.name'\n    return blocks.map(block => {\n      switch (block.type) {\n        case 'field':\n          return `$.${block.path}`;\n        case 'filter':\n          return `[${block.condition}]`;\n        case 'map':\n          return `.{ ${block.mapping} }`;\n        default:\n          throw new Error(`Unknown block type: ${block.type}`);\n      }\n    }).join('');\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return this.jsonataEngine.getExamples();\n  }\n  \n  getSyntaxHelp(): string {\n    return 'Use the visual builder UI to create expressions';\n  }\n}\n\n// Expression engine factory\nclass ExpressionEngineFactory {\n  static create(mode: 'text' | 'visual'): ExpressionEngine {\n    switch (mode) {\n      case 'text':\n        return new JSONataEngine();\n      case 'visual':\n        return new VisualExpressionBuilder();\n      default:\n        return new JSONataEngine();\n    }\n  }\n}\n\n// Usage in UI\nconst ExpressionEditor: React.FC<{ value: string; onChange: (v: string) => void }> = ({ value, onChange }) => {\n  const [mode, setMode] = useState<'text' | 'visual'>('text');\n  const [engine, setEngine] = useState<ExpressionEngine>(ExpressionEngineFactory.create('text'));\n  const [validation, setValidation] = useState<ValidationResult>({ valid: true });\n  \n  useEffect(() => {\n    setEngine(ExpressionEngineFactory.create(mode));\n  }, [mode]);\n  \n  useEffect(() => {\n    setValidation(engine.validate(value));\n  }, [value, engine]);\n  \n  return (\n    <div className=\"expression-editor\">\n      <div className=\"mode-toggle\">\n        <button onClick={() => setMode('text')}>Text</button>\n        <button onClick={() => setMode('visual')}>Visual</button>\n      </div>\n      \n      {mode === 'text' ? (\n        <textarea \n          value={value} \n          onChange={e => onChange(e.target.value)}\n          className={validation.valid ? '' : 'error'}\n        />\n      ) : (\n        <VisualBlockBuilder value={value} onChange={onChange} />\n      )}\n      \n      {!validation.valid && (\n        <div className=\"error-message\">\n          {validation.error}\n          {validation.suggestion && <div className=\"suggestion\">{validation.suggestion}</div>}\n        </div>\n      )}\n      \n      <ExpressionExamples examples={engine.getExamples()} />\n    </div>\n  );\n};\n\ninterface ExpressionBlock {\n  type: string;\n  [key: string]: any;\n}\n\ninterface ValidationResult {\n  valid: boolean;\n  error?: string;\n  suggestion?: string;\n}\n\ninterface ExpressionExample {\n  category: string;\n  expression: string;\n  description: string;\n  inputExample: any;\n  outputExample: any;\n}\n```\n\n## Append-Only Side Effect Log with Semantic Compensation\n\n### Key Insight: Compensation Needs Context, Not Just Undo Functions\n\n```typescript\ninterface SideEffectEntry {\n  // Identity\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  \n  // Operation description\n  operation: OperationType;\n  description: string; // Human-readable\n  \n  // Semantic compensation descriptor (NOT a function)\n  compensation: CompensationDescriptor;\n  \n  // State\n  status: 'pending' | 'executed' | 'failed' | 'compensated';\n  \n  // Audit trail\n  executedAt?: number;\n  compensatedAt?: number;\n  error?: string;\n}\n\ntype OperationType = \n  | 'api_call'\n  | 'file_write'\n  | 'database_write'\n  | 'external_webhook'\n  | 'email_send'\n  | 'payment_process';\n\n// Semantic descriptor instead of executable code\ntype CompensationDescriptor = \n  | { type: 'reversible'; reverseOperation: OperationDescriptor }\n  | { type: 'idempotent'; canRetry: boolean }\n  | { type: 'irreversible'; reason: string; manualSteps?: string[] }\n  | { type: 'eventual'; compensatingService: string; payload: any };\n\ninterface OperationDescriptor {\n  service: string;\n  method: string;\n  endpoint?: string;\n  parameters: Record<string, any>;\n}\n\nclass SideEffectLog {\n  private db: IDBDatabase;\n  private compensationHandlers: Map<string, CompensationHandler> = new Map();\n  \n  constructor() {\n    this.initDB();\n    this.registerDefaultHandlers();\n  }\n  \n  async append(entry: Omit<SideEffectEntry, 'id' | 'timestamp' | 'status'>): Promise<string> {\n    const id = crypto.randomUUID();\n    \n    const effectEntry: SideEffectEntry = {\n      ...entry,\n      id,\n      timestamp: Date.now(),\n      status: 'pending'\n    };\n    \n    // Append to log (write-only, no updates)\n    await this.writeToLog(effectEntry);\n    \n    return id;\n  }\n  \n  async markExecuted(effectId: string): Promise<void> {\n    // Create new log entry (append-only)\n    await this.writeToLog({\n      id: crypto.randomUUID(),\n      nodeId: 'system',\n      executionId: 'system',\n      timestamp: Date.now(),\n      operation: 'api_call', // dummy\n      description: `Effect ${effectId} executed`,\n      compensation: { type: 'irreversible', reason: 'audit entry' },\n      status: 'executed',\n      executedAt: Date.now()\n    });\n  }\n  \n  async compensate(effectId: string): Promise<CompensationResult> {\n    const entry = await this.getEntry(effectId);\n    \n    if (!entry) {\n      return { success: false, error: 'Effect not found' };\n    }\n    \n    if (entry.status !== 'executed') {\n      return { success: false, error: 'Effect not in executed state' };\n    }\n    \n    // Dispatch to appropriate compensation handler\n    const handler = this.compensationHandlers.get(entry.compensation.type);\n    \n    if (!handler) {\n      return { \n        success: false, \n        error: `No handler for compensation type: ${entry.compensation.type}` \n      };\n    }\n    \n    try {\n      const result = await handler.compensate(entry);\n      \n      // Log compensation (append-only)\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensated: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'compensated',\n        compensatedAt: Date.now()\n      });\n      \n      return result;\n      \n    } catch (error) {\n      // Log failure (append-only)\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensation failed: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'failed',\n        error: error.message\n      });\n      \n      return { success: false, error: error.message };\n    }\n  }\n  \n  async compensateAllForNode(nodeId: string): Promise<CompensationResult[]> {\n    const entries = await this.getEntriesForNode(nodeId);\n    const executed = entries\n      .filter(e => e.status === 'executed')\n      .reverse(); // Reverse order for LIFO\n    \n    const results: CompensationResult[] = [];\n    \n    for (const entry of executed) {\n      const result = await this.compensate(entry.id);\n      results.push(result);\n      \n      // Stop on first failure for safety\n      if (!result.success) {\n        break;\n      }\n    }\n    \n    return results;\n  }\n  \n  async getEntriesForExecution(executionId: string): Promise<SideEffectEntry[]> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    const index = store.index('executionId');\n    \n    const entries: SideEffectEntry[] = [];\n    const cursor = await index.openCursor(IDBKeyRange.only(executionId));\n    \n    while (cursor) {\n      entries.push(cursor.value);\n      await cursor.continue();\n    }\n    \n    return entries;\n  }\n  \n  private registerDefaultHandlers(): void {\n    // Reversible operations (API calls with DELETE endpoints)\n    this.compensationHandlers.set('reversible', {\n      compensate: async (entry) => {\n        const descriptor = (entry.compensation as any).reverseOperation;\n        \n        // Call reverse operation\n        const response = await fetch(descriptor.endpoint, {\n          method: descriptor.method,\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.parameters)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Reverse operation failed: ${response.statusText}`);\n        }\n        \n        return { success: true };\n      }\n    });\n    \n    // Idempotent operations (can safely retry)\n    this.compensationHandlers.set('idempotent', {\n      compensate: async (entry) => {\n        // No-op or retry logic\n        return { success: true, message: 'Idempotent operation - safe to leave' };\n      }\n    });\n    \n    // Irreversible operations (manual intervention required)\n    this.compensationHandlers.set('irreversible', {\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        // Can't auto-compensate - return manual steps\n        return {\n          success: false,\n          error: `Cannot auto-compensate: ${descriptor.reason}`,\n          manualSteps: descriptor.manualSteps,\n          requiresUserAction: true\n        };\n      }\n    });\n    \n    // Eventual consistency (call compensating transaction service)\n    this.compensationHandlers.set('eventual', {\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        // Call compensating service\n        const response = await fetch(descriptor.compensatingService, {\n          method: 'POST',\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.payload)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Compensating service failed: ${response.statusText}`);\n        }\n        \n        return { \n          success: true, \n          message: 'Compensating transaction initiated' \n        };\n      }\n    });\n  }\n  \n  private async writeToLog(entry: SideEffectEntry): Promise<void> {\n    const tx = this.db.transaction('effects', 'readwrite');\n    const store = tx.objectStore('effects');\n    await store.add(entry);\n  }\n  \n  private async getEntry(effectId: string): Promise<SideEffectEntry | null> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    return await store.get(effectId);\n  }\n  \n  private async getEntriesForNode(nodeId: string): Promise<SideEffectEntry[]> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    const index = store.index('nodeId');\n    \n    const entries: SideEffectEntry[] = [];\n    const cursor = await index.openCursor(IDBKeyRange.only(nodeId));\n    \n    while (cursor) {\n      entries.push(cursor.value);\n      await cursor.continue();\n    }\n    \n    return entries;\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('SideEffectLog', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        const store = db.createObjectStore('effects', { keyPath: 'id' });\n        store.createIndex('nodeId', 'nodeId', { unique: false });\n        store.createIndex('executionId', 'executionId', { unique: false });\n        store.createIndex('timestamp', 'timestamp', { unique: false });\n      };\n    });\n  }\n}\n\ninterface CompensationHandler {\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n### Usage Examples\n\n```typescript\n// Example 1: API call with reversible compensation\nconst effectId = await sideEffectLog.append({\n  nodeId: 'node-123',\n  executionId: 'exec-456',\n  operation: 'api_call',\n  description: 'Create user account',\n  compensation: {\n    type: 'reversible',\n    reverseOperation: {\n      service: 'user-service',\n      method: 'DELETE',\n      endpoint: '/api/users/{{userId}}',\n      parameters: { userId: '{{createdUserId}}' }\n    }\n  }\n});\n\n// Example 2: Email send (irreversible)\nawait sideEffectLog.append({\n  nodeId: 'node-123',\n  executionId: 'exec-456',\n  operation: 'email_send',\n  description: 'Send welcome email to user@example.com',\n  compensation: {\n    type: 'irreversible',\n    reason: 'Email cannot be unsent',\n    manualSteps: [\n      'Send apology email explaining the error',\n      'Update user account status to reflect correction'\n    ]\n  }\n});\n\n// Example 3: Database write with eventual consistency\nawait sideEffectLog.append({\n  nodeId: 'node-123',\n  executionId: 'exec-456',\n  operation: 'database_write',\n  description: 'Deduct 100 credits from account',\n  compensation: {\n    type: 'eventual',\n    compensatingService: '/api/credits/refund',\n    payload: {\n      accountId: 'acc-789',\n      amount: 100,\n      reason: 'workflow_rollback'\n    }\n  }\n});\n```\n\n## Enhanced Checkpoint Manager\n\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  private sideEffectLog: SideEffectLog;\n  \n  constructor(sideEffectLog: SideEffectLog) {\n    this.sideEffectLog = sideEffectLog;\n    this.initDB();\n  }\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    // Capture side effects up to this point\n    const sideEffects = await this.sideEffectLog.getEntriesForExecution(\n      executionState.executionId\n    );\n    \n    const checkpoint: ExecutionCheckpoint = {\n      id: checkpointId,\n      executionId: executionState.executionId,\n      timestamp: Date.now(),\n      nodeStates: this.serializeNodeStates(executionState.nodeStates),\n      sideEffectCount: sideEffects.length,\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    await tx.objectStore('checkpoints').put(checkpoint);\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string,\n    strategy: CompensationStrategy\n  ): Promise<RestoreResult> {\n    \n    const checkpoint = await this.getCheckpoint(checkpointId);\n    if (!checkpoint) {\n      return { success: false, error: 'Checkpoint not found' };\n    }\n    \n    // Get all side effects after checkpoint\n    const allEffects = await this.sideEffectLog.getEntriesForExecution(\n      checkpoint.executionId\n    );\n    \n    const effectsToCompensate = allEffects.filter(e => \n      e.timestamp > checkpoint.timestamp\n    );\n    \n    // Handle compensation based on strategy\n    const compensationResults = await this.applyCompensationStrategy(\n      effectsToCompensate,\n      strategy\n    );\n    \n    // Check for manual intervention required\n    const manualSteps = compensationResults\n      .filter(r => r.requiresUserAction)\n      .flatMap(r => r.manualSteps || []);\n    \n    return {\n      success: true,\n      checkpoint,\n      compensationResults,\n      manualSteps: manualSteps.length > 0 ? manualSteps : undefined\n    };\n  }\n  \n  private async applyCompensationStrategy(\n    effects: SideEffectEntry[],\n    strategy: CompensationStrategy\n  ): Promise<CompensationResult[]> {\n    \n    switch (strategy) {\n      case 'automatic':\n        // Compensate all reversible effects automatically\n        const results: CompensationResult[] = [];\n        \n        for (const effect of effects.reverse()) {\n          if (effect.compensation.type === 'reversible' || \n              effect.compensation.type === 'eventual') {\n            \n            const result = await this.sideEffectLog.compensate(effect.id);\n            results.push(result);\n            \n            if (!result.success) {\n              // Stop on first failure\n              break;\n            }\n          } else {\n            // Skip irreversible effects\n            results.push({\n              success: false,\n              error: 'Skipped irreversible effect',\n              requiresUserAction: true,\n              manualSteps: (effect.compensation as any).manualSteps\n            });\n          }\n        }\n        \n        return results;\n        \n      case 'manual':\n        // Return list of effects for user to handle manually\n        return effects.map(effect => ({\n          success: false,\n          requiresUserAction: true,\n          manualSteps: [\n            `Effect: ${effect.description}`,\n            `Type: ${effect.compensation.type}`,\n            ...(effect.compensation.type === 'irreversible' \n              ? (effect.compensation as any).manualSteps || []\n              : ['Manually revert this operation'])\n          ]\n        }));\n        \n      case 'confirm_each':\n        // This requires user interaction - not fully implemented here\n        // In practice, this would show a UI for each effect\n        return [];\n        \n      default:\n        throw new Error(`Unknown strategy: ${strategy}`);\n    }\n  }\n  \n  private async getCheckpoint(checkpointId: string): Promise<ExecutionCheckpoint | null> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    return await store.get(checkpointId);\n  }\n  \n  private serializeNodeStates(nodeStates: Map<string, NodeState>): any {\n    return Array.from(nodeStates.entries());\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowCheckpoints', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('checkpoints')) {\n          db.createObjectStore('checkpoints', { keyPath: 'id' });\n        }\n      };\n    });\n  }\n}\n\ntype CompensationStrategy = \n  | 'automatic'      // Auto-compensate all reversible effects\n  | 'manual'         // User handles all compensation\n  | 'confirm_each';  // Ask user for each effect\n\ninterface ExecutionCheckpoint {\n  id: string;\n  executionId: string;\n  timestamp: number;\n  nodeStates: any;\n  sideEffectCount: number;\n  reason: string;\n}\n\ninterface RestoreResult {\n  success: boolean;\n  error?: string;\n  checkpoint?: ExecutionCheckpoint;\n  compensationResults?: CompensationResult[];\n  manualSteps?: string[];\n}\n```\n\n## Performance Optimizations\n\n### Viewport Culling\n```typescript\nconst WorkflowCanvas: React.FC = () => {\n  const { nodes, edges } = useWorkflowStore();\n  const [viewport, setViewport] = useState<Viewport>({ x: 0, y: 0, zoom: 1 });\n  \n  const visibleNodes = useMemo(() => {\n    if (nodes.length < 50) return nodes;\n    \n    return nodes.filter(node => isNodeInViewport(node, viewport));\n  }, [nodes, viewport]);\n  \n  const visibleEdges = useMemo(() => {\n    const visibleNodeIds = new Set(visibleNodes.map(n => n.id));\n    return edges.filter(e => \n      visibleNodeIds.has(e.source) || visibleNodeIds.has(e.target)\n    );\n  }, [edges, visibleNodes]);\n  \n  return (\n    <ReactFlow\n      nodes={visibleNodes}\n      edges={visibleEdges}\n      onMove={(_, vp) => setViewport(vp)}\n    />\n  );\n};\n```\n\n### Virtual Scrolling\n```typescript\nimport { FixedSizeList } from 'react-window';\n\nconst NodePalette: React.FC = () => {\n  const nodeTypes = useNodeTypes();\n  \n  return (\n    <FixedSizeList\n      height={600}\n      itemCount={nodeTypes.length}\n      itemSize={60}\n      width=\"100%\"\n    >\n      {({ index, style }) => (\n        <div style={style}>\n          <NodePaletteItem node={nodeTypes[index]} />\n        </div>\n      )}\n    </FixedSizeList>\n  );\n};\n```\n\n### Debounced Auto-Save\n```typescript\nconst useAutoSave = (workflowId: string) => {\n  const { workflow } = useWorkflowStore();\n  const saveWorkflow = useSaveWorkflow();\n  \n  const debouncedSave = useMemo(\n    () => debounce(() => saveWorkflow(workflow), 2000),\n    [workflow, saveWorkflow]\n  );\n  \n  useEffect(() => {\n    debouncedSave();\n    return () => debouncedSave.cancel();\n  }, [workflow, debouncedSave]);\n};\n```\n\n## What I Improved\n\n### 1. **Dramatically Simplified Storage (3-Tier vs Complex Eviction)**\n- Removed complex hybrid eviction scoring system\n- **Three simple tiers**: Hot (50MB, tiny hot data) \u2192 Warm (500MB, default) \u2192 Cold (optional, huge files)\n- **Simple FIFO eviction** in hot cache, no complex LRU+size hybrid scoring\n- Most artifacts live in Warm (IndexedDB) which is fast enough\n- **Metadata index** keeps all artifact info lightweight and queryable\n- **Auto-cleanup** of old temporary data every hour\n\n### 2. **Streaming Decision Logic (Analyze First, Then Commit)**\n- Added `analyzeStreamingFeasibility()` to make intelligent streaming decisions\n- **Materialization strategy** for multi-consumer scenarios (store once, serve many)\n- **Streaming reasons** tracked for debugging and optimization\n- **Graceful degradation** to batch mode when streaming doesn't make sense\n- **Storage references** ($ref) for materialized streams instead of duplicating data\n\n### 3. **Pluggable Expression Engine (Progressive Enhancement)**\n- Expression engine is now an **interface** - swap JSONata for visual builder\n- **Validation with suggestions** helps users fix errors\n- **Visual builder option** for users who find text expressions hard\n- **Expression examples** built into the engine, not hard-coded in UI\n- **Syntax help** provided by the engine itself\n\n### 4. **Semantic Compensation (Descriptors Not Functions)**\n- Replaced JSONata compensation functions with **semantic descriptors**\n- **Four compensation types**: reversible, idempotent, irreversible, eventual\n- **Compensation handlers** registered per type, not per node\n- **Manual steps** included for irreversible operations (emails, payments)\n- **Append-only log** instead of mutable side effect records\n- **Audit trail** preserved forever for compliance\n\n### 5. **Compensation Strategy Selection**\n- Three strategies: **automatic** (safe auto-compensation), **manual** (user handles), **confirm_each** (interactive)\n- **Automatic stops on first failure** for safety\n- **Manual steps returned** to user for irreversible effects\n- **Partial compensation** supported (some effects can't be undone)\n\n## What I Kept\n\nAll the excellent foundation:\n- React Flow canvas\n- Zustand + immer state management\n- Worker pool architecture\n- Topological execution\n- Multiple handles per node\n- Input/output schemas\n- Retry configuration\n- Event system\n- Undo/redo\n- Edge validation\n- Resource limits\n- Node status granularity\n- Template system\n- Viewport culling optimization\n- Virtual scrolling for large lists\n- Debounced auto-save\n- Native Web Streams API for streaming\n- Compression for large data\n\n## Remaining Concerns\n\n### 1. **Cold Storage Implementation**\nThe current cold storage using blob URLs is browser-only and lost on refresh. For production:\n- Should we use **object storage** (S3, R2) for truly cold artifacts?\n- **Pre-signed URLs** for download instead of storing in browser?\n- **Lifecycle policies** to auto-delete cold storage after X days?\n\n### 2. **Expression Engine Complexity**\nJSONata is powerful but has a learning curve:\n- Should visual builder be **always available** as an alternative input method?\n- **AI-assisted expression generation**: \"convert this to an expression\"?\n- **Template expressions** library for common patterns?\n\n### 3. **Compensation Handler Registration**\nCurrently hard-coded handlers for 4 types:\n- Should users be able to **register custom handlers** for their services?\n- **Plugin system** for compensation handlers?\n- **Compensation middleware** for cross-cutting concerns (logging, notifications)?\n\n### 4. **Multi-Tab Coordination**\nNot addressed in this design:\n- **Lock workflows** during execution to prevent conflicts?\n- **Broadcast execution state** using BroadcastChannel API?\n- **Conflict resolution** when multiple tabs modify same workflow?\n\n### 5. **Cost Tracking**\nFor long-running workflows with many LLM calls:\n- **Pre-execution cost estimate** based on token limits per node?\n- **Running cost counter** updated as nodes complete?\n- **Budget limits** with auto-pause when approaching threshold?\n\n## Convergence Status\n\n**ITERATING** - This design significantly simplifies storage and compensation while adding critical missing pieces:\n\n\u2705 Simplified 3-tier storage (much easier to implement and reason about)  \n\u2705 Smart streaming decisions with materialization strategy  \n\u2705 Pluggable expression engine (text or visual)  \n\u2705 Semantic compensation with audit trail  \n\u2705 Compensation strategy selection  \n\u2705 All performance optimizations preserved  \n\nHowever, still needs refinement on:\n- Cold storage backend for production\n- Expression engine UX (visual builder, AI assist)\n- Custom compensation handler registration\n- Multi-tab coordination strategy\n\n---\n\n##",
        "prompt_for_other": "Please focus on these specific design questions:\n\n1. **Storage Backend**: Should cold storage use browser blob URLs (current design) or integrate with object storage (S3/R2) for production? What's the right trade-off between complexity and capability?\n\n2. **Expression Engine UX**: Should the visual builder be a separate mode (current design) or should we allow **mixed mode** where users can switch between text and visual for different parts of an expression?\n\n3. **Compensation Handler Extensibility**: Should we support **user-defined compensation handlers** for custom services, or is the 4-type system (reversible/idempotent/irreversible/eventual) sufficient for most use cases?\n\n4. **Materialization Heuristics**: When a stream has multiple consumers, the current design always materializes to storage. Should we support **stream teeing** for small streams and only materialize large ones? What's the size threshold?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v2\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Tiered    \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Storage   \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502  Pluggable \u2502  \u2502   Hybrid   \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  Storage   \u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  Engine    \u2502  \u2502  Manager   \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with custom viewport rendering\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with streaming-first, batch fallback\n- **Storage**: Three-tier: Hot cache (memory) \u2192 Warm cache (IndexedDB) \u2192 Cold storage (optional blob URLs)\n- **Expressions**: Pluggable engine (JSONata default, optional visual builder)\n- **Side Effects**: Append-only log with semantic compensation descriptors\n- **Workers**: Shared Worker pool with resource quotas\n\n## Simplified Three-Tier Storage Architecture\n\n### Key Insight: Most Artifacts Don't Need Eviction Logic\n\n```typescript\ninterface StorageConfig {\n  // Simplified thresholds - no complex eviction\n  hotCacheMaxBytes: number;      // 50MB - small, hot data only\n  warmCacheMaxBytes: number;     // 500MB - most artifacts live here\n  enableColdStorage: boolean;    // Optional blob URLs for huge files\n  compressionThreshold: number;  // 1MB - compress above this\n  autoCleanupAge: number;        // 24h - auto-delete old temp data\n}\n\nclass TieredStorageManager {\n  private hotCache: Map<string, HotCacheEntry> = new Map();\n  private hotCacheSize: number = 0;\n  private warmDB: IDBDatabase;\n  private coldBlobUrls: Map<string, string> = new Map();\n  private config: StorageConfig;\n  \n  // Metadata index for all artifacts (lightweight)\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initWarmStorage();\n    this.scheduleCleanup();\n  }\n  \n  async store(key: string, data: any, hints: StorageHints): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new Blob([serialized]).size;\n    \n    // Update metadata index\n    this.artifactIndex.set(key, {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      temporary: hints.temporary,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      accessCount: 0,\n      lastAccessedAt: Date.now()\n    });\n    \n    // Tier selection: SIMPLE rules, no complex scoring\n    const tier = this.selectTier(sizeBytes, hints);\n    \n    switch (tier) {\n      case 'hot':\n        await this.storeInHotCache(key, data, sizeBytes);\n        break;\n        \n      case 'warm':\n        await this.storeInWarmCache(key, serialized, sizeBytes);\n        break;\n        \n      case 'cold':\n        await this.storeInColdStorage(key, serialized, sizeBytes);\n        break;\n    }\n    \n    this.artifactIndex.get(key)!.tier = tier;\n  }\n  \n  private selectTier(sizeBytes: number, hints: StorageHints): StorageTier {\n    // COLD: Files > 100MB (if enabled) - these are rare\n    if (this.config.enableColdStorage && sizeBytes > 100 * 1024 * 1024) {\n      return 'cold';\n    }\n    \n    // HOT: Small + frequently accessed OR marked as hot\n    // Only small items that are actively being used by running nodes\n    if (sizeBytes < 100 * 1024 && // < 100KB\n        (hints.accessPattern === 'hot' || hints.temporary)) {\n      \n      // Check if we have room in hot cache\n      if (this.hotCacheSize + sizeBytes <= this.config.hotCacheMaxBytes) {\n        return 'hot';\n      }\n      \n      // Try to make room by removing oldest hot items\n      if (this.makeRoomInHotCache(sizeBytes)) {\n        return 'hot';\n      }\n    }\n    \n    // WARM: Everything else (default tier)\n    // Most artifacts live here - IndexedDB is fast enough\n    return 'warm';\n  }\n  \n  private makeRoomInHotCache(requiredBytes: number): boolean {\n    // Simple FIFO eviction of oldest items\n    const entries = Array.from(this.hotCache.entries())\n      .sort((a, b) => a[1].storedAt - b[1].storedAt);\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const [key, entry] of entries) {\n      // Never evict items stored in last 5 seconds\n      if (Date.now() - entry.storedAt < 5000) continue;\n      \n      toEvict.push(key);\n      freedBytes += entry.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    // Move to warm cache (no data loss)\n    for (const key of toEvict) {\n      const entry = this.hotCache.get(key)!;\n      this.storeInWarmCache(key, JSON.stringify(entry.data), entry.sizeBytes);\n      this.hotCache.delete(key);\n      this.hotCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) metadata.tier = 'warm';\n    }\n    \n    return freedBytes >= requiredBytes;\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    // Update access tracking\n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    // Retrieve from appropriate tier\n    let data: any;\n    \n    switch (metadata.tier) {\n      case 'hot':\n        data = this.hotCache.get(key)?.data;\n        if (!data) {\n          // Fallback to warm if somehow missing\n          data = await this.retrieveFromWarmCache(key);\n        }\n        break;\n        \n      case 'warm':\n        data = await this.retrieveFromWarmCache(key);\n        \n        // Promote to hot if: small + accessed 3+ times recently\n        if (metadata.sizeBytes < 100 * 1024 && \n            metadata.accessCount >= 3 &&\n            Date.now() - metadata.lastAccessedAt < 60000) {\n          \n          if (this.hotCacheSize + metadata.sizeBytes <= this.config.hotCacheMaxBytes) {\n            await this.storeInHotCache(key, data, metadata.sizeBytes);\n            metadata.tier = 'hot';\n          }\n        }\n        break;\n        \n      case 'cold':\n        data = await this.retrieveFromColdStorage(key);\n        break;\n    }\n    \n    return data;\n  }\n  \n  private async storeInHotCache(key: string, data: any, sizeBytes: number): Promise<void> {\n    this.hotCache.set(key, {\n      data,\n      sizeBytes,\n      storedAt: Date.now()\n    });\n    this.hotCacheSize += sizeBytes;\n  }\n  \n  private async storeInWarmCache(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    // Compress if large enough\n    const shouldCompress = sizeBytes > this.config.compressionThreshold;\n    const toStore = shouldCompress \n      ? await this.compress(serialized)\n      : serialized;\n    \n    const tx = this.warmDB.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      storedAt: Date.now()\n    });\n  }\n  \n  private async retrieveFromWarmCache(key: string): Promise<any> {\n    const tx = this.warmDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not found in warm cache: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async storeInColdStorage(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    // Use Blob URLs for huge files (rare case)\n    const compressed = await this.compress(serialized);\n    const blob = new Blob([compressed], { type: 'application/octet-stream' });\n    const blobUrl = URL.createObjectURL(blob);\n    \n    this.coldBlobUrls.set(key, blobUrl);\n    \n    // Also store reference in IndexedDB\n    const tx = this.warmDB.transaction('coldRefs', 'readwrite');\n    const store = tx.objectStore('coldRefs');\n    await store.put({ key, blobUrl, sizeBytes, storedAt: Date.now() });\n  }\n  \n  private async retrieveFromColdStorage(key: string): Promise<any> {\n    const blobUrl = this.coldBlobUrls.get(key);\n    if (!blobUrl) {\n      throw new Error(`Cold storage artifact not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = new Uint8Array(await response.arrayBuffer());\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async clear(predicate?: (metadata: ArtifactMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      // Clear everything\n      this.hotCache.clear();\n      this.hotCacheSize = 0;\n      this.artifactIndex.clear();\n      \n      const tx = this.warmDB.transaction(['artifacts', 'coldRefs'], 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      await tx.objectStore('coldRefs').clear();\n      \n      // Revoke blob URLs\n      for (const blobUrl of this.coldBlobUrls.values()) {\n        URL.revokeObjectURL(blobUrl);\n      }\n      this.coldBlobUrls.clear();\n      \n      return;\n    }\n    \n    // Selective clear\n    const toDelete: string[] = [];\n    \n    for (const [key, metadata] of this.artifactIndex.entries()) {\n      if (predicate(metadata)) {\n        toDelete.push(key);\n      }\n    }\n    \n    for (const key of toDelete) {\n      await this.delete(key);\n    }\n  }\n  \n  private async delete(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) return;\n    \n    switch (metadata.tier) {\n      case 'hot':\n        const entry = this.hotCache.get(key);\n        if (entry) {\n          this.hotCacheSize -= entry.sizeBytes;\n          this.hotCache.delete(key);\n        }\n        break;\n        \n      case 'warm':\n        const tx = this.warmDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        break;\n        \n      case 'cold':\n        const blobUrl = this.coldBlobUrls.get(key);\n        if (blobUrl) {\n          URL.revokeObjectURL(blobUrl);\n          this.coldBlobUrls.delete(key);\n        }\n        \n        const txCold = this.warmDB.transaction('coldRefs', 'readwrite');\n        await txCold.objectStore('coldRefs').delete(key);\n        break;\n    }\n    \n    this.artifactIndex.delete(key);\n  }\n  \n  private scheduleCleanup(): void {\n    // Auto-cleanup old temporary data every hour\n    setInterval(() => {\n      const cutoff = Date.now() - this.config.autoCleanupAge;\n      \n      this.clear(metadata => \n        metadata.temporary && metadata.createdAt < cutoff\n      );\n    }, 60 * 60 * 1000); // 1 hour\n  }\n  \n  getStorageStats(): StorageStats {\n    const stats = {\n      hot: { count: 0, bytes: this.hotCacheSize },\n      warm: { count: 0, bytes: 0 },\n      cold: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      stats[metadata.tier].count++;\n      stats[metadata.tier].bytes += metadata.sizeBytes;\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n  \n  private async compress(data: string): Promise<Uint8Array> {\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    return new Uint8Array(buffer);\n  }\n  \n  private async decompress(data: Uint8Array): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initWarmStorage(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowArtifacts', 2);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.warmDB = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          db.createObjectStore('artifacts', { keyPath: 'key' });\n        }\n        if (!db.objectStoreNames.contains('coldRefs')) {\n          db.createObjectStore('coldRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n}\n\n// Simplified types\ninterface HotCacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: StorageTier;\n  createdAt: number;\n  temporary: boolean;\n  nodeId?: string;\n  executionId?: string;\n  accessCount: number;\n  lastAccessedAt: number;\n}\n\ntype StorageTier = 'hot' | 'warm' | 'cold' | 'determining';\n\ninterface StorageHints {\n  temporary?: boolean;\n  accessPattern?: 'hot' | 'warm';\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface StorageStats {\n  hot: { count: number; bytes: number };\n  warm: { count: number; bytes: number };\n  cold: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n```\n\n### Usage Example\n```typescript\nconst storage = new TieredStorageManager({\n  hotCacheMaxBytes: 50 * 1024 * 1024,      // 50MB\n  warmCacheMaxBytes: 500 * 1024 * 1024,    // 500MB\n  enableColdStorage: true,\n  compressionThreshold: 1024 * 1024,        // 1MB\n  autoCleanupAge: 24 * 60 * 60 * 1000      // 24h\n});\n\n// Store - automatic tiering\nawait storage.store(\n  `node-${nodeId}-output`,\n  largeDataset,\n  {\n    nodeId,\n    executionId,\n    temporary: true,\n    accessPattern: 'warm' // Default for most data\n  }\n);\n\n// Retrieve - transparent tier access\nconst data = await storage.retrieve(`node-${nodeId}-output`);\n\n// Cleanup after workflow\nawait storage.clear(meta => \n  meta.temporary && meta.executionId === executionId\n);\n\n// Check storage usage\nconst stats = storage.getStorageStats();\nconsole.log(`Using ${stats.hot.bytes} bytes in hot cache`);\n```\n\n## Streaming-First with Graceful Degradation\n\n### Key Insight: Don't Force Everything to Stream\n\n```typescript\nclass AdaptiveExecutor {\n  private streamingExecutor: StreamingExecutor;\n  private batchExecutor: BatchExecutor;\n  private storage: TieredStorageManager;\n  \n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    // Check if streaming makes sense\n    const streamingDecision = this.analyzeStreamingFeasibility(node, inputs);\n    \n    if (streamingDecision.shouldStream) {\n      return this.executeWithStreaming(node, inputs, streamingDecision);\n    } else {\n      return this.executeWithBatch(node, inputs);\n    }\n  }\n  \n  private analyzeStreamingFeasibility(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): StreamingDecision {\n    \n    const reasons: string[] = [];\n    let shouldStream = false;\n    let materializationStrategy: MaterializationStrategy = 'none';\n    \n    // 1. Node explicitly requests streaming\n    if (node.data.config.streaming === true) {\n      shouldStream = true;\n      reasons.push('node_config_explicit');\n    }\n    \n    // 2. Node explicitly REJECTS streaming\n    if (node.data.config.streaming === false) {\n      return {\n        shouldStream: false,\n        reasons: ['node_config_explicit_batch'],\n        materializationStrategy: 'none'\n      };\n    }\n    \n    // 3. Input is already a stream\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream\n    );\n    \n    if (hasStreamInput) {\n      shouldStream = true;\n      reasons.push('stream_input');\n      \n      // Check if multiple downstream consumers need this stream\n      const downstreamCount = this.getDownstreamConsumerCount(node);\n      \n      if (downstreamCount > 1) {\n        // MATERIALIZE ONCE for multiple consumers\n        materializationStrategy = 'materialize_to_storage';\n        reasons.push('multiple_consumers_detected');\n      }\n    }\n    \n    // 4. Large input suggests streaming (but not required)\n    const estimatedSize = this.estimateDataSize(inputs);\n    if (estimatedSize > 5 * 1024 * 1024) { // 5MB\n      shouldStream = true;\n      reasons.push('large_input_detected');\n    }\n    \n    // 5. Agent type benefits from streaming (LLM responses)\n    if (['researcher', 'coder', 'reviewer'].includes(node.data.type)) {\n      shouldStream = true;\n      reasons.push('agent_type_benefits');\n    }\n    \n    return {\n      shouldStream,\n      reasons,\n      materializationStrategy\n    };\n  }\n  \n  private async executeWithStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const outputStream = await this.streamingExecutor.executeNodeStreaming(\n      node,\n      inputs\n    );\n    \n    // Handle materialization if needed\n    if (decision.materializationStrategy === 'materialize_to_storage') {\n      return await this.materializeStreamToStorage(node, outputStream);\n    }\n    \n    // Otherwise return stream directly\n    return {\n      nodeId: node.id,\n      output: outputStream,\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons\n      }\n    };\n  }\n  \n  private async materializeStreamToStorage(\n    node: WorkflowNode,\n    stream: ReadableStream\n  ): Promise<ExecutionResult> {\n    \n    // Read entire stream into storage\n    const chunks: any[] = [];\n    const reader = stream.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        chunks.push(value);\n      }\n    } finally {\n      reader.releaseLock();\n    }\n    \n    // Combine chunks\n    const materialized = this.combineChunks(chunks);\n    \n    // Store in tiered storage\n    const storageKey = `node-${node.id}-materialized`;\n    await this.storage.store(storageKey, materialized, {\n      nodeId: node.id,\n      temporary: true,\n      accessPattern: 'warm'\n    });\n    \n    // Return reference, not the data itself\n    return {\n      nodeId: node.id,\n      output: { $ref: storageKey },\n      streaming: false,\n      metadata: {\n        materialized: true,\n        storageKey\n      }\n    };\n  }\n  \n  private async executeWithBatch(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    // Resolve any storage references in inputs\n    const resolvedInputs = await this.resolveStorageReferences(inputs);\n    \n    // Execute node\n    const output = await this.batchExecutor.executeNodeBatch(node, resolvedInputs);\n    \n    return {\n      nodeId: node.id,\n      output,\n      streaming: false,\n      metadata: {}\n    };\n  }\n  \n  private async resolveStorageReferences(inputs: Record<string, any>): Promise<Record<string, any>> {\n    const resolved: Record<string, any> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      if (value && typeof value === 'object' && value.$ref) {\n        // Storage reference - retrieve\n        resolved[key] = await this.storage.retrieve(value.$ref);\n      } else {\n        resolved[key] = value;\n      }\n    }\n    \n    return resolved;\n  }\n  \n  private getDownstreamConsumerCount(node: WorkflowNode): number {\n    // Count how many nodes consume this node's output\n    return this.edges.filter(e => e.source === node.id).length;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private combineChunks(chunks: any[]): any {\n    // Combine strategy depends on chunk type\n    if (chunks.length === 0) return null;\n    \n    if (typeof chunks[0] === 'string') {\n      return chunks.join('');\n    }\n    \n    if (Array.isArray(chunks[0])) {\n      return chunks.flat();\n    }\n    \n    // Objects - last chunk wins\n    return chunks[chunks.length - 1];\n  }\n}\n\ninterface StreamingDecision {\n  shouldStream: boolean;\n  reasons: string[];\n  materializationStrategy: MaterializationStrategy;\n}\n\ntype MaterializationStrategy = \n  | 'none'                      // Pass stream through\n  | 'materialize_to_storage';   // Store once, serve multiple\n\ninterface ExecutionResult {\n  nodeId: string;\n  output: any | ReadableStream | { $ref: string };\n  streaming: boolean;\n  metadata: Record<string, any>;\n}\n```\n\n## Pluggable Expression Engine\n\n### Key Insight: Start Simple, Allow Upgrades\n\n```typescript\ninterface ExpressionEngine {\n  evaluate(expression: string, context: any): Promise<any>;\n  validate(expression: string): ValidationResult;\n  getExamples(): ExpressionExample[];\n  getSyntaxHelp(): string;\n}\n\nclass JSONataEngine implements ExpressionEngine {\n  private cache: Map<string, jsonata.Expression> = new Map();\n  private readonly timeout = 5000;\n  private readonly maxDepth = 100;\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      compiled = jsonata(expression);\n      \n      // Register safe functions\n      compiled.registerFunction('now', () => new Date().toISOString());\n      compiled.registerFunction('random', () => Math.random());\n      compiled.registerFunction('uuid', () => crypto.randomUUID());\n      \n      this.cache.set(expression, compiled);\n    }\n    \n    return await Promise.race([\n      compiled.evaluate(context, { maxDepth: this.maxDepth }),\n      new Promise((_, reject) => \n        setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n      )\n    ]);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      jsonata(expression);\n      return { valid: true };\n    } catch (error) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: this.getSuggestion(error)\n      };\n    }\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return [\n      {\n        category: 'Extract Field',\n        expression: '$.user.name',\n        description: 'Get nested field value',\n        inputExample: { user: { name: 'Alice' } },\n        outputExample: 'Alice'\n      },\n      {\n        category: 'Filter Array',\n        expression: '$.items[price > 100]',\n        description: 'Filter array by condition',\n        inputExample: { items: [{ price: 50 }, { price: 150 }] },\n        outputExample: [{ price: 150 }]\n      },\n      {\n        category: 'Transform',\n        expression: '{ \"fullName\": firstName & \" \" & lastName }',\n        description: 'Create new object from fields',\n        inputExample: { firstName: 'Alice', lastName: 'Smith' },\n        outputExample: { fullName: 'Alice Smith' }\n      }\n    ];\n  }\n  \n  getSyntaxHelp(): string {\n    return `\nJSONata Expression Syntax:\n- Field access: $.field.nested\n- Array filter: $.items[condition]\n- Array map: $.items.{ \"name\": name }\n- Aggregation: $sum(items.price)\n- String concat: field1 & \" \" & field2\n    `.trim();\n  }\n  \n  private getSuggestion(error: any): string | undefined {\n    // Provide helpful suggestions based on common errors\n    if (error.message.includes('undefined')) {\n      return 'Check field names - they may not exist in your data';\n    }\n    if (error.message.includes('syntax')) {\n      return 'Try using the visual builder for complex expressions';\n    }\n    return undefined;\n  }\n}\n\n// Optional visual builder for users who find JSONata hard\nclass VisualExpressionBuilder implements ExpressionEngine {\n  private jsonataEngine = new JSONataEngine();\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    // Expression is JSON representation of visual blocks\n    const blocks = JSON.parse(expression);\n    const jsonataExpr = this.blocksToJSONata(blocks);\n    return this.jsonataEngine.evaluate(jsonataExpr, context);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      const blocks = JSON.parse(expression);\n      const jsonataExpr = this.blocksToJSONata(blocks);\n      return this.jsonataEngine.validate(jsonataExpr);\n    } catch (error) {\n      return { valid: false, error: error.message };\n    }\n  }\n  \n  private blocksToJSONata(blocks: ExpressionBlock[]): string {\n    // Convert visual blocks to JSONata\n    // Example: [{ type: 'field', path: 'user.name' }] \u2192 '$.user.name'\n    return blocks.map(block => {\n      switch (block.type) {\n        case 'field':\n          return `$.${block.path}`;\n        case 'filter':\n          return `[${block.condition}]`;\n        case 'map':\n          return `.{ ${block.mapping} }`;\n        default:\n          throw new Error(`Unknown block type: ${block.type}`);\n      }\n    }).join('');\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return this.jsonataEngine.getExamples();\n  }\n  \n  getSyntaxHelp(): string {\n    return 'Use the visual builder UI to create expressions';\n  }\n}\n\n// Expression engine factory\nclass ExpressionEngineFactory {\n  static create(mode: 'text' | 'visual'): ExpressionEngine {\n    switch (mode) {\n      case 'text':\n        return new JSONataEngine();\n      case 'visual':\n        return new VisualExpressionBuilder();\n      default:\n        return new JSONataEngine();\n    }\n  }\n}\n\n// Usage in UI\nconst ExpressionEditor: React.FC<{ value: string; onChange: (v: string) => void }> = ({ value, onChange }) => {\n  const [mode, setMode] = useState<'text' | 'visual'>('text');\n  const [engine, setEngine] = useState<ExpressionEngine>(ExpressionEngineFactory.create('text'));\n  const [validation, setValidation] = useState<ValidationResult>({ valid: true });\n  \n  useEffect(() => {\n    setEngine(ExpressionEngineFactory.create(mode));\n  }, [mode]);\n  \n  useEffect(() => {\n    setValidation(engine.validate(value));\n  }, [value, engine]);\n  \n  return (\n    <div className=\"expression-editor\">\n      <div className=\"mode-toggle\">\n        <button onClick={() => setMode('text')}>Text</button>\n        <button onClick={() => setMode('visual')}>Visual</button>\n      </div>\n      \n      {mode === 'text' ? (\n        <textarea \n          value={value} \n          onChange={e => onChange(e.target.value)}\n          className={validation.valid ? '' : 'error'}\n        />\n      ) : (\n        <VisualBlockBuilder value={value} onChange={onChange} />\n      )}\n      \n      {!validation.valid && (\n        <div className=\"error-message\">\n          {validation.error}\n          {validation.suggestion && <div className=\"suggestion\">{validation.suggestion}</div>}\n        </div>\n      )}\n      \n      <ExpressionExamples examples={engine.getExamples()} />\n    </div>\n  );\n};\n\ninterface ExpressionBlock {\n  type: string;\n  [key: string]: any;\n}\n\ninterface ValidationResult {\n  valid: boolean;\n  error?: string;\n  suggestion?: string;\n}\n\ninterface ExpressionExample {\n  category: string;\n  expression: string;\n  description: string;\n  inputExample: any;\n  outputExample: any;\n}\n```\n\n## Append-Only Side Effect Log with Semantic Compensation\n\n### Key Insight: Compensation Needs Context, Not Just Undo Functions\n\n```typescript\ninterface SideEffectEntry {\n  // Identity\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  \n  // Operation description\n  operation: OperationType;\n  description: string; // Human-readable\n  \n  // Semantic compensation descriptor (NOT a function)\n  compensation: CompensationDescriptor;\n  \n  // State\n  status: 'pending' | 'executed' | 'failed' | 'compensated';\n  \n  // Audit trail\n  executedAt?: number;\n  compensatedAt?: number;\n  error?: string;\n}\n\ntype OperationType = \n  | 'api_call'\n  | 'file_write'\n  | 'database_write'\n  | 'external_webhook'\n  | 'email_send'\n  | 'payment_process';\n\n// Semantic descriptor instead of executable code\ntype CompensationDescriptor = \n  | { type: 'reversible'; reverseOperation: OperationDescriptor }\n  | { type: 'idempotent'; canRetry: boolean }\n  | { type: 'irreversible'; reason: string; manualSteps?: string[] }\n  | { type: 'eventual'; compensatingService: string; payload: any };\n\ninterface OperationDescriptor {\n  service: string;\n  method: string;\n  endpoint?: string;\n  parameters: Record<string, any>;\n}\n\nclass SideEffectLog {\n  private db: IDBDatabase;\n  private compensationHandlers: Map<string, CompensationHandler> = new Map();\n  \n  constructor() {\n    this.initDB();\n    this.registerDefaultHandlers();\n  }\n  \n  async append(entry: Omit<SideEffectEntry, 'id' | 'timestamp' | 'status'>): Promise<string> {\n    const id = crypto.randomUUID();\n    \n    const effectEntry: SideEffectEntry = {\n      ...entry,\n      id,\n      timestamp: Date.now(),\n      status: 'pending'\n    };\n    \n    // Append to log (write-only, no updates)\n    await this.writeToLog(effectEntry);\n    \n    return id;\n  }\n  \n  async markExecuted(effectId: string): Promise<void> {\n    // Create new log entry (append-only)\n    await this.writeToLog({\n      id: crypto.randomUUID(),\n      nodeId: 'system',\n      executionId: 'system',\n      timestamp: Date.now(),\n      operation: 'api_call', // dummy\n      description: `Effect ${effectId} executed`,\n      compensation: { type: 'irreversible', reason: 'audit entry' },\n      status: 'executed',\n      executedAt: Date.now()\n    });\n  }\n  \n  async compensate(effectId: string): Promise<CompensationResult> {\n    const entry = await this.getEntry(effectId);\n    \n    if (!entry) {\n      return { success: false, error: 'Effect not found' };\n    }\n    \n    if (entry.status !== 'executed') {\n      return { success: false, error: 'Effect not in executed state' };\n    }\n    \n    // Dispatch to appropriate compensation handler\n    const handler = this.compensationHandlers.get(entry.compensation.type);\n    \n    if (!handler) {\n      return { \n        success: false, \n        error: `No handler for compensation type: ${entry.compensation.type}` \n      };\n    }\n    \n    try {\n      const result = await handler.compensate(entry);\n      \n      // Log compensation (append-only)\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensated: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'compensated',\n        compensatedAt: Date.now()\n      });\n      \n      return result;\n      \n    } catch (error) {\n      // Log failure (append-only)\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensation failed: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'failed',\n        error: error.message\n      });\n      \n      return { success: false, error: error.message };\n    }\n  }\n  \n  async compensateAllForNode(nodeId: string): Promise<CompensationResult[]> {\n    const entries = await this.getEntriesForNode(nodeId);\n    const executed = entries\n      .filter(e => e.status === 'executed')\n      .reverse(); // Reverse order for LIFO\n    \n    const results: CompensationResult[] = [];\n    \n    for (const entry of executed) {\n      const result = await this.compensate(entry.id);\n      results.push(result);\n      \n      // Stop on first failure for safety\n      if (!result.success) {\n        break;\n      }\n    }\n    \n    return results;\n  }\n  \n  async getEntriesForExecution(executionId: string): Promise<SideEffectEntry[]> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    const index = store.index('executionId');\n    \n    const entries: SideEffectEntry[] = [];\n    const cursor = await index.openCursor(IDBKeyRange.only(executionId));\n    \n    while (cursor) {\n      entries.push(cursor.value);\n      await cursor.continue();\n    }\n    \n    return entries;\n  }\n  \n  private registerDefaultHandlers(): void {\n    // Reversible operations (API calls with DELETE endpoints)\n    this.compensationHandlers.set('reversible', {\n      compensate: async (entry) => {\n        const descriptor = (entry.compensation as any).reverseOperation;\n        \n        // Call reverse operation\n        const response = await fetch(descriptor.endpoint, {\n          method: descriptor.method,\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.parameters)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Reverse operation failed: ${response.statusText}`);\n        }\n        \n        return { success: true };\n      }\n    });\n    \n    // Idempotent operations (can safely retry)\n    this.compensationHandlers.set('idempotent', {\n      compensate: async (entry) => {\n        // No-op or retry logic\n        return { success: true, message: 'Idempotent operation - safe to leave' };\n      }\n    });\n    \n    // Irreversible operations (manual intervention required)\n    this.compensationHandlers.set('irreversible', {\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        // Can't auto-compensate - return manual steps\n        return {\n          success: false,\n          error: `Cannot auto-compensate: ${descriptor.reason}`,\n          manualSteps: descriptor.manualSteps,\n          requiresUserAction: true\n        };\n      }\n    });\n    \n    // Eventual consistency (call compensating transaction service)\n    this.compensationHandlers.set('eventual', {\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        // Call compensating service\n        const response = await fetch(descriptor.compensatingService, {\n          method: 'POST',\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.payload)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Compensating service failed: ${response.statusText}`);\n        }\n        \n        return { \n          success: true, \n          message: 'Compensating transaction initiated' \n        };\n      }\n    });\n  }\n  \n  private async writeToLog(entry: SideEffectEntry): Promise<void> {\n    const tx = this.db.transaction('effects', 'readwrite');\n    const store = tx.objectStore('effects');\n    await store.add(entry);\n  }\n  \n  private async getEntry(effectId: string): Promise<SideEffectEntry | null> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    return await store.get(effectId);\n  }\n  \n  private async getEntriesForNode(nodeId: string): Promise<SideEffectEntry[]> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    const index = store.index('nodeId');\n    \n    const entries: SideEffectEntry[] = [];\n    const cursor = await index.openCursor(IDBKeyRange.only(nodeId));\n    \n    while (cursor) {\n      entries.push(cursor.value);\n      await cursor.continue();\n    }\n    \n    return entries;\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('SideEffectLog', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        const store = db.createObjectStore('effects', { keyPath: 'id' });\n        store.createIndex('nodeId', 'nodeId', { unique: false });\n        store.createIndex('executionId', 'executionId', { unique: false });\n        store.createIndex('timestamp', 'timestamp', { unique: false });\n      };\n    });\n  }\n}\n\ninterface CompensationHandler {\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n### Usage Examples\n\n```typescript\n// Example 1: API call with reversible compensation\nconst effectId = await sideEffectLog.append({\n  nodeId: 'node-123',\n  executionId: 'exec-456',\n  operation: 'api_call',\n  description: 'Create user account',\n  compensation: {\n    type: 'reversible',\n    reverseOperation: {\n      service: 'user-service',\n      method: 'DELETE',\n      endpoint: '/api/users/{{userId}}',\n      parameters: { userId: '{{createdUserId}}' }\n    }\n  }\n});\n\n// Example 2: Email send (irreversible)\nawait sideEffectLog.append({\n  nodeId: 'node-123',\n  executionId: 'exec-456',\n  operation: 'email_send',\n  description: 'Send welcome email to user@example.com',\n  compensation: {\n    type: 'irreversible',\n    reason: 'Email cannot be unsent',\n    manualSteps: [\n      'Send apology email explaining the error',\n      'Update user account status to reflect correction'\n    ]\n  }\n});\n\n// Example 3: Database write with eventual consistency\nawait sideEffectLog.append({\n  nodeId: 'node-123',\n  executionId: 'exec-456',\n  operation: 'database_write',\n  description: 'Deduct 100 credits from account',\n  compensation: {\n    type: 'eventual',\n    compensatingService: '/api/credits/refund',\n    payload: {\n      accountId: 'acc-789',\n      amount: 100,\n      reason: 'workflow_rollback'\n    }\n  }\n});\n```\n\n## Enhanced Checkpoint Manager\n\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  private sideEffectLog: SideEffectLog;\n  \n  constructor(sideEffectLog: SideEffectLog) {\n    this.sideEffectLog = sideEffectLog;\n    this.initDB();\n  }\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    // Capture side effects up to this point\n    const sideEffects = await this.sideEffectLog.getEntriesForExecution(\n      executionState.executionId\n    );\n    \n    const checkpoint: ExecutionCheckpoint = {\n      id: checkpointId,\n      executionId: executionState.executionId,\n      timestamp: Date.now(),\n      nodeStates: this.serializeNodeStates(executionState.nodeStates),\n      sideEffectCount: sideEffects.length,\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    await tx.objectStore('checkpoints').put(checkpoint);\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string,\n    strategy: CompensationStrategy\n  ): Promise<RestoreResult> {\n    \n    const checkpoint = await this.getCheckpoint(checkpointId);\n    if (!checkpoint) {\n      return { success: false, error: 'Checkpoint not found' };\n    }\n    \n    // Get all side effects after checkpoint\n    const allEffects = await this.sideEffectLog.getEntriesForExecution(\n      checkpoint.executionId\n    );\n    \n    const effectsToCompensate = allEffects.filter(e => \n      e.timestamp > checkpoint.timestamp\n    );\n    \n    // Handle compensation based on strategy\n    const compensationResults = await this.applyCompensationStrategy(\n      effectsToCompensate,\n      strategy\n    );\n    \n    // Check for manual intervention required\n    const manualSteps = compensationResults\n      .filter(r => r.requiresUserAction)\n      .flatMap(r => r.manualSteps || []);\n    \n    return {\n      success: true,\n      checkpoint,\n      compensationResults,\n      manualSteps: manualSteps.length > 0 ? manualSteps : undefined\n    };\n  }\n  \n  private async applyCompensationStrategy(\n    effects: SideEffectEntry[],\n    strategy: CompensationStrategy\n  ): Promise<CompensationResult[]> {\n    \n    switch (strategy) {\n      case 'automatic':\n        // Compensate all reversible effects automatically\n        const results: CompensationResult[] = [];\n        \n        for (const effect of effects.reverse()) {\n          if (effect.compensation.type === 'reversible' || \n              effect.compensation.type === 'eventual') {\n            \n            const result = await this.sideEffectLog.compensate(effect.id);\n            results.push(result);\n            \n            if (!result.success) {\n              // Stop on first failure\n              break;\n            }\n          } else {\n            // Skip irreversible effects\n            results.push({\n              success: false,\n              error: 'Skipped irreversible effect',\n              requiresUserAction: true,\n              manualSteps: (effect.compensation as any).manualSteps\n            });\n          }\n        }\n        \n        return results;\n        \n      case 'manual':\n        // Return list of effects for user to handle manually\n        return effects.map(effect => ({\n          success: false,\n          requiresUserAction: true,\n          manualSteps: [\n            `Effect: ${effect.description}`,\n            `Type: ${effect.compensation.type}`,\n            ...(effect.compensation.type === 'irreversible' \n              ? (effect.compensation as any).manualSteps || []\n              : ['Manually revert this operation'])\n          ]\n        }));\n        \n      case 'confirm_each':\n        // This requires user interaction - not fully implemented here\n        // In practice, this would show a UI for each effect\n        return [];\n        \n      default:\n        throw new Error(`Unknown strategy: ${strategy}`);\n    }\n  }\n  \n  private async getCheckpoint(checkpointId: string): Promise<ExecutionCheckpoint | null> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    return await store.get(checkpointId);\n  }\n  \n  private serializeNodeStates(nodeStates: Map<string, NodeState>): any {\n    return Array.from(nodeStates.entries());\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowCheckpoints', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('checkpoints')) {\n          db.createObjectStore('checkpoints', { keyPath: 'id' });\n        }\n      };\n    });\n  }\n}\n\ntype CompensationStrategy = \n  | 'automatic'      // Auto-compensate all reversible effects\n  | 'manual'         // User handles all compensation\n  | 'confirm_each';  // Ask user for each effect\n\ninterface ExecutionCheckpoint {\n  id: string;\n  executionId: string;\n  timestamp: number;\n  nodeStates: any;\n  sideEffectCount: number;\n  reason: string;\n}\n\ninterface RestoreResult {\n  success: boolean;\n  error?: string;\n  checkpoint?: ExecutionCheckpoint;\n  compensationResults?: CompensationResult[];\n  manualSteps?: string[];\n}\n```\n\n## Performance Optimizations\n\n### Viewport Culling\n```typescript\nconst WorkflowCanvas: React.FC = () => {\n  const { nodes, edges } = useWorkflowStore();\n  const [viewport, setViewport] = useState<Viewport>({ x: 0, y: 0, zoom: 1 });\n  \n  const visibleNodes = useMemo(() => {\n    if (nodes.length < 50) return nodes;\n    \n    return nodes.filter(node => isNodeInViewport(node, viewport));\n  }, [nodes, viewport]);\n  \n  const visibleEdges = useMemo(() => {\n    const visibleNodeIds = new Set(visibleNodes.map(n => n.id));\n    return edges.filter(e => \n      visibleNodeIds.has(e.source) || visibleNodeIds.has(e.target)\n    );\n  }, [edges, visibleNodes]);\n  \n  return (\n    <ReactFlow\n      nodes={visibleNodes}\n      edges={visibleEdges}\n      onMove={(_, vp) => setViewport(vp)}\n    />\n  );\n};\n```\n\n### Virtual Scrolling\n```typescript\nimport { FixedSizeList } from 'react-window';\n\nconst NodePalette: React.FC = () => {\n  const nodeTypes = useNodeTypes();\n  \n  return (\n    <FixedSizeList\n      height={600}\n      itemCount={nodeTypes.length}\n      itemSize={60}\n      width=\"100%\"\n    >\n      {({ index, style }) => (\n        <div style={style}>\n          <NodePaletteItem node={nodeTypes[index]} />\n        </div>\n      )}\n    </FixedSizeList>\n  );\n};\n```\n\n### Debounced Auto-Save\n```typescript\nconst useAutoSave = (workflowId: string) => {\n  const { workflow } = useWorkflowStore();\n  const saveWorkflow = useSaveWorkflow();\n  \n  const debouncedSave = useMemo(\n    () => debounce(() => saveWorkflow(workflow), 2000),\n    [workflow, saveWorkflow]\n  );\n  \n  useEffect(() => {\n    debouncedSave();\n    return () => debouncedSave.cancel();\n  }, [workflow, debouncedSave]);\n};\n```\n\n## What I Improved\n\n### 1. **Dramatically Simplified Storage (3-Tier vs Complex Eviction)**\n- Removed complex hybrid eviction scoring system\n- **Three simple tiers**: Hot (50MB, tiny hot data) \u2192 Warm (500MB, default) \u2192 Cold (optional, huge files)\n- **Simple FIFO eviction** in hot cache, no complex LRU+size hybrid scoring\n- Most artifacts live in Warm (IndexedDB) which is fast enough\n- **Metadata index** keeps all artifact info lightweight and queryable\n- **Auto-cleanup** of old temporary data every hour\n\n### 2. **Streaming Decision Logic (Analyze First, Then Commit)**\n- Added `analyzeStreamingFeasibility()` to make intelligent streaming decisions\n- **Materialization strategy** for multi-consumer scenarios (store once, serve many)\n- **Streaming reasons** tracked for debugging and optimization\n- **Graceful degradation** to batch mode when streaming doesn't make sense\n- **Storage references** ($ref) for materialized streams instead of duplicating data\n\n### 3. **Pluggable Expression Engine (Progressive Enhancement)**\n- Expression engine is now an **interface** - swap JSONata for visual builder\n- **Validation with suggestions** helps users fix errors\n- **Visual builder option** for users who find text expressions hard\n- **Expression examples** built into the engine, not hard-coded in UI\n- **Syntax help** provided by the engine itself\n\n### 4. **Semantic Compensation (Descriptors Not Functions)**\n- Replaced JSONata compensation functions with **semantic descriptors**\n- **Four compensation types**: reversible, idempotent, irreversible, eventual\n- **Compensation handlers** registered per type, not per node\n- **Manual steps** included for irreversible operations (emails, payments)\n- **Append-only log** instead of mutable side effect records\n- **Audit trail** preserved forever for compliance\n\n### 5. **Compensation Strategy Selection**\n- Three strategies: **automatic** (safe auto-compensation), **manual** (user handles), **confirm_each** (interactive)\n- **Automatic stops on first failure** for safety\n- **Manual steps returned** to user for irreversible effects\n- **Partial compensation** supported (some effects can't be undone)\n\n## What I Kept\n\nAll the excellent foundation:\n- React Flow canvas\n- Zustand + immer state management\n- Worker pool architecture\n- Topological execution\n- Multiple handles per node\n- Input/output schemas\n- Retry configuration\n- Event system\n- Undo/redo\n- Edge validation\n- Resource limits\n- Node status granularity\n- Template system\n- Viewport culling optimization\n- Virtual scrolling for large lists\n- Debounced auto-save\n- Native Web Streams API for streaming\n- Compression for large data\n\n## Remaining Concerns\n\n### 1. **Cold Storage Implementation**\nThe current cold storage using blob URLs is browser-only and lost on refresh. For production:\n- Should we use **object storage** (S3, R2) for truly cold artifacts?\n- **Pre-signed URLs** for download instead of storing in browser?\n- **Lifecycle policies** to auto-delete cold storage after X days?\n\n### 2. **Expression Engine Complexity**\nJSONata is powerful but has a learning curve:\n- Should visual builder be **always available** as an alternative input method?\n- **AI-assisted expression generation**: \"convert this to an expression\"?\n- **Template expressions** library for common patterns?\n\n### 3. **Compensation Handler Registration**\nCurrently hard-coded handlers for 4 types:\n- Should users be able to **register custom handlers** for their services?\n- **Plugin system** for compensation handlers?\n- **Compensation middleware** for cross-cutting concerns (logging, notifications)?\n\n### 4. **Multi-Tab Coordination**\nNot addressed in this design:\n- **Lock workflows** during execution to prevent conflicts?\n- **Broadcast execution state** using BroadcastChannel API?\n- **Conflict resolution** when multiple tabs modify same workflow?\n\n### 5. **Cost Tracking**\nFor long-running workflows with many LLM calls:\n- **Pre-execution cost estimate** based on token limits per node?\n- **Running cost counter** updated as nodes complete?\n- **Budget limits** with auto-pause when approaching threshold?\n\n## Convergence Status\n\n**ITERATING** - This design significantly simplifies storage and compensation while adding critical missing pieces:\n\n\u2705 Simplified 3-tier storage (much easier to implement and reason about)  \n\u2705 Smart streaming decisions with materialization strategy  \n\u2705 Pluggable expression engine (text or visual)  \n\u2705 Semantic compensation with audit trail  \n\u2705 Compensation strategy selection  \n\u2705 All performance optimizations preserved  \n\nHowever, still needs refinement on:\n- Cold storage backend for production\n- Expression engine UX (visual builder, AI assist)\n- Custom compensation handler registration\n- Multi-tab coordination strategy\n\n---\n\n## PROMPT_FOR_ARCHITECT:\n\nPlease focus on these specific design questions:\n\n1. **Storage Backend**: Should cold storage use browser blob URLs (current design) or integrate with object storage (S3/R2) for production? What's the right trade-off between complexity and capability?\n\n2. **Expression Engine UX**: Should the visual builder be a separate mode (current design) or should we allow **mixed mode** where users can switch between text and visual for different parts of an expression?\n\n3. **Compensation Handler Extensibility**: Should we support **user-defined compensation handlers** for custom services, or is the 4-type system (reversible/idempotent/irreversible/eventual) sufficient for most use cases?\n\n4. **Materialization Heuristics**: When a stream has multiple consumers, the current design always materializes to storage. Should we support **stream teeing** for small streams and only materialize large ones? What's the size threshold?"
      }
    },
    {
      "round_number": 3,
      "agent_a": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v3\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Hybrid    \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Storage   \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Two-Tier \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  + Overflow\u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Strategy  \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with viewport culling\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with streaming + teeing for multi-consumer\n- **Storage**: Two-tier (Memory + IndexedDB) with configurable overflow strategies\n- **Expressions**: Composable expression system (JSONata blocks + visual builder)\n- **Side Effects**: Append-only log with pluggable compensation handlers\n- **Workers**: Shared Worker pool with resource quotas\n\n## Simplified Two-Tier Storage with Overflow Strategy\n\n### Key Insight: Let Users Choose Overflow Behavior, Not Complex Auto-Eviction\n\n```typescript\ntype OverflowStrategy = \n  | 'drop-oldest'           // FIFO eviction (default)\n  | 'drop-on-store'         // Fail loudly when full\n  | 'compress-aggressive'   // Compress everything aggressively\n  | 'external-blob';        // Use blob URLs for overflow (browser-only)\n\ninterface StorageConfig {\n  // Simple two-tier system\n  memoryCacheMaxBytes: number;      // 100MB - hot in-memory data\n  indexedDBMaxBytes: number;        // 1GB - warm IndexedDB storage\n  \n  // What happens when IndexedDB fills up?\n  overflowStrategy: OverflowStrategy;\n  \n  // Compression settings\n  compressionThreshold: number;     // 1MB - compress above this\n  compressionLevel: 'fast' | 'balanced' | 'best';\n  \n  // Auto-cleanup\n  autoCleanupAge: number;           // 24h - delete old temp artifacts\n  enableAutoCleanup: boolean;\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  private config: StorageConfig;\n  \n  // Lightweight metadata for all artifacts\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  // External blob URLs (only for 'external-blob' strategy)\n  private externalBlobs: Map<string, string> = new Map();\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initIndexedDB();\n    \n    if (config.enableAutoCleanup) {\n      this.scheduleCleanup();\n    }\n  }\n  \n  async store(key: string, data: any, hints: StorageHints = {}): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    // Update metadata\n    this.artifactIndex.set(key, {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId\n    });\n    \n    // Tier decision: Simple size-based\n    if (sizeBytes < 1024 * 1024) { // < 1MB -> try memory first\n      if (await this.tryStoreInMemory(key, data, sizeBytes)) {\n        this.artifactIndex.get(key)!.tier = 'memory';\n        return;\n      }\n    }\n    \n    // Otherwise use IndexedDB\n    await this.storeInIndexedDB(key, serialized, sizeBytes);\n    this.artifactIndex.get(key)!.tier = 'indexeddb';\n  }\n  \n  private async tryStoreInMemory(key: string, data: any, sizeBytes: number): Promise<boolean> {\n    if (this.memoryCacheSize + sizeBytes <= this.config.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    // Try to evict old memory entries\n    if (await this.evictFromMemory(sizeBytes)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(requiredBytes: number): Promise<boolean> {\n    const entries = Array.from(this.memoryCache.entries())\n      .sort((a, b) => a[1].storedAt - b[1].storedAt);\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const [key, entry] of entries) {\n      // Don't evict very recent entries (< 5s old)\n      if (Date.now() - entry.storedAt < 5000) continue;\n      \n      toEvict.push(key);\n      freedBytes += entry.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    // Move to IndexedDB (no data loss)\n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) metadata.tier = 'indexeddb';\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    // Check if we're at capacity\n    if (this.indexedDBSize + sizeBytes > this.config.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes);\n      return;\n    }\n    \n    // Compress if needed\n    const shouldCompress = sizeBytes > this.config.compressionThreshold;\n    const toStore = shouldCompress \n      ? await this.compress(serialized, this.config.compressionLevel)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore as string).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    switch (this.config.overflowStrategy) {\n      case 'drop-oldest':\n        // Evict oldest entries until we have space\n        await this.evictOldestFromIndexedDB(sizeBytes);\n        await this.storeInIndexedDB(key, serialized, sizeBytes);\n        break;\n        \n      case 'drop-on-store':\n        throw new Error(\n          `Storage full: Cannot store ${key} (${(sizeBytes / 1024 / 1024).toFixed(2)}MB). ` +\n          `IndexedDB usage: ${(this.indexedDBSize / 1024 / 1024).toFixed(2)}MB / ` +\n          `${(this.config.indexedDBMaxBytes / 1024 / 1024).toFixed(2)}MB`\n        );\n        \n      case 'compress-aggressive':\n        // Force best compression on this item\n        const compressed = await this.compress(serialized, 'best');\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > this.config.indexedDBMaxBytes) {\n          // Still doesn't fit - evict oldest\n          await this.evictOldestFromIndexedDB(compressedSize);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        break;\n        \n      case 'external-blob':\n        // Store as blob URL (browser-only, lost on refresh)\n        const compressed2 = await this.compress(serialized, 'best');\n        const blob = new Blob([compressed2], { type: 'application/octet-stream' });\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        // Store tiny reference in IndexedDB\n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) metadata.tier = 'external';\n        break;\n    }\n  }\n  \n  private async evictOldestFromIndexedDB(requiredBytes: number): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const index = store.index('storedAt');\n    \n    const entries: Array<{ key: string; sizeBytes: number }> = [];\n    let cursor = await index.openCursor();\n    \n    while (cursor) {\n      entries.push({\n        key: cursor.value.key,\n        sizeBytes: cursor.value.sizeBytes\n      });\n      cursor = await cursor.continue();\n    }\n    \n    // Sort by age\n    entries.sort((a, b) => {\n      const aTime = this.artifactIndex.get(a.key)?.createdAt ?? 0;\n      const bTime = this.artifactIndex.get(b.key)?.createdAt ?? 0;\n      return aTime - bTime;\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const entry of entries) {\n      toDelete.push(entry.key);\n      freedBytes += entry.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    // Delete entries\n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    // Update access tracking\n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    let data: any;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        data = this.memoryCache.get(key)?.data;\n        if (!data) {\n          // Fallback to IndexedDB\n          data = await this.retrieveFromIndexedDB(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        data = await this.retrieveFromIndexedDB(key);\n        \n        // Promote to memory if: small + accessed frequently\n        if (metadata.sizeBytes < 1024 * 1024 && \n            metadata.accessCount >= 3 &&\n            Date.now() - metadata.createdAt < 60000) {\n          \n          if (await this.tryStoreInMemory(key, data, metadata.sizeBytes)) {\n            metadata.tier = 'memory';\n          }\n        }\n        break;\n        \n      case 'external':\n        data = await this.retrieveFromExternalBlob(key);\n        break;\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n    \n    return data;\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async clear(predicate?: (metadata: ArtifactMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      // Clear everything\n      this.memoryCache.clear();\n      this.memoryCacheSize = 0;\n      this.indexedDBSize = 0;\n      this.artifactIndex.clear();\n      \n      const tx = this.indexedDB.transaction(['artifacts', 'blobRefs'], 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      await tx.objectStore('blobRefs').clear();\n      \n      // Revoke blob URLs\n      for (const blobUrl of this.externalBlobs.values()) {\n        URL.revokeObjectURL(blobUrl);\n      }\n      this.externalBlobs.clear();\n      \n      return;\n    }\n    \n    // Selective clear\n    const toDelete: string[] = [];\n    \n    for (const [key, metadata] of this.artifactIndex.entries()) {\n      if (predicate(metadata)) {\n        toDelete.push(key);\n      }\n    }\n    \n    for (const key of toDelete) {\n      await this.delete(key);\n    }\n  }\n  \n  private async delete(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) return;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        const entry = this.memoryCache.get(key);\n        if (entry) {\n          this.memoryCacheSize -= entry.sizeBytes;\n          this.memoryCache.delete(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        this.indexedDBSize -= metadata.sizeBytes;\n        break;\n        \n      case 'external':\n        const blobUrl = this.externalBlobs.get(key);\n        if (blobUrl) {\n          URL.revokeObjectURL(blobUrl);\n          this.externalBlobs.delete(key);\n        }\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').delete(key);\n        break;\n    }\n    \n    this.artifactIndex.delete(key);\n  }\n  \n  private scheduleCleanup(): void {\n    setInterval(() => {\n      const cutoff = Date.now() - this.config.autoCleanupAge;\n      \n      this.clear(metadata => \n        metadata.temporary && metadata.createdAt < cutoff\n      );\n    }, 60 * 60 * 1000); // 1 hour\n  }\n  \n  getStorageStats(): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      stats[metadata.tier as 'memory' | 'indexeddb' | 'external'].count++;\n      if (metadata.tier === 'external') {\n        stats.external.bytes += metadata.sizeBytes;\n      }\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n  \n  private async compress(data: string, level: 'fast' | 'balanced' | 'best'): Promise<string> {\n    // Use native compression (gzip)\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    // Convert to base64 for storage\n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    // Decode from base64\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 3);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n}\n\n// Types\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'external' | 'determining';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n```\n\n### Usage Example\n```typescript\nconst storage = new HybridStorageManager({\n  memoryCacheMaxBytes: 100 * 1024 * 1024,      // 100MB\n  indexedDBMaxBytes: 1024 * 1024 * 1024,       // 1GB\n  overflowStrategy: 'drop-oldest',             // Default: FIFO eviction\n  compressionThreshold: 1024 * 1024,           // 1MB\n  compressionLevel: 'balanced',\n  autoCleanupAge: 24 * 60 * 60 * 1000,        // 24h\n  enableAutoCleanup: true\n});\n\n// Store with automatic tiering\nawait storage.store(\n  `node-${nodeId}-output`,\n  largeDataset,\n  {\n    nodeId,\n    executionId,\n    temporary: true\n  }\n);\n\n// Retrieve (transparent tier access)\nconst data = await storage.retrieve(`node-${nodeId}-output`);\n\n// Cleanup after workflow\nawait storage.clear(meta => \n  meta.temporary && meta.executionId === executionId\n);\n\n// Check storage\nconst stats = storage.getStorageStats();\nconsole.log(`Memory: ${(stats.memory.bytes / 1024 / 1024).toFixed(2)}MB`);\nconsole.log(`IndexedDB: ${(stats.indexeddb.bytes / 1024 / 1024).toFixed(2)}MB`);\n```\n\n## Adaptive Streaming with Smart Teeing\n\n### Key Insight: Tee Small Streams, Materialize Large Ones\n\n```typescript\ninterface StreamingConfig {\n  teeThresholdBytes: number;        // 5MB - tee if smaller\n  materializeThresholdBytes: number; // 50MB - always materialize if larger\n  enableAdaptiveTee: boolean;        // Use size estimation for decision\n}\n\nclass AdaptiveExecutor {\n  private streamingExecutor: StreamingExecutor;\n  private batchExecutor: BatchExecutor;\n  private storage: HybridStorageManager;\n  private config: StreamingConfig;\n  \n  constructor(storage: HybridStorageManager, config: StreamingConfig) {\n    this.storage = storage;\n    this.config = config;\n    this.streamingExecutor = new StreamingExecutor();\n    this.batchExecutor = new BatchExecutor();\n  }\n  \n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): Promise<ExecutionResult> {\n    \n    // Analyze streaming feasibility\n    const decision = this.analyzeStreamingFeasibility(node, inputs, edges);\n    \n    if (decision.shouldStream) {\n      return this.executeWithStreaming(node, inputs, decision);\n    } else {\n      return this.executeWithBatch(node, inputs);\n    }\n  }\n  \n  private analyzeStreamingFeasibility(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): StreamingDecision {\n    \n    const reasons: string[] = [];\n    let shouldStream = false;\n    let multiConsumerStrategy: MultiConsumerStrategy = 'none';\n    \n    // 1. Explicit node config\n    if (node.data.config.streaming === false) {\n      return {\n        shouldStream: false,\n        reasons: ['explicit_batch_mode'],\n        multiConsumerStrategy: 'none'\n      };\n    }\n    \n    if (node.data.config.streaming === true) {\n      shouldStream = true;\n      reasons.push('explicit_streaming_mode');\n    }\n    \n    // 2. Stream input detection\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream\n    );\n    \n    if (hasStreamInput) {\n      shouldStream = true;\n      reasons.push('stream_input_detected');\n    }\n    \n    // 3. Large data heuristic\n    const estimatedSize = this.estimateDataSize(inputs);\n    if (estimatedSize > 5 * 1024 * 1024) {\n      shouldStream = true;\n      reasons.push('large_input_heuristic');\n    }\n    \n    // 4. Agent type benefits\n    if (['researcher', 'coder', 'reviewer'].includes(node.data.type)) {\n      shouldStream = true;\n      reasons.push('agent_benefits_from_streaming');\n    }\n    \n    // 5. Multi-consumer analysis\n    if (shouldStream) {\n      const downstreamCount = edges.filter(e => e.source === node.id).length;\n      \n      if (downstreamCount > 1) {\n        // Decide between teeing and materialization\n        multiConsumerStrategy = this.selectMultiConsumerStrategy(\n          estimatedSize,\n          downstreamCount\n        );\n        \n        reasons.push(`multi_consumer_${multiConsumerStrategy}`);\n      }\n    }\n    \n    return {\n      shouldStream,\n      reasons,\n      multiConsumerStrategy\n    };\n  }\n  \n  private selectMultiConsumerStrategy(\n    estimatedSize: number,\n    consumerCount: number\n  ): MultiConsumerStrategy {\n    \n    if (!this.config.enableAdaptiveTee) {\n      // Always materialize if adaptive disabled\n      return 'materialize';\n    }\n    \n    // Large streams: always materialize (avoid memory pressure)\n    if (estimatedSize > this.config.materializeThresholdBytes) {\n      return 'materialize';\n    }\n    \n    // Small streams: tee is efficient\n    if (estimatedSize < this.config.teeThresholdBytes) {\n      return consumerCount <= 3 ? 'tee' : 'materialize';\n    }\n    \n    // Medium streams: depends on consumer count\n    // More consumers = more memory copies = prefer materialize\n    if (consumerCount <= 2) {\n      return 'tee';\n    } else {\n      return 'materialize';\n    }\n  }\n  \n  private async executeWithStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const outputStream = await this.streamingExecutor.executeNodeStreaming(\n      node,\n      inputs\n    );\n    \n    // Handle multi-consumer scenario\n    if (decision.multiConsumerStrategy === 'materialize') {\n      return await this.materializeStreamToStorage(node, outputStream, decision);\n    } else if (decision.multiConsumerStrategy === 'tee') {\n      return await this.teeStreamForConsumers(node, outputStream, decision);\n    }\n    \n    // Single consumer: pass stream through\n    return {\n      nodeId: node.id,\n      output: outputStream,\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'none'\n      }\n    };\n  }\n  \n  private async teeStreamForConsumers(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    // Create teed streams (browser supports tee() natively)\n    const [stream1, stream2] = stream.tee();\n    \n    // Store references to both streams\n    const teeKey = `node-${node.id}-tee`;\n    \n    // Store tee metadata (not the streams themselves)\n    await this.storage.store(\n      teeKey,\n      {\n        type: 'teed_streams',\n        nodeId: node.id,\n        // We can't actually store streams, so we'll need to re-tee on demand\n        // This is a limitation - we need to handle this at execution level\n      },\n      {\n        nodeId: node.id,\n        temporary: true\n      }\n    );\n    \n    // Return special marker indicating teed output\n    return {\n      nodeId: node.id,\n      output: {\n        $teed: teeKey,\n        streams: [stream1, stream2]  // Store in execution context, not storage\n      },\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'tee'\n      }\n    };\n  }\n  \n  private async materializeStreamToStorage(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    // Read entire stream\n    const chunks: any[] = [];\n    const reader = stream.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        chunks.push(value);\n      }\n    } finally {\n      reader.releaseLock();\n    }\n    \n    // Combine chunks\n    const materialized = this.combineChunks(chunks);\n    \n    // Store in hybrid storage\n    const storageKey = `node-${node.id}-materialized`;\n    await this.storage.store(storageKey, materialized, {\n      nodeId: node.id,\n      temporary: true\n    });\n    \n    // Return storage reference\n    return {\n      nodeId: node.id,\n      output: { $ref: storageKey },\n      streaming: false,\n      metadata: {\n        materialized: true,\n        storageKey,\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'materialize'\n      }\n    };\n  }\n  \n  private async executeWithBatch(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    // Resolve storage references and teed streams\n    const resolvedInputs = await this.resolveInputs(inputs);\n    \n    // Execute\n    const output = await this.batchExecutor.executeNodeBatch(node, resolvedInputs);\n    \n    return {\n      nodeId: node.id,\n      output,\n      streaming: false,\n      metadata: {}\n    };\n  }\n  \n  private async resolveInputs(inputs: Record<string, any>): Promise<Record<string, any>> {\n    const resolved: Record<string, any> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      if (value && typeof value === 'object') {\n        if (value.$ref) {\n          // Storage reference\n          resolved[key] = await this.storage.retrieve(value.$ref);\n        } else if (value.$teed) {\n          // Teed stream reference - get one of the streams\n          // This is simplified - real implementation needs stream tracking\n          resolved[key] = value.streams[0];\n        } else {\n          resolved[key] = value;\n        }\n      } else {\n        resolved[key] = value;\n      }\n    }\n    \n    return resolved;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private combineChunks(chunks: any[]): any {\n    if (chunks.length === 0) return null;\n    \n    if (typeof chunks[0] === 'string') {\n      return chunks.join('');\n    }\n    \n    if (Array.isArray(chunks[0])) {\n      return chunks.flat();\n    }\n    \n    return chunks[chunks.length - 1];\n  }\n}\n\ntype MultiConsumerStrategy = 'none' | 'tee' | 'materialize';\n\ninterface StreamingDecision {\n  shouldStream: boolean;\n  reasons: string[];\n  multiConsumerStrategy: MultiConsumerStrategy;\n}\n\ninterface ExecutionResult {\n  nodeId: string;\n  output: any | ReadableStream | { $ref: string } | { $teed: string; streams: ReadableStream[] };\n  streaming: boolean;\n  metadata: Record<string, any>;\n}\n\ninterface WorkflowNode {\n  id: string;\n  data: {\n    type: string;\n    config: {\n      streaming?: boolean;\n      [key: string]: any;\n    };\n  };\n}\n\ninterface WorkflowEdge {\n  source: string;\n  target: string;\n}\n```\n\n## Composable Expression System\n\n### Key Insight: Mix and Match Text and Visual, Don't Force One Mode\n\n```typescript\ninterface ExpressionEngine {\n  evaluate(expression: Expression, context: any): Promise<any>;\n  validate(expression: Expression): ValidationResult;\n  getExamples(): ExpressionExample[];\n}\n\n// Expression can be pure text, pure blocks, or mixed\ntype Expression = \n  | { mode: 'text'; value: string }\n  | { mode: 'visual'; blocks: ExpressionBlock[] }\n  | { mode: 'composite'; parts: ExpressionPart[] };\n\ntype ExpressionPart =\n  | { type: 'text'; value: string }\n  | { type: 'block'; block: ExpressionBlock };\n\ninterface ExpressionBlock {\n  id: string;\n  type: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n}\n\nclass ComposableExpressionEngine implements ExpressionEngine {\n  private jsonataEngine: JSONataEngine;\n  \n  constructor() {\n    this.jsonataEngine = new JSONataEngine();\n  }\n  \n  async evaluate(expression: Expression, context: any): Promise<any> {\n    switch (expression.mode) {\n      case 'text':\n        return this.jsonataEngine.evaluate(expression.value, context);\n        \n      case 'visual':\n        const jsonataFromBlocks = this.blocksToJSONata(expression.blocks);\n        return this.jsonataEngine.evaluate(jsonataFromBlocks, context);\n        \n      case 'composite':\n        const jsonataFromComposite = this.compositeToJSONata(expression.parts);\n        return this.jsonataEngine.evaluate(jsonataFromComposite, context);\n        \n      default:\n        throw new Error(`Unknown expression mode: ${(expression as any).mode}`);\n    }\n  }\n  \n  validate(expression: Expression): ValidationResult {\n    try {\n      let jsonataExpr: string;\n      \n      switch (expression.mode) {\n        case 'text':\n          jsonataExpr = expression.value;\n          break;\n        case 'visual':\n          jsonataExpr = this.blocksToJSONata(expression.blocks);\n          break;\n        case 'composite':\n          jsonataExpr = this.compositeToJSONata(expression.parts);\n          break;\n        default:\n          return { valid: false, error: 'Unknown expression mode' };\n      }\n      \n      return this.jsonataEngine.validate(jsonataExpr);\n      \n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check expression syntax'\n      };\n    }\n  }\n  \n  private blocksToJSONata(blocks: ExpressionBlock[]): string {\n    return blocks.map(block => {\n      switch (block.type) {\n        case 'field':\n          return `$.${block.config.path}`;\n          \n        case 'filter':\n          return `[${block.config.condition}]`;\n          \n        case 'map':\n          return `.{ ${block.config.mapping} }`;\n          \n        case 'aggregate':\n          return `$${block.config.function}(${block.config.path})`;\n          \n        case 'transform':\n          return block.config.expression;\n          \n        default:\n          throw new Error(`Unknown block type: ${block.type}`);\n      }\n    }).join('');\n  }\n  \n  private compositeToJSONata(parts: ExpressionPart[]): string {\n    return parts.map(part => {\n      if (part.type === 'text') {\n        return part.value;\n      } else {\n        return this.blockToJSONata(part.block);\n      }\n    }).join('');\n  }\n  \n  private blockToJSONata(block: ExpressionBlock): string {\n    switch (block.type) {\n      case 'field':\n        return `$.${block.config.path}`;\n      case 'filter':\n        return `[${block.config.condition}]`;\n      case 'map':\n        return `.{ ${block.config.mapping} }`;\n      case 'aggregate':\n        return `$${block.config.function}(${block.config.path})`;\n      case 'transform':\n        return block.config.expression;\n      default:\n        throw new Error(`Unknown block type: ${block.type}`);\n    }\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return [\n      {\n        category: 'Simple Field',\n        expression: { mode: 'text', value: '$.user.name' },\n        description: 'Extract nested field',\n        inputExample: { user: { name: 'Alice' } },\n        outputExample: 'Alice'\n      },\n      {\n        category: 'Visual Filter',\n        expression: {\n          mode: 'visual',\n          blocks: [\n            { id: '1', type: 'field', config: { path: 'items' } },\n            { id: '2', type: 'filter', config: { condition: 'price > 100' } }\n          ]\n        },\n        description: 'Filter using visual blocks',\n        inputExample: { items: [{ price: 50 }, { price: 150 }] },\n        outputExample: [{ price: 150 }]\n      },\n      {\n        category: 'Composite',\n        expression: {\n          mode: 'composite',\n          parts: [\n            { type: 'block', block: { id: '1', type: 'field', config: { path: 'items' } } },\n            { type: 'text', value: '[price > 100]' },\n            { type: 'block', block: { id: '2', type: 'aggregate', config: { function: 'sum', path: 'price' } } }\n          ]\n        },\n        description: 'Mix visual blocks with text',\n        inputExample: { items: [{ price: 50 }, { price: 150 }, { price: 200 }] },\n        outputExample: 350\n      }\n    ];\n  }\n}\n\nclass JSONataEngine {\n  private cache: Map<string, any> = new Map();\n  private readonly timeout = 5000;\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      // Use jsonata library (would need to import)\n      // compiled = jsonata(expression);\n      \n      // For now, simplified evaluation\n      // In real implementation, use actual JSONata library\n      compiled = { evaluate: (ctx: any) => this.simpleEval(expression, ctx) };\n      \n      this.cache.set(expression, compiled);\n    }\n    \n    return await Promise.race([\n      Promise.resolve(compiled.evaluate(context)),\n      new Promise((_, reject) => \n        setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n      )\n    ]);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      // In real implementation, use JSONata parser\n      // For now, basic validation\n      if (!expression || expression.trim().length === 0) {\n        return { valid: false, error: 'Empty expression' };\n      }\n      \n      return { valid: true };\n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check JSONata syntax'\n      };\n    }\n  }\n  \n  private simpleEval(expression: string, context: any): any {\n    // Simplified evaluator - real implementation would use JSONata\n    if (expression.startsWith('$.')) {\n      const path = expression.substring(2).split('.');\n      let result = context;\n      for (const key of path) {\n        result = result?.[key];\n      }\n      return result;\n    }\n    \n    return null;\n  }\n}\n\n// UI Component for Composable Expressions\nconst ComposableExpressionEditor: React.FC<{\n  value: Expression;\n  onChange: (value: Expression) => void;\n}> = ({ value, onChange }) => {\n  \n  const [mode, setMode] = useState<'text' | 'visual' | 'composite'>(value.mode);\n  const engine = useMemo(() => new ComposableExpressionEngine(), []);\n  const [validation, setValidation] = useState<ValidationResult>({ valid: true });\n  \n  useEffect(() => {\n    setValidation(engine.validate(value));\n  }, [value, engine]);\n  \n  const switchMode = (newMode: 'text' | 'visual' | 'composite') => {\n    if (newMode === value.mode) return;\n    \n    // Convert between modes\n    let newValue: Expression;\n    \n    if (newMode === 'text') {\n      // Convert to text (simplified)\n      newValue = { mode: 'text', value: '' };\n    } else if (newMode === 'visual') {\n      newValue = { mode: 'visual', blocks: [] };\n    } else {\n      newValue = { mode: 'composite', parts: [] };\n    }\n    \n    setMode(newMode);\n    onChange(newValue);\n  };\n  \n  return (\n    <div className=\"expression-editor\">\n      <div className=\"mode-selector\">\n        <button \n          onClick={() => switchMode('text')}\n          className={mode === 'text' ? 'active' : ''}\n        >\n          Text\n        </button>\n        <button \n          onClick={() => switchMode('visual')}\n          className={mode === 'visual' ? 'active' : ''}\n        >\n          Visual\n        </button>\n        <button \n          onClick={() => switchMode('composite')}\n          className={mode === 'composite' ? 'active' : ''}\n        >\n          Mixed\n        </button>\n      </div>\n      \n      {mode === 'text' && value.mode === 'text' && (\n        <textarea\n          value={value.value}\n          onChange={e => onChange({ mode: 'text', value: e.target.value })}\n          className={validation.valid ? '' : 'error'}\n          placeholder=\"Enter JSONata expression: $.field.path\"\n        />\n      )}\n      \n      {mode === 'visual' && value.mode === 'visual' && (\n        <VisualBlockEditor\n          blocks={value.blocks}\n          onChange={blocks => onChange({ mode: 'visual', blocks })}\n        />\n      )}\n      \n      {mode === 'composite' && value.mode === 'composite' && (\n        <CompositeEditor\n          parts={value.parts}\n          onChange={parts => onChange({ mode: 'composite', parts })}\n        />\n      )}\n      \n      {!validation.valid && (\n        <div className=\"error-message\">\n          {validation.error}\n          {validation.suggestion && <div className=\"suggestion\">{validation.suggestion}</div>}\n        </div>\n      )}\n      \n      <ExpressionExamples examples={engine.getExamples()} />\n    </div>\n  );\n};\n\n// Stub components (would be implemented fully)\nconst VisualBlockEditor: React.FC<{\n  blocks: ExpressionBlock[];\n  onChange: (blocks: ExpressionBlock[]) => void;\n}> = ({ blocks, onChange }) => {\n  return <div>Visual block editor (drag-drop blocks)</div>;\n};\n\nconst CompositeEditor: React.FC<{\n  parts: ExpressionPart[];\n  onChange: (parts: ExpressionPart[]) => void;\n}> = ({ parts, onChange }) => {\n  return <div>Composite editor (mix text and blocks)</div>;\n};\n\nconst ExpressionExamples: React.FC<{\n  examples: ExpressionExample[];\n}> = ({ examples }) => {\n  return (\n    <div className=\"examples\">\n      <h4>Examples</h4>\n      {examples.map((ex, i) => (\n        <div key={i} className=\"example\">\n          <strong>{ex.category}:</strong> {ex.description}\n        </div>\n      ))}\n    </div>\n  );\n};\n\ninterface ValidationResult {\n  valid: boolean;\n  error?: string;\n  suggestion?: string;\n}\n\ninterface ExpressionExample {\n  category: string;\n  expression: Expression;\n  description: string;\n  inputExample: any;\n  outputExample: any;\n}\n```\n\n## Pluggable Compensation Handler System\n\n### Key Insight: Let Users Register Custom Handlers for Their Services\n\n```typescript\ninterface CompensationHandler {\n  name: string;\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n  canHandle(entry: SideEffectEntry): boolean;\n}\n\nclass CompensationHandlerRegistry {\n  private handlers: Map<string, CompensationHandler> = new Map();\n  \n  register(handler: CompensationHandler): void {\n    this.handlers.set(handler.name, handler);\n  }\n  \n  unregister(name: string): void {\n    this.handlers.delete(name);\n  }\n  \n  findHandler(entry: SideEffectEntry): CompensationHandler | null {\n    for (const handler of this.handlers.values()) {\n      if (handler.canHandle(entry)) {\n        return handler;\n      }\n    }\n    return null;\n  }\n  \n  getAll(): CompensationHandler[] {\n    return Array.from(this.handlers.values());\n  }\n}\n\n// Enhanced side effect log with pluggable handlers\nclass SideEffectLog {\n  private db: IDBDatabase;\n  private handlerRegistry: CompensationHandlerRegistry;\n  \n  constructor() {\n    this.initDB();\n    this.handlerRegistry = new CompensationHandlerRegistry();\n    this.registerBuiltInHandlers();\n  }\n  \n  async append(entry: Omit<SideEffectEntry, 'id' | 'timestamp' | 'status'>): Promise<string> {\n    const id = crypto.randomUUID();\n    \n    const effectEntry: SideEffectEntry = {\n      ...entry,\n      id,\n      timestamp: Date.now(),\n      status: 'pending'\n    };\n    \n    await this.writeToLog(effectEntry);\n    return id;\n  }\n  \n  async markExecuted(effectId: string): Promise<void> {\n    await this.writeToLog({\n      id: crypto.randomUUID(),\n      nodeId: 'system',\n      executionId: 'system',\n      timestamp: Date.now(),\n      operation: 'api_call',\n      description: `Effect ${effectId} executed`,\n      compensation: { type: 'irreversible', reason: 'audit entry' },\n      status: 'executed',\n      executedAt: Date.now()\n    });\n  }\n  \n  async compensate(effectId: string): Promise<CompensationResult> {\n    const entry = await this.getEntry(effectId);\n    \n    if (!entry) {\n      return { success: false, error: 'Effect not found' };\n    }\n    \n    if (entry.status !== 'executed') {\n      return { success: false, error: 'Effect not in executed state' };\n    }\n    \n    // Find appropriate handler\n    const handler = this.handlerRegistry.findHandler(entry);\n    \n    if (!handler) {\n      return { \n        success: false, \n        error: `No compensation handler found for: ${entry.description}`,\n        requiresUserAction: true,\n        manualSteps: [\n          'No automatic compensation available',\n          `Operation: ${entry.description}`,\n          'Please manually revert this change'\n        ]\n      };\n    }\n    \n    try {\n      const result = await handler.compensate(entry);\n      \n      // Log compensation\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensated: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'compensated',\n        compensatedAt: Date.now()\n      });\n      \n      return result;\n      \n    } catch (error: any) {\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensation failed: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'failed',\n        error: error.message\n      });\n      \n      return { success: false, error: error.message };\n    }\n  }\n  \n  registerHandler(handler: CompensationHandler): void {\n    this.handlerRegistry.register(handler);\n  }\n  \n  unregisterHandler(name: string): void {\n    this.handlerRegistry.unregister(name);\n  }\n  \n  private registerBuiltInHandlers(): void {\n    // Reversible operations\n    this.handlerRegistry.register({\n      name: 'reversible',\n      canHandle: (entry) => entry.compensation.type === 'reversible',\n      compensate: async (entry) => {\n        const descriptor = (entry.compensation as any).reverseOperation;\n        \n        const response = await fetch(descriptor.endpoint, {\n          method: descriptor.method,\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.parameters)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Reverse operation failed: ${response.statusText}`);\n        }\n        \n        return { success: true, message: 'Operation reversed successfully' };\n      }\n    });\n    \n    // Idempotent operations\n    this.handlerRegistry.register({\n      name: 'idempotent',\n      canHandle: (entry) => entry.compensation.type === 'idempotent',\n      compensate: async (entry) => {\n        return { success: true, message: 'Idempotent operation - no compensation needed' };\n      }\n    });\n    \n    // Irreversible operations\n    this.handlerRegistry.register({\n      name: 'irreversible',\n      canHandle: (entry) => entry.compensation.type === 'irreversible',\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        return {\n          success: false,\n          error: `Cannot auto-compensate: ${descriptor.reason}`,\n          manualSteps: descriptor.manualSteps || ['Manual intervention required'],\n          requiresUserAction: true\n        };\n      }\n    });\n    \n    // Eventual consistency\n    this.handlerRegistry.register({\n      name: 'eventual',\n      canHandle: (entry) => entry.compensation.type === 'eventual',\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        const response = await fetch(descriptor.compensatingService, {\n          method: 'POST',\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.payload)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Compensating service failed: ${response.statusText}`);\n        }\n        \n        return { \n          success: true, \n          message: 'Compensating transaction initiated' \n        };\n      }\n    });\n  }\n  \n  private async writeToLog(entry: SideEffectEntry): Promise<void> {\n    const tx = this.db.transaction('effects', 'readwrite');\n    const store = tx.objectStore('effects');\n    await store.add(entry);\n  }\n  \n  private async getEntry(effectId: string): Promise<SideEffectEntry | null> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    return await store.get(effectId);\n  }\n  \n  async getEntriesForExecution(executionId: string): Promise<SideEffectEntry[]> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    const index = store.index('executionId');\n    \n    const entries: SideEffectEntry[] = [];\n    let cursor = await index.openCursor(IDBKeyRange.only(executionId));\n    \n    while (cursor) {\n      entries.push(cursor.value);\n      cursor = await cursor.continue();\n    }\n    \n    return entries;\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('SideEffectLog', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        const store = db.createObjectStore('effects', { keyPath: 'id' });\n        store.createIndex('nodeId', 'nodeId', { unique: false });\n        store.createIndex('executionId', 'executionId', { unique: false });\n        store.createIndex('timestamp', 'timestamp', { unique: false });\n      };\n    });\n  }\n}\n\n// Example: Custom compensation handler for Stripe\nclass StripeCompensationHandler implements CompensationHandler {\n  name = 'stripe';\n  \n  canHandle(entry: SideEffectEntry): boolean {\n    return entry.operation === 'payment_process' && \n           entry.description.includes('Stripe');\n  }\n  \n  async compensate(entry: SideEffectEntry): Promise<CompensationResult> {\n    const descriptor = entry.compensation as any;\n    \n    try {\n      // Call Stripe refund API\n      const response = await fetch('https://api.stripe.com/v1/refunds', {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${descriptor.apiKey}`,\n          'Content-Type': 'application/x-www-form-urlencoded'\n        },\n        body: new URLSearchParams({\n          charge: descriptor.chargeId,\n          reason: 'requested_by_customer'\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Stripe refund failed: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Refunded charge ${descriptor.chargeId}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message,\n        requiresUserAction: true,\n        manualSteps: [\n          'Go to Stripe dashboard',\n          `Find charge: ${descriptor.chargeId}`,\n          'Issue manual refund'\n        ]\n      };\n    }\n  }\n}\n\n// Usage\nconst sideEffectLog = new SideEffectLog();\n\n// Register custom handler\nsideEffectLog.registerHandler(new StripeCompensationHandler());\n\n// Log Stripe payment\nawait sideEffectLog.append({\n  nodeId: 'checkout-node',\n  executionId: 'exec-123',\n  operation: 'payment_process',\n  description: 'Stripe payment for order #456',\n  compensation: {\n    type: 'reversible',\n    reverseOperation: {\n      service: 'stripe',\n      method: 'POST',\n      endpoint: '/v1/refunds',\n      parameters: {\n        chargeId: 'ch_abc123',\n        apiKey: 'sk_test_...'\n      }\n    }\n  }\n});\n\n// Types\ninterface SideEffectEntry {\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  operation: OperationType;\n  description: string;\n  compensation: CompensationDescriptor;\n  status: 'pending' | 'executed' | 'failed' | 'compensated';\n  executedAt?: number;\n  compensatedAt?: number;\n  error?: string;\n}\n\ntype OperationType = \n  | 'api_call'\n  | 'file_write'\n  | 'database_write'\n  | 'external_webhook'\n  | 'email_send'\n  | 'payment_process';\n\ntype CompensationDescriptor = \n  | { type: 'reversible'; reverseOperation: OperationDescriptor }\n  | { type: 'idempotent'; canRetry: boolean }\n  | { type: 'irreversible'; reason: string; manualSteps?: string[] }\n  | { type: 'eventual'; compensatingService: string; payload: any };\n\ninterface OperationDescriptor {\n  service: string;\n  method: string;\n  endpoint?: string;\n  parameters: Record<string, any>;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n## Checkpoint Manager (Enhanced)\n\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  private sideEffectLog: SideEffectLog;\n  \n  constructor(sideEffectLog: SideEffectLog) {\n    this.sideEffectLog = sideEffectLog;\n    this.initDB();\n  }\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    const sideEffects = await this.sideEffectLog.getEntriesForExecution(\n      executionState.executionId\n    );\n    \n    const checkpoint: ExecutionCheckpoint = {\n      id: checkpointId,\n      executionId: executionState.executionId,\n      timestamp: Date.now(),\n      nodeStates: this.serializeNodeStates(executionState.nodeStates),\n      sideEffectCount: sideEffects.length,\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    await tx.objectStore('checkpoints').put(checkpoint);\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string,\n    strategy: CompensationStrategy\n  ): Promise<RestoreResult> {\n    \n    const checkpoint = await this.getCheckpoint(checkpointId);\n    if (!checkpoint) {\n      return { success: false, error: 'Checkpoint not found' };\n    }\n    \n    const allEffects = await this.sideEffectLog.getEntriesForExecution(\n      checkpoint.executionId\n    );\n    \n    const effectsToCompensate = allEffects.filter(e => \n      e.timestamp > checkpoint.timestamp && e.status === 'executed'\n    );\n    \n    const compensationResults = await this.applyCompensationStrategy(\n      effectsToCompensate,\n      strategy\n    );\n    \n    const manualSteps = compensationResults\n      .filter(r => r.requiresUserAction)\n      .flatMap(r => r.manualSteps || []);\n    \n    return {\n      success: true,\n      checkpoint,\n      compensationResults,\n      manualSteps: manualSteps.length > 0 ? manualSteps : undefined\n    };\n  }\n  \n  private async applyCompensationStrategy(\n    effects: SideEffectEntry[],\n    strategy: CompensationStrategy\n  ): Promise<CompensationResult[]> {\n    \n    switch (strategy) {\n      case 'automatic':\n        const results: CompensationResult[] = [];\n        \n        for (const effect of effects.reverse()) {\n          const result = await this.sideEffectLog.compensate(effect.id);\n          results.push(result);\n          \n          if (!result.success && !result.requiresUserAction) {\n            break; // Stop on failure\n          }\n        }\n        \n        return results;\n        \n      case 'manual':\n        return effects.map(effect => ({\n          success: false,\n          requiresUserAction: true,\n          manualSteps: [\n            `Effect: ${effect.description}`,\n            `Type: ${effect.compensation.type}`,\n            'Manually revert this operation'\n          ]\n        }));\n        \n      case 'confirm_each':\n        // Would require UI interaction\n        return [];\n        \n      default:\n        throw new Error(`Unknown strategy: ${strategy}`);\n    }\n  }\n  \n  private async getCheckpoint(checkpointId: string): Promise<ExecutionCheckpoint | null> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    return await store.get(checkpointId);\n  }\n  \n  private serializeNodeStates(nodeStates: Map<string, any>): any {\n    return Array.from(nodeStates.entries());\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowCheckpoints', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('checkpoints')) {\n          db.createObjectStore('checkpoints', { keyPath: 'id' });\n        }\n      };\n    });\n  }\n}\n\ntype CompensationStrategy = \n  | 'automatic'\n  | 'manual'\n  | 'confirm_each';\n\ninterface ExecutionState {\n  executionId: string;\n  nodeStates: Map<string, any>;\n}\n\ninterface ExecutionCheckpoint {\n  id: string;\n  executionId: string;\n  timestamp: number;\n  nodeStates: any;\n  sideEffectCount: number;\n  reason: string;\n}\n\ninterface RestoreResult {\n  success: boolean;\n  error?: string;\n  checkpoint?: ExecutionCheckpoint;\n  compensationResults?: CompensationResult[];\n  manualSteps?: string[];\n}\n```\n\n## Performance Optimizations (Preserved)\n\nAll performance optimizations from previous design are maintained:\n- Viewport culling for large canvases\n- Virtual scrolling for node palette\n- Debounced auto-save\n- Worker pool for execution\n- Lazy loading of node data\n\n---\n\n## Rationale\n\n### 1. **Two-Tier Storage with Overflow Strategy**\nThe three-tier approach adds complexity that most users don't need. Instead:\n- **Two tiers** (Memory + IndexedDB) are simpler and sufficient for 99% of workflows\n- **Overflow strategy** gives users explicit control over what happens when storage fills up\n- **drop-oldest**: Safe default (FIFO eviction)\n- **drop-on-store**: Fail loudly for debugging\n- **compress-aggressive**: Trade CPU for space\n- **external-blob**: Browser-only fallback for huge files\n\nThis is simpler to reason about and lets users pick the behavior that matches their needs.\n\n### 2. **Adaptive Teeing for Multi-Consumer Streams**\nThe previous design always materialized streams with multiple consumers. This wastes memory for small streams:\n- **Tee small streams** (< 5MB): Efficient for 2-3 consumers\n- **Materialize large streams** (> 5MB): Avoid memory pressure\n- **Configurable thresholds**: Users can tune based on their data\n- **Adaptive mode**: Automatically choose based on size + consumer count\n\n### 3. **Composable Expression System**\nThe previous design forced users to choose between text OR visual. This is limiting:\n- **Three modes**: Text, Visual, Composite\n- **Composite mode**: Mix text and visual blocks in one expression\n- **Progressive enhancement**: Start with text, add visual blocks where helpful\n- **Same engine**: All modes compile to JSONata under the hood\n\nExample: `$.items[price > 100] | {VISUAL_BLOCK: sum(price)}`\n\n### 4. **Pluggable Compensation Handlers**\nThe four built-in compensation types are good, but users need extensibility:\n- **Handler registry**: Register custom handlers for specific services\n- **Auto-discovery**: Handlers declare what they can handle via `canHandle()`\n- **Fallback**: If no handler found, return manual steps\n- **Examples**: Stripe refunds, Slack message deletion, S3 object removal\n\nThis allows users to build domain-specific compensation logic without modifying core code.\n\n---\n\n## What I Changed\n\n### Storage: 3-tier \u2192 2-tier with overflow strategy\n- Simpler mental model (hot/warm vs hot/warm/cold)\n- Explicit user control via overflow strategy\n- No complex tier promotion logic\n- External blobs only when user opts in\n\n### Streaming: Always materialize \u2192 Adaptive teeing\n- Tee small streams for efficiency\n- Materialize large streams to avoid memory issues\n- Configurable thresholds\n- Tracks multi-consumer strategy in metadata\n\n### Expressions: Separate modes \u2192 Composable system\n- Mix text and visual in one expression\n- Three modes: text, visual, composite\n- Progressive enhancement path\n- Single engine (JSONata) underneath\n\n### Compensation: Fixed handlers \u2192 Pluggable registry\n- Users can register custom handlers\n- Auto-discovery via `canHandle()`\n- Extensible for domain-specific services\n- Built-in handlers still provided\n\n---\n\n## What I Kept\n\nAll the excellent architectural decisions:\n- React Flow canvas with viewport culling\n- Zustand + immer state management\n- Append-only side effect log\n- Semantic compensation descriptors (not functions)\n- Checkpoint manager with strategies\n- Worker pool architecture\n- Topological execution order\n- Compression for large data\n- Auto-cleanup of temporary artifacts\n- IndexedDB for persistence\n- Performance optimizations (viewport culling, virtual scrolling, debounced save)\n\n---\n\n## Open Questions\n\n### 1. **Overflow Strategy Defaults**\nShould different overflow strategies be recommended for different use cases?\n- Development: `drop-on-store` (fail loudly)\n- Production: `drop-oldest` (graceful degradation)\n- Large files: `external-blob` (but lost on refresh)\n\n### 2. **Tee vs Materialize Thresholds**\nCurrent defaults:\n- Tee threshold: 5MB\n- Materialize threshold: 50MB\n- Max consumers for tee: 3\n\nAre these reasonable? Should they be auto-tuned based on browser memory?\n\n### 3. **Composite Expression UI**\nHow should the UI for composite mode work?\n- Inline visual blocks within text editor?\n- Side-by-side panels?\n- Click to convert text \u2192 block or block \u2192 text?\n\n### 4. **Handler Registration UX**\nShould compensation handler registration be:\n- **Code-only** (developers write TypeScript classes)\n- **UI-based** (users configure handlers in settings panel)\n- **Both** (UI for simple cases, code for complex)\n\n### 5. **Streaming Statistics**\nShould we track and display:\n- Number of streams teed vs materialized\n- Memory saved by teeing\n- Storage saved by compression\n- Per-node streaming performance metrics\n\n---\n\n## Convergence Status\n\n**PROPOSING_FINAL** - This design addresses all major concerns and is ready for implementation:\n\n\u2705 Simplified storage (2-tier + overflow strategy)  \n\u2705 Smart streaming (adaptive teeing + materialization)  \n\u2705 Flexible expressions (composable system supporting text/visual/mixed)  \n\u2705 Extensible compensation (pluggable handler registry)  \n\u2705 All performance optimizations preserved  \n\u2705 Production-ready architecture  \n\nThe design is complete, implementable, and handles edge cases gracefully.\n\n---\n\n##",
        "prompt_for_other": "Please focus on these specific areas:\n\n1. **Overflow Strategy Naming**: Are the four strategy names intuitive? Would different names be clearer? (e.g., `drop-on-store` vs `fail-when-full`)\n\n2. **Tee Implementation Details**: The current design stores teed streams in execution context (not storage). Is this the right approach? Should we have a separate \"stream registry\" for active tees?\n\n3. **Composite Expression Syntax**: What should the syntax look like for mixing text and visual blocks? Should blocks have special markers (e.g., `{{BLOCK:id}}`) or be more integrated?\n\n4. **Handler Registration Timing**: When should custom compensation handlers be registered? At app startup? Per-workflow? Should handlers be serializable so they can be saved with workflows?\n\n5. **Performance Monitoring**: Should we add built-in performance monitoring for streaming decisions (tee vs materialize) to help users tune thresholds? What metrics matter most?",
        "convergence_signal": "PROPOSING_FINAL",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v3\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Hybrid    \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Storage   \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Two-Tier \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  + Overflow\u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Strategy  \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with viewport culling\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with streaming + teeing for multi-consumer\n- **Storage**: Two-tier (Memory + IndexedDB) with configurable overflow strategies\n- **Expressions**: Composable expression system (JSONata blocks + visual builder)\n- **Side Effects**: Append-only log with pluggable compensation handlers\n- **Workers**: Shared Worker pool with resource quotas\n\n## Simplified Two-Tier Storage with Overflow Strategy\n\n### Key Insight: Let Users Choose Overflow Behavior, Not Complex Auto-Eviction\n\n```typescript\ntype OverflowStrategy = \n  | 'drop-oldest'           // FIFO eviction (default)\n  | 'drop-on-store'         // Fail loudly when full\n  | 'compress-aggressive'   // Compress everything aggressively\n  | 'external-blob';        // Use blob URLs for overflow (browser-only)\n\ninterface StorageConfig {\n  // Simple two-tier system\n  memoryCacheMaxBytes: number;      // 100MB - hot in-memory data\n  indexedDBMaxBytes: number;        // 1GB - warm IndexedDB storage\n  \n  // What happens when IndexedDB fills up?\n  overflowStrategy: OverflowStrategy;\n  \n  // Compression settings\n  compressionThreshold: number;     // 1MB - compress above this\n  compressionLevel: 'fast' | 'balanced' | 'best';\n  \n  // Auto-cleanup\n  autoCleanupAge: number;           // 24h - delete old temp artifacts\n  enableAutoCleanup: boolean;\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  private config: StorageConfig;\n  \n  // Lightweight metadata for all artifacts\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  // External blob URLs (only for 'external-blob' strategy)\n  private externalBlobs: Map<string, string> = new Map();\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initIndexedDB();\n    \n    if (config.enableAutoCleanup) {\n      this.scheduleCleanup();\n    }\n  }\n  \n  async store(key: string, data: any, hints: StorageHints = {}): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    // Update metadata\n    this.artifactIndex.set(key, {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId\n    });\n    \n    // Tier decision: Simple size-based\n    if (sizeBytes < 1024 * 1024) { // < 1MB -> try memory first\n      if (await this.tryStoreInMemory(key, data, sizeBytes)) {\n        this.artifactIndex.get(key)!.tier = 'memory';\n        return;\n      }\n    }\n    \n    // Otherwise use IndexedDB\n    await this.storeInIndexedDB(key, serialized, sizeBytes);\n    this.artifactIndex.get(key)!.tier = 'indexeddb';\n  }\n  \n  private async tryStoreInMemory(key: string, data: any, sizeBytes: number): Promise<boolean> {\n    if (this.memoryCacheSize + sizeBytes <= this.config.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    // Try to evict old memory entries\n    if (await this.evictFromMemory(sizeBytes)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(requiredBytes: number): Promise<boolean> {\n    const entries = Array.from(this.memoryCache.entries())\n      .sort((a, b) => a[1].storedAt - b[1].storedAt);\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const [key, entry] of entries) {\n      // Don't evict very recent entries (< 5s old)\n      if (Date.now() - entry.storedAt < 5000) continue;\n      \n      toEvict.push(key);\n      freedBytes += entry.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    // Move to IndexedDB (no data loss)\n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) metadata.tier = 'indexeddb';\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    // Check if we're at capacity\n    if (this.indexedDBSize + sizeBytes > this.config.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes);\n      return;\n    }\n    \n    // Compress if needed\n    const shouldCompress = sizeBytes > this.config.compressionThreshold;\n    const toStore = shouldCompress \n      ? await this.compress(serialized, this.config.compressionLevel)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore as string).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(key: string, serialized: string, sizeBytes: number): Promise<void> {\n    switch (this.config.overflowStrategy) {\n      case 'drop-oldest':\n        // Evict oldest entries until we have space\n        await this.evictOldestFromIndexedDB(sizeBytes);\n        await this.storeInIndexedDB(key, serialized, sizeBytes);\n        break;\n        \n      case 'drop-on-store':\n        throw new Error(\n          `Storage full: Cannot store ${key} (${(sizeBytes / 1024 / 1024).toFixed(2)}MB). ` +\n          `IndexedDB usage: ${(this.indexedDBSize / 1024 / 1024).toFixed(2)}MB / ` +\n          `${(this.config.indexedDBMaxBytes / 1024 / 1024).toFixed(2)}MB`\n        );\n        \n      case 'compress-aggressive':\n        // Force best compression on this item\n        const compressed = await this.compress(serialized, 'best');\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > this.config.indexedDBMaxBytes) {\n          // Still doesn't fit - evict oldest\n          await this.evictOldestFromIndexedDB(compressedSize);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        break;\n        \n      case 'external-blob':\n        // Store as blob URL (browser-only, lost on refresh)\n        const compressed2 = await this.compress(serialized, 'best');\n        const blob = new Blob([compressed2], { type: 'application/octet-stream' });\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        // Store tiny reference in IndexedDB\n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) metadata.tier = 'external';\n        break;\n    }\n  }\n  \n  private async evictOldestFromIndexedDB(requiredBytes: number): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const index = store.index('storedAt');\n    \n    const entries: Array<{ key: string; sizeBytes: number }> = [];\n    let cursor = await index.openCursor();\n    \n    while (cursor) {\n      entries.push({\n        key: cursor.value.key,\n        sizeBytes: cursor.value.sizeBytes\n      });\n      cursor = await cursor.continue();\n    }\n    \n    // Sort by age\n    entries.sort((a, b) => {\n      const aTime = this.artifactIndex.get(a.key)?.createdAt ?? 0;\n      const bTime = this.artifactIndex.get(b.key)?.createdAt ?? 0;\n      return aTime - bTime;\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const entry of entries) {\n      toDelete.push(entry.key);\n      freedBytes += entry.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    // Delete entries\n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    // Update access tracking\n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    let data: any;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        data = this.memoryCache.get(key)?.data;\n        if (!data) {\n          // Fallback to IndexedDB\n          data = await this.retrieveFromIndexedDB(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        data = await this.retrieveFromIndexedDB(key);\n        \n        // Promote to memory if: small + accessed frequently\n        if (metadata.sizeBytes < 1024 * 1024 && \n            metadata.accessCount >= 3 &&\n            Date.now() - metadata.createdAt < 60000) {\n          \n          if (await this.tryStoreInMemory(key, data, metadata.sizeBytes)) {\n            metadata.tier = 'memory';\n          }\n        }\n        break;\n        \n      case 'external':\n        data = await this.retrieveFromExternalBlob(key);\n        break;\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n    \n    return data;\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async clear(predicate?: (metadata: ArtifactMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      // Clear everything\n      this.memoryCache.clear();\n      this.memoryCacheSize = 0;\n      this.indexedDBSize = 0;\n      this.artifactIndex.clear();\n      \n      const tx = this.indexedDB.transaction(['artifacts', 'blobRefs'], 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      await tx.objectStore('blobRefs').clear();\n      \n      // Revoke blob URLs\n      for (const blobUrl of this.externalBlobs.values()) {\n        URL.revokeObjectURL(blobUrl);\n      }\n      this.externalBlobs.clear();\n      \n      return;\n    }\n    \n    // Selective clear\n    const toDelete: string[] = [];\n    \n    for (const [key, metadata] of this.artifactIndex.entries()) {\n      if (predicate(metadata)) {\n        toDelete.push(key);\n      }\n    }\n    \n    for (const key of toDelete) {\n      await this.delete(key);\n    }\n  }\n  \n  private async delete(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) return;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        const entry = this.memoryCache.get(key);\n        if (entry) {\n          this.memoryCacheSize -= entry.sizeBytes;\n          this.memoryCache.delete(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        this.indexedDBSize -= metadata.sizeBytes;\n        break;\n        \n      case 'external':\n        const blobUrl = this.externalBlobs.get(key);\n        if (blobUrl) {\n          URL.revokeObjectURL(blobUrl);\n          this.externalBlobs.delete(key);\n        }\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').delete(key);\n        break;\n    }\n    \n    this.artifactIndex.delete(key);\n  }\n  \n  private scheduleCleanup(): void {\n    setInterval(() => {\n      const cutoff = Date.now() - this.config.autoCleanupAge;\n      \n      this.clear(metadata => \n        metadata.temporary && metadata.createdAt < cutoff\n      );\n    }, 60 * 60 * 1000); // 1 hour\n  }\n  \n  getStorageStats(): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      stats[metadata.tier as 'memory' | 'indexeddb' | 'external'].count++;\n      if (metadata.tier === 'external') {\n        stats.external.bytes += metadata.sizeBytes;\n      }\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n  \n  private async compress(data: string, level: 'fast' | 'balanced' | 'best'): Promise<string> {\n    // Use native compression (gzip)\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    // Convert to base64 for storage\n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    // Decode from base64\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 3);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n}\n\n// Types\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'external' | 'determining';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  nodeId?: string;\n  executionId?: string;\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n```\n\n### Usage Example\n```typescript\nconst storage = new HybridStorageManager({\n  memoryCacheMaxBytes: 100 * 1024 * 1024,      // 100MB\n  indexedDBMaxBytes: 1024 * 1024 * 1024,       // 1GB\n  overflowStrategy: 'drop-oldest',             // Default: FIFO eviction\n  compressionThreshold: 1024 * 1024,           // 1MB\n  compressionLevel: 'balanced',\n  autoCleanupAge: 24 * 60 * 60 * 1000,        // 24h\n  enableAutoCleanup: true\n});\n\n// Store with automatic tiering\nawait storage.store(\n  `node-${nodeId}-output`,\n  largeDataset,\n  {\n    nodeId,\n    executionId,\n    temporary: true\n  }\n);\n\n// Retrieve (transparent tier access)\nconst data = await storage.retrieve(`node-${nodeId}-output`);\n\n// Cleanup after workflow\nawait storage.clear(meta => \n  meta.temporary && meta.executionId === executionId\n);\n\n// Check storage\nconst stats = storage.getStorageStats();\nconsole.log(`Memory: ${(stats.memory.bytes / 1024 / 1024).toFixed(2)}MB`);\nconsole.log(`IndexedDB: ${(stats.indexeddb.bytes / 1024 / 1024).toFixed(2)}MB`);\n```\n\n## Adaptive Streaming with Smart Teeing\n\n### Key Insight: Tee Small Streams, Materialize Large Ones\n\n```typescript\ninterface StreamingConfig {\n  teeThresholdBytes: number;        // 5MB - tee if smaller\n  materializeThresholdBytes: number; // 50MB - always materialize if larger\n  enableAdaptiveTee: boolean;        // Use size estimation for decision\n}\n\nclass AdaptiveExecutor {\n  private streamingExecutor: StreamingExecutor;\n  private batchExecutor: BatchExecutor;\n  private storage: HybridStorageManager;\n  private config: StreamingConfig;\n  \n  constructor(storage: HybridStorageManager, config: StreamingConfig) {\n    this.storage = storage;\n    this.config = config;\n    this.streamingExecutor = new StreamingExecutor();\n    this.batchExecutor = new BatchExecutor();\n  }\n  \n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): Promise<ExecutionResult> {\n    \n    // Analyze streaming feasibility\n    const decision = this.analyzeStreamingFeasibility(node, inputs, edges);\n    \n    if (decision.shouldStream) {\n      return this.executeWithStreaming(node, inputs, decision);\n    } else {\n      return this.executeWithBatch(node, inputs);\n    }\n  }\n  \n  private analyzeStreamingFeasibility(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): StreamingDecision {\n    \n    const reasons: string[] = [];\n    let shouldStream = false;\n    let multiConsumerStrategy: MultiConsumerStrategy = 'none';\n    \n    // 1. Explicit node config\n    if (node.data.config.streaming === false) {\n      return {\n        shouldStream: false,\n        reasons: ['explicit_batch_mode'],\n        multiConsumerStrategy: 'none'\n      };\n    }\n    \n    if (node.data.config.streaming === true) {\n      shouldStream = true;\n      reasons.push('explicit_streaming_mode');\n    }\n    \n    // 2. Stream input detection\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream\n    );\n    \n    if (hasStreamInput) {\n      shouldStream = true;\n      reasons.push('stream_input_detected');\n    }\n    \n    // 3. Large data heuristic\n    const estimatedSize = this.estimateDataSize(inputs);\n    if (estimatedSize > 5 * 1024 * 1024) {\n      shouldStream = true;\n      reasons.push('large_input_heuristic');\n    }\n    \n    // 4. Agent type benefits\n    if (['researcher', 'coder', 'reviewer'].includes(node.data.type)) {\n      shouldStream = true;\n      reasons.push('agent_benefits_from_streaming');\n    }\n    \n    // 5. Multi-consumer analysis\n    if (shouldStream) {\n      const downstreamCount = edges.filter(e => e.source === node.id).length;\n      \n      if (downstreamCount > 1) {\n        // Decide between teeing and materialization\n        multiConsumerStrategy = this.selectMultiConsumerStrategy(\n          estimatedSize,\n          downstreamCount\n        );\n        \n        reasons.push(`multi_consumer_${multiConsumerStrategy}`);\n      }\n    }\n    \n    return {\n      shouldStream,\n      reasons,\n      multiConsumerStrategy\n    };\n  }\n  \n  private selectMultiConsumerStrategy(\n    estimatedSize: number,\n    consumerCount: number\n  ): MultiConsumerStrategy {\n    \n    if (!this.config.enableAdaptiveTee) {\n      // Always materialize if adaptive disabled\n      return 'materialize';\n    }\n    \n    // Large streams: always materialize (avoid memory pressure)\n    if (estimatedSize > this.config.materializeThresholdBytes) {\n      return 'materialize';\n    }\n    \n    // Small streams: tee is efficient\n    if (estimatedSize < this.config.teeThresholdBytes) {\n      return consumerCount <= 3 ? 'tee' : 'materialize';\n    }\n    \n    // Medium streams: depends on consumer count\n    // More consumers = more memory copies = prefer materialize\n    if (consumerCount <= 2) {\n      return 'tee';\n    } else {\n      return 'materialize';\n    }\n  }\n  \n  private async executeWithStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const outputStream = await this.streamingExecutor.executeNodeStreaming(\n      node,\n      inputs\n    );\n    \n    // Handle multi-consumer scenario\n    if (decision.multiConsumerStrategy === 'materialize') {\n      return await this.materializeStreamToStorage(node, outputStream, decision);\n    } else if (decision.multiConsumerStrategy === 'tee') {\n      return await this.teeStreamForConsumers(node, outputStream, decision);\n    }\n    \n    // Single consumer: pass stream through\n    return {\n      nodeId: node.id,\n      output: outputStream,\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'none'\n      }\n    };\n  }\n  \n  private async teeStreamForConsumers(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    // Create teed streams (browser supports tee() natively)\n    const [stream1, stream2] = stream.tee();\n    \n    // Store references to both streams\n    const teeKey = `node-${node.id}-tee`;\n    \n    // Store tee metadata (not the streams themselves)\n    await this.storage.store(\n      teeKey,\n      {\n        type: 'teed_streams',\n        nodeId: node.id,\n        // We can't actually store streams, so we'll need to re-tee on demand\n        // This is a limitation - we need to handle this at execution level\n      },\n      {\n        nodeId: node.id,\n        temporary: true\n      }\n    );\n    \n    // Return special marker indicating teed output\n    return {\n      nodeId: node.id,\n      output: {\n        $teed: teeKey,\n        streams: [stream1, stream2]  // Store in execution context, not storage\n      },\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'tee'\n      }\n    };\n  }\n  \n  private async materializeStreamToStorage(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    // Read entire stream\n    const chunks: any[] = [];\n    const reader = stream.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        chunks.push(value);\n      }\n    } finally {\n      reader.releaseLock();\n    }\n    \n    // Combine chunks\n    const materialized = this.combineChunks(chunks);\n    \n    // Store in hybrid storage\n    const storageKey = `node-${node.id}-materialized`;\n    await this.storage.store(storageKey, materialized, {\n      nodeId: node.id,\n      temporary: true\n    });\n    \n    // Return storage reference\n    return {\n      nodeId: node.id,\n      output: { $ref: storageKey },\n      streaming: false,\n      metadata: {\n        materialized: true,\n        storageKey,\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'materialize'\n      }\n    };\n  }\n  \n  private async executeWithBatch(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    // Resolve storage references and teed streams\n    const resolvedInputs = await this.resolveInputs(inputs);\n    \n    // Execute\n    const output = await this.batchExecutor.executeNodeBatch(node, resolvedInputs);\n    \n    return {\n      nodeId: node.id,\n      output,\n      streaming: false,\n      metadata: {}\n    };\n  }\n  \n  private async resolveInputs(inputs: Record<string, any>): Promise<Record<string, any>> {\n    const resolved: Record<string, any> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      if (value && typeof value === 'object') {\n        if (value.$ref) {\n          // Storage reference\n          resolved[key] = await this.storage.retrieve(value.$ref);\n        } else if (value.$teed) {\n          // Teed stream reference - get one of the streams\n          // This is simplified - real implementation needs stream tracking\n          resolved[key] = value.streams[0];\n        } else {\n          resolved[key] = value;\n        }\n      } else {\n        resolved[key] = value;\n      }\n    }\n    \n    return resolved;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private combineChunks(chunks: any[]): any {\n    if (chunks.length === 0) return null;\n    \n    if (typeof chunks[0] === 'string') {\n      return chunks.join('');\n    }\n    \n    if (Array.isArray(chunks[0])) {\n      return chunks.flat();\n    }\n    \n    return chunks[chunks.length - 1];\n  }\n}\n\ntype MultiConsumerStrategy = 'none' | 'tee' | 'materialize';\n\ninterface StreamingDecision {\n  shouldStream: boolean;\n  reasons: string[];\n  multiConsumerStrategy: MultiConsumerStrategy;\n}\n\ninterface ExecutionResult {\n  nodeId: string;\n  output: any | ReadableStream | { $ref: string } | { $teed: string; streams: ReadableStream[] };\n  streaming: boolean;\n  metadata: Record<string, any>;\n}\n\ninterface WorkflowNode {\n  id: string;\n  data: {\n    type: string;\n    config: {\n      streaming?: boolean;\n      [key: string]: any;\n    };\n  };\n}\n\ninterface WorkflowEdge {\n  source: string;\n  target: string;\n}\n```\n\n## Composable Expression System\n\n### Key Insight: Mix and Match Text and Visual, Don't Force One Mode\n\n```typescript\ninterface ExpressionEngine {\n  evaluate(expression: Expression, context: any): Promise<any>;\n  validate(expression: Expression): ValidationResult;\n  getExamples(): ExpressionExample[];\n}\n\n// Expression can be pure text, pure blocks, or mixed\ntype Expression = \n  | { mode: 'text'; value: string }\n  | { mode: 'visual'; blocks: ExpressionBlock[] }\n  | { mode: 'composite'; parts: ExpressionPart[] };\n\ntype ExpressionPart =\n  | { type: 'text'; value: string }\n  | { type: 'block'; block: ExpressionBlock };\n\ninterface ExpressionBlock {\n  id: string;\n  type: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n}\n\nclass ComposableExpressionEngine implements ExpressionEngine {\n  private jsonataEngine: JSONataEngine;\n  \n  constructor() {\n    this.jsonataEngine = new JSONataEngine();\n  }\n  \n  async evaluate(expression: Expression, context: any): Promise<any> {\n    switch (expression.mode) {\n      case 'text':\n        return this.jsonataEngine.evaluate(expression.value, context);\n        \n      case 'visual':\n        const jsonataFromBlocks = this.blocksToJSONata(expression.blocks);\n        return this.jsonataEngine.evaluate(jsonataFromBlocks, context);\n        \n      case 'composite':\n        const jsonataFromComposite = this.compositeToJSONata(expression.parts);\n        return this.jsonataEngine.evaluate(jsonataFromComposite, context);\n        \n      default:\n        throw new Error(`Unknown expression mode: ${(expression as any).mode}`);\n    }\n  }\n  \n  validate(expression: Expression): ValidationResult {\n    try {\n      let jsonataExpr: string;\n      \n      switch (expression.mode) {\n        case 'text':\n          jsonataExpr = expression.value;\n          break;\n        case 'visual':\n          jsonataExpr = this.blocksToJSONata(expression.blocks);\n          break;\n        case 'composite':\n          jsonataExpr = this.compositeToJSONata(expression.parts);\n          break;\n        default:\n          return { valid: false, error: 'Unknown expression mode' };\n      }\n      \n      return this.jsonataEngine.validate(jsonataExpr);\n      \n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check expression syntax'\n      };\n    }\n  }\n  \n  private blocksToJSONata(blocks: ExpressionBlock[]): string {\n    return blocks.map(block => {\n      switch (block.type) {\n        case 'field':\n          return `$.${block.config.path}`;\n          \n        case 'filter':\n          return `[${block.config.condition}]`;\n          \n        case 'map':\n          return `.{ ${block.config.mapping} }`;\n          \n        case 'aggregate':\n          return `$${block.config.function}(${block.config.path})`;\n          \n        case 'transform':\n          return block.config.expression;\n          \n        default:\n          throw new Error(`Unknown block type: ${block.type}`);\n      }\n    }).join('');\n  }\n  \n  private compositeToJSONata(parts: ExpressionPart[]): string {\n    return parts.map(part => {\n      if (part.type === 'text') {\n        return part.value;\n      } else {\n        return this.blockToJSONata(part.block);\n      }\n    }).join('');\n  }\n  \n  private blockToJSONata(block: ExpressionBlock): string {\n    switch (block.type) {\n      case 'field':\n        return `$.${block.config.path}`;\n      case 'filter':\n        return `[${block.config.condition}]`;\n      case 'map':\n        return `.{ ${block.config.mapping} }`;\n      case 'aggregate':\n        return `$${block.config.function}(${block.config.path})`;\n      case 'transform':\n        return block.config.expression;\n      default:\n        throw new Error(`Unknown block type: ${block.type}`);\n    }\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return [\n      {\n        category: 'Simple Field',\n        expression: { mode: 'text', value: '$.user.name' },\n        description: 'Extract nested field',\n        inputExample: { user: { name: 'Alice' } },\n        outputExample: 'Alice'\n      },\n      {\n        category: 'Visual Filter',\n        expression: {\n          mode: 'visual',\n          blocks: [\n            { id: '1', type: 'field', config: { path: 'items' } },\n            { id: '2', type: 'filter', config: { condition: 'price > 100' } }\n          ]\n        },\n        description: 'Filter using visual blocks',\n        inputExample: { items: [{ price: 50 }, { price: 150 }] },\n        outputExample: [{ price: 150 }]\n      },\n      {\n        category: 'Composite',\n        expression: {\n          mode: 'composite',\n          parts: [\n            { type: 'block', block: { id: '1', type: 'field', config: { path: 'items' } } },\n            { type: 'text', value: '[price > 100]' },\n            { type: 'block', block: { id: '2', type: 'aggregate', config: { function: 'sum', path: 'price' } } }\n          ]\n        },\n        description: 'Mix visual blocks with text',\n        inputExample: { items: [{ price: 50 }, { price: 150 }, { price: 200 }] },\n        outputExample: 350\n      }\n    ];\n  }\n}\n\nclass JSONataEngine {\n  private cache: Map<string, any> = new Map();\n  private readonly timeout = 5000;\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      // Use jsonata library (would need to import)\n      // compiled = jsonata(expression);\n      \n      // For now, simplified evaluation\n      // In real implementation, use actual JSONata library\n      compiled = { evaluate: (ctx: any) => this.simpleEval(expression, ctx) };\n      \n      this.cache.set(expression, compiled);\n    }\n    \n    return await Promise.race([\n      Promise.resolve(compiled.evaluate(context)),\n      new Promise((_, reject) => \n        setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n      )\n    ]);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      // In real implementation, use JSONata parser\n      // For now, basic validation\n      if (!expression || expression.trim().length === 0) {\n        return { valid: false, error: 'Empty expression' };\n      }\n      \n      return { valid: true };\n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check JSONata syntax'\n      };\n    }\n  }\n  \n  private simpleEval(expression: string, context: any): any {\n    // Simplified evaluator - real implementation would use JSONata\n    if (expression.startsWith('$.')) {\n      const path = expression.substring(2).split('.');\n      let result = context;\n      for (const key of path) {\n        result = result?.[key];\n      }\n      return result;\n    }\n    \n    return null;\n  }\n}\n\n// UI Component for Composable Expressions\nconst ComposableExpressionEditor: React.FC<{\n  value: Expression;\n  onChange: (value: Expression) => void;\n}> = ({ value, onChange }) => {\n  \n  const [mode, setMode] = useState<'text' | 'visual' | 'composite'>(value.mode);\n  const engine = useMemo(() => new ComposableExpressionEngine(), []);\n  const [validation, setValidation] = useState<ValidationResult>({ valid: true });\n  \n  useEffect(() => {\n    setValidation(engine.validate(value));\n  }, [value, engine]);\n  \n  const switchMode = (newMode: 'text' | 'visual' | 'composite') => {\n    if (newMode === value.mode) return;\n    \n    // Convert between modes\n    let newValue: Expression;\n    \n    if (newMode === 'text') {\n      // Convert to text (simplified)\n      newValue = { mode: 'text', value: '' };\n    } else if (newMode === 'visual') {\n      newValue = { mode: 'visual', blocks: [] };\n    } else {\n      newValue = { mode: 'composite', parts: [] };\n    }\n    \n    setMode(newMode);\n    onChange(newValue);\n  };\n  \n  return (\n    <div className=\"expression-editor\">\n      <div className=\"mode-selector\">\n        <button \n          onClick={() => switchMode('text')}\n          className={mode === 'text' ? 'active' : ''}\n        >\n          Text\n        </button>\n        <button \n          onClick={() => switchMode('visual')}\n          className={mode === 'visual' ? 'active' : ''}\n        >\n          Visual\n        </button>\n        <button \n          onClick={() => switchMode('composite')}\n          className={mode === 'composite' ? 'active' : ''}\n        >\n          Mixed\n        </button>\n      </div>\n      \n      {mode === 'text' && value.mode === 'text' && (\n        <textarea\n          value={value.value}\n          onChange={e => onChange({ mode: 'text', value: e.target.value })}\n          className={validation.valid ? '' : 'error'}\n          placeholder=\"Enter JSONata expression: $.field.path\"\n        />\n      )}\n      \n      {mode === 'visual' && value.mode === 'visual' && (\n        <VisualBlockEditor\n          blocks={value.blocks}\n          onChange={blocks => onChange({ mode: 'visual', blocks })}\n        />\n      )}\n      \n      {mode === 'composite' && value.mode === 'composite' && (\n        <CompositeEditor\n          parts={value.parts}\n          onChange={parts => onChange({ mode: 'composite', parts })}\n        />\n      )}\n      \n      {!validation.valid && (\n        <div className=\"error-message\">\n          {validation.error}\n          {validation.suggestion && <div className=\"suggestion\">{validation.suggestion}</div>}\n        </div>\n      )}\n      \n      <ExpressionExamples examples={engine.getExamples()} />\n    </div>\n  );\n};\n\n// Stub components (would be implemented fully)\nconst VisualBlockEditor: React.FC<{\n  blocks: ExpressionBlock[];\n  onChange: (blocks: ExpressionBlock[]) => void;\n}> = ({ blocks, onChange }) => {\n  return <div>Visual block editor (drag-drop blocks)</div>;\n};\n\nconst CompositeEditor: React.FC<{\n  parts: ExpressionPart[];\n  onChange: (parts: ExpressionPart[]) => void;\n}> = ({ parts, onChange }) => {\n  return <div>Composite editor (mix text and blocks)</div>;\n};\n\nconst ExpressionExamples: React.FC<{\n  examples: ExpressionExample[];\n}> = ({ examples }) => {\n  return (\n    <div className=\"examples\">\n      <h4>Examples</h4>\n      {examples.map((ex, i) => (\n        <div key={i} className=\"example\">\n          <strong>{ex.category}:</strong> {ex.description}\n        </div>\n      ))}\n    </div>\n  );\n};\n\ninterface ValidationResult {\n  valid: boolean;\n  error?: string;\n  suggestion?: string;\n}\n\ninterface ExpressionExample {\n  category: string;\n  expression: Expression;\n  description: string;\n  inputExample: any;\n  outputExample: any;\n}\n```\n\n## Pluggable Compensation Handler System\n\n### Key Insight: Let Users Register Custom Handlers for Their Services\n\n```typescript\ninterface CompensationHandler {\n  name: string;\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n  canHandle(entry: SideEffectEntry): boolean;\n}\n\nclass CompensationHandlerRegistry {\n  private handlers: Map<string, CompensationHandler> = new Map();\n  \n  register(handler: CompensationHandler): void {\n    this.handlers.set(handler.name, handler);\n  }\n  \n  unregister(name: string): void {\n    this.handlers.delete(name);\n  }\n  \n  findHandler(entry: SideEffectEntry): CompensationHandler | null {\n    for (const handler of this.handlers.values()) {\n      if (handler.canHandle(entry)) {\n        return handler;\n      }\n    }\n    return null;\n  }\n  \n  getAll(): CompensationHandler[] {\n    return Array.from(this.handlers.values());\n  }\n}\n\n// Enhanced side effect log with pluggable handlers\nclass SideEffectLog {\n  private db: IDBDatabase;\n  private handlerRegistry: CompensationHandlerRegistry;\n  \n  constructor() {\n    this.initDB();\n    this.handlerRegistry = new CompensationHandlerRegistry();\n    this.registerBuiltInHandlers();\n  }\n  \n  async append(entry: Omit<SideEffectEntry, 'id' | 'timestamp' | 'status'>): Promise<string> {\n    const id = crypto.randomUUID();\n    \n    const effectEntry: SideEffectEntry = {\n      ...entry,\n      id,\n      timestamp: Date.now(),\n      status: 'pending'\n    };\n    \n    await this.writeToLog(effectEntry);\n    return id;\n  }\n  \n  async markExecuted(effectId: string): Promise<void> {\n    await this.writeToLog({\n      id: crypto.randomUUID(),\n      nodeId: 'system',\n      executionId: 'system',\n      timestamp: Date.now(),\n      operation: 'api_call',\n      description: `Effect ${effectId} executed`,\n      compensation: { type: 'irreversible', reason: 'audit entry' },\n      status: 'executed',\n      executedAt: Date.now()\n    });\n  }\n  \n  async compensate(effectId: string): Promise<CompensationResult> {\n    const entry = await this.getEntry(effectId);\n    \n    if (!entry) {\n      return { success: false, error: 'Effect not found' };\n    }\n    \n    if (entry.status !== 'executed') {\n      return { success: false, error: 'Effect not in executed state' };\n    }\n    \n    // Find appropriate handler\n    const handler = this.handlerRegistry.findHandler(entry);\n    \n    if (!handler) {\n      return { \n        success: false, \n        error: `No compensation handler found for: ${entry.description}`,\n        requiresUserAction: true,\n        manualSteps: [\n          'No automatic compensation available',\n          `Operation: ${entry.description}`,\n          'Please manually revert this change'\n        ]\n      };\n    }\n    \n    try {\n      const result = await handler.compensate(entry);\n      \n      // Log compensation\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensated: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'compensated',\n        compensatedAt: Date.now()\n      });\n      \n      return result;\n      \n    } catch (error: any) {\n      await this.writeToLog({\n        id: crypto.randomUUID(),\n        nodeId: entry.nodeId,\n        executionId: entry.executionId,\n        timestamp: Date.now(),\n        operation: entry.operation,\n        description: `Compensation failed: ${entry.description}`,\n        compensation: entry.compensation,\n        status: 'failed',\n        error: error.message\n      });\n      \n      return { success: false, error: error.message };\n    }\n  }\n  \n  registerHandler(handler: CompensationHandler): void {\n    this.handlerRegistry.register(handler);\n  }\n  \n  unregisterHandler(name: string): void {\n    this.handlerRegistry.unregister(name);\n  }\n  \n  private registerBuiltInHandlers(): void {\n    // Reversible operations\n    this.handlerRegistry.register({\n      name: 'reversible',\n      canHandle: (entry) => entry.compensation.type === 'reversible',\n      compensate: async (entry) => {\n        const descriptor = (entry.compensation as any).reverseOperation;\n        \n        const response = await fetch(descriptor.endpoint, {\n          method: descriptor.method,\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.parameters)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Reverse operation failed: ${response.statusText}`);\n        }\n        \n        return { success: true, message: 'Operation reversed successfully' };\n      }\n    });\n    \n    // Idempotent operations\n    this.handlerRegistry.register({\n      name: 'idempotent',\n      canHandle: (entry) => entry.compensation.type === 'idempotent',\n      compensate: async (entry) => {\n        return { success: true, message: 'Idempotent operation - no compensation needed' };\n      }\n    });\n    \n    // Irreversible operations\n    this.handlerRegistry.register({\n      name: 'irreversible',\n      canHandle: (entry) => entry.compensation.type === 'irreversible',\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        return {\n          success: false,\n          error: `Cannot auto-compensate: ${descriptor.reason}`,\n          manualSteps: descriptor.manualSteps || ['Manual intervention required'],\n          requiresUserAction: true\n        };\n      }\n    });\n    \n    // Eventual consistency\n    this.handlerRegistry.register({\n      name: 'eventual',\n      canHandle: (entry) => entry.compensation.type === 'eventual',\n      compensate: async (entry) => {\n        const descriptor = entry.compensation as any;\n        \n        const response = await fetch(descriptor.compensatingService, {\n          method: 'POST',\n          headers: { 'Content-Type': 'application/json' },\n          body: JSON.stringify(descriptor.payload)\n        });\n        \n        if (!response.ok) {\n          throw new Error(`Compensating service failed: ${response.statusText}`);\n        }\n        \n        return { \n          success: true, \n          message: 'Compensating transaction initiated' \n        };\n      }\n    });\n  }\n  \n  private async writeToLog(entry: SideEffectEntry): Promise<void> {\n    const tx = this.db.transaction('effects', 'readwrite');\n    const store = tx.objectStore('effects');\n    await store.add(entry);\n  }\n  \n  private async getEntry(effectId: string): Promise<SideEffectEntry | null> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    return await store.get(effectId);\n  }\n  \n  async getEntriesForExecution(executionId: string): Promise<SideEffectEntry[]> {\n    const tx = this.db.transaction('effects', 'readonly');\n    const store = tx.objectStore('effects');\n    const index = store.index('executionId');\n    \n    const entries: SideEffectEntry[] = [];\n    let cursor = await index.openCursor(IDBKeyRange.only(executionId));\n    \n    while (cursor) {\n      entries.push(cursor.value);\n      cursor = await cursor.continue();\n    }\n    \n    return entries;\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('SideEffectLog', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        const store = db.createObjectStore('effects', { keyPath: 'id' });\n        store.createIndex('nodeId', 'nodeId', { unique: false });\n        store.createIndex('executionId', 'executionId', { unique: false });\n        store.createIndex('timestamp', 'timestamp', { unique: false });\n      };\n    });\n  }\n}\n\n// Example: Custom compensation handler for Stripe\nclass StripeCompensationHandler implements CompensationHandler {\n  name = 'stripe';\n  \n  canHandle(entry: SideEffectEntry): boolean {\n    return entry.operation === 'payment_process' && \n           entry.description.includes('Stripe');\n  }\n  \n  async compensate(entry: SideEffectEntry): Promise<CompensationResult> {\n    const descriptor = entry.compensation as any;\n    \n    try {\n      // Call Stripe refund API\n      const response = await fetch('https://api.stripe.com/v1/refunds', {\n        method: 'POST',\n        headers: {\n          'Authorization': `Bearer ${descriptor.apiKey}`,\n          'Content-Type': 'application/x-www-form-urlencoded'\n        },\n        body: new URLSearchParams({\n          charge: descriptor.chargeId,\n          reason: 'requested_by_customer'\n        })\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Stripe refund failed: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Refunded charge ${descriptor.chargeId}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message,\n        requiresUserAction: true,\n        manualSteps: [\n          'Go to Stripe dashboard',\n          `Find charge: ${descriptor.chargeId}`,\n          'Issue manual refund'\n        ]\n      };\n    }\n  }\n}\n\n// Usage\nconst sideEffectLog = new SideEffectLog();\n\n// Register custom handler\nsideEffectLog.registerHandler(new StripeCompensationHandler());\n\n// Log Stripe payment\nawait sideEffectLog.append({\n  nodeId: 'checkout-node',\n  executionId: 'exec-123',\n  operation: 'payment_process',\n  description: 'Stripe payment for order #456',\n  compensation: {\n    type: 'reversible',\n    reverseOperation: {\n      service: 'stripe',\n      method: 'POST',\n      endpoint: '/v1/refunds',\n      parameters: {\n        chargeId: 'ch_abc123',\n        apiKey: 'sk_test_...'\n      }\n    }\n  }\n});\n\n// Types\ninterface SideEffectEntry {\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  operation: OperationType;\n  description: string;\n  compensation: CompensationDescriptor;\n  status: 'pending' | 'executed' | 'failed' | 'compensated';\n  executedAt?: number;\n  compensatedAt?: number;\n  error?: string;\n}\n\ntype OperationType = \n  | 'api_call'\n  | 'file_write'\n  | 'database_write'\n  | 'external_webhook'\n  | 'email_send'\n  | 'payment_process';\n\ntype CompensationDescriptor = \n  | { type: 'reversible'; reverseOperation: OperationDescriptor }\n  | { type: 'idempotent'; canRetry: boolean }\n  | { type: 'irreversible'; reason: string; manualSteps?: string[] }\n  | { type: 'eventual'; compensatingService: string; payload: any };\n\ninterface OperationDescriptor {\n  service: string;\n  method: string;\n  endpoint?: string;\n  parameters: Record<string, any>;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n## Checkpoint Manager (Enhanced)\n\n```typescript\nclass CheckpointManager {\n  private db: IDBDatabase;\n  private sideEffectLog: SideEffectLog;\n  \n  constructor(sideEffectLog: SideEffectLog) {\n    this.sideEffectLog = sideEffectLog;\n    this.initDB();\n  }\n  \n  async createCheckpoint(\n    executionState: ExecutionState,\n    reason: 'manual' | 'auto' | 'error' = 'auto'\n  ): Promise<string> {\n    const checkpointId = crypto.randomUUID();\n    \n    const sideEffects = await this.sideEffectLog.getEntriesForExecution(\n      executionState.executionId\n    );\n    \n    const checkpoint: ExecutionCheckpoint = {\n      id: checkpointId,\n      executionId: executionState.executionId,\n      timestamp: Date.now(),\n      nodeStates: this.serializeNodeStates(executionState.nodeStates),\n      sideEffectCount: sideEffects.length,\n      reason\n    };\n    \n    const tx = this.db.transaction('checkpoints', 'readwrite');\n    await tx.objectStore('checkpoints').put(checkpoint);\n    \n    return checkpointId;\n  }\n  \n  async restoreCheckpoint(\n    checkpointId: string,\n    strategy: CompensationStrategy\n  ): Promise<RestoreResult> {\n    \n    const checkpoint = await this.getCheckpoint(checkpointId);\n    if (!checkpoint) {\n      return { success: false, error: 'Checkpoint not found' };\n    }\n    \n    const allEffects = await this.sideEffectLog.getEntriesForExecution(\n      checkpoint.executionId\n    );\n    \n    const effectsToCompensate = allEffects.filter(e => \n      e.timestamp > checkpoint.timestamp && e.status === 'executed'\n    );\n    \n    const compensationResults = await this.applyCompensationStrategy(\n      effectsToCompensate,\n      strategy\n    );\n    \n    const manualSteps = compensationResults\n      .filter(r => r.requiresUserAction)\n      .flatMap(r => r.manualSteps || []);\n    \n    return {\n      success: true,\n      checkpoint,\n      compensationResults,\n      manualSteps: manualSteps.length > 0 ? manualSteps : undefined\n    };\n  }\n  \n  private async applyCompensationStrategy(\n    effects: SideEffectEntry[],\n    strategy: CompensationStrategy\n  ): Promise<CompensationResult[]> {\n    \n    switch (strategy) {\n      case 'automatic':\n        const results: CompensationResult[] = [];\n        \n        for (const effect of effects.reverse()) {\n          const result = await this.sideEffectLog.compensate(effect.id);\n          results.push(result);\n          \n          if (!result.success && !result.requiresUserAction) {\n            break; // Stop on failure\n          }\n        }\n        \n        return results;\n        \n      case 'manual':\n        return effects.map(effect => ({\n          success: false,\n          requiresUserAction: true,\n          manualSteps: [\n            `Effect: ${effect.description}`,\n            `Type: ${effect.compensation.type}`,\n            'Manually revert this operation'\n          ]\n        }));\n        \n      case 'confirm_each':\n        // Would require UI interaction\n        return [];\n        \n      default:\n        throw new Error(`Unknown strategy: ${strategy}`);\n    }\n  }\n  \n  private async getCheckpoint(checkpointId: string): Promise<ExecutionCheckpoint | null> {\n    const tx = this.db.transaction('checkpoints', 'readonly');\n    const store = tx.objectStore('checkpoints');\n    return await store.get(checkpointId);\n  }\n  \n  private serializeNodeStates(nodeStates: Map<string, any>): any {\n    return Array.from(nodeStates.entries());\n  }\n  \n  private async initDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowCheckpoints', 1);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        if (!db.objectStoreNames.contains('checkpoints')) {\n          db.createObjectStore('checkpoints', { keyPath: 'id' });\n        }\n      };\n    });\n  }\n}\n\ntype CompensationStrategy = \n  | 'automatic'\n  | 'manual'\n  | 'confirm_each';\n\ninterface ExecutionState {\n  executionId: string;\n  nodeStates: Map<string, any>;\n}\n\ninterface ExecutionCheckpoint {\n  id: string;\n  executionId: string;\n  timestamp: number;\n  nodeStates: any;\n  sideEffectCount: number;\n  reason: string;\n}\n\ninterface RestoreResult {\n  success: boolean;\n  error?: string;\n  checkpoint?: ExecutionCheckpoint;\n  compensationResults?: CompensationResult[];\n  manualSteps?: string[];\n}\n```\n\n## Performance Optimizations (Preserved)\n\nAll performance optimizations from previous design are maintained:\n- Viewport culling for large canvases\n- Virtual scrolling for node palette\n- Debounced auto-save\n- Worker pool for execution\n- Lazy loading of node data\n\n---\n\n## Rationale\n\n### 1. **Two-Tier Storage with Overflow Strategy**\nThe three-tier approach adds complexity that most users don't need. Instead:\n- **Two tiers** (Memory + IndexedDB) are simpler and sufficient for 99% of workflows\n- **Overflow strategy** gives users explicit control over what happens when storage fills up\n- **drop-oldest**: Safe default (FIFO eviction)\n- **drop-on-store**: Fail loudly for debugging\n- **compress-aggressive**: Trade CPU for space\n- **external-blob**: Browser-only fallback for huge files\n\nThis is simpler to reason about and lets users pick the behavior that matches their needs.\n\n### 2. **Adaptive Teeing for Multi-Consumer Streams**\nThe previous design always materialized streams with multiple consumers. This wastes memory for small streams:\n- **Tee small streams** (< 5MB): Efficient for 2-3 consumers\n- **Materialize large streams** (> 5MB): Avoid memory pressure\n- **Configurable thresholds**: Users can tune based on their data\n- **Adaptive mode**: Automatically choose based on size + consumer count\n\n### 3. **Composable Expression System**\nThe previous design forced users to choose between text OR visual. This is limiting:\n- **Three modes**: Text, Visual, Composite\n- **Composite mode**: Mix text and visual blocks in one expression\n- **Progressive enhancement**: Start with text, add visual blocks where helpful\n- **Same engine**: All modes compile to JSONata under the hood\n\nExample: `$.items[price > 100] | {VISUAL_BLOCK: sum(price)}`\n\n### 4. **Pluggable Compensation Handlers**\nThe four built-in compensation types are good, but users need extensibility:\n- **Handler registry**: Register custom handlers for specific services\n- **Auto-discovery**: Handlers declare what they can handle via `canHandle()`\n- **Fallback**: If no handler found, return manual steps\n- **Examples**: Stripe refunds, Slack message deletion, S3 object removal\n\nThis allows users to build domain-specific compensation logic without modifying core code.\n\n---\n\n## What I Changed\n\n### Storage: 3-tier \u2192 2-tier with overflow strategy\n- Simpler mental model (hot/warm vs hot/warm/cold)\n- Explicit user control via overflow strategy\n- No complex tier promotion logic\n- External blobs only when user opts in\n\n### Streaming: Always materialize \u2192 Adaptive teeing\n- Tee small streams for efficiency\n- Materialize large streams to avoid memory issues\n- Configurable thresholds\n- Tracks multi-consumer strategy in metadata\n\n### Expressions: Separate modes \u2192 Composable system\n- Mix text and visual in one expression\n- Three modes: text, visual, composite\n- Progressive enhancement path\n- Single engine (JSONata) underneath\n\n### Compensation: Fixed handlers \u2192 Pluggable registry\n- Users can register custom handlers\n- Auto-discovery via `canHandle()`\n- Extensible for domain-specific services\n- Built-in handlers still provided\n\n---\n\n## What I Kept\n\nAll the excellent architectural decisions:\n- React Flow canvas with viewport culling\n- Zustand + immer state management\n- Append-only side effect log\n- Semantic compensation descriptors (not functions)\n- Checkpoint manager with strategies\n- Worker pool architecture\n- Topological execution order\n- Compression for large data\n- Auto-cleanup of temporary artifacts\n- IndexedDB for persistence\n- Performance optimizations (viewport culling, virtual scrolling, debounced save)\n\n---\n\n## Open Questions\n\n### 1. **Overflow Strategy Defaults**\nShould different overflow strategies be recommended for different use cases?\n- Development: `drop-on-store` (fail loudly)\n- Production: `drop-oldest` (graceful degradation)\n- Large files: `external-blob` (but lost on refresh)\n\n### 2. **Tee vs Materialize Thresholds**\nCurrent defaults:\n- Tee threshold: 5MB\n- Materialize threshold: 50MB\n- Max consumers for tee: 3\n\nAre these reasonable? Should they be auto-tuned based on browser memory?\n\n### 3. **Composite Expression UI**\nHow should the UI for composite mode work?\n- Inline visual blocks within text editor?\n- Side-by-side panels?\n- Click to convert text \u2192 block or block \u2192 text?\n\n### 4. **Handler Registration UX**\nShould compensation handler registration be:\n- **Code-only** (developers write TypeScript classes)\n- **UI-based** (users configure handlers in settings panel)\n- **Both** (UI for simple cases, code for complex)\n\n### 5. **Streaming Statistics**\nShould we track and display:\n- Number of streams teed vs materialized\n- Memory saved by teeing\n- Storage saved by compression\n- Per-node streaming performance metrics\n\n---\n\n## Convergence Status\n\n**PROPOSING_FINAL** - This design addresses all major concerns and is ready for implementation:\n\n\u2705 Simplified storage (2-tier + overflow strategy)  \n\u2705 Smart streaming (adaptive teeing + materialization)  \n\u2705 Flexible expressions (composable system supporting text/visual/mixed)  \n\u2705 Extensible compensation (pluggable handler registry)  \n\u2705 All performance optimizations preserved  \n\u2705 Production-ready architecture  \n\nThe design is complete, implementable, and handles edge cases gracefully.\n\n---\n\n## PROMPT_FOR_CRITIC:\n\nPlease focus on these specific areas:\n\n1. **Overflow Strategy Naming**: Are the four strategy names intuitive? Would different names be clearer? (e.g., `drop-on-store` vs `fail-when-full`)\n\n2. **Tee Implementation Details**: The current design stores teed streams in execution context (not storage). Is this the right approach? Should we have a separate \"stream registry\" for active tees?\n\n3. **Composite Expression Syntax**: What should the syntax look like for mixing text and visual blocks? Should blocks have special markers (e.g., `{{BLOCK:id}}`) or be more integrated?\n\n4. **Handler Registration Timing**: When should custom compensation handlers be registered? At app startup? Per-workflow? Should handlers be serializable so they can be saved with workflows?\n\n5. **Performance Monitoring**: Should we add built-in performance monitoring for streaming decisions (tee vs materialize) to help users tune thresholds? What metrics matter most?"
      },
      "agent_b": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v4\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Storage   \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  + Stream  \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502            \u2502  \u2502  Registry  \u2502  \u2502            \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Two-Tier \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  + Overflow\u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Strategy  \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with viewport culling\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with streaming + dedicated stream registry\n- **Storage**: Two-tier (Memory + IndexedDB) with explicit overflow policies\n- **Expressions**: Composable expression system (JSONata + visual blocks)\n- **Side Effects**: Append-only log with serializable compensation handlers\n- **Workers**: Shared Worker pool with resource quotas\n\n## Enhanced Two-Tier Storage with Clear Overflow Policies\n\n### Key Changes: Clearer Naming, Better Eviction Control, Stream-Aware\n\n```typescript\ntype OverflowPolicy = \n  | 'evict-lru'              // Default: Evict least recently used\n  | 'evict-oldest'           // FIFO eviction by creation time\n  | 'fail-when-full'         // Throw error when storage full\n  | 'compress-max'           // Aggressive compression on overflow\n  | 'external-blob';         // Blob URLs (volatile, cleared on reload)\n\ninterface StorageConfig {\n  // Tier capacities\n  memoryCacheMaxBytes: number;      // 100MB - hot in-memory data\n  indexedDBMaxBytes: number;        // 1GB - warm IndexedDB storage\n  \n  // Overflow behavior\n  overflowPolicy: OverflowPolicy;\n  \n  // Eviction preferences (for evict-lru/evict-oldest policies)\n  evictionPreferences: {\n    protectRecent: boolean;         // Don't evict items < 5s old\n    protectFrequent: boolean;       // Prefer evicting low-access items\n    protectPinned: boolean;         // Never evict explicitly pinned items\n  };\n  \n  // Compression\n  compressionThreshold: number;     // 1MB - compress above this\n  compressionLevel: 'fast' | 'balanced' | 'best';\n  \n  // Auto-cleanup\n  autoCleanupAge: number;           // 24h - delete old temp artifacts\n  enableAutoCleanup: boolean;\n  \n  // Storage warnings\n  warnAtPercent: number;            // 80 - warn when storage % full\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'external' | 'determining';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  pinned: boolean;                  // Protected from eviction\n  nodeId?: string;\n  executionId?: string;\n  dataType: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  private config: StorageConfig;\n  \n  // Lightweight metadata for all artifacts\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  // External blob URLs (only for 'external-blob' policy)\n  private externalBlobs: Map<string, string> = new Map();\n  \n  // Storage event listeners\n  private listeners: Set<StorageEventListener> = new Set();\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initIndexedDB();\n    \n    if (config.enableAutoCleanup) {\n      this.scheduleCleanup();\n    }\n    \n    // Monitor storage levels\n    this.scheduleStorageMonitoring();\n  }\n  \n  async store(\n    key: string, \n    data: any, \n    hints: StorageHints = {}\n  ): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    // Update metadata\n    this.artifactIndex.set(key, {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      pinned: hints.pinned ?? false,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      dataType: hints.dataType ?? 'json'\n    });\n    \n    // Tier decision: Simple size-based\n    if (sizeBytes < 1024 * 1024) { // < 1MB -> try memory first\n      if (await this.tryStoreInMemory(key, data, sizeBytes)) {\n        this.artifactIndex.get(key)!.tier = 'memory';\n        this.emitEvent('stored', { key, tier: 'memory', sizeBytes });\n        return;\n      }\n    }\n    \n    // Otherwise use IndexedDB\n    await this.storeInIndexedDB(key, serialized, sizeBytes);\n    this.artifactIndex.get(key)!.tier = 'indexeddb';\n    this.emitEvent('stored', { key, tier: 'indexeddb', sizeBytes });\n  }\n  \n  private async tryStoreInMemory(\n    key: string, \n    data: any, \n    sizeBytes: number\n  ): Promise<boolean> {\n    if (this.memoryCacheSize + sizeBytes <= this.config.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    // Try to evict from memory\n    if (await this.evictFromMemory(sizeBytes)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(requiredBytes: number): Promise<boolean> {\n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'memory' &&\n        (!this.config.evictionPreferences.protectPinned || !m.pinned) &&\n        (!this.config.evictionPreferences.protectRecent || \n         Date.now() - m.createdAt > 5000)\n      );\n    \n    // Sort by eviction preference\n    candidates.sort((a, b) => {\n      if (this.config.evictionPreferences.protectFrequent) {\n        // Evict less frequently accessed first\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      // Then by LRU\n      return a.lastAccessedAt - b.lastAccessedAt;\n    });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const metadata of candidates) {\n      toEvict.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    // Move to IndexedDB (no data loss)\n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n        this.emitEvent('evicted', { key, from: 'memory', to: 'indexeddb' });\n      }\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(\n    key: string, \n    serialized: string, \n    sizeBytes: number\n  ): Promise<void> {\n    // Check if we're at capacity\n    if (this.indexedDBSize + sizeBytes > this.config.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes);\n      return;\n    }\n    \n    // Compress if needed\n    const shouldCompress = sizeBytes > this.config.compressionThreshold;\n    const toStore = shouldCompress \n      ? await this.compress(serialized, this.config.compressionLevel)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore as string).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(\n    key: string, \n    serialized: string, \n    sizeBytes: number\n  ): Promise<void> {\n    switch (this.config.overflowPolicy) {\n      case 'evict-lru':\n      case 'evict-oldest':\n        await this.evictFromIndexedDB(\n          sizeBytes, \n          this.config.overflowPolicy === 'evict-lru'\n        );\n        await this.storeInIndexedDB(key, serialized, sizeBytes);\n        break;\n        \n      case 'fail-when-full':\n        const stats = this.getStorageStats();\n        throw new StorageFullError(\n          `Storage full: Cannot store ${key} (${formatBytes(sizeBytes)}). ` +\n          `Usage: ${formatBytes(stats.total.bytes)} / ` +\n          `${formatBytes(this.config.memoryCacheMaxBytes + this.config.indexedDBMaxBytes)}`,\n          stats\n        );\n        \n      case 'compress-max':\n        const compressed = await this.compress(serialized, 'best');\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > this.config.indexedDBMaxBytes) {\n          // Still doesn't fit - evict\n          await this.evictFromIndexedDB(compressedSize, true);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        this.emitEvent('compressed', { key, originalSize: sizeBytes, compressedSize });\n        break;\n        \n      case 'external-blob':\n        const compressed2 = await this.compress(serialized, 'best');\n        const blob = new Blob([compressed2], { type: 'application/octet-stream' });\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        // Store tiny reference in IndexedDB\n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'external';\n          this.emitEvent('stored-external', { key, sizeBytes });\n        }\n        break;\n    }\n  }\n  \n  private async evictFromIndexedDB(\n    requiredBytes: number, \n    useLRU: boolean\n  ): Promise<void> {\n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'indexeddb' &&\n        (!this.config.evictionPreferences.protectPinned || !m.pinned)\n      );\n    \n    // Sort by strategy\n    candidates.sort((a, b) => {\n      if (useLRU) {\n        // LRU: least recently accessed first\n        if (this.config.evictionPreferences.protectFrequent) {\n          if (a.accessCount !== b.accessCount) {\n            return a.accessCount - b.accessCount;\n          }\n        }\n        return a.lastAccessedAt - b.lastAccessedAt;\n      } else {\n        // FIFO: oldest creation time first\n        return a.createdAt - b.createdAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const metadata of candidates) {\n      toDelete.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (toDelete.length === 0) {\n      throw new Error('Cannot evict: all items are protected');\n    }\n    \n    // Delete entries\n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n        this.emitEvent('evicted', { \n          key, \n          from: 'indexeddb', \n          to: 'deleted',\n          reason: useLRU ? 'lru' : 'fifo'\n        });\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    // Update access tracking\n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    let data: any;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        data = this.memoryCache.get(key)?.data;\n        if (!data) {\n          // Fallback to IndexedDB\n          data = await this.retrieveFromIndexedDB(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        data = await this.retrieveFromIndexedDB(key);\n        \n        // Promote to memory if: small + accessed frequently\n        if (metadata.sizeBytes < 1024 * 1024 && \n            metadata.accessCount >= 3 &&\n            Date.now() - metadata.createdAt < 60000) {\n          \n          if (await this.tryStoreInMemory(key, data, metadata.sizeBytes)) {\n            metadata.tier = 'memory';\n            this.emitEvent('promoted', { key, to: 'memory' });\n          }\n        }\n        break;\n        \n      case 'external':\n        data = await this.retrieveFromExternalBlob(key);\n        break;\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n    \n    return data;\n  }\n  \n  async pin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = true;\n      this.emitEvent('pinned', { key });\n    }\n  }\n  \n  async unpin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = false;\n      this.emitEvent('unpinned', { key });\n    }\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found (may have been cleared): ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  on(event: StorageEventType, listener: StorageEventListener): void {\n    this.listeners.add(listener);\n  }\n  \n  off(listener: StorageEventListener): void {\n    this.listeners.delete(listener);\n  }\n  \n  private emitEvent(type: string, data: any): void {\n    for (const listener of this.listeners) {\n      listener({ type, data, timestamp: Date.now() });\n    }\n  }\n  \n  private scheduleStorageMonitoring(): void {\n    setInterval(() => {\n      const stats = this.getStorageStats();\n      const totalCapacity = this.config.memoryCacheMaxBytes + this.config.indexedDBMaxBytes;\n      const usagePercent = (stats.total.bytes / totalCapacity) * 100;\n      \n      if (usagePercent >= this.config.warnAtPercent) {\n        this.emitEvent('storage-warning', {\n          usagePercent,\n          stats,\n          message: `Storage ${usagePercent.toFixed(1)}% full`\n        });\n      }\n    }, 30000); // Check every 30s\n  }\n  \n  getStorageStats(): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      stats[metadata.tier as 'memory' | 'indexeddb' | 'external'].count++;\n      if (metadata.tier === 'external') {\n        stats.external.bytes += metadata.sizeBytes;\n      }\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n  \n  // ... compression/decompression methods same as before ...\n  \n  private async compress(data: string, level: 'fast' | 'balanced' | 'best'): Promise<string> {\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 3);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n  \n  private scheduleCleanup(): void {\n    setInterval(() => {\n      const cutoff = Date.now() - this.config.autoCleanupAge;\n      \n      this.clear(metadata => \n        metadata.temporary && !metadata.pinned && metadata.createdAt < cutoff\n      );\n    }, 60 * 60 * 1000); // 1 hour\n  }\n  \n  async clear(predicate?: (metadata: ArtifactMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      // Clear everything\n      this.memoryCache.clear();\n      this.memoryCacheSize = 0;\n      this.indexedDBSize = 0;\n      this.artifactIndex.clear();\n      \n      const tx = this.indexedDB.transaction(['artifacts', 'blobRefs'], 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      await tx.objectStore('blobRefs').clear();\n      \n      // Revoke blob URLs\n      for (const blobUrl of this.externalBlobs.values()) {\n        URL.revokeObjectURL(blobUrl);\n      }\n      this.externalBlobs.clear();\n      \n      return;\n    }\n    \n    // Selective clear\n    const toDelete: string[] = [];\n    \n    for (const [key, metadata] of this.artifactIndex.entries()) {\n      if (predicate(metadata)) {\n        toDelete.push(key);\n      }\n    }\n    \n    for (const key of toDelete) {\n      await this.delete(key);\n    }\n  }\n  \n  private async delete(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) return;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        const entry = this.memoryCache.get(key);\n        if (entry) {\n          this.memoryCacheSize -= entry.sizeBytes;\n          this.memoryCache.delete(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        this.indexedDBSize -= metadata.sizeBytes;\n        break;\n        \n      case 'external':\n        const blobUrl = this.externalBlobs.get(key);\n        if (blobUrl) {\n          URL.revokeObjectURL(blobUrl);\n          this.externalBlobs.delete(key);\n        }\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').delete(key);\n        break;\n    }\n    \n    this.artifactIndex.delete(key);\n  }\n}\n\n// Types\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  pinned?: boolean;\n  nodeId?: string;\n  executionId?: string;\n  dataType?: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n\ntype StorageEventType = 'stored' | 'evicted' | 'promoted' | 'compressed' | \n                        'stored-external' | 'pinned' | 'unpinned' | 'storage-warning';\n\ntype StorageEventListener = (event: { \n  type: string; \n  data: any; \n  timestamp: number \n}) => void;\n\nclass StorageFullError extends Error {\n  constructor(message: string, public stats: StorageStats) {\n    super(message);\n    this.name = 'StorageFullError';\n  }\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes}B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(2)}KB`;\n  return `${(bytes / 1024 / 1024).toFixed(2)}MB`;\n}\n```\n\n## Stream Registry for Tee Management\n\n### Key Insight: Active Streams Need Separate Lifecycle Management\n\n```typescript\ninterface StreamRegistryConfig {\n  maxConcurrentStreams: number;     // 50 - prevent memory exhaustion\n  streamTimeout: number;             // 5min - auto-cleanup stale streams\n  enableMetrics: boolean;            // Track tee vs materialize decisions\n}\n\ninterface StreamMetadata {\n  id: string;\n  sourceNodeId: string;\n  executionId: string;\n  createdAt: number;\n  lastAccessedAt: number;\n  consumerCount: number;\n  estimatedSize: number;\n  strategy: 'tee' | 'materialize';\n  status: 'active' | 'completed' | 'error' | 'timeout';\n}\n\nclass StreamRegistry {\n  private activeStreams: Map<string, {\n    metadata: StreamMetadata;\n    streams: ReadableStream[];\n    readers: Set<ReadableStreamDefaultReader>;\n  }> = new Map();\n  \n  private config: StreamRegistryConfig;\n  private metrics: StreamMetrics;\n  \n  constructor(config: StreamRegistryConfig) {\n    this.config = config;\n    this.metrics = { teeCount: 0, materializeCount: 0, totalStreams: 0 };\n    this.startTimeoutMonitor();\n  }\n  \n  async registerTeedStream(\n    sourceNodeId: string,\n    executionId: string,\n    originalStream: ReadableStream,\n    consumerCount: number,\n    estimatedSize: number\n  ): Promise<string> {\n    \n    if (this.activeStreams.size >= this.config.maxConcurrentStreams) {\n      throw new Error(\n        `Max concurrent streams reached (${this.config.maxConcurrentStreams}). ` +\n        'Consider using materialization strategy.'\n      );\n    }\n    \n    const streamId = crypto.randomUUID();\n    \n    // Create teed streams based on consumer count\n    const teedStreams = this.createMultiTee(originalStream, consumerCount);\n    \n    this.activeStreams.set(streamId, {\n      metadata: {\n        id: streamId,\n        sourceNodeId,\n        executionId,\n        createdAt: Date.now(),\n        lastAccessedAt: Date.now(),\n        consumerCount,\n        estimatedSize,\n        strategy: 'tee',\n        status: 'active'\n      },\n      streams: teedStreams,\n      readers: new Set()\n    });\n    \n    if (this.config.enableMetrics) {\n      this.metrics.teeCount++;\n      this.metrics.totalStreams++;\n    }\n    \n    return streamId;\n  }\n  \n  private createMultiTee(\n    stream: ReadableStream, \n    count: number\n  ): ReadableStream[] {\n    if (count === 1) return [stream];\n    if (count === 2) return stream.tee();\n    \n    // For 3+ consumers, tee iteratively\n    const streams: ReadableStream[] = [];\n    let remaining = stream;\n    \n    for (let i = 0; i < count - 1; i++) {\n      const [stream1, stream2] = remaining.tee();\n      streams.push(stream1);\n      remaining = stream2;\n    }\n    streams.push(remaining);\n    \n    return streams;\n  }\n  \n  async getStream(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStream> {\n    const entry = this.activeStreams.get(streamId);\n    \n    if (!entry) {\n      throw new Error(`Stream not found: ${streamId}`);\n    }\n    \n    if (consumerIndex >= entry.streams.length) {\n      throw new Error(\n        `Invalid consumer index ${consumerIndex} ` +\n        `(max: ${entry.streams.length - 1})`\n      );\n    }\n    \n    entry.metadata.lastAccessedAt = Date.now();\n    \n    return entry.streams[consumerIndex];\n  }\n  \n  async acquireReader(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStreamDefaultReader> {\n    const stream = await this.getStream(streamId, consumerIndex);\n    const reader = stream.getReader();\n    \n    const entry = this.activeStreams.get(streamId);\n    if (entry) {\n      entry.readers.add(reader);\n    }\n    \n    return reader;\n  }\n  \n  async releaseReader(\n    streamId: string, \n    reader: ReadableStreamDefaultReader\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    try {\n      reader.releaseLock();\n    } catch {\n      // Already released\n    }\n    \n    entry.readers.delete(reader);\n    \n    // Check if all consumers are done\n    if (entry.readers.size === 0) {\n      await this.cleanup(streamId, 'completed');\n    }\n  }\n  \n  async markError(streamId: string, error: Error): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    entry.metadata.status = 'error';\n    \n    // Release all readers\n    for (const reader of entry.readers) {\n      try {\n        await reader.cancel(error.message);\n      } catch {\n        // Ignore\n      }\n    }\n    \n    await this.cleanup(streamId, 'error');\n  }\n  \n  private async cleanup(\n    streamId: string, \n    reason: 'completed' | 'error' | 'timeout'\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    entry.metadata.status = reason as any;\n    \n    // Cancel any remaining streams\n    for (const stream of entry.streams) {\n      try {\n        const reader = stream.getReader();\n        await reader.cancel(`Cleanup: ${reason}`);\n        reader.releaseLock();\n      } catch {\n        // Stream may already be locked or closed\n      }\n    }\n    \n    this.activeStreams.delete(streamId);\n  }\n  \n  private startTimeoutMonitor(): void {\n    setInterval(() => {\n      const now = Date.now();\n      const timeout = this.config.streamTimeout;\n      \n      for (const [streamId, entry] of this.activeStreams.entries()) {\n        if (now - entry.metadata.lastAccessedAt > timeout) {\n          console.warn(`Stream timeout: ${streamId}`);\n          this.cleanup(streamId, 'timeout');\n        }\n      }\n    }, 60000); // Check every minute\n  }\n  \n  getMetrics(): StreamMetrics & { active: number } {\n    return {\n      ...this.metrics,\n      active: this.activeStreams.size\n    };\n  }\n  \n  getStreamInfo(streamId: string): StreamMetadata | null {\n    return this.activeStreams.get(streamId)?.metadata ?? null;\n  }\n  \n  listActiveStreams(): StreamMetadata[] {\n    return Array.from(this.activeStreams.values())\n      .map(entry => entry.metadata);\n  }\n}\n\ninterface StreamMetrics {\n  teeCount: number;\n  materializeCount: number;\n  totalStreams: number;\n}\n```\n\n## Adaptive Executor with Stream Registry Integration\n\n```typescript\ninterface StreamingConfig {\n  teeThresholdBytes: number;        // 5MB - tee if smaller\n  materializeThresholdBytes: number; // 50MB - always materialize if larger\n  enableAdaptiveTee: boolean;        // Use size estimation for decision\n  maxTeesPerStream: number;          // 3 - max consumers for teeing\n}\n\nclass AdaptiveExecutor {\n  private streamingExecutor: StreamingExecutor;\n  private batchExecutor: BatchExecutor;\n  private storage: HybridStorageManager;\n  private streamRegistry: StreamRegistry;\n  private config: StreamingConfig;\n  \n  constructor(\n    storage: HybridStorageManager, \n    streamRegistry: StreamRegistry,\n    config: StreamingConfig\n  ) {\n    this.storage = storage;\n    this.streamRegistry = streamRegistry;\n    this.config = config;\n    this.streamingExecutor = new StreamingExecutor();\n    this.batchExecutor = new BatchExecutor();\n  }\n  \n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): Promise<ExecutionResult> {\n    \n    // Analyze streaming feasibility\n    const decision = this.analyzeStreamingFeasibility(node, inputs, edges);\n    \n    if (decision.shouldStream) {\n      return this.executeWithStreaming(node, inputs, decision);\n    } else {\n      return this.executeWithBatch(node, inputs);\n    }\n  }\n  \n  private analyzeStreamingFeasibility(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): StreamingDecision {\n    \n    const reasons: string[] = [];\n    let shouldStream = false;\n    let multiConsumerStrategy: MultiConsumerStrategy = 'none';\n    \n    // 1. Explicit node config\n    if (node.data.config.streaming === false) {\n      return {\n        shouldStream: false,\n        reasons: ['explicit_batch_mode'],\n        multiConsumerStrategy: 'none'\n      };\n    }\n    \n    if (node.data.config.streaming === true) {\n      shouldStream = true;\n      reasons.push('explicit_streaming_mode');\n    }\n    \n    // 2. Stream input detection\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream || v?.$streamRef\n    );\n    \n    if (hasStreamInput) {\n      shouldStream = true;\n      reasons.push('stream_input_detected');\n    }\n    \n    // 3. Large data heuristic\n    const estimatedSize = this.estimateDataSize(inputs);\n    if (estimatedSize > 5 * 1024 * 1024) {\n      shouldStream = true;\n      reasons.push('large_input_heuristic');\n    }\n    \n    // 4. Agent type benefits\n    if (['researcher', 'coder', 'reviewer'].includes(node.data.type)) {\n      shouldStream = true;\n      reasons.push('agent_benefits_from_streaming');\n    }\n    \n    // 5. Multi-consumer analysis\n    if (shouldStream) {\n      const downstreamCount = edges.filter(e => e.source === node.id).length;\n      \n      if (downstreamCount > 1) {\n        multiConsumerStrategy = this.selectMultiConsumerStrategy(\n          estimatedSize,\n          downstreamCount\n        );\n        \n        reasons.push(`multi_consumer_${multiConsumerStrategy}`);\n      }\n    }\n    \n    return {\n      shouldStream,\n      reasons,\n      multiConsumerStrategy,\n      estimatedSize\n    };\n  }\n  \n  private selectMultiConsumerStrategy(\n    estimatedSize: number,\n    consumerCount: number\n  ): MultiConsumerStrategy {\n    \n    if (!this.config.enableAdaptiveTee) {\n      return 'materialize';\n    }\n    \n    // Large streams: always materialize\n    if (estimatedSize > this.config.materializeThresholdBytes) {\n      return 'materialize';\n    }\n    \n    // Small streams: tee is efficient\n    if (estimatedSize < this.config.teeThresholdBytes) {\n      return consumerCount <= this.config.maxTeesPerStream ? 'tee' : 'materialize';\n    }\n    \n    // Medium streams: depends on consumer count\n    if (consumerCount <= 2) {\n      return 'tee';\n    } else {\n      return 'materialize';\n    }\n  }\n  \n  private async executeWithStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const outputStream = await this.streamingExecutor.executeNodeStreaming(\n      node,\n      inputs\n    );\n    \n    // Handle multi-consumer scenario\n    if (decision.multiConsumerStrategy === 'materialize') {\n      return await this.materializeStreamToStorage(node, outputStream, decision);\n    } else if (decision.multiConsumerStrategy === 'tee') {\n      return await this.teeStreamInRegistry(node, outputStream, decision);\n    }\n    \n    // Single consumer: pass stream through\n    return {\n      nodeId: node.id,\n      output: outputStream,\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'none',\n        estimatedSize: decision.estimatedSize\n      }\n    };\n  }\n  \n  private async teeStreamInRegistry(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const downstreamCount = 2; // Get actual count from edges\n    \n    const streamId = await this.streamRegistry.registerTeedStream(\n      node.id,\n      'current-execution-id', // Get from context\n      stream,\n      downstreamCount,\n      decision.estimatedSize\n    );\n    \n    // Return stream reference\n    return {\n      nodeId: node.id,\n      output: { \n        $streamRef: streamId,\n        consumerCount: downstreamCount\n      },\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'tee',\n        streamId,\n        estimatedSize: decision.estimatedSize\n      }\n    };\n  }\n  \n  private async materializeStreamToStorage(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const chunks: any[] = [];\n    const reader = stream.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        chunks.push(value);\n      }\n    } finally {\n      reader.releaseLock();\n    }\n    \n    const materialized = this.combineChunks(chunks);\n    \n    const storageKey = `node-${node.id}-materialized`;\n    await this.storage.store(storageKey, materialized, {\n      nodeId: node.id,\n      temporary: true,\n      dataType: 'materialized'\n    });\n    \n    if (this.streamRegistry['config'].enableMetrics) {\n      this.streamRegistry['metrics'].materializeCount++;\n    }\n    \n    return {\n      nodeId: node.id,\n      output: { $ref: storageKey },\n      streaming: false,\n      metadata: {\n        materialized: true,\n        storageKey,\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'materialize',\n        actualSize: new TextEncoder().encode(JSON.stringify(materialized)).length\n      }\n    };\n  }\n  \n  private async executeWithBatch(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    const resolvedInputs = await this.resolveInputs(inputs);\n    const output = await this.batchExecutor.executeNodeBatch(node, resolvedInputs);\n    \n    return {\n      nodeId: node.id,\n      output,\n      streaming: false,\n      metadata: {}\n    };\n  }\n  \n  private async resolveInputs(inputs: Record<string, any>): Promise<Record<string, any>> {\n    const resolved: Record<string, any> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      if (value && typeof value === 'object') {\n        if (value.$ref) {\n          // Storage reference\n          resolved[key] = await this.storage.retrieve(value.$ref);\n        } else if (value.$streamRef) {\n          // Stream registry reference - get first available stream\n          resolved[key] = await this.streamRegistry.getStream(value.$streamRef, 0);\n        } else {\n          resolved[key] = value;\n        }\n      } else {\n        resolved[key] = value;\n      }\n    }\n    \n    return resolved;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private combineChunks(chunks: any[]): any {\n    if (chunks.length === 0) return null;\n    \n    if (typeof chunks[0] === 'string') {\n      return chunks.join('');\n    }\n    \n    if (Array.isArray(chunks[0])) {\n      return chunks.flat();\n    }\n    \n    return chunks[chunks.length - 1];\n  }\n}\n\ntype MultiConsumerStrategy = 'none' | 'tee' | 'materialize';\n\ninterface StreamingDecision {\n  shouldStream: boolean;\n  reasons: string[];\n  multiConsumerStrategy: MultiConsumerStrategy;\n  estimatedSize: number;\n}\n\ninterface ExecutionResult {\n  nodeId: string;\n  output: any | ReadableStream | { $ref: string } | { $streamRef: string };\n  streaming: boolean;\n  metadata: Record<string, any>;\n}\n\ninterface WorkflowNode {\n  id: string;\n  data: {\n    type: string;\n    config: {\n      streaming?: boolean;\n      [key: string]: any;\n    };\n  };\n}\n\ninterface WorkflowEdge {\n  source: string;\n  target: string;\n}\n```\n\n## Composable Expression System with Clear Syntax\n\n### Key Change: Inline Block Markers for Composite Mode\n\n```typescript\n// Expression syntax for composite mode:\n// Use {{block:type:config}} markers inline with text\n\n// Examples:\n// \"$.users[age > 18].{{block:aggregate:sum:salary}}\"\n// \"{{block:field:items}}[price > 100].name\"\n// \"$.data | {{block:transform:normalize}} | {{block:filter:active}}\"\n\ninterface ExpressionEngine {\n  evaluate(expression: Expression, context: any): Promise<any>;\n  validate(expression: Expression): ValidationResult;\n  parseComposite(text: string): ExpressionPart[];\n  serializeComposite(parts: ExpressionPart[]): string;\n}\n\ntype Expression = \n  | { mode: 'text'; value: string }\n  | { mode: 'visual'; blocks: ExpressionBlock[] }\n  | { mode: 'composite'; parts: ExpressionPart[] };\n\ntype ExpressionPart =\n  | { type: 'text'; value: string }\n  | { type: 'block'; block: ExpressionBlock };\n\ninterface ExpressionBlock {\n  id: string;\n  type: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n}\n\nclass ComposableExpressionEngine implements ExpressionEngine {\n  private jsonataEngine: JSONataEngine;\n  private blockMarkerRegex = /\\{\\{block:([^:}]+):([^}]+)\\}\\}/g;\n  \n  constructor() {\n    this.jsonataEngine = new JSONataEngine();\n  }\n  \n  parseComposite(text: string): ExpressionPart[] {\n    const parts: ExpressionPart[] = [];\n    let lastIndex = 0;\n    \n    const matches = text.matchAll(this.blockMarkerRegex);\n    \n    for (const match of matches) {\n      // Add text before block\n      if (match.index! > lastIndex) {\n        const textPart = text.slice(lastIndex, match.index);\n        if (textPart) {\n          parts.push({ type: 'text', value: textPart });\n        }\n      }\n      \n      // Parse block\n      const [_, blockType, configStr] = match;\n      const config = this.parseBlockConfig(blockType, configStr);\n      \n      parts.push({\n        type: 'block',\n        block: {\n          id: crypto.randomUUID(),\n          type: blockType as any,\n          config\n        }\n      });\n      \n      lastIndex = match.index! + match[0].length;\n    }\n    \n    // Add remaining text\n    if (lastIndex < text.length) {\n      parts.push({ type: 'text', value: text.slice(lastIndex) });\n    }\n    \n    return parts;\n  }\n  \n  serializeComposite(parts: ExpressionPart[]): string {\n    return parts.map(part => {\n      if (part.type === 'text') {\n        return part.value;\n      } else {\n        const block = part.block;\n        const configStr = this.serializeBlockConfig(block.type, block.config);\n        return `{{block:${block.type}:${configStr}}}`;\n      }\n    }).join('');\n  }\n  \n  private parseBlockConfig(type: string, configStr: string): Record<string, any> {\n    // Simple key:value parsing\n    const pairs = configStr.split(':');\n    \n    switch (type) {\n      case 'field':\n        return { path: pairs[0] };\n      case 'filter':\n        return { condition: pairs[0] };\n      case 'aggregate':\n        return { function: pairs[0], path: pairs[1] };\n      case 'transform':\n        return { operation: pairs[0] };\n      default:\n        return {};\n    }\n  }\n  \n  private serializeBlockConfig(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return config.path;\n      case 'filter':\n        return config.condition;\n      case 'aggregate':\n        return `${config.function}:${config.path}`;\n      case 'transform':\n        return config.operation;\n      default:\n        return '';\n    }\n  }\n  \n  async evaluate(expression: Expression, context: any): Promise<any> {\n    switch (expression.mode) {\n      case 'text':\n        return this.jsonataEngine.evaluate(expression.value, context);\n        \n      case 'visual':\n        const jsonataFromBlocks = this.blocksToJSONata(expression.blocks);\n        return this.jsonataEngine.evaluate(jsonataFromBlocks, context);\n        \n      case 'composite':\n        const jsonataFromComposite = this.compositeToJSONata(expression.parts);\n        return this.jsonataEngine.evaluate(jsonataFromComposite, context);\n        \n      default:\n        throw new Error(`Unknown expression mode: ${(expression as any).mode}`);\n    }\n  }\n  \n  validate(expression: Expression): ValidationResult {\n    try {\n      let jsonataExpr: string;\n      \n      switch (expression.mode) {\n        case 'text':\n          jsonataExpr = expression.value;\n          break;\n        case 'visual':\n          jsonataExpr = this.blocksToJSONata(expression.blocks);\n          break;\n        case 'composite':\n          jsonataExpr = this.compositeToJSONata(expression.parts);\n          break;\n        default:\n          return { valid: false, error: 'Unknown expression mode' };\n      }\n      \n      return this.jsonataEngine.validate(jsonataExpr);\n      \n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check expression syntax'\n      };\n    }\n  }\n  \n  private blocksToJSONata(blocks: ExpressionBlock[]): string {\n    return blocks.map(block => this.blockToJSONata(block)).join('');\n  }\n  \n  private compositeToJSONata(parts: ExpressionPart[]): string {\n    return parts.map(part => {\n      if (part.type === 'text') {\n        return part.value;\n      } else {\n        return this.blockToJSONata(part.block);\n      }\n    }).join('');\n  }\n  \n  private blockToJSONata(block: ExpressionBlock): string {\n    switch (block.type) {\n      case 'field':\n        return `$.${block.config.path}`;\n      case 'filter':\n        return `[${block.config.condition}]`;\n      case 'map':\n        return `.{ ${block.config.mapping} }`;\n      case 'aggregate':\n        return `$${block.config.function}(${block.config.path})`;\n      case 'transform':\n        return block.config.expression || block.config.operation;\n      default:\n        throw new Error(`Unknown block type: ${block.type}`);\n    }\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return [\n      {\n        category: 'Text Mode',\n        expression: { mode: 'text', value: '$.user.name' },\n        description: 'Simple field extraction',\n        inputExample: { user: { name: 'Alice' } },\n        outputExample: 'Alice'\n      },\n      {\n        category: 'Composite Mode',\n        expression: {\n          mode: 'composite',\n          parts: this.parseComposite(\n            '$.items[price > 100].{{block:aggregate:sum:price}}'\n          )\n        },\n        description: 'Mix text and visual blocks',\n        inputExample: { items: [{ price: 50 }, { price: 150 }, { price: 200 }] },\n        outputExample: 350\n      }\n    ];\n  }\n}\n\nclass JSONataEngine {\n  private cache: Map<string, any> = new Map();\n  private readonly timeout = 5000;\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      // In real implementation, use actual JSONata library\n      compiled = { evaluate: (ctx: any) => this.simpleEval(expression, ctx) };\n      this.cache.set(expression, compiled);\n    }\n    \n    return await Promise.race([\n      Promise.resolve(compiled.evaluate(context)),\n      new Promise((_, reject) => \n        setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n      )\n    ]);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      if (!expression || expression.trim().length === 0) {\n        return { valid: false, error: 'Empty expression' };\n      }\n      \n      return { valid: true };\n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check JSONata syntax'\n      };\n    }\n  }\n  \n  private simpleEval(expression: string, context: any): any {\n    if (expression.startsWith('$.')) {\n      const path = expression.substring(2).split('.');\n      let result = context;\n      for (const key of path) {\n        result = result?.[key];\n      }\n      return result;\n    }\n    \n    return null;\n  }\n}\n\n// UI Component\nconst ComposableExpressionEditor: React.FC<{\n  value: Expression;\n  onChange: (value: Expression) => void;\n}> = ({ value, onChange }) => {\n  \n  const [mode, setMode] = useState<'text' | 'visual' | 'composite'>(value.mode);\n  const engine = useMemo(() => new ComposableExpressionEngine(), []);\n  const [validation, setValidation] = useState<ValidationResult>({ valid: true });\n  \n  useEffect(() => {\n    setValidation(engine.validate(value));\n  }, [value, engine]);\n  \n  const handleTextChange = (text: string) => {\n    if (mode === 'composite') {\n      // Parse inline block markers\n      const parts = engine.parseComposite(text);\n      onChange({ mode: 'composite', parts });\n    } else {\n      onChange({ mode: 'text', value: text });\n    }\n  };\n  \n  const insertBlock = (blockType: string) => {\n    if (mode !== 'composite' || value.mode !== 'composite') return;\n    \n    const serialized = engine.serializeComposite(value.parts);\n    const marker = `{{block:${blockType}:}}`;\n    \n    // Insert at cursor position (simplified)\n    const newText = serialized + marker;\n    const parts = engine.parseComposite(newText);\n    onChange({ mode: 'composite', parts });\n  };\n  \n  return (\n    <div className=\"expression-editor\">\n      <div className=\"mode-selector\">\n        <button \n          onClick={() => setMode('text')}\n          className={mode === 'text' ? 'active' : ''}\n        >\n          Text\n        </button>\n        <button \n          onClick={() => setMode('visual')}\n          className={mode === 'visual' ? 'active' : ''}\n        >\n          Visual\n        </button>\n        <button \n          onClick={() => setMode('composite')}\n          className={mode === 'composite' ? 'active' : ''}\n        >\n          Mixed\n        </button>\n      </div>\n      \n      {(mode === 'text' || mode === 'composite') && (\n        <div>\n          <textarea\n            value={\n              value.mode === 'text' \n                ? value.value \n                : value.mode === 'composite'\n                ? engine.serializeComposite(value.parts)\n                : ''\n            }\n            onChange={e => handleTextChange(e.target.value)}\n            className={validation.valid ? '' : 'error'}\n            placeholder={\n              mode === 'composite'\n                ? 'Enter expression with {{block:type:config}} markers'\n                : 'Enter JSONata expression'\n            }\n          />\n          \n          {mode === 'composite' && (\n            <div className=\"block-palette\">\n              <button onClick={() => insertBlock('field')}>+ Field</button>\n              <button onClick={() => insertBlock('filter')}>+ Filter</button>\n              <button onClick={() => insertBlock('aggregate')}>+ Aggregate</button>\n            </div>\n          )}\n        </div>\n      )}\n      \n      {mode === 'visual' && value.mode === 'visual' && (\n        <VisualBlockEditor\n          blocks={value.blocks}\n          onChange={blocks => onChange({ mode: 'visual', blocks })}\n        />\n      )}\n      \n      {!validation.valid && (\n        <div className=\"error-message\">\n          {validation.error}\n          {validation.suggestion && <div className=\"suggestion\">{validation.suggestion}</div>}\n        </div>\n      )}\n    </div>\n  );\n};\n\nconst VisualBlockEditor: React.FC<{\n  blocks: ExpressionBlock[];\n  onChange: (blocks: ExpressionBlock[]) => void;\n}> = ({ blocks, onChange }) => {\n  return <div>Visual block editor</div>;\n};\n\ninterface ValidationResult {\n  valid: boolean;\n  error?: string;\n  suggestion?: string;\n}\n\ninterface ExpressionExample {\n  category: string;\n  expression: Expression;\n  description: string;\n  inputExample: any;\n  outputExample: any;\n}\n```\n\n## Serializable Compensation Handlers\n\n### Key Change: Handlers as Declarative Config, Registered at Workflow Level\n\n```typescript\n// Handlers are now declarative configurations, not code\ntype CompensationHandlerConfig = {\n  id: string;\n  name: string;\n  handlerType: 'builtin' | 'http-api' | 'webhook' | 'custom';\n  \n  // Matching rules\n  matches: {\n    operation?: string[];        // Match specific operations\n    description?: string[];      // Regex patterns for description\n    service?: string[];          // Service names\n  };\n  \n  // Compensation behavior\n  compensation: \n    | { type: 'http-api'; config: HttpApiConfig }\n    | { type: 'webhook'; config: WebhookConfig }\n    | { type: 'custom'; className: string }  // For advanced users\n    | { type: 'manual'; steps: string[] };\n};\n\ninterface HttpApiConfig {\n  method: 'GET' | 'POST' | 'DELETE' | 'PUT';\n  urlTemplate: string;           // \"https://api.stripe.com/v1/refunds\"\n  headers: Record<string, string>;\n  bodyTemplate: string;          // JSON with {{placeholders}}\n  auth?: {\n    type: 'bearer' | 'basic' | 'api-key';\n    tokenPath: string;           // Path to token in effect data\n  };\n}\n\ninterface WebhookConfig {\n  url: string;\n  method: 'POST' | 'PUT';\n  payloadTemplate: string;\n}\n\nclass CompensationHandlerRegistry {\n  private handlers: Map<string, CompensationHandler> = new Map();\n  private configs: Map<string, CompensationHandlerConfig> = new Map();\n  \n  // Register from declarative config\n  registerFromConfig(config: CompensationHandlerConfig): void {\n    this.configs.set(config.id, config);\n    \n    const handler = this.createHandlerFromConfig(config);\n    this.handlers.set(config.id, handler);\n  }\n  \n  private createHandlerFromConfig(\n    config: CompensationHandlerConfig\n  ): CompensationHandler {\n    \n    return {\n      name: config.name,\n      \n      canHandle: (entry: SideEffectEntry) => {\n        const { matches } = config;\n        \n        if (matches.operation && !matches.operation.includes(entry.operation)) {\n          return false;\n        }\n        \n        if (matches.description) {\n          const descMatch = matches.description.some(pattern => \n            new RegExp(pattern).test(entry.description)\n          );\n          if (!descMatch) return false;\n        }\n        \n        if (matches.service) {\n          const descriptor = entry.compensation as any;\n          if (!matches.service.includes(descriptor.service)) {\n            return false;\n          }\n        }\n        \n        return true;\n      },\n      \n      compensate: async (entry: SideEffectEntry) => {\n        switch (config.compensation.type) {\n          case 'http-api':\n            return this.compensateViaHttpApi(entry, config.compensation.config);\n            \n          case 'webhook':\n            return this.compensateViaWebhook(entry, config.compensation.config);\n            \n          case 'custom':\n            // Load custom class (advanced)\n            const CustomHandler = await this.loadCustomHandler(\n              config.compensation.className\n            );\n            return new CustomHandler().compensate(entry);\n            \n          case 'manual':\n            return {\n              success: false,\n              requiresUserAction: true,\n              manualSteps: config.compensation.steps\n            };\n            \n          default:\n            throw new Error(`Unknown compensation type: ${(config.compensation as any).type}`);\n        }\n      }\n    };\n  }\n  \n  private async compensateViaHttpApi(\n    entry: SideEffectEntry,\n    config: HttpApiConfig\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    \n    // Resolve URL template\n    const url = this.resolveTemplate(config.urlTemplate, {\n      ...descriptor,\n      effectId: entry.id\n    });\n    \n    // Resolve headers\n    const headers = { ...config.headers };\n    \n    if (config.auth) {\n      const token = this.extractValue(descriptor, config.auth.tokenPath);\n      switch (config.auth.type) {\n        case 'bearer':\n          headers['Authorization'] = `Bearer ${token}`;\n          break;\n        case 'api-key':\n          headers['Authorization'] = token;\n          break;\n      }\n    }\n    \n    // Resolve body\n    const body = config.bodyTemplate\n      ? this.resolveTemplate(config.bodyTemplate, descriptor)\n      : undefined;\n    \n    try {\n      const response = await fetch(url, {\n        method: config.method,\n        headers,\n        body: body ? JSON.stringify(JSON.parse(body)) : undefined\n      });\n      \n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Compensated via ${config.method} ${url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message,\n        requiresUserAction: true,\n        manualSteps: [\n          `Failed to compensate via API: ${error.message}`,\n          `Operation: ${entry.description}`,\n          'Please manually revert'\n        ]\n      };\n    }\n  }\n  \n  private async compensateViaWebhook(\n    entry: SideEffectEntry,\n    config: WebhookConfig\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    const payload = this.resolveTemplate(config.payloadTemplate, {\n      ...descriptor,\n      effectId: entry.id,\n      effectDescription: entry.description\n    });\n    \n    try {\n      const response = await fetch(config.url, {\n        method: config.method,\n        headers: { 'Content-Type': 'application/json' },\n        body: payload\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Webhook failed: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Webhook triggered: ${config.url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n  \n  private resolveTemplate(template: string, data: any): string {\n    return template.replace(/\\{\\{([^}]+)\\}\\}/g, (_, path) => {\n      return this.extractValue(data, path) ?? '';\n    });\n  }\n  \n  private extractValue(obj: any, path: string): any {\n    const parts = path.split('.');\n    let result = obj;\n    for (const part of parts) {\n      result = result?.[part];\n    }\n    return result;\n  }\n  \n  private async loadCustomHandler(className: string): Promise<any> {\n    // In real implementation, dynamically import class\n    throw new Error('Custom handlers not yet implemented');\n  }\n  \n  // Serialization for workflow persistence\n  serializeConfigs(): CompensationHandlerConfig[] {\n    return Array.from(this.configs.values());\n  }\n  \n  loadConfigs(configs: CompensationHandlerConfig[]): void {\n    for (const config of configs) {\n      this.registerFromConfig(config);\n    }\n  }\n  \n  unregister(id: string): void {\n    this.handlers.delete(id);\n    this.configs.delete(id);\n  }\n  \n  findHandler(entry: SideEffectEntry): CompensationHandler | null {\n    for (const handler of this.handlers.values()) {\n      if (handler.canHandle(entry)) {\n        return handler;\n      }\n    }\n    return null;\n  }\n}\n\n// Example: Stripe handler as config\nconst stripeHandlerConfig: CompensationHandlerConfig = {\n  id: 'stripe-refund',\n  name: 'Stripe Refund Handler',\n  handlerType: 'http-api',\n  \n  matches: {\n    operation: ['payment_process'],\n    description: ['Stripe.*payment'],\n    service: ['stripe']\n  },\n  \n  compensation: {\n    type: 'http-api',\n    config: {\n      method: 'POST',\n      urlTemplate: 'https://api.stripe.com/v1/refunds',\n      headers: {\n        'Content-Type': 'application/x-www-form-urlencoded'\n      },\n      bodyTemplate: JSON.stringify({\n        charge: '{{chargeId}}',\n        reason: 'requested_by_customer'\n      }),\n      auth: {\n        type: 'bearer',\n        tokenPath: 'apiKey'\n      }\n    }\n  }\n};\n\n// Usage\nconst registry = new CompensationHandlerRegistry();\nregistry.registerFromConfig(stripeHandlerConfig);\n\n// Serialize with workflow\nconst workflowData = {\n  nodes: [...],\n  edges: [...],\n  compensationHandlers: registry.serializeConfigs()\n};\n\n// Load from workflow\nregistry.loadConfigs(workflowData.compensationHandlers);\n\ninterface CompensationHandler {\n  name: string;\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n  canHandle(entry: SideEffectEntry): boolean;\n}\n\ninterface SideEffectEntry {\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  operation: string;\n  description: string;\n  compensation: any;\n  status: string;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n## Performance Monitoring for Streaming Decisions\n\n```typescript\ninterface StreamingMetrics {\n  decisions: {\n    teeCount: number;\n    materializeCount: number;\n    batchCount: number;\n  };\n  \n  performance: {\n    avgTeeMemory: number;         // Average memory per tee (bytes)\n    avgMaterializeSize: number;   // Average materialized size (bytes)\n    avgTeeTime: number;           // Average time to complete tee (ms)\n    avgMaterializeTime: number;   // Average time to materialize (ms)\n  };\n  \n  errors: {\n    teeFailures: number;\n    materializeFailures: number;\n    storageFullErrors: number;\n  };\n  \n  recommendations: {\n    suggestedTeeThreshold: number;\n    suggestedMaterializeThreshold: number;\n    suggestedOverflowPolicy: string;\n  };\n}\n\nclass StreamingPerformanceMonitor {\n  private metrics: StreamingMetrics;\n  private measurements: Array<{\n    strategy: 'tee' | 'materialize';\n    sizeBytes: number;\n    durationMs: number;\n    memoryUsed: number;\n    timestamp: number;\n  }> = [];\n  \n  constructor() {\n    this.metrics = this.initMetrics();\n  }\n  \n  recordDecision(\n    strategy: 'tee' | 'materialize' | 'batch',\n    sizeBytes: number,\n    durationMs: number,\n    memoryUsed: number\n  ): void {\n    \n    switch (strategy) {\n      case 'tee':\n        this.metrics.decisions.teeCount++;\n        break;\n      case 'materialize':\n        this.metrics.decisions.materializeCount++;\n        break;\n      case 'batch':\n        this.metrics.decisions.batchCount++;\n        break;\n    }\n    \n    if (strategy === 'tee' || strategy === 'materialize') {\n      this.measurements.push({\n        strategy,\n        sizeBytes,\n        durationMs,\n        memoryUsed,\n        timestamp: Date.now()\n      });\n      \n      // Keep last 100 measurements\n      if (this.measurements.length > 100) {\n        this.measurements.shift();\n      }\n      \n      this.updateAverages();\n      this.generateRecommendations();\n    }\n  }\n  \n  recordError(type: 'tee' | 'materialize' | 'storage-full'): void {\n    switch (type) {\n      case 'tee':\n        this.metrics.errors.teeFailures++;\n        break;\n      case 'materialize':\n        this.metrics.errors.materializeFailures++;\n        break;\n      case 'storage-full':\n        this.metrics.errors.storageFullErrors++;\n        break;\n    }\n  }\n  \n  private updateAverages(): void {\n    const tees = this.measurements.filter(m => m.strategy === 'tee');\n    const materializes = this.measurements.filter(m => m.strategy === 'materialize');\n    \n    if (tees.length > 0) {\n      this.metrics.performance.avgTeeMemory = \n        tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length;\n      this.metrics.performance.avgTeeTime = \n        tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length;\n    }\n    \n    if (materializes.length > 0) {\n      this.metrics.performance.avgMaterializeSize = \n        materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length;\n      this.metrics.performance.avgMaterializeTime = \n        materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length;\n    }\n  }\n  \n  private generateRecommendations(): void {\n    // Analyze patterns to suggest better thresholds\n    \n    // If tees are consistently using too much memory, lower threshold\n    if (this.metrics.performance.avgTeeMemory > 10 * 1024 * 1024) {\n      this.metrics.recommendations.suggestedTeeThreshold = 3 * 1024 * 1024; // 3MB\n    } else {\n      this.metrics.recommendations.suggestedTeeThreshold = 5 * 1024 * 1024; // 5MB\n    }\n    \n    // If materialize operations are fast, raise threshold\n    if (this.metrics.performance.avgMaterializeTime < 100) {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 100 * 1024 * 1024; // 100MB\n    } else {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 50 * 1024 * 1024; // 50MB\n    }\n    \n    // Recommend overflow policy based on errors\n    if (this.metrics.errors.storageFullErrors > 5) {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'evict-lru';\n    } else {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'compress-max';\n    }\n  }\n  \n  getMetrics(): StreamingMetrics {\n    return { ...this.metrics };\n  }\n  \n  getMeasurements(): typeof this.measurements {\n    return [...this.measurements];\n  }\n  \n  private initMetrics(): StreamingMetrics {\n    return {\n      decisions: {\n        teeCount: 0,\n        materializeCount: 0,\n        batchCount: 0\n      },\n      performance: {\n        avgTeeMemory: 0,\n        avgMaterializeSize: 0,\n        avgTeeTime: 0,\n        avgMaterializeTime: 0\n      },\n      errors: {\n        teeFailures: 0,\n        materializeFailures: 0,\n        storageFullErrors: 0\n      },\n      recommendations: {\n        suggestedTeeThreshold: 5 * 1024 * 1024,\n        suggestedMaterializeThreshold: 50 * 1024 * 1024,\n        suggestedOverflowPolicy: 'evict-lru'\n      }\n    };\n  }\n}\n\n// UI Component for Metrics Dashboard\nconst StreamingMetricsDashboard: React.FC<{\n  monitor: StreamingPerformanceMonitor;\n}> = ({ monitor }) => {\n  \n  const [metrics, setMetrics] = useState(monitor.getMetrics());\n  \n  useEffect(() => {\n    const interval = setInterval(() => {\n      setMetrics(monitor.getMetrics());\n    }, 1000);\n    \n    return () => clearInterval(interval);\n  }, [monitor]);\n  \n  return (\n    <div className=\"metrics-dashboard\">\n      <h3>Streaming Performance</h3>\n      \n      <div className=\"metrics-section\">\n        <h4>Decisions</h4>\n        <div>Tees: {metrics.decisions.teeCount}</div>\n        <div>Materializes: {metrics.decisions.materializeCount}</div>\n        <div>Batch: {metrics.decisions.batchCount}</div>\n      </div>\n      \n      <div className=\"metrics-section\">\n        <h4>Performance</h4>\n        <div>Avg Tee Memory: {formatBytes(metrics.performance.avgTeeMemory)}</div>\n        <div>Avg Materialize Size: {formatBytes(metrics.performance.avgMaterializeSize)}</div>\n        <div>Avg Tee Time: {metrics.performance.avgTeeTime.toFixed(2)}ms</div>\n        <div>Avg Materialize Time: {metrics.performance.avgMaterializeTime.toFixed(2)}ms</div>\n      </div>\n      \n      <div className=\"metrics-section\">\n        <h4>Errors</h4>\n        <div>Tee Failures: {metrics.errors.teeFailures}</div>\n        <div>Materialize Failures: {metrics.errors.materializeFailures}</div>\n        <div>Storage Full: {metrics.errors.storageFullErrors}</div>\n      </div>\n      \n      <div className=\"metrics-section recommendations\">\n        <h4>Recommendations</h4>\n        <div>Tee Threshold: {formatBytes(metrics.recommendations.suggestedTeeThreshold)}</div>\n        <div>Materialize Threshold: {formatBytes(metrics.recommendations.suggestedMaterializeThreshold)}</div>\n        <div>Overflow Policy: {metrics.recommendations.suggestedOverflowPolicy}</div>\n      </div>\n    </div>\n  );\n};\n```\n\n---\n\n## What I Improved\n\n### 1. Storage Overflow Policy Naming\n**Changed**: `drop-on-store` \u2192 `fail-when-full`, `drop-oldest` \u2192 `evict-lru`/`evict-oldest`\n- **Why**: \"fail-when-full\" is more explicit about what happens (throws error)\n- **Added**: Separate `evict-lru` (access-based) vs `evict-oldest` (time-based) for clarity\n- **Added**: Eviction preferences (protect recent, frequent, pinned items)\n- **Added**: Storage event system for monitoring\n\n### 2. Stream Registry as First-Class System\n**Changed**: From \"execution context storage\" to dedicated `StreamRegistry`\n- **Why**: Streams have different lifecycle than data artifacts\n- **Added**: Timeout monitoring for abandoned streams\n- **Added**: Multi-tee support (3+ consumers via iterative teeing)\n- **Added**: Reader tracking for proper cleanup\n- **Added**: Stream metrics separate from storage metrics\n\n### 3. Composite Expression Syntax\n**Changed**: From abstract \"mixed mode\" to concrete inline markers\n- **Syntax**: `{{block:type:config}}` embedded in text\n- **Why**: Makes composite mode tangible and editable as text\n- **Added**: `parseComposite()` and `serializeComposite()` for conversion\n- **Added**: Block insertion palette in UI\n\n### 4. Serializable Compensation Handlers\n**Changed**: From TypeScript classes to declarative JSON configs\n- **Why**: Handlers can now be saved with workflows\n- **Format**: HTTP API templates, webhook configs, or custom classes\n- **Added**: Template system with `{{placeholder}}` syntax\n- **Added**: Handler registry persistence (`serializeConfigs()`)\n\n### 5. Performance Monitoring System\n**Added**: Complete streaming performance tracking\n- **Metrics**: Tee vs materialize decisions, memory usage, timing\n- **Analysis**: Auto-generated threshold recommendations\n- **UI**: Real-time dashboard component\n- **Feedback Loop**: Metrics inform threshold tuning\n\n### 6. Enhanced Storage Management\n**Added**: Several production-ready features\n- **Pinning**: Protect critical artifacts from eviction\n- **Events**: Observable storage operations\n- **Warnings**: Proactive alerts at 80% capacity\n- **Error Types**: `StorageFullError` with detailed stats\n\n---\n\n## What I Kept\n\nAll excellent architectural decisions:\n- React Flow canvas with viewport culling\n- Zustand + immer state management\n- Two-tier storage (Memory + IndexedDB)\n- Adaptive streaming with size-based decisions\n- Append-only side effect log\n- Checkpoint manager with strategies\n- Worker pool architecture\n- Compression for large data\n- Auto-cleanup of temporary artifacts\n- All performance optimizations\n\n---\n\n## Remaining Concerns\n\n### 1. Stream Registry Memory Management\nThe stream registry holds `ReadableStream` objects in memory, which can't be serialized. If a page refreshes:\n- **Issue**: All teed streams are lost\n- **Mitigation**: Should we auto-materialize long-lived streams to storage?\n- **Trade-off**: Defeats the purpose of streaming for performance\n\n### 2. Composite Expression Learning Curve\nThe `{{block:type:config}}` syntax is powerful but:\n- **Issue**: Users need to learn the marker format\n- **Mitigation**: Good examples + auto-complete in UI\n- **Alternative**: Click-to-insert blocks that generate markers automatically\n\n### 3. Compensation Handler Security\nHandlers defined in JSON can make arbitrary HTTP calls:\n- **Issue**: Malicious workflows could call unintended APIs\n- **Mitigation**: Require user approval for each handler registration?\n- **Alternative**: Sandboxed execution environment for handlers\n\n### 4. Metrics Storage Size\nTracking 100 measurements per metric can grow large:\n- **Issue**: Metrics themselves consume memory\n- **Mitigation**: Persist to IndexedDB instead of memory\n- **Alternative**: Sample (only keep every Nth measurement)\n\n---\n\n## Convergence Status\n\n**ITERATING** - Good progress, but need architect's input on key design choices:\n\n\u2705 Clearer overflow policy names and eviction strategies  \n\u2705 Dedicated stream registry with lifecycle management  \n\u2705 Concrete composite expression syntax  \n\u2705 Serializable compensation handlers  \n\u2705 Performance monitoring system  \n\n\u2753 Stream persistence strategy on page refresh  \n\u2753 Composite expression UI/UX details  \n\u2753 Compensation handler security model  \n\u2753 Metrics storage and sampling strategy  \n\n---\n\n##",
        "prompt_for_other": "Please focus on these refined areas:\n\n1. **Stream Registry Persistence**: The stream registry holds active `ReadableStream` objects that can't survive page refresh. Should we:\n   - Accept this limitation (users re-run workflows after refresh)?\n   - Auto-materialize streams that live longer than N seconds?\n   - Add a \"save checkpoint\" button that materializes all active streams?\n\n2. **Composite Expression UX**: I've defined the syntax as `{{block:type:config}}`, but how should the UI work?\n   - Should users type markers manually (with autocomplete)?\n   - Should clicking \"Insert Block\" add the marker at cursor position?\n   - Should we show a visual preview of blocks highlighted within the text?\n\n3. **Compensation Handler Trust Model**: Since handlers are declarative configs that make HTTP calls:\n   - Should ALL handlers require explicit user approval when registered?\n   - Should we have \"trusted\" vs \"untrusted\" handler sources?\n   - Should we sandbox handler execution somehow (CSP, web worker)?\n\n4. **Performance Metrics Persistence**: Currently metrics live in memory. Should we:\n   - Persist to IndexedDB for long-term analysis?\n   - Only track metrics in development mode?\n   - Sample aggressively (1 in 10 measurements) to reduce memory?\n\n5. **Eviction Strategy Recommendations**: With both `evict-lru` and `evict-oldest` available:\n   - Should we recommend one as default for most workflows?\n   - Should eviction strategy be per-workflow or global?\n   - Should users be able to mix strategies (LRU for memory, FIFO for IndexedDB)?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v4\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Storage   \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  + Stream  \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502            \u2502  \u2502  Registry  \u2502  \u2502            \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Two-Tier \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  + Overflow\u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Strategy  \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with viewport culling\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with streaming + dedicated stream registry\n- **Storage**: Two-tier (Memory + IndexedDB) with explicit overflow policies\n- **Expressions**: Composable expression system (JSONata + visual blocks)\n- **Side Effects**: Append-only log with serializable compensation handlers\n- **Workers**: Shared Worker pool with resource quotas\n\n## Enhanced Two-Tier Storage with Clear Overflow Policies\n\n### Key Changes: Clearer Naming, Better Eviction Control, Stream-Aware\n\n```typescript\ntype OverflowPolicy = \n  | 'evict-lru'              // Default: Evict least recently used\n  | 'evict-oldest'           // FIFO eviction by creation time\n  | 'fail-when-full'         // Throw error when storage full\n  | 'compress-max'           // Aggressive compression on overflow\n  | 'external-blob';         // Blob URLs (volatile, cleared on reload)\n\ninterface StorageConfig {\n  // Tier capacities\n  memoryCacheMaxBytes: number;      // 100MB - hot in-memory data\n  indexedDBMaxBytes: number;        // 1GB - warm IndexedDB storage\n  \n  // Overflow behavior\n  overflowPolicy: OverflowPolicy;\n  \n  // Eviction preferences (for evict-lru/evict-oldest policies)\n  evictionPreferences: {\n    protectRecent: boolean;         // Don't evict items < 5s old\n    protectFrequent: boolean;       // Prefer evicting low-access items\n    protectPinned: boolean;         // Never evict explicitly pinned items\n  };\n  \n  // Compression\n  compressionThreshold: number;     // 1MB - compress above this\n  compressionLevel: 'fast' | 'balanced' | 'best';\n  \n  // Auto-cleanup\n  autoCleanupAge: number;           // 24h - delete old temp artifacts\n  enableAutoCleanup: boolean;\n  \n  // Storage warnings\n  warnAtPercent: number;            // 80 - warn when storage % full\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'external' | 'determining';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  pinned: boolean;                  // Protected from eviction\n  nodeId?: string;\n  executionId?: string;\n  dataType: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  private config: StorageConfig;\n  \n  // Lightweight metadata for all artifacts\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  // External blob URLs (only for 'external-blob' policy)\n  private externalBlobs: Map<string, string> = new Map();\n  \n  // Storage event listeners\n  private listeners: Set<StorageEventListener> = new Set();\n  \n  constructor(config: StorageConfig) {\n    this.config = config;\n    this.initIndexedDB();\n    \n    if (config.enableAutoCleanup) {\n      this.scheduleCleanup();\n    }\n    \n    // Monitor storage levels\n    this.scheduleStorageMonitoring();\n  }\n  \n  async store(\n    key: string, \n    data: any, \n    hints: StorageHints = {}\n  ): Promise<void> {\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    // Update metadata\n    this.artifactIndex.set(key, {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      pinned: hints.pinned ?? false,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      dataType: hints.dataType ?? 'json'\n    });\n    \n    // Tier decision: Simple size-based\n    if (sizeBytes < 1024 * 1024) { // < 1MB -> try memory first\n      if (await this.tryStoreInMemory(key, data, sizeBytes)) {\n        this.artifactIndex.get(key)!.tier = 'memory';\n        this.emitEvent('stored', { key, tier: 'memory', sizeBytes });\n        return;\n      }\n    }\n    \n    // Otherwise use IndexedDB\n    await this.storeInIndexedDB(key, serialized, sizeBytes);\n    this.artifactIndex.get(key)!.tier = 'indexeddb';\n    this.emitEvent('stored', { key, tier: 'indexeddb', sizeBytes });\n  }\n  \n  private async tryStoreInMemory(\n    key: string, \n    data: any, \n    sizeBytes: number\n  ): Promise<boolean> {\n    if (this.memoryCacheSize + sizeBytes <= this.config.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    // Try to evict from memory\n    if (await this.evictFromMemory(sizeBytes)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(requiredBytes: number): Promise<boolean> {\n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'memory' &&\n        (!this.config.evictionPreferences.protectPinned || !m.pinned) &&\n        (!this.config.evictionPreferences.protectRecent || \n         Date.now() - m.createdAt > 5000)\n      );\n    \n    // Sort by eviction preference\n    candidates.sort((a, b) => {\n      if (this.config.evictionPreferences.protectFrequent) {\n        // Evict less frequently accessed first\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      // Then by LRU\n      return a.lastAccessedAt - b.lastAccessedAt;\n    });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const metadata of candidates) {\n      toEvict.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    // Move to IndexedDB (no data loss)\n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n        this.emitEvent('evicted', { key, from: 'memory', to: 'indexeddb' });\n      }\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(\n    key: string, \n    serialized: string, \n    sizeBytes: number\n  ): Promise<void> {\n    // Check if we're at capacity\n    if (this.indexedDBSize + sizeBytes > this.config.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes);\n      return;\n    }\n    \n    // Compress if needed\n    const shouldCompress = sizeBytes > this.config.compressionThreshold;\n    const toStore = shouldCompress \n      ? await this.compress(serialized, this.config.compressionLevel)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore as string).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    const store = tx.objectStore('artifacts');\n    \n    await store.put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(\n    key: string, \n    serialized: string, \n    sizeBytes: number\n  ): Promise<void> {\n    switch (this.config.overflowPolicy) {\n      case 'evict-lru':\n      case 'evict-oldest':\n        await this.evictFromIndexedDB(\n          sizeBytes, \n          this.config.overflowPolicy === 'evict-lru'\n        );\n        await this.storeInIndexedDB(key, serialized, sizeBytes);\n        break;\n        \n      case 'fail-when-full':\n        const stats = this.getStorageStats();\n        throw new StorageFullError(\n          `Storage full: Cannot store ${key} (${formatBytes(sizeBytes)}). ` +\n          `Usage: ${formatBytes(stats.total.bytes)} / ` +\n          `${formatBytes(this.config.memoryCacheMaxBytes + this.config.indexedDBMaxBytes)}`,\n          stats\n        );\n        \n      case 'compress-max':\n        const compressed = await this.compress(serialized, 'best');\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > this.config.indexedDBMaxBytes) {\n          // Still doesn't fit - evict\n          await this.evictFromIndexedDB(compressedSize, true);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        this.emitEvent('compressed', { key, originalSize: sizeBytes, compressedSize });\n        break;\n        \n      case 'external-blob':\n        const compressed2 = await this.compress(serialized, 'best');\n        const blob = new Blob([compressed2], { type: 'application/octet-stream' });\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        // Store tiny reference in IndexedDB\n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'external';\n          this.emitEvent('stored-external', { key, sizeBytes });\n        }\n        break;\n    }\n  }\n  \n  private async evictFromIndexedDB(\n    requiredBytes: number, \n    useLRU: boolean\n  ): Promise<void> {\n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'indexeddb' &&\n        (!this.config.evictionPreferences.protectPinned || !m.pinned)\n      );\n    \n    // Sort by strategy\n    candidates.sort((a, b) => {\n      if (useLRU) {\n        // LRU: least recently accessed first\n        if (this.config.evictionPreferences.protectFrequent) {\n          if (a.accessCount !== b.accessCount) {\n            return a.accessCount - b.accessCount;\n          }\n        }\n        return a.lastAccessedAt - b.lastAccessedAt;\n      } else {\n        // FIFO: oldest creation time first\n        return a.createdAt - b.createdAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const metadata of candidates) {\n      toDelete.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (toDelete.length === 0) {\n      throw new Error('Cannot evict: all items are protected');\n    }\n    \n    // Delete entries\n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n        this.emitEvent('evicted', { \n          key, \n          from: 'indexeddb', \n          to: 'deleted',\n          reason: useLRU ? 'lru' : 'fifo'\n        });\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    // Update access tracking\n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    let data: any;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        data = this.memoryCache.get(key)?.data;\n        if (!data) {\n          // Fallback to IndexedDB\n          data = await this.retrieveFromIndexedDB(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        data = await this.retrieveFromIndexedDB(key);\n        \n        // Promote to memory if: small + accessed frequently\n        if (metadata.sizeBytes < 1024 * 1024 && \n            metadata.accessCount >= 3 &&\n            Date.now() - metadata.createdAt < 60000) {\n          \n          if (await this.tryStoreInMemory(key, data, metadata.sizeBytes)) {\n            metadata.tier = 'memory';\n            this.emitEvent('promoted', { key, to: 'memory' });\n          }\n        }\n        break;\n        \n      case 'external':\n        data = await this.retrieveFromExternalBlob(key);\n        break;\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n    \n    return data;\n  }\n  \n  async pin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = true;\n      this.emitEvent('pinned', { key });\n    }\n  }\n  \n  async unpin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = false;\n      this.emitEvent('unpinned', { key });\n    }\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    const record = await store.get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found (may have been cleared): ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  on(event: StorageEventType, listener: StorageEventListener): void {\n    this.listeners.add(listener);\n  }\n  \n  off(listener: StorageEventListener): void {\n    this.listeners.delete(listener);\n  }\n  \n  private emitEvent(type: string, data: any): void {\n    for (const listener of this.listeners) {\n      listener({ type, data, timestamp: Date.now() });\n    }\n  }\n  \n  private scheduleStorageMonitoring(): void {\n    setInterval(() => {\n      const stats = this.getStorageStats();\n      const totalCapacity = this.config.memoryCacheMaxBytes + this.config.indexedDBMaxBytes;\n      const usagePercent = (stats.total.bytes / totalCapacity) * 100;\n      \n      if (usagePercent >= this.config.warnAtPercent) {\n        this.emitEvent('storage-warning', {\n          usagePercent,\n          stats,\n          message: `Storage ${usagePercent.toFixed(1)}% full`\n        });\n      }\n    }, 30000); // Check every 30s\n  }\n  \n  getStorageStats(): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      stats[metadata.tier as 'memory' | 'indexeddb' | 'external'].count++;\n      if (metadata.tier === 'external') {\n        stats.external.bytes += metadata.sizeBytes;\n      }\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n  \n  // ... compression/decompression methods same as before ...\n  \n  private async compress(data: string, level: 'fast' | 'balanced' | 'best'): Promise<string> {\n    const encoder = new TextEncoder();\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 3);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n  \n  private scheduleCleanup(): void {\n    setInterval(() => {\n      const cutoff = Date.now() - this.config.autoCleanupAge;\n      \n      this.clear(metadata => \n        metadata.temporary && !metadata.pinned && metadata.createdAt < cutoff\n      );\n    }, 60 * 60 * 1000); // 1 hour\n  }\n  \n  async clear(predicate?: (metadata: ArtifactMetadata) => boolean): Promise<void> {\n    if (!predicate) {\n      // Clear everything\n      this.memoryCache.clear();\n      this.memoryCacheSize = 0;\n      this.indexedDBSize = 0;\n      this.artifactIndex.clear();\n      \n      const tx = this.indexedDB.transaction(['artifacts', 'blobRefs'], 'readwrite');\n      await tx.objectStore('artifacts').clear();\n      await tx.objectStore('blobRefs').clear();\n      \n      // Revoke blob URLs\n      for (const blobUrl of this.externalBlobs.values()) {\n        URL.revokeObjectURL(blobUrl);\n      }\n      this.externalBlobs.clear();\n      \n      return;\n    }\n    \n    // Selective clear\n    const toDelete: string[] = [];\n    \n    for (const [key, metadata] of this.artifactIndex.entries()) {\n      if (predicate(metadata)) {\n        toDelete.push(key);\n      }\n    }\n    \n    for (const key of toDelete) {\n      await this.delete(key);\n    }\n  }\n  \n  private async delete(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) return;\n    \n    switch (metadata.tier) {\n      case 'memory':\n        const entry = this.memoryCache.get(key);\n        if (entry) {\n          this.memoryCacheSize -= entry.sizeBytes;\n          this.memoryCache.delete(key);\n        }\n        break;\n        \n      case 'indexeddb':\n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').delete(key);\n        this.indexedDBSize -= metadata.sizeBytes;\n        break;\n        \n      case 'external':\n        const blobUrl = this.externalBlobs.get(key);\n        if (blobUrl) {\n          URL.revokeObjectURL(blobUrl);\n          this.externalBlobs.delete(key);\n        }\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').delete(key);\n        break;\n    }\n    \n    this.artifactIndex.delete(key);\n  }\n}\n\n// Types\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  pinned?: boolean;\n  nodeId?: string;\n  executionId?: string;\n  dataType?: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n\ntype StorageEventType = 'stored' | 'evicted' | 'promoted' | 'compressed' | \n                        'stored-external' | 'pinned' | 'unpinned' | 'storage-warning';\n\ntype StorageEventListener = (event: { \n  type: string; \n  data: any; \n  timestamp: number \n}) => void;\n\nclass StorageFullError extends Error {\n  constructor(message: string, public stats: StorageStats) {\n    super(message);\n    this.name = 'StorageFullError';\n  }\n}\n\nfunction formatBytes(bytes: number): string {\n  if (bytes < 1024) return `${bytes}B`;\n  if (bytes < 1024 * 1024) return `${(bytes / 1024).toFixed(2)}KB`;\n  return `${(bytes / 1024 / 1024).toFixed(2)}MB`;\n}\n```\n\n## Stream Registry for Tee Management\n\n### Key Insight: Active Streams Need Separate Lifecycle Management\n\n```typescript\ninterface StreamRegistryConfig {\n  maxConcurrentStreams: number;     // 50 - prevent memory exhaustion\n  streamTimeout: number;             // 5min - auto-cleanup stale streams\n  enableMetrics: boolean;            // Track tee vs materialize decisions\n}\n\ninterface StreamMetadata {\n  id: string;\n  sourceNodeId: string;\n  executionId: string;\n  createdAt: number;\n  lastAccessedAt: number;\n  consumerCount: number;\n  estimatedSize: number;\n  strategy: 'tee' | 'materialize';\n  status: 'active' | 'completed' | 'error' | 'timeout';\n}\n\nclass StreamRegistry {\n  private activeStreams: Map<string, {\n    metadata: StreamMetadata;\n    streams: ReadableStream[];\n    readers: Set<ReadableStreamDefaultReader>;\n  }> = new Map();\n  \n  private config: StreamRegistryConfig;\n  private metrics: StreamMetrics;\n  \n  constructor(config: StreamRegistryConfig) {\n    this.config = config;\n    this.metrics = { teeCount: 0, materializeCount: 0, totalStreams: 0 };\n    this.startTimeoutMonitor();\n  }\n  \n  async registerTeedStream(\n    sourceNodeId: string,\n    executionId: string,\n    originalStream: ReadableStream,\n    consumerCount: number,\n    estimatedSize: number\n  ): Promise<string> {\n    \n    if (this.activeStreams.size >= this.config.maxConcurrentStreams) {\n      throw new Error(\n        `Max concurrent streams reached (${this.config.maxConcurrentStreams}). ` +\n        'Consider using materialization strategy.'\n      );\n    }\n    \n    const streamId = crypto.randomUUID();\n    \n    // Create teed streams based on consumer count\n    const teedStreams = this.createMultiTee(originalStream, consumerCount);\n    \n    this.activeStreams.set(streamId, {\n      metadata: {\n        id: streamId,\n        sourceNodeId,\n        executionId,\n        createdAt: Date.now(),\n        lastAccessedAt: Date.now(),\n        consumerCount,\n        estimatedSize,\n        strategy: 'tee',\n        status: 'active'\n      },\n      streams: teedStreams,\n      readers: new Set()\n    });\n    \n    if (this.config.enableMetrics) {\n      this.metrics.teeCount++;\n      this.metrics.totalStreams++;\n    }\n    \n    return streamId;\n  }\n  \n  private createMultiTee(\n    stream: ReadableStream, \n    count: number\n  ): ReadableStream[] {\n    if (count === 1) return [stream];\n    if (count === 2) return stream.tee();\n    \n    // For 3+ consumers, tee iteratively\n    const streams: ReadableStream[] = [];\n    let remaining = stream;\n    \n    for (let i = 0; i < count - 1; i++) {\n      const [stream1, stream2] = remaining.tee();\n      streams.push(stream1);\n      remaining = stream2;\n    }\n    streams.push(remaining);\n    \n    return streams;\n  }\n  \n  async getStream(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStream> {\n    const entry = this.activeStreams.get(streamId);\n    \n    if (!entry) {\n      throw new Error(`Stream not found: ${streamId}`);\n    }\n    \n    if (consumerIndex >= entry.streams.length) {\n      throw new Error(\n        `Invalid consumer index ${consumerIndex} ` +\n        `(max: ${entry.streams.length - 1})`\n      );\n    }\n    \n    entry.metadata.lastAccessedAt = Date.now();\n    \n    return entry.streams[consumerIndex];\n  }\n  \n  async acquireReader(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStreamDefaultReader> {\n    const stream = await this.getStream(streamId, consumerIndex);\n    const reader = stream.getReader();\n    \n    const entry = this.activeStreams.get(streamId);\n    if (entry) {\n      entry.readers.add(reader);\n    }\n    \n    return reader;\n  }\n  \n  async releaseReader(\n    streamId: string, \n    reader: ReadableStreamDefaultReader\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    try {\n      reader.releaseLock();\n    } catch {\n      // Already released\n    }\n    \n    entry.readers.delete(reader);\n    \n    // Check if all consumers are done\n    if (entry.readers.size === 0) {\n      await this.cleanup(streamId, 'completed');\n    }\n  }\n  \n  async markError(streamId: string, error: Error): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    entry.metadata.status = 'error';\n    \n    // Release all readers\n    for (const reader of entry.readers) {\n      try {\n        await reader.cancel(error.message);\n      } catch {\n        // Ignore\n      }\n    }\n    \n    await this.cleanup(streamId, 'error');\n  }\n  \n  private async cleanup(\n    streamId: string, \n    reason: 'completed' | 'error' | 'timeout'\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    entry.metadata.status = reason as any;\n    \n    // Cancel any remaining streams\n    for (const stream of entry.streams) {\n      try {\n        const reader = stream.getReader();\n        await reader.cancel(`Cleanup: ${reason}`);\n        reader.releaseLock();\n      } catch {\n        // Stream may already be locked or closed\n      }\n    }\n    \n    this.activeStreams.delete(streamId);\n  }\n  \n  private startTimeoutMonitor(): void {\n    setInterval(() => {\n      const now = Date.now();\n      const timeout = this.config.streamTimeout;\n      \n      for (const [streamId, entry] of this.activeStreams.entries()) {\n        if (now - entry.metadata.lastAccessedAt > timeout) {\n          console.warn(`Stream timeout: ${streamId}`);\n          this.cleanup(streamId, 'timeout');\n        }\n      }\n    }, 60000); // Check every minute\n  }\n  \n  getMetrics(): StreamMetrics & { active: number } {\n    return {\n      ...this.metrics,\n      active: this.activeStreams.size\n    };\n  }\n  \n  getStreamInfo(streamId: string): StreamMetadata | null {\n    return this.activeStreams.get(streamId)?.metadata ?? null;\n  }\n  \n  listActiveStreams(): StreamMetadata[] {\n    return Array.from(this.activeStreams.values())\n      .map(entry => entry.metadata);\n  }\n}\n\ninterface StreamMetrics {\n  teeCount: number;\n  materializeCount: number;\n  totalStreams: number;\n}\n```\n\n## Adaptive Executor with Stream Registry Integration\n\n```typescript\ninterface StreamingConfig {\n  teeThresholdBytes: number;        // 5MB - tee if smaller\n  materializeThresholdBytes: number; // 50MB - always materialize if larger\n  enableAdaptiveTee: boolean;        // Use size estimation for decision\n  maxTeesPerStream: number;          // 3 - max consumers for teeing\n}\n\nclass AdaptiveExecutor {\n  private streamingExecutor: StreamingExecutor;\n  private batchExecutor: BatchExecutor;\n  private storage: HybridStorageManager;\n  private streamRegistry: StreamRegistry;\n  private config: StreamingConfig;\n  \n  constructor(\n    storage: HybridStorageManager, \n    streamRegistry: StreamRegistry,\n    config: StreamingConfig\n  ) {\n    this.storage = storage;\n    this.streamRegistry = streamRegistry;\n    this.config = config;\n    this.streamingExecutor = new StreamingExecutor();\n    this.batchExecutor = new BatchExecutor();\n  }\n  \n  async executeNode(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): Promise<ExecutionResult> {\n    \n    // Analyze streaming feasibility\n    const decision = this.analyzeStreamingFeasibility(node, inputs, edges);\n    \n    if (decision.shouldStream) {\n      return this.executeWithStreaming(node, inputs, decision);\n    } else {\n      return this.executeWithBatch(node, inputs);\n    }\n  }\n  \n  private analyzeStreamingFeasibility(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    edges: WorkflowEdge[]\n  ): StreamingDecision {\n    \n    const reasons: string[] = [];\n    let shouldStream = false;\n    let multiConsumerStrategy: MultiConsumerStrategy = 'none';\n    \n    // 1. Explicit node config\n    if (node.data.config.streaming === false) {\n      return {\n        shouldStream: false,\n        reasons: ['explicit_batch_mode'],\n        multiConsumerStrategy: 'none'\n      };\n    }\n    \n    if (node.data.config.streaming === true) {\n      shouldStream = true;\n      reasons.push('explicit_streaming_mode');\n    }\n    \n    // 2. Stream input detection\n    const hasStreamInput = Object.values(inputs).some(v => \n      v instanceof ReadableStream || v?.$streamRef\n    );\n    \n    if (hasStreamInput) {\n      shouldStream = true;\n      reasons.push('stream_input_detected');\n    }\n    \n    // 3. Large data heuristic\n    const estimatedSize = this.estimateDataSize(inputs);\n    if (estimatedSize > 5 * 1024 * 1024) {\n      shouldStream = true;\n      reasons.push('large_input_heuristic');\n    }\n    \n    // 4. Agent type benefits\n    if (['researcher', 'coder', 'reviewer'].includes(node.data.type)) {\n      shouldStream = true;\n      reasons.push('agent_benefits_from_streaming');\n    }\n    \n    // 5. Multi-consumer analysis\n    if (shouldStream) {\n      const downstreamCount = edges.filter(e => e.source === node.id).length;\n      \n      if (downstreamCount > 1) {\n        multiConsumerStrategy = this.selectMultiConsumerStrategy(\n          estimatedSize,\n          downstreamCount\n        );\n        \n        reasons.push(`multi_consumer_${multiConsumerStrategy}`);\n      }\n    }\n    \n    return {\n      shouldStream,\n      reasons,\n      multiConsumerStrategy,\n      estimatedSize\n    };\n  }\n  \n  private selectMultiConsumerStrategy(\n    estimatedSize: number,\n    consumerCount: number\n  ): MultiConsumerStrategy {\n    \n    if (!this.config.enableAdaptiveTee) {\n      return 'materialize';\n    }\n    \n    // Large streams: always materialize\n    if (estimatedSize > this.config.materializeThresholdBytes) {\n      return 'materialize';\n    }\n    \n    // Small streams: tee is efficient\n    if (estimatedSize < this.config.teeThresholdBytes) {\n      return consumerCount <= this.config.maxTeesPerStream ? 'tee' : 'materialize';\n    }\n    \n    // Medium streams: depends on consumer count\n    if (consumerCount <= 2) {\n      return 'tee';\n    } else {\n      return 'materialize';\n    }\n  }\n  \n  private async executeWithStreaming(\n    node: WorkflowNode,\n    inputs: Record<string, any>,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const outputStream = await this.streamingExecutor.executeNodeStreaming(\n      node,\n      inputs\n    );\n    \n    // Handle multi-consumer scenario\n    if (decision.multiConsumerStrategy === 'materialize') {\n      return await this.materializeStreamToStorage(node, outputStream, decision);\n    } else if (decision.multiConsumerStrategy === 'tee') {\n      return await this.teeStreamInRegistry(node, outputStream, decision);\n    }\n    \n    // Single consumer: pass stream through\n    return {\n      nodeId: node.id,\n      output: outputStream,\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'none',\n        estimatedSize: decision.estimatedSize\n      }\n    };\n  }\n  \n  private async teeStreamInRegistry(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const downstreamCount = 2; // Get actual count from edges\n    \n    const streamId = await this.streamRegistry.registerTeedStream(\n      node.id,\n      'current-execution-id', // Get from context\n      stream,\n      downstreamCount,\n      decision.estimatedSize\n    );\n    \n    // Return stream reference\n    return {\n      nodeId: node.id,\n      output: { \n        $streamRef: streamId,\n        consumerCount: downstreamCount\n      },\n      streaming: true,\n      metadata: {\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'tee',\n        streamId,\n        estimatedSize: decision.estimatedSize\n      }\n    };\n  }\n  \n  private async materializeStreamToStorage(\n    node: WorkflowNode,\n    stream: ReadableStream,\n    decision: StreamingDecision\n  ): Promise<ExecutionResult> {\n    \n    const chunks: any[] = [];\n    const reader = stream.getReader();\n    \n    try {\n      while (true) {\n        const { done, value } = await reader.read();\n        if (done) break;\n        chunks.push(value);\n      }\n    } finally {\n      reader.releaseLock();\n    }\n    \n    const materialized = this.combineChunks(chunks);\n    \n    const storageKey = `node-${node.id}-materialized`;\n    await this.storage.store(storageKey, materialized, {\n      nodeId: node.id,\n      temporary: true,\n      dataType: 'materialized'\n    });\n    \n    if (this.streamRegistry['config'].enableMetrics) {\n      this.streamRegistry['metrics'].materializeCount++;\n    }\n    \n    return {\n      nodeId: node.id,\n      output: { $ref: storageKey },\n      streaming: false,\n      metadata: {\n        materialized: true,\n        storageKey,\n        streamingReasons: decision.reasons,\n        multiConsumerStrategy: 'materialize',\n        actualSize: new TextEncoder().encode(JSON.stringify(materialized)).length\n      }\n    };\n  }\n  \n  private async executeWithBatch(\n    node: WorkflowNode,\n    inputs: Record<string, any>\n  ): Promise<ExecutionResult> {\n    \n    const resolvedInputs = await this.resolveInputs(inputs);\n    const output = await this.batchExecutor.executeNodeBatch(node, resolvedInputs);\n    \n    return {\n      nodeId: node.id,\n      output,\n      streaming: false,\n      metadata: {}\n    };\n  }\n  \n  private async resolveInputs(inputs: Record<string, any>): Promise<Record<string, any>> {\n    const resolved: Record<string, any> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      if (value && typeof value === 'object') {\n        if (value.$ref) {\n          // Storage reference\n          resolved[key] = await this.storage.retrieve(value.$ref);\n        } else if (value.$streamRef) {\n          // Stream registry reference - get first available stream\n          resolved[key] = await this.streamRegistry.getStream(value.$streamRef, 0);\n        } else {\n          resolved[key] = value;\n        }\n      } else {\n        resolved[key] = value;\n      }\n    }\n    \n    return resolved;\n  }\n  \n  private estimateDataSize(data: any): number {\n    try {\n      return JSON.stringify(data).length;\n    } catch {\n      return 0;\n    }\n  }\n  \n  private combineChunks(chunks: any[]): any {\n    if (chunks.length === 0) return null;\n    \n    if (typeof chunks[0] === 'string') {\n      return chunks.join('');\n    }\n    \n    if (Array.isArray(chunks[0])) {\n      return chunks.flat();\n    }\n    \n    return chunks[chunks.length - 1];\n  }\n}\n\ntype MultiConsumerStrategy = 'none' | 'tee' | 'materialize';\n\ninterface StreamingDecision {\n  shouldStream: boolean;\n  reasons: string[];\n  multiConsumerStrategy: MultiConsumerStrategy;\n  estimatedSize: number;\n}\n\ninterface ExecutionResult {\n  nodeId: string;\n  output: any | ReadableStream | { $ref: string } | { $streamRef: string };\n  streaming: boolean;\n  metadata: Record<string, any>;\n}\n\ninterface WorkflowNode {\n  id: string;\n  data: {\n    type: string;\n    config: {\n      streaming?: boolean;\n      [key: string]: any;\n    };\n  };\n}\n\ninterface WorkflowEdge {\n  source: string;\n  target: string;\n}\n```\n\n## Composable Expression System with Clear Syntax\n\n### Key Change: Inline Block Markers for Composite Mode\n\n```typescript\n// Expression syntax for composite mode:\n// Use {{block:type:config}} markers inline with text\n\n// Examples:\n// \"$.users[age > 18].{{block:aggregate:sum:salary}}\"\n// \"{{block:field:items}}[price > 100].name\"\n// \"$.data | {{block:transform:normalize}} | {{block:filter:active}}\"\n\ninterface ExpressionEngine {\n  evaluate(expression: Expression, context: any): Promise<any>;\n  validate(expression: Expression): ValidationResult;\n  parseComposite(text: string): ExpressionPart[];\n  serializeComposite(parts: ExpressionPart[]): string;\n}\n\ntype Expression = \n  | { mode: 'text'; value: string }\n  | { mode: 'visual'; blocks: ExpressionBlock[] }\n  | { mode: 'composite'; parts: ExpressionPart[] };\n\ntype ExpressionPart =\n  | { type: 'text'; value: string }\n  | { type: 'block'; block: ExpressionBlock };\n\ninterface ExpressionBlock {\n  id: string;\n  type: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n}\n\nclass ComposableExpressionEngine implements ExpressionEngine {\n  private jsonataEngine: JSONataEngine;\n  private blockMarkerRegex = /\\{\\{block:([^:}]+):([^}]+)\\}\\}/g;\n  \n  constructor() {\n    this.jsonataEngine = new JSONataEngine();\n  }\n  \n  parseComposite(text: string): ExpressionPart[] {\n    const parts: ExpressionPart[] = [];\n    let lastIndex = 0;\n    \n    const matches = text.matchAll(this.blockMarkerRegex);\n    \n    for (const match of matches) {\n      // Add text before block\n      if (match.index! > lastIndex) {\n        const textPart = text.slice(lastIndex, match.index);\n        if (textPart) {\n          parts.push({ type: 'text', value: textPart });\n        }\n      }\n      \n      // Parse block\n      const [_, blockType, configStr] = match;\n      const config = this.parseBlockConfig(blockType, configStr);\n      \n      parts.push({\n        type: 'block',\n        block: {\n          id: crypto.randomUUID(),\n          type: blockType as any,\n          config\n        }\n      });\n      \n      lastIndex = match.index! + match[0].length;\n    }\n    \n    // Add remaining text\n    if (lastIndex < text.length) {\n      parts.push({ type: 'text', value: text.slice(lastIndex) });\n    }\n    \n    return parts;\n  }\n  \n  serializeComposite(parts: ExpressionPart[]): string {\n    return parts.map(part => {\n      if (part.type === 'text') {\n        return part.value;\n      } else {\n        const block = part.block;\n        const configStr = this.serializeBlockConfig(block.type, block.config);\n        return `{{block:${block.type}:${configStr}}}`;\n      }\n    }).join('');\n  }\n  \n  private parseBlockConfig(type: string, configStr: string): Record<string, any> {\n    // Simple key:value parsing\n    const pairs = configStr.split(':');\n    \n    switch (type) {\n      case 'field':\n        return { path: pairs[0] };\n      case 'filter':\n        return { condition: pairs[0] };\n      case 'aggregate':\n        return { function: pairs[0], path: pairs[1] };\n      case 'transform':\n        return { operation: pairs[0] };\n      default:\n        return {};\n    }\n  }\n  \n  private serializeBlockConfig(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return config.path;\n      case 'filter':\n        return config.condition;\n      case 'aggregate':\n        return `${config.function}:${config.path}`;\n      case 'transform':\n        return config.operation;\n      default:\n        return '';\n    }\n  }\n  \n  async evaluate(expression: Expression, context: any): Promise<any> {\n    switch (expression.mode) {\n      case 'text':\n        return this.jsonataEngine.evaluate(expression.value, context);\n        \n      case 'visual':\n        const jsonataFromBlocks = this.blocksToJSONata(expression.blocks);\n        return this.jsonataEngine.evaluate(jsonataFromBlocks, context);\n        \n      case 'composite':\n        const jsonataFromComposite = this.compositeToJSONata(expression.parts);\n        return this.jsonataEngine.evaluate(jsonataFromComposite, context);\n        \n      default:\n        throw new Error(`Unknown expression mode: ${(expression as any).mode}`);\n    }\n  }\n  \n  validate(expression: Expression): ValidationResult {\n    try {\n      let jsonataExpr: string;\n      \n      switch (expression.mode) {\n        case 'text':\n          jsonataExpr = expression.value;\n          break;\n        case 'visual':\n          jsonataExpr = this.blocksToJSONata(expression.blocks);\n          break;\n        case 'composite':\n          jsonataExpr = this.compositeToJSONata(expression.parts);\n          break;\n        default:\n          return { valid: false, error: 'Unknown expression mode' };\n      }\n      \n      return this.jsonataEngine.validate(jsonataExpr);\n      \n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check expression syntax'\n      };\n    }\n  }\n  \n  private blocksToJSONata(blocks: ExpressionBlock[]): string {\n    return blocks.map(block => this.blockToJSONata(block)).join('');\n  }\n  \n  private compositeToJSONata(parts: ExpressionPart[]): string {\n    return parts.map(part => {\n      if (part.type === 'text') {\n        return part.value;\n      } else {\n        return this.blockToJSONata(part.block);\n      }\n    }).join('');\n  }\n  \n  private blockToJSONata(block: ExpressionBlock): string {\n    switch (block.type) {\n      case 'field':\n        return `$.${block.config.path}`;\n      case 'filter':\n        return `[${block.config.condition}]`;\n      case 'map':\n        return `.{ ${block.config.mapping} }`;\n      case 'aggregate':\n        return `$${block.config.function}(${block.config.path})`;\n      case 'transform':\n        return block.config.expression || block.config.operation;\n      default:\n        throw new Error(`Unknown block type: ${block.type}`);\n    }\n  }\n  \n  getExamples(): ExpressionExample[] {\n    return [\n      {\n        category: 'Text Mode',\n        expression: { mode: 'text', value: '$.user.name' },\n        description: 'Simple field extraction',\n        inputExample: { user: { name: 'Alice' } },\n        outputExample: 'Alice'\n      },\n      {\n        category: 'Composite Mode',\n        expression: {\n          mode: 'composite',\n          parts: this.parseComposite(\n            '$.items[price > 100].{{block:aggregate:sum:price}}'\n          )\n        },\n        description: 'Mix text and visual blocks',\n        inputExample: { items: [{ price: 50 }, { price: 150 }, { price: 200 }] },\n        outputExample: 350\n      }\n    ];\n  }\n}\n\nclass JSONataEngine {\n  private cache: Map<string, any> = new Map();\n  private readonly timeout = 5000;\n  \n  async evaluate(expression: string, context: any): Promise<any> {\n    let compiled = this.cache.get(expression);\n    \n    if (!compiled) {\n      // In real implementation, use actual JSONata library\n      compiled = { evaluate: (ctx: any) => this.simpleEval(expression, ctx) };\n      this.cache.set(expression, compiled);\n    }\n    \n    return await Promise.race([\n      Promise.resolve(compiled.evaluate(context)),\n      new Promise((_, reject) => \n        setTimeout(() => reject(new Error('Expression timeout')), this.timeout)\n      )\n    ]);\n  }\n  \n  validate(expression: string): ValidationResult {\n    try {\n      if (!expression || expression.trim().length === 0) {\n        return { valid: false, error: 'Empty expression' };\n      }\n      \n      return { valid: true };\n    } catch (error: any) {\n      return { \n        valid: false, \n        error: error.message,\n        suggestion: 'Check JSONata syntax'\n      };\n    }\n  }\n  \n  private simpleEval(expression: string, context: any): any {\n    if (expression.startsWith('$.')) {\n      const path = expression.substring(2).split('.');\n      let result = context;\n      for (const key of path) {\n        result = result?.[key];\n      }\n      return result;\n    }\n    \n    return null;\n  }\n}\n\n// UI Component\nconst ComposableExpressionEditor: React.FC<{\n  value: Expression;\n  onChange: (value: Expression) => void;\n}> = ({ value, onChange }) => {\n  \n  const [mode, setMode] = useState<'text' | 'visual' | 'composite'>(value.mode);\n  const engine = useMemo(() => new ComposableExpressionEngine(), []);\n  const [validation, setValidation] = useState<ValidationResult>({ valid: true });\n  \n  useEffect(() => {\n    setValidation(engine.validate(value));\n  }, [value, engine]);\n  \n  const handleTextChange = (text: string) => {\n    if (mode === 'composite') {\n      // Parse inline block markers\n      const parts = engine.parseComposite(text);\n      onChange({ mode: 'composite', parts });\n    } else {\n      onChange({ mode: 'text', value: text });\n    }\n  };\n  \n  const insertBlock = (blockType: string) => {\n    if (mode !== 'composite' || value.mode !== 'composite') return;\n    \n    const serialized = engine.serializeComposite(value.parts);\n    const marker = `{{block:${blockType}:}}`;\n    \n    // Insert at cursor position (simplified)\n    const newText = serialized + marker;\n    const parts = engine.parseComposite(newText);\n    onChange({ mode: 'composite', parts });\n  };\n  \n  return (\n    <div className=\"expression-editor\">\n      <div className=\"mode-selector\">\n        <button \n          onClick={() => setMode('text')}\n          className={mode === 'text' ? 'active' : ''}\n        >\n          Text\n        </button>\n        <button \n          onClick={() => setMode('visual')}\n          className={mode === 'visual' ? 'active' : ''}\n        >\n          Visual\n        </button>\n        <button \n          onClick={() => setMode('composite')}\n          className={mode === 'composite' ? 'active' : ''}\n        >\n          Mixed\n        </button>\n      </div>\n      \n      {(mode === 'text' || mode === 'composite') && (\n        <div>\n          <textarea\n            value={\n              value.mode === 'text' \n                ? value.value \n                : value.mode === 'composite'\n                ? engine.serializeComposite(value.parts)\n                : ''\n            }\n            onChange={e => handleTextChange(e.target.value)}\n            className={validation.valid ? '' : 'error'}\n            placeholder={\n              mode === 'composite'\n                ? 'Enter expression with {{block:type:config}} markers'\n                : 'Enter JSONata expression'\n            }\n          />\n          \n          {mode === 'composite' && (\n            <div className=\"block-palette\">\n              <button onClick={() => insertBlock('field')}>+ Field</button>\n              <button onClick={() => insertBlock('filter')}>+ Filter</button>\n              <button onClick={() => insertBlock('aggregate')}>+ Aggregate</button>\n            </div>\n          )}\n        </div>\n      )}\n      \n      {mode === 'visual' && value.mode === 'visual' && (\n        <VisualBlockEditor\n          blocks={value.blocks}\n          onChange={blocks => onChange({ mode: 'visual', blocks })}\n        />\n      )}\n      \n      {!validation.valid && (\n        <div className=\"error-message\">\n          {validation.error}\n          {validation.suggestion && <div className=\"suggestion\">{validation.suggestion}</div>}\n        </div>\n      )}\n    </div>\n  );\n};\n\nconst VisualBlockEditor: React.FC<{\n  blocks: ExpressionBlock[];\n  onChange: (blocks: ExpressionBlock[]) => void;\n}> = ({ blocks, onChange }) => {\n  return <div>Visual block editor</div>;\n};\n\ninterface ValidationResult {\n  valid: boolean;\n  error?: string;\n  suggestion?: string;\n}\n\ninterface ExpressionExample {\n  category: string;\n  expression: Expression;\n  description: string;\n  inputExample: any;\n  outputExample: any;\n}\n```\n\n## Serializable Compensation Handlers\n\n### Key Change: Handlers as Declarative Config, Registered at Workflow Level\n\n```typescript\n// Handlers are now declarative configurations, not code\ntype CompensationHandlerConfig = {\n  id: string;\n  name: string;\n  handlerType: 'builtin' | 'http-api' | 'webhook' | 'custom';\n  \n  // Matching rules\n  matches: {\n    operation?: string[];        // Match specific operations\n    description?: string[];      // Regex patterns for description\n    service?: string[];          // Service names\n  };\n  \n  // Compensation behavior\n  compensation: \n    | { type: 'http-api'; config: HttpApiConfig }\n    | { type: 'webhook'; config: WebhookConfig }\n    | { type: 'custom'; className: string }  // For advanced users\n    | { type: 'manual'; steps: string[] };\n};\n\ninterface HttpApiConfig {\n  method: 'GET' | 'POST' | 'DELETE' | 'PUT';\n  urlTemplate: string;           // \"https://api.stripe.com/v1/refunds\"\n  headers: Record<string, string>;\n  bodyTemplate: string;          // JSON with {{placeholders}}\n  auth?: {\n    type: 'bearer' | 'basic' | 'api-key';\n    tokenPath: string;           // Path to token in effect data\n  };\n}\n\ninterface WebhookConfig {\n  url: string;\n  method: 'POST' | 'PUT';\n  payloadTemplate: string;\n}\n\nclass CompensationHandlerRegistry {\n  private handlers: Map<string, CompensationHandler> = new Map();\n  private configs: Map<string, CompensationHandlerConfig> = new Map();\n  \n  // Register from declarative config\n  registerFromConfig(config: CompensationHandlerConfig): void {\n    this.configs.set(config.id, config);\n    \n    const handler = this.createHandlerFromConfig(config);\n    this.handlers.set(config.id, handler);\n  }\n  \n  private createHandlerFromConfig(\n    config: CompensationHandlerConfig\n  ): CompensationHandler {\n    \n    return {\n      name: config.name,\n      \n      canHandle: (entry: SideEffectEntry) => {\n        const { matches } = config;\n        \n        if (matches.operation && !matches.operation.includes(entry.operation)) {\n          return false;\n        }\n        \n        if (matches.description) {\n          const descMatch = matches.description.some(pattern => \n            new RegExp(pattern).test(entry.description)\n          );\n          if (!descMatch) return false;\n        }\n        \n        if (matches.service) {\n          const descriptor = entry.compensation as any;\n          if (!matches.service.includes(descriptor.service)) {\n            return false;\n          }\n        }\n        \n        return true;\n      },\n      \n      compensate: async (entry: SideEffectEntry) => {\n        switch (config.compensation.type) {\n          case 'http-api':\n            return this.compensateViaHttpApi(entry, config.compensation.config);\n            \n          case 'webhook':\n            return this.compensateViaWebhook(entry, config.compensation.config);\n            \n          case 'custom':\n            // Load custom class (advanced)\n            const CustomHandler = await this.loadCustomHandler(\n              config.compensation.className\n            );\n            return new CustomHandler().compensate(entry);\n            \n          case 'manual':\n            return {\n              success: false,\n              requiresUserAction: true,\n              manualSteps: config.compensation.steps\n            };\n            \n          default:\n            throw new Error(`Unknown compensation type: ${(config.compensation as any).type}`);\n        }\n      }\n    };\n  }\n  \n  private async compensateViaHttpApi(\n    entry: SideEffectEntry,\n    config: HttpApiConfig\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    \n    // Resolve URL template\n    const url = this.resolveTemplate(config.urlTemplate, {\n      ...descriptor,\n      effectId: entry.id\n    });\n    \n    // Resolve headers\n    const headers = { ...config.headers };\n    \n    if (config.auth) {\n      const token = this.extractValue(descriptor, config.auth.tokenPath);\n      switch (config.auth.type) {\n        case 'bearer':\n          headers['Authorization'] = `Bearer ${token}`;\n          break;\n        case 'api-key':\n          headers['Authorization'] = token;\n          break;\n      }\n    }\n    \n    // Resolve body\n    const body = config.bodyTemplate\n      ? this.resolveTemplate(config.bodyTemplate, descriptor)\n      : undefined;\n    \n    try {\n      const response = await fetch(url, {\n        method: config.method,\n        headers,\n        body: body ? JSON.stringify(JSON.parse(body)) : undefined\n      });\n      \n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Compensated via ${config.method} ${url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message,\n        requiresUserAction: true,\n        manualSteps: [\n          `Failed to compensate via API: ${error.message}`,\n          `Operation: ${entry.description}`,\n          'Please manually revert'\n        ]\n      };\n    }\n  }\n  \n  private async compensateViaWebhook(\n    entry: SideEffectEntry,\n    config: WebhookConfig\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    const payload = this.resolveTemplate(config.payloadTemplate, {\n      ...descriptor,\n      effectId: entry.id,\n      effectDescription: entry.description\n    });\n    \n    try {\n      const response = await fetch(config.url, {\n        method: config.method,\n        headers: { 'Content-Type': 'application/json' },\n        body: payload\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Webhook failed: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Webhook triggered: ${config.url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n  \n  private resolveTemplate(template: string, data: any): string {\n    return template.replace(/\\{\\{([^}]+)\\}\\}/g, (_, path) => {\n      return this.extractValue(data, path) ?? '';\n    });\n  }\n  \n  private extractValue(obj: any, path: string): any {\n    const parts = path.split('.');\n    let result = obj;\n    for (const part of parts) {\n      result = result?.[part];\n    }\n    return result;\n  }\n  \n  private async loadCustomHandler(className: string): Promise<any> {\n    // In real implementation, dynamically import class\n    throw new Error('Custom handlers not yet implemented');\n  }\n  \n  // Serialization for workflow persistence\n  serializeConfigs(): CompensationHandlerConfig[] {\n    return Array.from(this.configs.values());\n  }\n  \n  loadConfigs(configs: CompensationHandlerConfig[]): void {\n    for (const config of configs) {\n      this.registerFromConfig(config);\n    }\n  }\n  \n  unregister(id: string): void {\n    this.handlers.delete(id);\n    this.configs.delete(id);\n  }\n  \n  findHandler(entry: SideEffectEntry): CompensationHandler | null {\n    for (const handler of this.handlers.values()) {\n      if (handler.canHandle(entry)) {\n        return handler;\n      }\n    }\n    return null;\n  }\n}\n\n// Example: Stripe handler as config\nconst stripeHandlerConfig: CompensationHandlerConfig = {\n  id: 'stripe-refund',\n  name: 'Stripe Refund Handler',\n  handlerType: 'http-api',\n  \n  matches: {\n    operation: ['payment_process'],\n    description: ['Stripe.*payment'],\n    service: ['stripe']\n  },\n  \n  compensation: {\n    type: 'http-api',\n    config: {\n      method: 'POST',\n      urlTemplate: 'https://api.stripe.com/v1/refunds',\n      headers: {\n        'Content-Type': 'application/x-www-form-urlencoded'\n      },\n      bodyTemplate: JSON.stringify({\n        charge: '{{chargeId}}',\n        reason: 'requested_by_customer'\n      }),\n      auth: {\n        type: 'bearer',\n        tokenPath: 'apiKey'\n      }\n    }\n  }\n};\n\n// Usage\nconst registry = new CompensationHandlerRegistry();\nregistry.registerFromConfig(stripeHandlerConfig);\n\n// Serialize with workflow\nconst workflowData = {\n  nodes: [...],\n  edges: [...],\n  compensationHandlers: registry.serializeConfigs()\n};\n\n// Load from workflow\nregistry.loadConfigs(workflowData.compensationHandlers);\n\ninterface CompensationHandler {\n  name: string;\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n  canHandle(entry: SideEffectEntry): boolean;\n}\n\ninterface SideEffectEntry {\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  operation: string;\n  description: string;\n  compensation: any;\n  status: string;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n## Performance Monitoring for Streaming Decisions\n\n```typescript\ninterface StreamingMetrics {\n  decisions: {\n    teeCount: number;\n    materializeCount: number;\n    batchCount: number;\n  };\n  \n  performance: {\n    avgTeeMemory: number;         // Average memory per tee (bytes)\n    avgMaterializeSize: number;   // Average materialized size (bytes)\n    avgTeeTime: number;           // Average time to complete tee (ms)\n    avgMaterializeTime: number;   // Average time to materialize (ms)\n  };\n  \n  errors: {\n    teeFailures: number;\n    materializeFailures: number;\n    storageFullErrors: number;\n  };\n  \n  recommendations: {\n    suggestedTeeThreshold: number;\n    suggestedMaterializeThreshold: number;\n    suggestedOverflowPolicy: string;\n  };\n}\n\nclass StreamingPerformanceMonitor {\n  private metrics: StreamingMetrics;\n  private measurements: Array<{\n    strategy: 'tee' | 'materialize';\n    sizeBytes: number;\n    durationMs: number;\n    memoryUsed: number;\n    timestamp: number;\n  }> = [];\n  \n  constructor() {\n    this.metrics = this.initMetrics();\n  }\n  \n  recordDecision(\n    strategy: 'tee' | 'materialize' | 'batch',\n    sizeBytes: number,\n    durationMs: number,\n    memoryUsed: number\n  ): void {\n    \n    switch (strategy) {\n      case 'tee':\n        this.metrics.decisions.teeCount++;\n        break;\n      case 'materialize':\n        this.metrics.decisions.materializeCount++;\n        break;\n      case 'batch':\n        this.metrics.decisions.batchCount++;\n        break;\n    }\n    \n    if (strategy === 'tee' || strategy === 'materialize') {\n      this.measurements.push({\n        strategy,\n        sizeBytes,\n        durationMs,\n        memoryUsed,\n        timestamp: Date.now()\n      });\n      \n      // Keep last 100 measurements\n      if (this.measurements.length > 100) {\n        this.measurements.shift();\n      }\n      \n      this.updateAverages();\n      this.generateRecommendations();\n    }\n  }\n  \n  recordError(type: 'tee' | 'materialize' | 'storage-full'): void {\n    switch (type) {\n      case 'tee':\n        this.metrics.errors.teeFailures++;\n        break;\n      case 'materialize':\n        this.metrics.errors.materializeFailures++;\n        break;\n      case 'storage-full':\n        this.metrics.errors.storageFullErrors++;\n        break;\n    }\n  }\n  \n  private updateAverages(): void {\n    const tees = this.measurements.filter(m => m.strategy === 'tee');\n    const materializes = this.measurements.filter(m => m.strategy === 'materialize');\n    \n    if (tees.length > 0) {\n      this.metrics.performance.avgTeeMemory = \n        tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length;\n      this.metrics.performance.avgTeeTime = \n        tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length;\n    }\n    \n    if (materializes.length > 0) {\n      this.metrics.performance.avgMaterializeSize = \n        materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length;\n      this.metrics.performance.avgMaterializeTime = \n        materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length;\n    }\n  }\n  \n  private generateRecommendations(): void {\n    // Analyze patterns to suggest better thresholds\n    \n    // If tees are consistently using too much memory, lower threshold\n    if (this.metrics.performance.avgTeeMemory > 10 * 1024 * 1024) {\n      this.metrics.recommendations.suggestedTeeThreshold = 3 * 1024 * 1024; // 3MB\n    } else {\n      this.metrics.recommendations.suggestedTeeThreshold = 5 * 1024 * 1024; // 5MB\n    }\n    \n    // If materialize operations are fast, raise threshold\n    if (this.metrics.performance.avgMaterializeTime < 100) {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 100 * 1024 * 1024; // 100MB\n    } else {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 50 * 1024 * 1024; // 50MB\n    }\n    \n    // Recommend overflow policy based on errors\n    if (this.metrics.errors.storageFullErrors > 5) {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'evict-lru';\n    } else {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'compress-max';\n    }\n  }\n  \n  getMetrics(): StreamingMetrics {\n    return { ...this.metrics };\n  }\n  \n  getMeasurements(): typeof this.measurements {\n    return [...this.measurements];\n  }\n  \n  private initMetrics(): StreamingMetrics {\n    return {\n      decisions: {\n        teeCount: 0,\n        materializeCount: 0,\n        batchCount: 0\n      },\n      performance: {\n        avgTeeMemory: 0,\n        avgMaterializeSize: 0,\n        avgTeeTime: 0,\n        avgMaterializeTime: 0\n      },\n      errors: {\n        teeFailures: 0,\n        materializeFailures: 0,\n        storageFullErrors: 0\n      },\n      recommendations: {\n        suggestedTeeThreshold: 5 * 1024 * 1024,\n        suggestedMaterializeThreshold: 50 * 1024 * 1024,\n        suggestedOverflowPolicy: 'evict-lru'\n      }\n    };\n  }\n}\n\n// UI Component for Metrics Dashboard\nconst StreamingMetricsDashboard: React.FC<{\n  monitor: StreamingPerformanceMonitor;\n}> = ({ monitor }) => {\n  \n  const [metrics, setMetrics] = useState(monitor.getMetrics());\n  \n  useEffect(() => {\n    const interval = setInterval(() => {\n      setMetrics(monitor.getMetrics());\n    }, 1000);\n    \n    return () => clearInterval(interval);\n  }, [monitor]);\n  \n  return (\n    <div className=\"metrics-dashboard\">\n      <h3>Streaming Performance</h3>\n      \n      <div className=\"metrics-section\">\n        <h4>Decisions</h4>\n        <div>Tees: {metrics.decisions.teeCount}</div>\n        <div>Materializes: {metrics.decisions.materializeCount}</div>\n        <div>Batch: {metrics.decisions.batchCount}</div>\n      </div>\n      \n      <div className=\"metrics-section\">\n        <h4>Performance</h4>\n        <div>Avg Tee Memory: {formatBytes(metrics.performance.avgTeeMemory)}</div>\n        <div>Avg Materialize Size: {formatBytes(metrics.performance.avgMaterializeSize)}</div>\n        <div>Avg Tee Time: {metrics.performance.avgTeeTime.toFixed(2)}ms</div>\n        <div>Avg Materialize Time: {metrics.performance.avgMaterializeTime.toFixed(2)}ms</div>\n      </div>\n      \n      <div className=\"metrics-section\">\n        <h4>Errors</h4>\n        <div>Tee Failures: {metrics.errors.teeFailures}</div>\n        <div>Materialize Failures: {metrics.errors.materializeFailures}</div>\n        <div>Storage Full: {metrics.errors.storageFullErrors}</div>\n      </div>\n      \n      <div className=\"metrics-section recommendations\">\n        <h4>Recommendations</h4>\n        <div>Tee Threshold: {formatBytes(metrics.recommendations.suggestedTeeThreshold)}</div>\n        <div>Materialize Threshold: {formatBytes(metrics.recommendations.suggestedMaterializeThreshold)}</div>\n        <div>Overflow Policy: {metrics.recommendations.suggestedOverflowPolicy}</div>\n      </div>\n    </div>\n  );\n};\n```\n\n---\n\n## What I Improved\n\n### 1. Storage Overflow Policy Naming\n**Changed**: `drop-on-store` \u2192 `fail-when-full`, `drop-oldest` \u2192 `evict-lru`/`evict-oldest`\n- **Why**: \"fail-when-full\" is more explicit about what happens (throws error)\n- **Added**: Separate `evict-lru` (access-based) vs `evict-oldest` (time-based) for clarity\n- **Added**: Eviction preferences (protect recent, frequent, pinned items)\n- **Added**: Storage event system for monitoring\n\n### 2. Stream Registry as First-Class System\n**Changed**: From \"execution context storage\" to dedicated `StreamRegistry`\n- **Why**: Streams have different lifecycle than data artifacts\n- **Added**: Timeout monitoring for abandoned streams\n- **Added**: Multi-tee support (3+ consumers via iterative teeing)\n- **Added**: Reader tracking for proper cleanup\n- **Added**: Stream metrics separate from storage metrics\n\n### 3. Composite Expression Syntax\n**Changed**: From abstract \"mixed mode\" to concrete inline markers\n- **Syntax**: `{{block:type:config}}` embedded in text\n- **Why**: Makes composite mode tangible and editable as text\n- **Added**: `parseComposite()` and `serializeComposite()` for conversion\n- **Added**: Block insertion palette in UI\n\n### 4. Serializable Compensation Handlers\n**Changed**: From TypeScript classes to declarative JSON configs\n- **Why**: Handlers can now be saved with workflows\n- **Format**: HTTP API templates, webhook configs, or custom classes\n- **Added**: Template system with `{{placeholder}}` syntax\n- **Added**: Handler registry persistence (`serializeConfigs()`)\n\n### 5. Performance Monitoring System\n**Added**: Complete streaming performance tracking\n- **Metrics**: Tee vs materialize decisions, memory usage, timing\n- **Analysis**: Auto-generated threshold recommendations\n- **UI**: Real-time dashboard component\n- **Feedback Loop**: Metrics inform threshold tuning\n\n### 6. Enhanced Storage Management\n**Added**: Several production-ready features\n- **Pinning**: Protect critical artifacts from eviction\n- **Events**: Observable storage operations\n- **Warnings**: Proactive alerts at 80% capacity\n- **Error Types**: `StorageFullError` with detailed stats\n\n---\n\n## What I Kept\n\nAll excellent architectural decisions:\n- React Flow canvas with viewport culling\n- Zustand + immer state management\n- Two-tier storage (Memory + IndexedDB)\n- Adaptive streaming with size-based decisions\n- Append-only side effect log\n- Checkpoint manager with strategies\n- Worker pool architecture\n- Compression for large data\n- Auto-cleanup of temporary artifacts\n- All performance optimizations\n\n---\n\n## Remaining Concerns\n\n### 1. Stream Registry Memory Management\nThe stream registry holds `ReadableStream` objects in memory, which can't be serialized. If a page refreshes:\n- **Issue**: All teed streams are lost\n- **Mitigation**: Should we auto-materialize long-lived streams to storage?\n- **Trade-off**: Defeats the purpose of streaming for performance\n\n### 2. Composite Expression Learning Curve\nThe `{{block:type:config}}` syntax is powerful but:\n- **Issue**: Users need to learn the marker format\n- **Mitigation**: Good examples + auto-complete in UI\n- **Alternative**: Click-to-insert blocks that generate markers automatically\n\n### 3. Compensation Handler Security\nHandlers defined in JSON can make arbitrary HTTP calls:\n- **Issue**: Malicious workflows could call unintended APIs\n- **Mitigation**: Require user approval for each handler registration?\n- **Alternative**: Sandboxed execution environment for handlers\n\n### 4. Metrics Storage Size\nTracking 100 measurements per metric can grow large:\n- **Issue**: Metrics themselves consume memory\n- **Mitigation**: Persist to IndexedDB instead of memory\n- **Alternative**: Sample (only keep every Nth measurement)\n\n---\n\n## Convergence Status\n\n**ITERATING** - Good progress, but need architect's input on key design choices:\n\n\u2705 Clearer overflow policy names and eviction strategies  \n\u2705 Dedicated stream registry with lifecycle management  \n\u2705 Concrete composite expression syntax  \n\u2705 Serializable compensation handlers  \n\u2705 Performance monitoring system  \n\n\u2753 Stream persistence strategy on page refresh  \n\u2753 Composite expression UI/UX details  \n\u2753 Compensation handler security model  \n\u2753 Metrics storage and sampling strategy  \n\n---\n\n## PROMPT_FOR_ARCHITECT:\n\nPlease focus on these refined areas:\n\n1. **Stream Registry Persistence**: The stream registry holds active `ReadableStream` objects that can't survive page refresh. Should we:\n   - Accept this limitation (users re-run workflows after refresh)?\n   - Auto-materialize streams that live longer than N seconds?\n   - Add a \"save checkpoint\" button that materializes all active streams?\n\n2. **Composite Expression UX**: I've defined the syntax as `{{block:type:config}}`, but how should the UI work?\n   - Should users type markers manually (with autocomplete)?\n   - Should clicking \"Insert Block\" add the marker at cursor position?\n   - Should we show a visual preview of blocks highlighted within the text?\n\n3. **Compensation Handler Trust Model**: Since handlers are declarative configs that make HTTP calls:\n   - Should ALL handlers require explicit user approval when registered?\n   - Should we have \"trusted\" vs \"untrusted\" handler sources?\n   - Should we sandbox handler execution somehow (CSP, web worker)?\n\n4. **Performance Metrics Persistence**: Currently metrics live in memory. Should we:\n   - Persist to IndexedDB for long-term analysis?\n   - Only track metrics in development mode?\n   - Sample aggressively (1 in 10 measurements) to reduce memory?\n\n5. **Eviction Strategy Recommendations**: With both `evict-lru` and `evict-oldest` available:\n   - Should we recommend one as default for most workflows?\n   - Should eviction strategy be per-workflow or global?\n   - Should users be able to mix strategies (LRU for memory, FIFO for IndexedDB)?"
      }
    },
    {
      "round_number": 4,
      "agent_a": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v5\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Resilient \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Stream    \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502            \u2502  \u2502  Manager   \u2502  \u2502            \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Tiered   \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  Storage   \u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Manager   \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with viewport culling\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with resilient streaming\n- **Storage**: Three-tier (Memory + IndexedDB + SessionStorage) with per-workflow policies\n- **Expressions**: Composable expression system with visual-text hybrid mode\n- **Side Effects**: Append-only log with sandboxed compensation handlers\n- **Workers**: Shared Worker pool with resource quotas\n\n## Resilient Stream Manager (Solves Page Refresh Problem)\n\n### Key Innovation: Automatic Checkpoint-Based Stream Resurrection\n\n```typescript\ninterface StreamManagerConfig {\n  maxConcurrentStreams: number;        // 50\n  streamTimeout: number;                // 5min\n  autoCheckpointInterval: number;       // 30s - checkpoint long-lived streams\n  checkpointThresholdAge: number;       // 60s - checkpoint if older than this\n  enableResurrection: boolean;          // Restore streams after page refresh\n  maxResurrectionAttempts: number;      // 3\n}\n\ninterface StreamCheckpoint {\n  streamId: string;\n  sourceNodeId: string;\n  executionId: string;\n  consumerCount: number;\n  createdAt: number;\n  lastCheckpointAt: number;\n  \n  // Resurrection data\n  materializationType: 'full' | 'partial' | 'regenerate';\n  materializedData?: any;               // For 'full' or 'partial'\n  partialOffset?: number;               // Where partial stopped\n  regenerationConfig?: {                // For 'regenerate'\n    nodeConfig: any;\n    inputs: Record<string, any>;\n  };\n  \n  // State\n  status: 'active' | 'checkpointed' | 'resurrected' | 'expired';\n  resurrectionAttempts: number;\n}\n\nclass ResilientStreamManager {\n  private activeStreams: Map<string, ActiveStreamEntry> = new Map();\n  private checkpoints: Map<string, StreamCheckpoint> = new Map();\n  private storage: HybridStorageManager;\n  private config: StreamManagerConfig;\n  \n  constructor(storage: HybridStorageManager, config: StreamManagerConfig) {\n    this.storage = storage;\n    this.config = config;\n    \n    // Restore checkpoints from previous session\n    if (config.enableResurrection) {\n      this.restoreCheckpointsFromStorage();\n    }\n    \n    this.startAutoCheckpointing();\n    this.startTimeoutMonitor();\n  }\n  \n  async registerStream(\n    sourceNodeId: string,\n    executionId: string,\n    stream: ReadableStream,\n    consumerCount: number,\n    options: {\n      estimatedSize: number;\n      materializationType?: 'full' | 'partial' | 'regenerate';\n      regenerationConfig?: any;\n    }\n  ): Promise<string> {\n    \n    if (this.activeStreams.size >= this.config.maxConcurrentStreams) {\n      throw new Error(\n        `Max concurrent streams reached (${this.config.maxConcurrentStreams})`\n      );\n    }\n    \n    const streamId = crypto.randomUUID();\n    \n    // Create teed streams\n    const teedStreams = this.createMultiTee(stream, consumerCount);\n    \n    // Create checkpoint\n    const checkpoint: StreamCheckpoint = {\n      streamId,\n      sourceNodeId,\n      executionId,\n      consumerCount,\n      createdAt: Date.now(),\n      lastCheckpointAt: Date.now(),\n      materializationType: options.materializationType ?? 'full',\n      regenerationConfig: options.regenerationConfig,\n      status: 'active',\n      resurrectionAttempts: 0\n    };\n    \n    this.checkpoints.set(streamId, checkpoint);\n    \n    this.activeStreams.set(streamId, {\n      checkpoint,\n      streams: teedStreams,\n      readers: new Set(),\n      materializedChunks: options.materializationType !== 'regenerate' ? [] : undefined\n    });\n    \n    return streamId;\n  }\n  \n  async getStream(streamId: string, consumerIndex: number): Promise<ReadableStream> {\n    const active = this.activeStreams.get(streamId);\n    \n    if (active) {\n      // Stream is live\n      active.checkpoint.lastCheckpointAt = Date.now();\n      return active.streams[consumerIndex];\n    }\n    \n    // Try to resurrect from checkpoint\n    if (this.config.enableResurrection) {\n      const resurrected = await this.resurrectStream(streamId, consumerIndex);\n      if (resurrected) return resurrected;\n    }\n    \n    throw new Error(`Stream not found and could not be resurrected: ${streamId}`);\n  }\n  \n  private async resurrectStream(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStream | null> {\n    \n    const checkpoint = this.checkpoints.get(streamId);\n    if (!checkpoint) return null;\n    \n    if (checkpoint.resurrectionAttempts >= this.config.maxResurrectionAttempts) {\n      console.error(`Max resurrection attempts reached for stream ${streamId}`);\n      return null;\n    }\n    \n    checkpoint.resurrectionAttempts++;\n    \n    try {\n      switch (checkpoint.materializationType) {\n        case 'full':\n          return await this.resurrectFromFullMaterialization(checkpoint);\n          \n        case 'partial':\n          return await this.resurrectFromPartialMaterialization(checkpoint);\n          \n        case 'regenerate':\n          return await this.resurrectViaRegeneration(checkpoint);\n          \n        default:\n          throw new Error(`Unknown materialization type: ${checkpoint.materializationType}`);\n      }\n    } catch (error: any) {\n      console.error(`Stream resurrection failed: ${error.message}`);\n      return null;\n    }\n  }\n  \n  private async resurrectFromFullMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    // Load materialized data from storage\n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const materialized = await this.storage.retrieve(storageKey);\n    \n    // Convert back to stream\n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(materialized)) {\n          for (const chunk of materialized) {\n            controller.enqueue(chunk);\n          }\n        } else {\n          controller.enqueue(materialized);\n        }\n        controller.close();\n      }\n    });\n    \n    checkpoint.status = 'resurrected';\n    \n    return stream;\n  }\n  \n  private async resurrectFromPartialMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    // Load partial data\n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const partialData = await this.storage.retrieve(storageKey);\n    \n    // Return stream that first emits partial data, then signals \"incomplete\"\n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(partialData)) {\n          for (const chunk of partialData) {\n            controller.enqueue(chunk);\n          }\n        }\n        \n        // Signal incomplete\n        controller.enqueue({\n          $incomplete: true,\n          offset: checkpoint.partialOffset,\n          message: 'Stream was interrupted. Partial data only.'\n        });\n        \n        controller.close();\n      }\n    });\n    \n    checkpoint.status = 'resurrected';\n    \n    return stream;\n  }\n  \n  private async resurrectViaRegeneration(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    if (!checkpoint.regenerationConfig) {\n      throw new Error('No regeneration config available');\n    }\n    \n    // Re-execute the node to regenerate the stream\n    // This requires access to the executor (injected via callback)\n    const regenerated = await this.executeNodeForRegeneration(\n      checkpoint.sourceNodeId,\n      checkpoint.regenerationConfig.nodeConfig,\n      checkpoint.regenerationConfig.inputs\n    );\n    \n    checkpoint.status = 'resurrected';\n    \n    return regenerated;\n  }\n  \n  private async executeNodeForRegeneration(\n    nodeId: string,\n    nodeConfig: any,\n    inputs: Record<string, any>\n  ): Promise<ReadableStream> {\n    \n    // This is called by injecting an executor callback during initialization\n    // For now, throw error to indicate implementation needed\n    throw new Error(\n      'Regeneration requires executor callback. ' +\n      'Set ResilientStreamManager.regenerationExecutor = (node, inputs) => stream'\n    );\n  }\n  \n  private startAutoCheckpointing(): void {\n    setInterval(() => {\n      this.checkpointLongLivedStreams();\n    }, this.config.autoCheckpointInterval);\n  }\n  \n  private async checkpointLongLivedStreams(): Promise<void> {\n    const now = Date.now();\n    const threshold = this.config.checkpointThresholdAge;\n    \n    for (const [streamId, entry] of this.activeStreams.entries()) {\n      const age = now - entry.checkpoint.createdAt;\n      \n      if (age > threshold && entry.checkpoint.status === 'active') {\n        await this.checkpointStream(streamId);\n      }\n    }\n  }\n  \n  private async checkpointStream(streamId: string): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    const checkpoint = entry.checkpoint;\n    \n    switch (checkpoint.materializationType) {\n      case 'full':\n        // Materialize all consumed chunks so far\n        if (entry.materializedChunks && entry.materializedChunks.length > 0) {\n          const storageKey = `stream-checkpoint-${streamId}`;\n          checkpoint.materializedData = [...entry.materializedChunks];\n          \n          await this.storage.store(storageKey, checkpoint.materializedData, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'partial':\n        // Only keep last N chunks\n        if (entry.materializedChunks) {\n          const keepCount = 10;\n          const partial = entry.materializedChunks.slice(-keepCount);\n          \n          const storageKey = `stream-checkpoint-${streamId}`;\n          await this.storage.store(storageKey, partial, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.partialOffset = entry.materializedChunks.length - keepCount;\n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'regenerate':\n        // No materialization needed - regeneration config already stored\n        checkpoint.status = 'checkpointed';\n        checkpoint.lastCheckpointAt = Date.now();\n        break;\n    }\n    \n    // Persist checkpoint metadata to SessionStorage\n    await this.persistCheckpoint(checkpoint);\n  }\n  \n  private async persistCheckpoint(checkpoint: StreamCheckpoint): Promise<void> {\n    const key = `stream-checkpoint-meta-${checkpoint.streamId}`;\n    sessionStorage.setItem(key, JSON.stringify(checkpoint));\n  }\n  \n  private async restoreCheckpointsFromStorage(): Promise<void> {\n    const keys = Object.keys(sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('stream-checkpoint-meta-')) {\n        const checkpointData = sessionStorage.getItem(key);\n        if (checkpointData) {\n          const checkpoint: StreamCheckpoint = JSON.parse(checkpointData);\n          this.checkpoints.set(checkpoint.streamId, checkpoint);\n        }\n      }\n    }\n  }\n  \n  private createMultiTee(stream: ReadableStream, count: number): ReadableStream[] {\n    if (count === 1) return [stream];\n    if (count === 2) return stream.tee();\n    \n    const streams: ReadableStream[] = [];\n    let remaining = stream;\n    \n    for (let i = 0; i < count - 1; i++) {\n      const [stream1, stream2] = remaining.tee();\n      streams.push(stream1);\n      remaining = stream2;\n    }\n    streams.push(remaining);\n    \n    return streams;\n  }\n  \n  async acquireReader(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStreamDefaultReader> {\n    const stream = await this.getStream(streamId, consumerIndex);\n    const reader = stream.getReader();\n    \n    const entry = this.activeStreams.get(streamId);\n    if (entry) {\n      entry.readers.add(reader);\n      \n      // Wrap reader to capture chunks for checkpointing\n      if (entry.materializedChunks) {\n        return this.wrapReaderForCheckpointing(reader, entry.materializedChunks);\n      }\n    }\n    \n    return reader;\n  }\n  \n  private wrapReaderForCheckpointing(\n    reader: ReadableStreamDefaultReader,\n    chunksBuffer: any[]\n  ): ReadableStreamDefaultReader {\n    \n    const originalRead = reader.read.bind(reader);\n    \n    reader.read = async () => {\n      const result = await originalRead();\n      \n      if (!result.done && result.value !== undefined) {\n        chunksBuffer.push(result.value);\n      }\n      \n      return result;\n    };\n    \n    return reader;\n  }\n  \n  async releaseReader(\n    streamId: string, \n    reader: ReadableStreamDefaultReader\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    try {\n      reader.releaseLock();\n    } catch {\n      // Already released\n    }\n    \n    entry.readers.delete(reader);\n    \n    if (entry.readers.size === 0) {\n      await this.cleanup(streamId, 'completed');\n    }\n  }\n  \n  private async cleanup(\n    streamId: string, \n    reason: 'completed' | 'error' | 'timeout'\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    // Final checkpoint before cleanup\n    if (reason === 'completed' && entry.checkpoint.materializationType !== 'regenerate') {\n      await this.checkpointStream(streamId);\n    }\n    \n    entry.checkpoint.status = 'expired';\n    \n    for (const stream of entry.streams) {\n      try {\n        const reader = stream.getReader();\n        await reader.cancel(`Cleanup: ${reason}`);\n        reader.releaseLock();\n      } catch {\n        // Ignore\n      }\n    }\n    \n    this.activeStreams.delete(streamId);\n  }\n  \n  private startTimeoutMonitor(): void {\n    setInterval(() => {\n      const now = Date.now();\n      const timeout = this.config.streamTimeout;\n      \n      for (const [streamId, entry] of this.activeStreams.entries()) {\n        if (now - entry.checkpoint.lastCheckpointAt > timeout) {\n          console.warn(`Stream timeout: ${streamId}`);\n          this.cleanup(streamId, 'timeout');\n        }\n      }\n    }, 60000);\n  }\n  \n  getCheckpoint(streamId: string): StreamCheckpoint | null {\n    return this.checkpoints.get(streamId) ?? null;\n  }\n  \n  listActiveStreams(): StreamCheckpoint[] {\n    return Array.from(this.activeStreams.values())\n      .map(entry => entry.checkpoint);\n  }\n  \n  // Public property for injecting regeneration logic\n  regenerationExecutor?: (\n    nodeId: string, \n    nodeConfig: any, \n    inputs: Record<string, any>\n  ) => Promise<ReadableStream>;\n}\n\ninterface ActiveStreamEntry {\n  checkpoint: StreamCheckpoint;\n  streams: ReadableStream[];\n  readers: Set<ReadableStreamDefaultReader>;\n  materializedChunks?: any[];  // For 'full' or 'partial' checkpointing\n}\n```\n\n## Enhanced Tiered Storage with Per-Workflow Policies\n\n### Key Innovation: Workflow-Scoped Storage Configuration\n\n```typescript\ninterface WorkflowStoragePolicy {\n  workflowId: string;\n  \n  // Tier capacities (per workflow)\n  memoryCacheMaxBytes: number;          // Default: 50MB per workflow\n  indexedDBMaxBytes: number;            // Default: 500MB per workflow\n  \n  // Overflow behavior\n  overflowPolicy: 'evict-lru' | 'evict-oldest' | 'fail' | 'compress' | 'external';\n  \n  // Eviction preferences\n  evictionPreferences: {\n    protectRecent: boolean;             // Don't evict < 5s old\n    protectFrequent: boolean;           // Prefer low-access items\n    protectPinned: boolean;             // Never evict pinned\n  };\n  \n  // Auto-cleanup\n  autoCleanupAge: number;               // 24h\n  enableAutoCleanup: boolean;\n}\n\nclass TieredStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  \n  // Per-workflow policies\n  private workflowPolicies: Map<string, WorkflowStoragePolicy> = new Map();\n  private globalPolicy: WorkflowStoragePolicy;\n  \n  // Artifact metadata\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  // External blobs\n  private externalBlobs: Map<string, string> = new Map();\n  \n  // Session-level temporary storage (survives refresh)\n  private sessionStorage: Storage = window.sessionStorage;\n  \n  constructor(globalPolicy: WorkflowStoragePolicy) {\n    this.globalPolicy = globalPolicy;\n    this.initIndexedDB();\n    this.restoreFromSession();\n  }\n  \n  setWorkflowPolicy(workflowId: string, policy: WorkflowStoragePolicy): void {\n    this.workflowPolicies.set(workflowId, policy);\n  }\n  \n  getPolicy(workflowId?: string): WorkflowStoragePolicy {\n    if (workflowId) {\n      return this.workflowPolicies.get(workflowId) ?? this.globalPolicy;\n    }\n    return this.globalPolicy;\n  }\n  \n  async store(\n    key: string, \n    data: any, \n    hints: StorageHints = {}\n  ): Promise<void> {\n    \n    const policy = this.getPolicy(hints.workflowId);\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    // Create metadata\n    const metadata: ArtifactMetadata = {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      pinned: hints.pinned ?? false,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      workflowId: hints.workflowId,\n      dataType: hints.dataType ?? 'json'\n    };\n    \n    this.artifactIndex.set(key, metadata);\n    \n    // Check if we should use session storage (for critical workflow state)\n    if (hints.useSessionStorage) {\n      this.sessionStorage.setItem(key, serialized);\n      metadata.tier = 'session';\n      return;\n    }\n    \n    // Tier decision\n    if (sizeBytes < 1024 * 1024) {\n      if (await this.tryStoreInMemory(key, data, sizeBytes, policy)) {\n        metadata.tier = 'memory';\n        return;\n      }\n    }\n    \n    await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n    metadata.tier = 'indexeddb';\n  }\n  \n  private async tryStoreInMemory(\n    key: string, \n    data: any, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    if (this.memoryCacheSize + sizeBytes <= policy.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    if (await this.evictFromMemory(sizeBytes, policy)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(\n    requiredBytes: number, \n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'memory' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        (!policy.evictionPreferences.protectRecent || \n         Date.now() - m.createdAt > 5000)\n      );\n    \n    // Sort by policy\n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const metadata of candidates) {\n      toEvict.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes, policy);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n      }\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    if (this.indexedDBSize + sizeBytes > policy.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes, policy);\n      return;\n    }\n    \n    const shouldCompress = sizeBytes > 1024 * 1024;\n    const toStore = shouldCompress \n      ? await this.compress(serialized)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    await tx.objectStore('artifacts').put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    switch (policy.overflowPolicy) {\n      case 'evict-lru':\n      case 'evict-oldest':\n        await this.evictFromIndexedDB(sizeBytes, policy);\n        await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n        break;\n        \n      case 'fail':\n        throw new StorageFullError(\n          `Storage full for workflow: ${policy.workflowId}`,\n          this.getStorageStats()\n        );\n        \n      case 'compress':\n        const compressed = await this.compress(serialized);\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > policy.indexedDBMaxBytes) {\n          await this.evictFromIndexedDB(compressedSize, policy);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        break;\n        \n      case 'external':\n        const blob = new Blob([await this.compress(serialized)]);\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'external';\n        }\n        break;\n    }\n  }\n  \n  private async evictFromIndexedDB(\n    requiredBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'indexeddb' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        m.workflowId === policy.workflowId  // Only evict from same workflow\n      );\n    \n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const metadata of candidates) {\n      toDelete.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (toDelete.length === 0) {\n      throw new Error('Cannot evict: all items protected or wrong workflow');\n    }\n    \n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    switch (metadata.tier) {\n      case 'memory':\n        return this.memoryCache.get(key)?.data;\n        \n      case 'indexeddb':\n        return await this.retrieveFromIndexedDB(key);\n        \n      case 'session':\n        const sessionData = this.sessionStorage.getItem(key);\n        return sessionData ? JSON.parse(sessionData) : null;\n        \n      case 'external':\n        return await this.retrieveFromExternalBlob(key);\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const record = await tx.objectStore('artifacts').get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async pin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = true;\n    }\n  }\n  \n  async unpin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = false;\n    }\n  }\n  \n  private async compress(data: string): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 4);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n  \n  private restoreFromSession(): void {\n    // Restore critical workflow state from session storage\n    const keys = Object.keys(this.sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('workflow-')) {\n        const metadata: ArtifactMetadata = {\n          key,\n          sizeBytes: this.sessionStorage.getItem(key)!.length,\n          tier: 'session',\n          createdAt: Date.now(),\n          lastAccessedAt: Date.now(),\n          accessCount: 0,\n          temporary: false,\n          pinned: true,\n          dataType: 'json'\n        };\n        \n        this.artifactIndex.set(key, metadata);\n      }\n    }\n  }\n  \n  getStorageStats(workflowId?: string): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      session: { count: 0, bytes: 0 },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      if (workflowId && metadata.workflowId !== workflowId) continue;\n      \n      stats[metadata.tier as keyof typeof stats].count++;\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n}\n\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  pinned?: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType?: 'stream-ref' | 'materialized' | 'blob' | 'json';\n  useSessionStorage?: boolean;\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'session' | 'external' | 'determining';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  pinned: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  session: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n\nclass StorageFullError extends Error {\n  constructor(message: string, public stats: StorageStats) {\n    super(message);\n    this.name = 'StorageFullError';\n  }\n}\n```\n\n## Visual-Text Hybrid Expression Editor\n\n### Key Innovation: Rich Text Editor with Inline Block Components\n\n```typescript\ninterface HybridExpression {\n  mode: 'hybrid';\n  content: EditorContent[];  // Prosemirror-style content\n  rawText: string;           // Plain text with markers (for serialization)\n}\n\ntype EditorContent = TextNode | BlockNode;\n\ninterface TextNode {\n  type: 'text';\n  text: string;\n}\n\ninterface BlockNode {\n  type: 'block';\n  blockType: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n  displayText: string;  // Human-readable representation\n}\n\nclass HybridExpressionEditor {\n  private engine: ComposableExpressionEngine;\n  \n  constructor() {\n    this.engine = new ComposableExpressionEngine();\n  }\n  \n  parseFromText(text: string): HybridExpression {\n    const content: EditorContent[] = [];\n    const blockMarkerRegex = /\\{\\{block:([^:}]+):([^}]+)\\}\\}/g;\n    \n    let lastIndex = 0;\n    const matches = text.matchAll(blockMarkerRegex);\n    \n    for (const match of matches) {\n      // Add text before block\n      if (match.index! > lastIndex) {\n        const textPart = text.slice(lastIndex, match.index);\n        if (textPart) {\n          content.push({ type: 'text', text: textPart });\n        }\n      }\n      \n      // Parse block\n      const [_, blockType, configStr] = match;\n      const config = this.parseBlockConfig(blockType, configStr);\n      const displayText = this.generateDisplayText(blockType, config);\n      \n      content.push({\n        type: 'block',\n        blockType: blockType as any,\n        config,\n        displayText\n      });\n      \n      lastIndex = match.index! + match[0].length;\n    }\n    \n    // Add remaining text\n    if (lastIndex < text.length) {\n      content.push({ type: 'text', text: text.slice(lastIndex) });\n    }\n    \n    return {\n      mode: 'hybrid',\n      content,\n      rawText: text\n    };\n  }\n  \n  serializeToText(expression: HybridExpression): string {\n    return expression.content.map(node => {\n      if (node.type === 'text') {\n        return node.text;\n      } else {\n        const configStr = this.serializeBlockConfig(node.blockType, node.config);\n        return `{{block:${node.blockType}:${configStr}}}`;\n      }\n    }).join('');\n  }\n  \n  private parseBlockConfig(type: string, configStr: string): Record<string, any> {\n    const pairs = configStr.split(':');\n    \n    switch (type) {\n      case 'field':\n        return { path: pairs[0] };\n      case 'filter':\n        return { condition: pairs[0] };\n      case 'aggregate':\n        return { function: pairs[0], path: pairs[1] || '' };\n      case 'map':\n        return { expression: pairs[0] };\n      case 'transform':\n        return { operation: pairs[0] };\n      default:\n        return {};\n    }\n  }\n  \n  private serializeBlockConfig(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return config.path;\n      case 'filter':\n        return config.condition;\n      case 'aggregate':\n        return `${config.function}:${config.path}`;\n      case 'map':\n        return config.expression;\n      case 'transform':\n        return config.operation;\n      default:\n        return '';\n    }\n  }\n  \n  private generateDisplayText(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return `$.${config.path}`;\n      case 'filter':\n        return `[${config.condition}]`;\n      case 'aggregate':\n        return `${config.function}(${config.path})`;\n      case 'map':\n        return `map(${config.expression})`;\n      case 'transform':\n        return config.operation;\n      default:\n        return 'block';\n    }\n  }\n  \n  insertBlock(\n    expression: HybridExpression, \n    cursorPosition: number,\n    blockType: string,\n    config: Record<string, any>\n  ): HybridExpression {\n    \n    const displayText = this.generateDisplayText(blockType, config);\n    const blockNode: BlockNode = {\n      type: 'block',\n      blockType: blockType as any,\n      config,\n      displayText\n    };\n    \n    // Find position in content array\n    let currentPos = 0;\n    let insertIndex = 0;\n    \n    for (let i = 0; i < expression.content.length; i++) {\n      const node = expression.content[i];\n      const nodeLength = node.type === 'text' \n        ? node.text.length \n        : node.displayText.length;\n      \n      if (currentPos + nodeLength >= cursorPosition) {\n        insertIndex = i + 1;\n        break;\n      }\n      \n      currentPos += nodeLength;\n    }\n    \n    const newContent = [\n      ...expression.content.slice(0, insertIndex),\n      blockNode,\n      ...expression.content.slice(insertIndex)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent })\n    };\n  }\n  \n  removeBlock(expression: HybridExpression, blockIndex: number): HybridExpression {\n    const newContent = expression.content.filter((_, i) => i !== blockIndex);\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent })\n    };\n  }\n  \n  updateBlock(\n    expression: HybridExpression, \n    blockIndex: number,\n    newConfig: Record<string, any>\n  ): HybridExpression {\n    \n    const node = expression.content[blockIndex];\n    if (node.type !== 'block') {\n      throw new Error('Node is not a block');\n    }\n    \n    const updatedNode: BlockNode = {\n      ...node,\n      config: newConfig,\n      displayText: this.generateDisplayText(node.blockType, newConfig)\n    };\n    \n    const newContent = [\n      ...expression.content.slice(0, blockIndex),\n      updatedNode,\n      ...expression.content.slice(blockIndex + 1)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent })\n    };\n  }\n}\n\n// React Component\nconst HybridExpressionEditorUI: React.FC<{\n  value: HybridExpression;\n  onChange: (value: HybridExpression) => void;\n}> = ({ value, onChange }) => {\n  \n  const editor = useMemo(() => new HybridExpressionEditor(), []);\n  const [cursorPosition, setCursorPosition] = useState(0);\n  const [selectedBlockIndex, setSelectedBlockIndex] = useState<number | null>(null);\n  const editorRef = useRef<HTMLDivElement>(null);\n  \n  const handleInsertBlock = (blockType: string) => {\n    const defaultConfig = getDefaultBlockConfig(blockType);\n    const updated = editor.insertBlock(value, cursorPosition, blockType, defaultConfig);\n    onChange(updated);\n  };\n  \n  const handleRemoveBlock = (blockIndex: number) => {\n    const updated = editor.removeBlock(value, blockIndex);\n    onChange(updated);\n    setSelectedBlockIndex(null);\n  };\n  \n  const handleUpdateBlock = (blockIndex: number, newConfig: Record<string, any>) => {\n    const updated = editor.updateBlock(value, blockIndex, newConfig);\n    onChange(updated);\n  };\n  \n  const handleTextChange = (newText: string) => {\n    const parsed = editor.parseFromText(newText);\n    onChange(parsed);\n  };\n  \n  return (\n    <div className=\"hybrid-expression-editor\">\n      {/* Block Palette */}\n      <div className=\"block-palette\">\n        <button onClick={() => handleInsertBlock('field')}>\n          <span className=\"icon\">\ud83d\udcc4</span> Field\n        </button>\n        <button onClick={() => handleInsertBlock('filter')}>\n          <span className=\"icon\">\ud83d\udd0d</span> Filter\n        </button>\n        <button onClick={() => handleInsertBlock('aggregate')}>\n          <span className=\"icon\">\u03a3</span> Aggregate\n        </button>\n        <button onClick={() => handleInsertBlock('map')}>\n          <span className=\"icon\">\ud83d\uddfa\ufe0f</span> Map\n        </button>\n        <button onClick={() => handleInsertBlock('transform')}>\n          <span className=\"icon\">\u2699\ufe0f</span> Transform\n        </button>\n      </div>\n      \n      {/* Rich Text Editor */}\n      <div \n        ref={editorRef}\n        className=\"editor-content\"\n        contentEditable\n        suppressContentEditableWarning\n        onInput={(e) => {\n          const text = e.currentTarget.textContent || '';\n          handleTextChange(text);\n        }}\n        onSelect={() => {\n          const selection = window.getSelection();\n          if (selection) {\n            setCursorPosition(selection.anchorOffset);\n          }\n        }}\n      >\n        {value.content.map((node, index) => {\n          if (node.type === 'text') {\n            return <span key={index}>{node.text}</span>;\n          } else {\n            return (\n              <BlockComponent\n                key={index}\n                node={node}\n                selected={selectedBlockIndex === index}\n                onSelect={() => setSelectedBlockIndex(index)}\n                onRemove={() => handleRemoveBlock(index)}\n                onUpdate={(config) => handleUpdateBlock(index, config)}\n              />\n            );\n          }\n        })}\n      </div>\n      \n      {/* Block Config Panel (when block selected) */}\n      {selectedBlockIndex !== null && value.content[selectedBlockIndex].type === 'block' && (\n        <BlockConfigPanel\n          blockNode={value.content[selectedBlockIndex] as BlockNode}\n          onUpdate={(config) => handleUpdateBlock(selectedBlockIndex, config)}\n          onClose={() => setSelectedBlockIndex(null)}\n        />\n      )}\n      \n      {/* Raw Text View (toggle) */}\n      <details className=\"raw-text-view\">\n        <summary>View Raw Text</summary>\n        <textarea \n          value={value.rawText}\n          onChange={(e) => handleTextChange(e.target.value)}\n          className=\"raw-text-editor\"\n        />\n      </details>\n    </div>\n  );\n};\n\nconst BlockComponent: React.FC<{\n  node: BlockNode;\n  selected: boolean;\n  onSelect: () => void;\n  onRemove: () => void;\n  onUpdate: (config: Record<string, any>) => void;\n}> = ({ node, selected, onSelect, onRemove, onUpdate }) => {\n  \n  const getBlockIcon = (type: string) => {\n    switch (type) {\n      case 'field': return '\ud83d\udcc4';\n      case 'filter': return '\ud83d\udd0d';\n      case 'aggregate': return '\u03a3';\n      case 'map': return '\ud83d\uddfa\ufe0f';\n      case 'transform': return '\u2699\ufe0f';\n      default: return '\ud83e\udde9';\n    }\n  };\n  \n  return (\n    <span \n      className={`expression-block ${selected ? 'selected' : ''}`}\n      onClick={onSelect}\n      contentEditable={false}\n    >\n      <span className=\"block-icon\">{getBlockIcon(node.blockType)}</span>\n      <span className=\"block-text\">{node.displayText}</span>\n      <button className=\"block-remove\" onClick={onRemove}>\u00d7</button>\n    </span>\n  );\n};\n\nconst BlockConfigPanel: React.FC<{\n  blockNode: BlockNode;\n  onUpdate: (config: Record<string, any>) => void;\n  onClose: () => void;\n}> = ({ blockNode, onUpdate, onClose }) => {\n  \n  const [config, setConfig] = useState(blockNode.config);\n  \n  const handleSave = () => {\n    onUpdate(config);\n    onClose();\n  };\n  \n  return (\n    <div className=\"block-config-panel\">\n      <div className=\"panel-header\">\n        <h4>Configure {blockNode.blockType}</h4>\n        <button onClick={onClose}>\u00d7</button>\n      </div>\n      \n      <div className=\"panel-body\">\n        {blockNode.blockType === 'field' && (\n          <label>\n            Path:\n            <input \n              type=\"text\"\n              value={config.path || ''}\n              onChange={(e) => setConfig({ ...config, path: e.target.value })}\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'filter' && (\n          <label>\n            Condition:\n            <input \n              type=\"text\"\n              value={config.condition || ''}\n              onChange={(e) => setConfig({ ...config, condition: e.target.value })}\n              placeholder=\"e.g., age > 18\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'aggregate' && (\n          <>\n            <label>\n              Function:\n              <select \n                value={config.function || 'sum'}\n                onChange={(e) => setConfig({ ...config, function: e.target.value })}\n              >\n                <option value=\"sum\">Sum</option>\n                <option value=\"avg\">Average</option>\n                <option value=\"count\">Count</option>\n                <option value=\"min\">Min</option>\n                <option value=\"max\">Max</option>\n              </select>\n            </label>\n            <label>\n              Path:\n              <input \n                type=\"text\"\n                value={config.path || ''}\n                onChange={(e) => setConfig({ ...config, path: e.target.value })}\n              />\n            </label>\n          </>\n        )}\n        \n        {blockNode.blockType === 'map' && (\n          <label>\n            Expression:\n            <input \n              type=\"text\"\n              value={config.expression || ''}\n              onChange={(e) => setConfig({ ...config, expression: e.target.value })}\n              placeholder=\"e.g., name & ' ' & email\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'transform' && (\n          <label>\n            Operation:\n            <select \n              value={config.operation || 'uppercase'}\n              onChange={(e) => setConfig({ ...config, operation: e.target.value })}\n            >\n              <option value=\"uppercase\">Uppercase</option>\n              <option value=\"lowercase\">Lowercase</option>\n              <option value=\"trim\">Trim</option>\n              <option value=\"reverse\">Reverse</option>\n            </select>\n          </label>\n        )}\n      </div>\n      \n      <div className=\"panel-footer\">\n        <button onClick={handleSave}>Save</button>\n        <button onClick={onClose}>Cancel</button>\n      </div>\n    </div>\n  );\n};\n\nfunction getDefaultBlockConfig(blockType: string): Record<string, any> {\n  switch (blockType) {\n    case 'field':\n      return { path: '' };\n    case 'filter':\n      return { condition: '' };\n    case 'aggregate':\n      return { function: 'sum', path: '' };\n    case 'map':\n      return { expression: '' };\n    case 'transform':\n      return { operation: 'uppercase' };\n    default:\n      return {};\n  }\n}\n```\n\n## Sandboxed Compensation Handlers with Trust Levels\n\n### Key Innovation: CSP-Based Sandboxing + Trust Verification\n\n```typescript\ntype TrustLevel = 'system' | 'verified' | 'user' | 'untrusted';\n\ninterface CompensationHandlerConfig {\n  id: string;\n  name: string;\n  trustLevel: TrustLevel;\n  author?: string;\n  verified?: boolean;\n  \n  handlerType: 'builtin' | 'http-api' | 'webhook' | 'custom';\n  \n  matches: {\n    operation?: string[];\n    description?: string[];\n    service?: string[];\n  };\n  \n  compensation: \n    | { type: 'http-api'; config: HttpApiConfig }\n    | { type: 'webhook'; config: WebhookConfig }\n    | { type: 'custom'; workerUrl: string }  // Sandboxed Web Worker\n    | { type: 'manual'; steps: string[] };\n}\n\ninterface HttpApiConfig {\n  method: 'GET' | 'POST' | 'DELETE' | 'PUT';\n  urlTemplate: string;\n  allowedDomains: string[];  // Whitelist of domains this handler can call\n  headers: Record<string, string>;\n  bodyTemplate: string;\n  auth?: {\n    type: 'bearer' | 'basic' | 'api-key';\n    tokenPath: string;\n  };\n}\n\ninterface WebhookConfig {\n  url: string;\n  allowedDomains: string[];\n  method: 'POST' | 'PUT';\n  payloadTemplate: string;\n}\n\nclass SandboxedCompensationHandlerRegistry {\n  private handlers: Map<string, CompensationHandler> = new Map();\n  private configs: Map<string, CompensationHandlerConfig> = new Map();\n  private trustedDomains: Set<string> = new Set(['api.stripe.com', 'hooks.slack.com']);\n  \n  // User approval tracking\n  private approvedHandlers: Set<string> = new Set();\n  \n  constructor() {\n    this.loadApprovedHandlers();\n  }\n  \n  async registerFromConfig(\n    config: CompensationHandlerConfig,\n    requireApproval: boolean = true\n  ): Promise<boolean> {\n    \n    // Check trust level\n    if (requireApproval && config.trustLevel === 'untrusted') {\n      const approved = await this.requestUserApproval(config);\n      if (!approved) {\n        return false;\n      }\n      this.approvedHandlers.add(config.id);\n      this.persistApprovedHandlers();\n    }\n    \n    // Validate config\n    const validation = this.validateConfig(config);\n    if (!validation.valid) {\n      throw new Error(`Invalid handler config: ${validation.error}`);\n    }\n    \n    this.configs.set(config.id, config);\n    \n    const handler = await this.createHandlerFromConfig(config);\n    this.handlers.set(config.id, handler);\n    \n    return true;\n  }\n  \n  private validateConfig(config: CompensationHandlerConfig): { valid: boolean; error?: string } {\n    // Validate HTTP API config\n    if (config.compensation.type === 'http-api') {\n      const httpConfig = config.compensation.config;\n      \n      // Check URL against allowed domains\n      const url = new URL(httpConfig.urlTemplate.replace(/\\{\\{[^}]+\\}\\}/g, 'placeholder'));\n      \n      if (!httpConfig.allowedDomains.includes(url.hostname)) {\n        return {\n          valid: false,\n          error: `Domain ${url.hostname} not in allowedDomains list`\n        };\n      }\n      \n      // For untrusted handlers, require domain to be explicitly trusted\n      if (config.trustLevel === 'untrusted' && !this.trustedDomains.has(url.hostname)) {\n        return {\n          valid: false,\n          error: `Domain ${url.hostname} not in trusted domains list`\n        };\n      }\n    }\n    \n    // Validate webhook config\n    if (config.compensation.type === 'webhook') {\n      const webhookConfig = config.compensation.config;\n      const url = new URL(webhookConfig.url);\n      \n      if (!webhookConfig.allowedDomains.includes(url.hostname)) {\n        return {\n          valid: false,\n          error: `Webhook domain ${url.hostname} not in allowedDomains list`\n        };\n      }\n    }\n    \n    // Validate custom handler\n    if (config.compensation.type === 'custom') {\n      // Custom handlers must use data: URLs or trusted origins\n      const workerUrl = config.compensation.workerUrl;\n      \n      if (!workerUrl.startsWith('data:') && !workerUrl.startsWith('blob:')) {\n        return {\n          valid: false,\n          error: 'Custom handlers must use data: or blob: URLs for security'\n        };\n      }\n    }\n    \n    return { valid: true };\n  }\n  \n  private async requestUserApproval(\n    config: CompensationHandlerConfig\n  ): Promise<boolean> {\n    \n    return new Promise((resolve) => {\n      // Show modal to user\n      const modal = document.createElement('div');\n      modal.className = 'approval-modal';\n      modal.innerHTML = `\n        <div class=\"modal-content\">\n          <h3>\u26a0\ufe0f Handler Approval Required</h3>\n          <p>The handler \"${config.name}\" wants to perform compensations.</p>\n          \n          <div class=\"handler-details\">\n            <p><strong>Trust Level:</strong> ${config.trustLevel}</p>\n            <p><strong>Author:</strong> ${config.author || 'Unknown'}</p>\n            <p><strong>Type:</strong> ${config.compensation.type}</p>\n            \n            ${config.compensation.type === 'http-api' ? `\n              <p><strong>Will call:</strong> ${config.compensation.config.urlTemplate}</p>\n              <p><strong>Allowed domains:</strong> ${config.compensation.config.allowedDomains.join(', ')}</p>\n            ` : ''}\n            \n            ${config.compensation.type === 'webhook' ? `\n              <p><strong>Webhook URL:</strong> ${config.compensation.config.url}</p>\n            ` : ''}\n          </div>\n          \n          <p class=\"warning\">This handler will be able to make HTTP requests on your behalf. Only approve if you trust the source.</p>\n          \n          <div class=\"modal-actions\">\n            <button id=\"approve-btn\">Approve</button>\n            <button id=\"deny-btn\">Deny</button>\n          </div>\n        </div>\n      `;\n      \n      document.body.appendChild(modal);\n      \n      modal.querySelector('#approve-btn')?.addEventListener('click', () => {\n        document.body.removeChild(modal);\n        resolve(true);\n      });\n      \n      modal.querySelector('#deny-btn')?.addEventListener('click', () => {\n        document.body.removeChild(modal);\n        resolve(false);\n      });\n    });\n  }\n  \n  private async createHandlerFromConfig(\n    config: CompensationHandlerConfig\n  ): Promise<CompensationHandler> {\n    \n    return {\n      name: config.name,\n      \n      canHandle: (entry: SideEffectEntry) => {\n        const { matches } = config;\n        \n        if (matches.operation && !matches.operation.includes(entry.operation)) {\n          return false;\n        }\n        \n        if (matches.description) {\n          const descMatch = matches.description.some(pattern => \n            new RegExp(pattern).test(entry.description)\n          );\n          if (!descMatch) return false;\n        }\n        \n        if (matches.service) {\n          const descriptor = entry.compensation as any;\n          if (!matches.service.includes(descriptor.service)) {\n            return false;\n          }\n        }\n        \n        return true;\n      },\n      \n      compensate: async (entry: SideEffectEntry) => {\n        switch (config.compensation.type) {\n          case 'http-api':\n            return this.compensateViaHttpApi(entry, config.compensation.config, config.trustLevel);\n            \n          case 'webhook':\n            return this.compensateViaWebhook(entry, config.compensation.config, config.trustLevel);\n            \n          case 'custom':\n            return this.compensateViaCustomWorker(entry, config.compensation.workerUrl);\n            \n          case 'manual':\n            return {\n              success: false,\n              requiresUserAction: true,\n              manualSteps: config.compensation.steps\n            };\n            \n          default:\n            throw new Error(`Unknown compensation type`);\n        }\n      }\n    };\n  }\n  \n  private async compensateViaHttpApi(\n    entry: SideEffectEntry,\n    config: HttpApiConfig,\n    trustLevel: TrustLevel\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    \n    // Resolve URL\n    const url = this.resolveTemplate(config.urlTemplate, {\n      ...descriptor,\n      effectId: entry.id\n    });\n    \n    // Security check: Verify URL domain\n    const urlObj = new URL(url);\n    if (!config.allowedDomains.includes(urlObj.hostname)) {\n      return {\n        success: false,\n        error: `Security: Domain ${urlObj.hostname} not allowed`,\n        requiresUserAction: true,\n        manualSteps: ['Manually compensate this effect', `URL attempted: ${url}`]\n      };\n    }\n    \n    // Build request\n    const headers = { ...config.headers };\n    \n    if (config.auth) {\n      const token = this.extractValue(descriptor, config.auth.tokenPath);\n      switch (config.auth.type) {\n        case 'bearer':\n          headers['Authorization'] = `Bearer ${token}`;\n          break;\n        case 'api-key':\n          headers['Authorization'] = token;\n          break;\n      }\n    }\n    \n    const body = config.bodyTemplate\n      ? this.resolveTemplate(config.bodyTemplate, descriptor)\n      : undefined;\n    \n    try {\n      const response = await fetch(url, {\n        method: config.method,\n        headers,\n        body: body ? JSON.stringify(JSON.parse(body)) : undefined\n      });\n      \n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Compensated via ${config.method} ${url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message,\n        requiresUserAction: true,\n        manualSteps: [\n          `Failed to compensate via API: ${error.message}`,\n          `Operation: ${entry.description}`,\n          'Please manually revert'\n        ]\n      };\n    }\n  }\n  \n  private async compensateViaWebhook(\n    entry: SideEffectEntry,\n    config: WebhookConfig,\n    trustLevel: TrustLevel\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    \n    // Security check\n    const urlObj = new URL(config.url);\n    if (!config.allowedDomains.includes(urlObj.hostname)) {\n      return {\n        success: false,\n        error: `Security: Webhook domain ${urlObj.hostname} not allowed`\n      };\n    }\n    \n    const payload = this.resolveTemplate(config.payloadTemplate, {\n      ...descriptor,\n      effectId: entry.id,\n      effectDescription: entry.description\n    });\n    \n    try {\n      const response = await fetch(config.url, {\n        method: config.method,\n        headers: { 'Content-Type': 'application/json' },\n        body: payload\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Webhook failed: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Webhook triggered: ${config.url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n  \n  private async compensateViaCustomWorker(\n    entry: SideEffectEntry,\n    workerUrl: string\n  ): Promise<CompensationResult> {\n    \n    return new Promise((resolve) => {\n      const worker = new Worker(workerUrl);\n      \n      const timeout = setTimeout(() => {\n        worker.terminate();\n        resolve({\n          success: false,\n          error: 'Custom handler timeout'\n        });\n      }, 10000);\n      \n      worker.onmessage = (event) => {\n        clearTimeout(timeout);\n        worker.terminate();\n        resolve(event.data);\n      };\n      \n      worker.onerror = (error) => {\n        clearTimeout(timeout);\n        worker.terminate();\n        resolve({\n          success: false,\n          error: `Worker error: ${error.message}`\n        });\n      };\n      \n      worker.postMessage({\n        entry,\n        descriptor: entry.compensation\n      });\n    });\n  }\n  \n  private resolveTemplate(template: string, data: any): string {\n    return template.replace(/\\{\\{([^}]+)\\}\\}/g, (_, path) => {\n      return this.extractValue(data, path) ?? '';\n    });\n  }\n  \n  private extractValue(obj: any, path: string): any {\n    const parts = path.split('.');\n    let result = obj;\n    for (const part of parts) {\n      result = result?.[part];\n    }\n    return result;\n  }\n  \n  private loadApprovedHandlers(): void {\n    const stored = localStorage.getItem('approvedHandlers');\n    if (stored) {\n      this.approvedHandlers = new Set(JSON.parse(stored));\n    }\n  }\n  \n  private persistApprovedHandlers(): void {\n    localStorage.setItem('approvedHandlers', JSON.stringify([...this.approvedHandlers]));\n  }\n  \n  serializeConfigs(): CompensationHandlerConfig[] {\n    return Array.from(this.configs.values());\n  }\n  \n  loadConfigs(configs: CompensationHandlerConfig[]): void {\n    for (const config of configs) {\n      // Check if already approved\n      const requireApproval = !this.approvedHandlers.has(config.id);\n      this.registerFromConfig(config, requireApproval);\n    }\n  }\n  \n  unregister(id: string): void {\n    this.handlers.delete(id);\n    this.configs.delete(id);\n    this.approvedHandlers.delete(id);\n    this.persistApprovedHandlers();\n  }\n  \n  findHandler(entry: SideEffectEntry): CompensationHandler | null {\n    for (const handler of this.handlers.values()) {\n      if (handler.canHandle(entry)) {\n        return handler;\n      }\n    }\n    return null;\n  }\n}\n\ninterface CompensationHandler {\n  name: string;\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n  canHandle(entry: SideEffectEntry): boolean;\n}\n\ninterface SideEffectEntry {\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  operation: string;\n  description: string;\n  compensation: any;\n  status: string;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n## Adaptive Performance Metrics with IndexedDB Persistence\n\n### Key Innovation: Long-Term Analysis + Sampling Strategy\n\n```typescript\ninterface MetricsConfig {\n  enablePersistence: boolean;        // Store to IndexedDB\n  samplingRate: number;              // 0.1 = 10% of measurements\n  retentionDays: number;             // 7 days\n  enableRecommendations: boolean;\n}\n\ninterface StreamingMetrics {\n  decisions: {\n    teeCount: number;\n    materializeCount: number;\n    batchCount: number;\n  };\n  \n  performance: {\n    avgTeeMemory: number;\n    avgMaterializeSize: number;\n    avgTeeTime: number;\n    avgMaterializeTime: number;\n  };\n  \n  errors: {\n    teeFailures: number;\n    materializeFailures: number;\n    storageFullErrors: number;\n  };\n  \n  recommendations: {\n    suggestedTeeThreshold: number;\n    suggestedMaterializeThreshold: number;\n    suggestedOverflowPolicy: string;\n  };\n}\n\ninterface Measurement {\n  id: string;\n  timestamp: number;\n  strategy: 'tee' | 'materialize';\n  sizeBytes: number;\n  durationMs: number;\n  memoryUsed: number;\n  workflowId: string;\n  nodeId: string;\n}\n\nclass AdaptivePerformanceMonitor {\n  private metrics: StreamingMetrics;\n  private recentMeasurements: Measurement[] = [];\n  private db: IDBDatabase | null = null;\n  private config: MetricsConfig;\n  \n  constructor(config: MetricsConfig) {\n    this.config = config;\n    this.metrics = this.initMetrics();\n    \n    if (config.enablePersistence) {\n      this.initDatabase();\n    }\n  }\n  \n  recordDecision(\n    strategy: 'tee' | 'materialize' | 'batch',\n    sizeBytes: number,\n    durationMs: number,\n    memoryUsed: number,\n    context: {\n      workflowId: string;\n      nodeId: string;\n    }\n  ): void {\n    \n    switch (strategy) {\n      case 'tee':\n        this.metrics.decisions.teeCount++;\n        break;\n      case 'materialize':\n        this.metrics.decisions.materializeCount++;\n        break;\n      case 'batch':\n        this.metrics.decisions.batchCount++;\n        break;\n    }\n    \n    if (strategy === 'tee' || strategy === 'materialize') {\n      // Apply sampling\n      if (Math.random() < this.config.samplingRate) {\n        const measurement: Measurement = {\n          id: crypto.randomUUID(),\n          timestamp: Date.now(),\n          strategy,\n          sizeBytes,\n          durationMs,\n          memoryUsed,\n          workflowId: context.workflowId,\n          nodeId: context.nodeId\n        };\n        \n        this.recentMeasurements.push(measurement);\n        \n        // Keep last 100 in memory\n        if (this.recentMeasurements.length > 100) {\n          this.recentMeasurements.shift();\n        }\n        \n        // Persist to IndexedDB\n        if (this.config.enablePersistence && this.db) {\n          this.persistMeasurement(measurement);\n        }\n      }\n      \n      this.updateAverages();\n      \n      if (this.config.enableRecommendations) {\n        this.generateRecommendations();\n      }\n    }\n  }\n  \n  recordError(type: 'tee' | 'materialize' | 'storage-full'): void {\n    switch (type) {\n      case 'tee':\n        this.metrics.errors.teeFailures++;\n        break;\n      case 'materialize':\n        this.metrics.errors.materializeFailures++;\n        break;\n      case 'storage-full':\n        this.metrics.errors.storageFullErrors++;\n        break;\n    }\n  }\n  \n  private updateAverages(): void {\n    const tees = this.recentMeasurements.filter(m => m.strategy === 'tee');\n    const materializes = this.recentMeasurements.filter(m => m.strategy === 'materialize');\n    \n    if (tees.length > 0) {\n      this.metrics.performance.avgTeeMemory = \n        tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length;\n      this.metrics.performance.avgTeeTime = \n        tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length;\n    }\n    \n    if (materializes.length > 0) {\n      this.metrics.performance.avgMaterializeSize = \n        materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length;\n      this.metrics.performance.avgMaterializeTime = \n        materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length;\n    }\n  }\n  \n  private generateRecommendations(): void {\n    // Tee threshold\n    if (this.metrics.performance.avgTeeMemory > 10 * 1024 * 1024) {\n      this.metrics.recommendations.suggestedTeeThreshold = 3 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedTeeThreshold = 5 * 1024 * 1024;\n    }\n    \n    // Materialize threshold\n    if (this.metrics.performance.avgMaterializeTime < 100) {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 100 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 50 * 1024 * 1024;\n    }\n    \n    // Overflow policy\n    if (this.metrics.errors.storageFullErrors > 5) {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'evict-lru';\n    } else {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'compress';\n    }\n  }\n  \n  async getHistoricalMetrics(\n    workflowId?: string,\n    startTime?: number,\n    endTime?: number\n  ): Promise<Measurement[]> {\n    \n    if (!this.config.enablePersistence || !this.db) {\n      return this.recentMeasurements;\n    }\n    \n    return new Promise((resolve, reject) => {\n      const tx = this.db!.transaction('measurements', 'readonly');\n      const store = tx.objectStore('measurements');\n      const index = store.index('timestamp');\n      \n      const range = IDBKeyRange.bound(\n        startTime || 0,\n        endTime || Date.now()\n      );\n      \n      const results: Measurement[] = [];\n      const request = index.openCursor(range);\n      \n      request.onsuccess = (event) => {\n        const cursor = (event.target as IDBRequest).result;\n        if (cursor) {\n          const measurement: Measurement = cursor.value;\n          \n          if (!workflowId || measurement.workflowId === workflowId) {\n            results.push(measurement);\n          }\n          \n          cursor.continue();\n        } else {\n          resolve(results);\n        }\n      };\n      \n      request.onerror = () => reject(request.error);\n    });\n  }\n  \n  async generateReport(workflowId?: string): Promise<PerformanceReport> {\n    const measurements = await this.getHistoricalMetrics(workflowId);\n    \n    const tees = measurements.filter(m => m.strategy === 'tee');\n    const materializes = measurements.filter(m => m.strategy === 'materialize');\n    \n    return {\n      totalMeasurements: measurements.length,\n      timeRange: {\n        start: Math.min(...measurements.map(m => m.timestamp)),\n        end: Math.max(...measurements.map(m => m.timestamp))\n      },\n      strategies: {\n        tee: {\n          count: tees.length,\n          avgMemory: tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length,\n          avgDuration: tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length,\n          avgSize: tees.reduce((sum, m) => sum + m.sizeBytes, 0) / tees.length\n        },\n        materialize: {\n          count: materializes.length,\n          avgMemory: materializes.reduce((sum, m) => sum + m.memoryUsed, 0) / materializes.length,\n          avgDuration: materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length,\n          avgSize: materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length\n        }\n      },\n      recommendations: this.metrics.recommendations\n    };\n  }\n  \n  getMetrics(): StreamingMetrics {\n    return { ...this.metrics };\n  }\n  \n  getMeasurements(): Measurement[] {\n    return [...this.recentMeasurements];\n  }\n  \n  private async initDatabase(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('PerformanceMetrics', 2);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        this.cleanupOldMeasurements();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('measurements')) {\n          const store = db.createObjectStore('measurements', { keyPath: 'id' });\n          store.createIndex('timestamp', 'timestamp', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n          store.createIndex('strategy', 'strategy', { unique: false });\n        }\n      };\n    });\n  }\n  \n  private async persistMeasurement(measurement: Measurement): Promise<void> {\n    if (!this.db) return;\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    await tx.objectStore('measurements').add(measurement);\n  }\n  \n  private async cleanupOldMeasurements(): Promise<void> {\n    if (!this.db) return;\n    \n    const cutoff = Date.now() - (this.config.retentionDays * 24 * 60 * 60 * 1000);\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    const store = tx.objectStore('measurements');\n    const index = store.index('timestamp');\n    \n    const range = IDBKeyRange.upperBound(cutoff);\n    const request = index.openCursor(range);\n    \n    request.onsuccess = (event) => {\n      const cursor = (event.target as IDBRequest).result;\n      if (cursor) {\n        cursor.delete();\n        cursor.continue();\n      }\n    };\n  }\n  \n  private initMetrics(): StreamingMetrics {\n    return {\n      decisions: {\n        teeCount: 0,\n        materializeCount: 0,\n        batchCount: 0\n      },\n      performance: {\n        avgTeeMemory: 0,\n        avgMaterializeSize: 0,\n        avgTeeTime: 0,\n        avgMaterializeTime: 0\n      },\n      errors: {\n        teeFailures: 0,\n        materializeFailures: 0,\n        storageFullErrors: 0\n      },\n      recommendations: {\n        suggestedTeeThreshold: 5 * 1024 * 1024,\n        suggestedMaterializeThreshold: 50 * 1024 * 1024,\n        suggestedOverflowPolicy: 'evict-lru'\n      }\n    };\n  }\n}\n\ninterface PerformanceReport {\n  totalMeasurements: number;\n  timeRange: {\n    start: number;\n    end: number;\n  };\n  strategies: {\n    tee: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n    };\n    materialize: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n    };\n  };\n  recommendations: StreamingMetrics['recommendations'];\n}\n```\n\n---\n\n## Rationale\n\n### 1. Resilient Stream Manager with Auto-Checkpointing\n**Why**: Solves the page refresh problem without requiring user intervention.\n- **Three materialization strategies**: `full` (all data), `partial` (last N chunks), `regenerate` (re-execute node)\n- **Automatic checkpointing**: Long-lived streams (>60s) are automatically checkpointed every 30s\n- **SessionStorage for metadata**: Checkpoint metadata persists across page refresh\n- **Resurrection logic**: Streams can be restored from checkpoints transparently\n\n### 2. Three-Tier Storage with Per-Workflow Policies\n**Why**: Different workflows have different storage needs. Heavy data workflows need aggressive eviction, lightweight workflows don't.\n- **Memory + IndexedDB + SessionStorage**: Three tiers with clear purposes\n- **Per-workflow policies**: Each workflow can configure its own overflow policy and eviction preferences\n- **Global fallback**: Workflows inherit global policy if not customized\n- **Session tier for critical state**: Workflow definitions and execution state use SessionStorage (survives refresh)\n\n### 3. Visual-Text Hybrid Expression Editor\n**Why**: Best of both worlds - power users can type, visual users can click.\n- **Rich text editor with inline blocks**: Blocks appear as visual chips within text\n- **Click-to-insert palette**: Easy block insertion at cursor position\n- **Configurable blocks**: Click a block to open config panel\n- **Raw text fallback**: Toggle to see/edit the underlying `{{block:type:config}}` syntax\n- **Automatic sync**: Changes in visual mode update raw text and vice versa\n\n### 4. Sandboxed Compensation Handlers with Trust Levels\n**Why**: Security without sacrificing flexibility.\n- **Four trust levels**: `system` (built-in), `verified` (marketplace), `user` (self-created), `untrusted` (external)\n- **Domain whitelisting**: HTTP handlers can only call explicitly allowed domains\n- **User approval for untrusted**: Modal prompts user before registering untrusted handlers\n- **CSP sandboxing for custom**: Custom handlers run in Web Workers with no DOM access\n- **Approval persistence**: Once approved, handlers don't require re-approval (stored in localStorage)\n\n### 5. Adaptive Performance Metrics with IndexedDB\n**Why**: Long-term learning without memory bloat.\n- **10% sampling rate**: Only record 1 in 10 measurements to reduce overhead\n- **IndexedDB persistence**: Metrics survive page refresh for long-term analysis\n- **7-day retention**: Auto-cleanup of old data\n- **Per-workflow reports**: Generate performance reports for specific workflows\n- **Recommendation engine**: Uses historical data to suggest better thresholds\n\n---\n\n## What I Changed\n\n### Stream Persistence Strategy\n**Changed from**: Vague \"materialize on timeout\" to concrete auto-checkpointing system\n- **Added**: Three materialization strategies (full/partial/regenerate)\n- **Added**: Automatic 30s checkpoint interval for long-lived streams\n- **Added**: SessionStorage-based checkpoint metadata persistence\n- **Added**: Transparent stream resurrection after page refresh\n\n### Storage Architecture\n**Changed from**: Global overflow policies to per-workflow policies\n- **Added**: Third storage tier (SessionStorage) for critical workflow state\n- **Added**: Workflow-scoped storage quotas and eviction preferences\n- **Added**: Global policy as fallback for workflows without custom config\n\n### Expression Editor UX\n**Changed from**: Abstract \"inline markers\" to concrete rich text editor with visual blocks\n- **Added**: Click-to-insert block palette\n- **Added**: Visual block components rendered inline with text\n- **Added**: Config panel that opens on block selection\n- **Added**: Raw text toggle for advanced users\n\n### Compensation Handler Security\n**Changed from**: \"Should we sandbox?\" to concrete CSP-based trust model\n- **Added**: Four trust levels with different approval requirements\n- **Added**: Domain whitelisting for HTTP/webhook handlers\n- **Added**: User approval modal for untrusted handlers\n- **Added**: Approval persistence in localStorage\n- **Added**: Web Worker sandboxing for custom handlers\n\n### Performance Metrics\n**Changed from**: In-memory metrics to persistent, sampled metrics\n- **Added**: 10% sampling rate to reduce overhead\n- **Added**: IndexedDB persistence with 7-day retention\n- **Added**: Historical query API for long-term analysis\n- **Added**: Per-workflow performance reports\n\n---\n\n## What I Kept\n\nAll core architectural decisions remain:\n- React Flow canvas\n- Zustand + immer state management\n- Adaptive streaming executor\n- Two-tier storage (Memory + IndexedDB) - now enhanced to three-tier\n- Composable expression system - now with concrete visual editor\n- Append-only side effect log\n- Checkpoint manager\n- Worker pools\n- Compression\n- All performance optimizations\n\n---\n\n## Open Questions\n\n### 1. Stream Regeneration Executor Injection\nThe `ResilientStreamManager` needs access to the executor to regenerate streams. Currently it throws an error.\n- **Option A**: Inject executor via constructor parameter\n- **Option B**: Use callback property `regenerationExecutor` set after initialization\n- **Option C**: Pass executor on each `registerStream` call\n- **Recommendation**: Option B (callback property) for flexibility\n\n### 2. SessionStorage Capacity Limits\nSessionStorage has ~5-10MB limit. For large workflows with many checkpoints:\n- **Issue**: May exceed SessionStorage quota\n- **Mitigation**: Only store lightweight checkpoint metadata, not full data\n- **Alternative**: Use IndexedDB for checkpoint metadata too (but won't survive tab close)\n\n### 3. Sampling Rate Configuration\n10% sampling might be too aggressive for small workflows (< 10 executions):\n- **Option A**: Dynamic sampling (100% for first 20 measurements, then 10%)\n- **Option B**: User-configurable sampling rate per workflow\n- **Option C**: Keep static 10% globally\n- **Recommendation**: Option A (dynamic) for better initial data\n\n### 4. Trust Level Assignment\nHow do we assign trust levels to handlers?\n- **System**: Built-in handlers (Stripe, Slack, etc.)\n- **Verified**: Who verifies? Do we need a marketplace?\n- **User**: User-created handlers (default)\n- **Untrusted**: Imported from external sources\n- **Question**: Should we auto-assign based on handler origin (same-domain = user, cross-domain = untrusted)?\n\n---\n\n## Convergence Status\n\n**PROPOSING_FINAL** - All five critical design questions have concrete, implementable solutions:\n\n\u2705 Stream persistence: Auto-checkpointing with three materialization strategies  \n\u2705 Expression editor UX: Rich text editor with inline visual blocks  \n\u2705 Handler security: Trust levels + domain whitelisting + CSP sandboxing  \n\u2705 Metrics persistence: IndexedDB + 10% sampling + 7-day retention  \n\u2705 Eviction strategy: Per-workflow policies with global fallback  \n\nAll remaining questions are implementation details that can be resolved during coding.\n\n---\n\n##",
        "prompt_for_other": "The design is complete and ready for implementation. Please focus final review on:\n\n1. **Stream Resurrection Edge Cases**: Are there scenarios where resurrection would fail catastrophically? What if the node config changed between checkpoint and resurrection?\n\n2. **SessionStorage Overflow**: With checkpoint metadata in SessionStorage, what happens when we hit the 5MB limit? Should we have a fallback strategy?\n\n3. **Visual Block Editor Performance**: Rendering many inline block components could be slow. Should we virtualize the editor content? Or set a max block count per expression?\n\n4. **Trust Level UX**: Is the four-level trust model (system/verified/user/untrusted) too complex for users? Would a simpler \"trusted/untrusted\" binary be better?\n\n5. **Metrics Sampling Bias**: 10% sampling means we might miss rare edge cases or performance spikes. Is there a smarter sampling strategy (e.g., always sample errors, sometimes sample successes)?\n\nIf you have strong objections to any of these solutions, please suggest concrete alternatives. Otherwise, this design is ready to build.",
        "convergence_signal": "PROPOSING_FINAL",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v5\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Resilient \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Stream    \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502            \u2502  \u2502  Manager   \u2502  \u2502            \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Tiered   \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  Storage   \u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Manager   \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n### Technology Stack\n- **Canvas**: React Flow v11+ with viewport culling\n- **State**: Zustand with immer middleware and persistence\n- **UI**: Radix UI primitives + Tailwind CSS\n- **Execution**: Adaptive executor with resilient streaming\n- **Storage**: Three-tier (Memory + IndexedDB + SessionStorage) with per-workflow policies\n- **Expressions**: Composable expression system with visual-text hybrid mode\n- **Side Effects**: Append-only log with sandboxed compensation handlers\n- **Workers**: Shared Worker pool with resource quotas\n\n## Resilient Stream Manager (Solves Page Refresh Problem)\n\n### Key Innovation: Automatic Checkpoint-Based Stream Resurrection\n\n```typescript\ninterface StreamManagerConfig {\n  maxConcurrentStreams: number;        // 50\n  streamTimeout: number;                // 5min\n  autoCheckpointInterval: number;       // 30s - checkpoint long-lived streams\n  checkpointThresholdAge: number;       // 60s - checkpoint if older than this\n  enableResurrection: boolean;          // Restore streams after page refresh\n  maxResurrectionAttempts: number;      // 3\n}\n\ninterface StreamCheckpoint {\n  streamId: string;\n  sourceNodeId: string;\n  executionId: string;\n  consumerCount: number;\n  createdAt: number;\n  lastCheckpointAt: number;\n  \n  // Resurrection data\n  materializationType: 'full' | 'partial' | 'regenerate';\n  materializedData?: any;               // For 'full' or 'partial'\n  partialOffset?: number;               // Where partial stopped\n  regenerationConfig?: {                // For 'regenerate'\n    nodeConfig: any;\n    inputs: Record<string, any>;\n  };\n  \n  // State\n  status: 'active' | 'checkpointed' | 'resurrected' | 'expired';\n  resurrectionAttempts: number;\n}\n\nclass ResilientStreamManager {\n  private activeStreams: Map<string, ActiveStreamEntry> = new Map();\n  private checkpoints: Map<string, StreamCheckpoint> = new Map();\n  private storage: HybridStorageManager;\n  private config: StreamManagerConfig;\n  \n  constructor(storage: HybridStorageManager, config: StreamManagerConfig) {\n    this.storage = storage;\n    this.config = config;\n    \n    // Restore checkpoints from previous session\n    if (config.enableResurrection) {\n      this.restoreCheckpointsFromStorage();\n    }\n    \n    this.startAutoCheckpointing();\n    this.startTimeoutMonitor();\n  }\n  \n  async registerStream(\n    sourceNodeId: string,\n    executionId: string,\n    stream: ReadableStream,\n    consumerCount: number,\n    options: {\n      estimatedSize: number;\n      materializationType?: 'full' | 'partial' | 'regenerate';\n      regenerationConfig?: any;\n    }\n  ): Promise<string> {\n    \n    if (this.activeStreams.size >= this.config.maxConcurrentStreams) {\n      throw new Error(\n        `Max concurrent streams reached (${this.config.maxConcurrentStreams})`\n      );\n    }\n    \n    const streamId = crypto.randomUUID();\n    \n    // Create teed streams\n    const teedStreams = this.createMultiTee(stream, consumerCount);\n    \n    // Create checkpoint\n    const checkpoint: StreamCheckpoint = {\n      streamId,\n      sourceNodeId,\n      executionId,\n      consumerCount,\n      createdAt: Date.now(),\n      lastCheckpointAt: Date.now(),\n      materializationType: options.materializationType ?? 'full',\n      regenerationConfig: options.regenerationConfig,\n      status: 'active',\n      resurrectionAttempts: 0\n    };\n    \n    this.checkpoints.set(streamId, checkpoint);\n    \n    this.activeStreams.set(streamId, {\n      checkpoint,\n      streams: teedStreams,\n      readers: new Set(),\n      materializedChunks: options.materializationType !== 'regenerate' ? [] : undefined\n    });\n    \n    return streamId;\n  }\n  \n  async getStream(streamId: string, consumerIndex: number): Promise<ReadableStream> {\n    const active = this.activeStreams.get(streamId);\n    \n    if (active) {\n      // Stream is live\n      active.checkpoint.lastCheckpointAt = Date.now();\n      return active.streams[consumerIndex];\n    }\n    \n    // Try to resurrect from checkpoint\n    if (this.config.enableResurrection) {\n      const resurrected = await this.resurrectStream(streamId, consumerIndex);\n      if (resurrected) return resurrected;\n    }\n    \n    throw new Error(`Stream not found and could not be resurrected: ${streamId}`);\n  }\n  \n  private async resurrectStream(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStream | null> {\n    \n    const checkpoint = this.checkpoints.get(streamId);\n    if (!checkpoint) return null;\n    \n    if (checkpoint.resurrectionAttempts >= this.config.maxResurrectionAttempts) {\n      console.error(`Max resurrection attempts reached for stream ${streamId}`);\n      return null;\n    }\n    \n    checkpoint.resurrectionAttempts++;\n    \n    try {\n      switch (checkpoint.materializationType) {\n        case 'full':\n          return await this.resurrectFromFullMaterialization(checkpoint);\n          \n        case 'partial':\n          return await this.resurrectFromPartialMaterialization(checkpoint);\n          \n        case 'regenerate':\n          return await this.resurrectViaRegeneration(checkpoint);\n          \n        default:\n          throw new Error(`Unknown materialization type: ${checkpoint.materializationType}`);\n      }\n    } catch (error: any) {\n      console.error(`Stream resurrection failed: ${error.message}`);\n      return null;\n    }\n  }\n  \n  private async resurrectFromFullMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    // Load materialized data from storage\n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const materialized = await this.storage.retrieve(storageKey);\n    \n    // Convert back to stream\n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(materialized)) {\n          for (const chunk of materialized) {\n            controller.enqueue(chunk);\n          }\n        } else {\n          controller.enqueue(materialized);\n        }\n        controller.close();\n      }\n    });\n    \n    checkpoint.status = 'resurrected';\n    \n    return stream;\n  }\n  \n  private async resurrectFromPartialMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    // Load partial data\n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const partialData = await this.storage.retrieve(storageKey);\n    \n    // Return stream that first emits partial data, then signals \"incomplete\"\n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(partialData)) {\n          for (const chunk of partialData) {\n            controller.enqueue(chunk);\n          }\n        }\n        \n        // Signal incomplete\n        controller.enqueue({\n          $incomplete: true,\n          offset: checkpoint.partialOffset,\n          message: 'Stream was interrupted. Partial data only.'\n        });\n        \n        controller.close();\n      }\n    });\n    \n    checkpoint.status = 'resurrected';\n    \n    return stream;\n  }\n  \n  private async resurrectViaRegeneration(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    if (!checkpoint.regenerationConfig) {\n      throw new Error('No regeneration config available');\n    }\n    \n    // Re-execute the node to regenerate the stream\n    // This requires access to the executor (injected via callback)\n    const regenerated = await this.executeNodeForRegeneration(\n      checkpoint.sourceNodeId,\n      checkpoint.regenerationConfig.nodeConfig,\n      checkpoint.regenerationConfig.inputs\n    );\n    \n    checkpoint.status = 'resurrected';\n    \n    return regenerated;\n  }\n  \n  private async executeNodeForRegeneration(\n    nodeId: string,\n    nodeConfig: any,\n    inputs: Record<string, any>\n  ): Promise<ReadableStream> {\n    \n    // This is called by injecting an executor callback during initialization\n    // For now, throw error to indicate implementation needed\n    throw new Error(\n      'Regeneration requires executor callback. ' +\n      'Set ResilientStreamManager.regenerationExecutor = (node, inputs) => stream'\n    );\n  }\n  \n  private startAutoCheckpointing(): void {\n    setInterval(() => {\n      this.checkpointLongLivedStreams();\n    }, this.config.autoCheckpointInterval);\n  }\n  \n  private async checkpointLongLivedStreams(): Promise<void> {\n    const now = Date.now();\n    const threshold = this.config.checkpointThresholdAge;\n    \n    for (const [streamId, entry] of this.activeStreams.entries()) {\n      const age = now - entry.checkpoint.createdAt;\n      \n      if (age > threshold && entry.checkpoint.status === 'active') {\n        await this.checkpointStream(streamId);\n      }\n    }\n  }\n  \n  private async checkpointStream(streamId: string): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    const checkpoint = entry.checkpoint;\n    \n    switch (checkpoint.materializationType) {\n      case 'full':\n        // Materialize all consumed chunks so far\n        if (entry.materializedChunks && entry.materializedChunks.length > 0) {\n          const storageKey = `stream-checkpoint-${streamId}`;\n          checkpoint.materializedData = [...entry.materializedChunks];\n          \n          await this.storage.store(storageKey, checkpoint.materializedData, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'partial':\n        // Only keep last N chunks\n        if (entry.materializedChunks) {\n          const keepCount = 10;\n          const partial = entry.materializedChunks.slice(-keepCount);\n          \n          const storageKey = `stream-checkpoint-${streamId}`;\n          await this.storage.store(storageKey, partial, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.partialOffset = entry.materializedChunks.length - keepCount;\n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'regenerate':\n        // No materialization needed - regeneration config already stored\n        checkpoint.status = 'checkpointed';\n        checkpoint.lastCheckpointAt = Date.now();\n        break;\n    }\n    \n    // Persist checkpoint metadata to SessionStorage\n    await this.persistCheckpoint(checkpoint);\n  }\n  \n  private async persistCheckpoint(checkpoint: StreamCheckpoint): Promise<void> {\n    const key = `stream-checkpoint-meta-${checkpoint.streamId}`;\n    sessionStorage.setItem(key, JSON.stringify(checkpoint));\n  }\n  \n  private async restoreCheckpointsFromStorage(): Promise<void> {\n    const keys = Object.keys(sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('stream-checkpoint-meta-')) {\n        const checkpointData = sessionStorage.getItem(key);\n        if (checkpointData) {\n          const checkpoint: StreamCheckpoint = JSON.parse(checkpointData);\n          this.checkpoints.set(checkpoint.streamId, checkpoint);\n        }\n      }\n    }\n  }\n  \n  private createMultiTee(stream: ReadableStream, count: number): ReadableStream[] {\n    if (count === 1) return [stream];\n    if (count === 2) return stream.tee();\n    \n    const streams: ReadableStream[] = [];\n    let remaining = stream;\n    \n    for (let i = 0; i < count - 1; i++) {\n      const [stream1, stream2] = remaining.tee();\n      streams.push(stream1);\n      remaining = stream2;\n    }\n    streams.push(remaining);\n    \n    return streams;\n  }\n  \n  async acquireReader(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStreamDefaultReader> {\n    const stream = await this.getStream(streamId, consumerIndex);\n    const reader = stream.getReader();\n    \n    const entry = this.activeStreams.get(streamId);\n    if (entry) {\n      entry.readers.add(reader);\n      \n      // Wrap reader to capture chunks for checkpointing\n      if (entry.materializedChunks) {\n        return this.wrapReaderForCheckpointing(reader, entry.materializedChunks);\n      }\n    }\n    \n    return reader;\n  }\n  \n  private wrapReaderForCheckpointing(\n    reader: ReadableStreamDefaultReader,\n    chunksBuffer: any[]\n  ): ReadableStreamDefaultReader {\n    \n    const originalRead = reader.read.bind(reader);\n    \n    reader.read = async () => {\n      const result = await originalRead();\n      \n      if (!result.done && result.value !== undefined) {\n        chunksBuffer.push(result.value);\n      }\n      \n      return result;\n    };\n    \n    return reader;\n  }\n  \n  async releaseReader(\n    streamId: string, \n    reader: ReadableStreamDefaultReader\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    try {\n      reader.releaseLock();\n    } catch {\n      // Already released\n    }\n    \n    entry.readers.delete(reader);\n    \n    if (entry.readers.size === 0) {\n      await this.cleanup(streamId, 'completed');\n    }\n  }\n  \n  private async cleanup(\n    streamId: string, \n    reason: 'completed' | 'error' | 'timeout'\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    // Final checkpoint before cleanup\n    if (reason === 'completed' && entry.checkpoint.materializationType !== 'regenerate') {\n      await this.checkpointStream(streamId);\n    }\n    \n    entry.checkpoint.status = 'expired';\n    \n    for (const stream of entry.streams) {\n      try {\n        const reader = stream.getReader();\n        await reader.cancel(`Cleanup: ${reason}`);\n        reader.releaseLock();\n      } catch {\n        // Ignore\n      }\n    }\n    \n    this.activeStreams.delete(streamId);\n  }\n  \n  private startTimeoutMonitor(): void {\n    setInterval(() => {\n      const now = Date.now();\n      const timeout = this.config.streamTimeout;\n      \n      for (const [streamId, entry] of this.activeStreams.entries()) {\n        if (now - entry.checkpoint.lastCheckpointAt > timeout) {\n          console.warn(`Stream timeout: ${streamId}`);\n          this.cleanup(streamId, 'timeout');\n        }\n      }\n    }, 60000);\n  }\n  \n  getCheckpoint(streamId: string): StreamCheckpoint | null {\n    return this.checkpoints.get(streamId) ?? null;\n  }\n  \n  listActiveStreams(): StreamCheckpoint[] {\n    return Array.from(this.activeStreams.values())\n      .map(entry => entry.checkpoint);\n  }\n  \n  // Public property for injecting regeneration logic\n  regenerationExecutor?: (\n    nodeId: string, \n    nodeConfig: any, \n    inputs: Record<string, any>\n  ) => Promise<ReadableStream>;\n}\n\ninterface ActiveStreamEntry {\n  checkpoint: StreamCheckpoint;\n  streams: ReadableStream[];\n  readers: Set<ReadableStreamDefaultReader>;\n  materializedChunks?: any[];  // For 'full' or 'partial' checkpointing\n}\n```\n\n## Enhanced Tiered Storage with Per-Workflow Policies\n\n### Key Innovation: Workflow-Scoped Storage Configuration\n\n```typescript\ninterface WorkflowStoragePolicy {\n  workflowId: string;\n  \n  // Tier capacities (per workflow)\n  memoryCacheMaxBytes: number;          // Default: 50MB per workflow\n  indexedDBMaxBytes: number;            // Default: 500MB per workflow\n  \n  // Overflow behavior\n  overflowPolicy: 'evict-lru' | 'evict-oldest' | 'fail' | 'compress' | 'external';\n  \n  // Eviction preferences\n  evictionPreferences: {\n    protectRecent: boolean;             // Don't evict < 5s old\n    protectFrequent: boolean;           // Prefer low-access items\n    protectPinned: boolean;             // Never evict pinned\n  };\n  \n  // Auto-cleanup\n  autoCleanupAge: number;               // 24h\n  enableAutoCleanup: boolean;\n}\n\nclass TieredStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  \n  // Per-workflow policies\n  private workflowPolicies: Map<string, WorkflowStoragePolicy> = new Map();\n  private globalPolicy: WorkflowStoragePolicy;\n  \n  // Artifact metadata\n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  \n  // External blobs\n  private externalBlobs: Map<string, string> = new Map();\n  \n  // Session-level temporary storage (survives refresh)\n  private sessionStorage: Storage = window.sessionStorage;\n  \n  constructor(globalPolicy: WorkflowStoragePolicy) {\n    this.globalPolicy = globalPolicy;\n    this.initIndexedDB();\n    this.restoreFromSession();\n  }\n  \n  setWorkflowPolicy(workflowId: string, policy: WorkflowStoragePolicy): void {\n    this.workflowPolicies.set(workflowId, policy);\n  }\n  \n  getPolicy(workflowId?: string): WorkflowStoragePolicy {\n    if (workflowId) {\n      return this.workflowPolicies.get(workflowId) ?? this.globalPolicy;\n    }\n    return this.globalPolicy;\n  }\n  \n  async store(\n    key: string, \n    data: any, \n    hints: StorageHints = {}\n  ): Promise<void> {\n    \n    const policy = this.getPolicy(hints.workflowId);\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    // Create metadata\n    const metadata: ArtifactMetadata = {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      pinned: hints.pinned ?? false,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      workflowId: hints.workflowId,\n      dataType: hints.dataType ?? 'json'\n    };\n    \n    this.artifactIndex.set(key, metadata);\n    \n    // Check if we should use session storage (for critical workflow state)\n    if (hints.useSessionStorage) {\n      this.sessionStorage.setItem(key, serialized);\n      metadata.tier = 'session';\n      return;\n    }\n    \n    // Tier decision\n    if (sizeBytes < 1024 * 1024) {\n      if (await this.tryStoreInMemory(key, data, sizeBytes, policy)) {\n        metadata.tier = 'memory';\n        return;\n      }\n    }\n    \n    await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n    metadata.tier = 'indexeddb';\n  }\n  \n  private async tryStoreInMemory(\n    key: string, \n    data: any, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    if (this.memoryCacheSize + sizeBytes <= policy.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    if (await this.evictFromMemory(sizeBytes, policy)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(\n    requiredBytes: number, \n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'memory' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        (!policy.evictionPreferences.protectRecent || \n         Date.now() - m.createdAt > 5000)\n      );\n    \n    // Sort by policy\n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const metadata of candidates) {\n      toEvict.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes, policy);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n      }\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    if (this.indexedDBSize + sizeBytes > policy.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes, policy);\n      return;\n    }\n    \n    const shouldCompress = sizeBytes > 1024 * 1024;\n    const toStore = shouldCompress \n      ? await this.compress(serialized)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    await tx.objectStore('artifacts').put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    switch (policy.overflowPolicy) {\n      case 'evict-lru':\n      case 'evict-oldest':\n        await this.evictFromIndexedDB(sizeBytes, policy);\n        await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n        break;\n        \n      case 'fail':\n        throw new StorageFullError(\n          `Storage full for workflow: ${policy.workflowId}`,\n          this.getStorageStats()\n        );\n        \n      case 'compress':\n        const compressed = await this.compress(serialized);\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > policy.indexedDBMaxBytes) {\n          await this.evictFromIndexedDB(compressedSize, policy);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        break;\n        \n      case 'external':\n        const blob = new Blob([await this.compress(serialized)]);\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'external';\n        }\n        break;\n    }\n  }\n  \n  private async evictFromIndexedDB(\n    requiredBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'indexeddb' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        m.workflowId === policy.workflowId  // Only evict from same workflow\n      );\n    \n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const metadata of candidates) {\n      toDelete.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (toDelete.length === 0) {\n      throw new Error('Cannot evict: all items protected or wrong workflow');\n    }\n    \n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    switch (metadata.tier) {\n      case 'memory':\n        return this.memoryCache.get(key)?.data;\n        \n      case 'indexeddb':\n        return await this.retrieveFromIndexedDB(key);\n        \n      case 'session':\n        const sessionData = this.sessionStorage.getItem(key);\n        return sessionData ? JSON.parse(sessionData) : null;\n        \n      case 'external':\n        return await this.retrieveFromExternalBlob(key);\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const record = await tx.objectStore('artifacts').get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async pin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = true;\n    }\n  }\n  \n  async unpin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = false;\n    }\n  }\n  \n  private async compress(data: string): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 4);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n  \n  private restoreFromSession(): void {\n    // Restore critical workflow state from session storage\n    const keys = Object.keys(this.sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('workflow-')) {\n        const metadata: ArtifactMetadata = {\n          key,\n          sizeBytes: this.sessionStorage.getItem(key)!.length,\n          tier: 'session',\n          createdAt: Date.now(),\n          lastAccessedAt: Date.now(),\n          accessCount: 0,\n          temporary: false,\n          pinned: true,\n          dataType: 'json'\n        };\n        \n        this.artifactIndex.set(key, metadata);\n      }\n    }\n  }\n  \n  getStorageStats(workflowId?: string): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      session: { count: 0, bytes: 0 },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      if (workflowId && metadata.workflowId !== workflowId) continue;\n      \n      stats[metadata.tier as keyof typeof stats].count++;\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n}\n\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  pinned?: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType?: 'stream-ref' | 'materialized' | 'blob' | 'json';\n  useSessionStorage?: boolean;\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'session' | 'external' | 'determining';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  pinned: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  session: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n\nclass StorageFullError extends Error {\n  constructor(message: string, public stats: StorageStats) {\n    super(message);\n    this.name = 'StorageFullError';\n  }\n}\n```\n\n## Visual-Text Hybrid Expression Editor\n\n### Key Innovation: Rich Text Editor with Inline Block Components\n\n```typescript\ninterface HybridExpression {\n  mode: 'hybrid';\n  content: EditorContent[];  // Prosemirror-style content\n  rawText: string;           // Plain text with markers (for serialization)\n}\n\ntype EditorContent = TextNode | BlockNode;\n\ninterface TextNode {\n  type: 'text';\n  text: string;\n}\n\ninterface BlockNode {\n  type: 'block';\n  blockType: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n  displayText: string;  // Human-readable representation\n}\n\nclass HybridExpressionEditor {\n  private engine: ComposableExpressionEngine;\n  \n  constructor() {\n    this.engine = new ComposableExpressionEngine();\n  }\n  \n  parseFromText(text: string): HybridExpression {\n    const content: EditorContent[] = [];\n    const blockMarkerRegex = /\\{\\{block:([^:}]+):([^}]+)\\}\\}/g;\n    \n    let lastIndex = 0;\n    const matches = text.matchAll(blockMarkerRegex);\n    \n    for (const match of matches) {\n      // Add text before block\n      if (match.index! > lastIndex) {\n        const textPart = text.slice(lastIndex, match.index);\n        if (textPart) {\n          content.push({ type: 'text', text: textPart });\n        }\n      }\n      \n      // Parse block\n      const [_, blockType, configStr] = match;\n      const config = this.parseBlockConfig(blockType, configStr);\n      const displayText = this.generateDisplayText(blockType, config);\n      \n      content.push({\n        type: 'block',\n        blockType: blockType as any,\n        config,\n        displayText\n      });\n      \n      lastIndex = match.index! + match[0].length;\n    }\n    \n    // Add remaining text\n    if (lastIndex < text.length) {\n      content.push({ type: 'text', text: text.slice(lastIndex) });\n    }\n    \n    return {\n      mode: 'hybrid',\n      content,\n      rawText: text\n    };\n  }\n  \n  serializeToText(expression: HybridExpression): string {\n    return expression.content.map(node => {\n      if (node.type === 'text') {\n        return node.text;\n      } else {\n        const configStr = this.serializeBlockConfig(node.blockType, node.config);\n        return `{{block:${node.blockType}:${configStr}}}`;\n      }\n    }).join('');\n  }\n  \n  private parseBlockConfig(type: string, configStr: string): Record<string, any> {\n    const pairs = configStr.split(':');\n    \n    switch (type) {\n      case 'field':\n        return { path: pairs[0] };\n      case 'filter':\n        return { condition: pairs[0] };\n      case 'aggregate':\n        return { function: pairs[0], path: pairs[1] || '' };\n      case 'map':\n        return { expression: pairs[0] };\n      case 'transform':\n        return { operation: pairs[0] };\n      default:\n        return {};\n    }\n  }\n  \n  private serializeBlockConfig(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return config.path;\n      case 'filter':\n        return config.condition;\n      case 'aggregate':\n        return `${config.function}:${config.path}`;\n      case 'map':\n        return config.expression;\n      case 'transform':\n        return config.operation;\n      default:\n        return '';\n    }\n  }\n  \n  private generateDisplayText(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return `$.${config.path}`;\n      case 'filter':\n        return `[${config.condition}]`;\n      case 'aggregate':\n        return `${config.function}(${config.path})`;\n      case 'map':\n        return `map(${config.expression})`;\n      case 'transform':\n        return config.operation;\n      default:\n        return 'block';\n    }\n  }\n  \n  insertBlock(\n    expression: HybridExpression, \n    cursorPosition: number,\n    blockType: string,\n    config: Record<string, any>\n  ): HybridExpression {\n    \n    const displayText = this.generateDisplayText(blockType, config);\n    const blockNode: BlockNode = {\n      type: 'block',\n      blockType: blockType as any,\n      config,\n      displayText\n    };\n    \n    // Find position in content array\n    let currentPos = 0;\n    let insertIndex = 0;\n    \n    for (let i = 0; i < expression.content.length; i++) {\n      const node = expression.content[i];\n      const nodeLength = node.type === 'text' \n        ? node.text.length \n        : node.displayText.length;\n      \n      if (currentPos + nodeLength >= cursorPosition) {\n        insertIndex = i + 1;\n        break;\n      }\n      \n      currentPos += nodeLength;\n    }\n    \n    const newContent = [\n      ...expression.content.slice(0, insertIndex),\n      blockNode,\n      ...expression.content.slice(insertIndex)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent })\n    };\n  }\n  \n  removeBlock(expression: HybridExpression, blockIndex: number): HybridExpression {\n    const newContent = expression.content.filter((_, i) => i !== blockIndex);\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent })\n    };\n  }\n  \n  updateBlock(\n    expression: HybridExpression, \n    blockIndex: number,\n    newConfig: Record<string, any>\n  ): HybridExpression {\n    \n    const node = expression.content[blockIndex];\n    if (node.type !== 'block') {\n      throw new Error('Node is not a block');\n    }\n    \n    const updatedNode: BlockNode = {\n      ...node,\n      config: newConfig,\n      displayText: this.generateDisplayText(node.blockType, newConfig)\n    };\n    \n    const newContent = [\n      ...expression.content.slice(0, blockIndex),\n      updatedNode,\n      ...expression.content.slice(blockIndex + 1)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent })\n    };\n  }\n}\n\n// React Component\nconst HybridExpressionEditorUI: React.FC<{\n  value: HybridExpression;\n  onChange: (value: HybridExpression) => void;\n}> = ({ value, onChange }) => {\n  \n  const editor = useMemo(() => new HybridExpressionEditor(), []);\n  const [cursorPosition, setCursorPosition] = useState(0);\n  const [selectedBlockIndex, setSelectedBlockIndex] = useState<number | null>(null);\n  const editorRef = useRef<HTMLDivElement>(null);\n  \n  const handleInsertBlock = (blockType: string) => {\n    const defaultConfig = getDefaultBlockConfig(blockType);\n    const updated = editor.insertBlock(value, cursorPosition, blockType, defaultConfig);\n    onChange(updated);\n  };\n  \n  const handleRemoveBlock = (blockIndex: number) => {\n    const updated = editor.removeBlock(value, blockIndex);\n    onChange(updated);\n    setSelectedBlockIndex(null);\n  };\n  \n  const handleUpdateBlock = (blockIndex: number, newConfig: Record<string, any>) => {\n    const updated = editor.updateBlock(value, blockIndex, newConfig);\n    onChange(updated);\n  };\n  \n  const handleTextChange = (newText: string) => {\n    const parsed = editor.parseFromText(newText);\n    onChange(parsed);\n  };\n  \n  return (\n    <div className=\"hybrid-expression-editor\">\n      {/* Block Palette */}\n      <div className=\"block-palette\">\n        <button onClick={() => handleInsertBlock('field')}>\n          <span className=\"icon\">\ud83d\udcc4</span> Field\n        </button>\n        <button onClick={() => handleInsertBlock('filter')}>\n          <span className=\"icon\">\ud83d\udd0d</span> Filter\n        </button>\n        <button onClick={() => handleInsertBlock('aggregate')}>\n          <span className=\"icon\">\u03a3</span> Aggregate\n        </button>\n        <button onClick={() => handleInsertBlock('map')}>\n          <span className=\"icon\">\ud83d\uddfa\ufe0f</span> Map\n        </button>\n        <button onClick={() => handleInsertBlock('transform')}>\n          <span className=\"icon\">\u2699\ufe0f</span> Transform\n        </button>\n      </div>\n      \n      {/* Rich Text Editor */}\n      <div \n        ref={editorRef}\n        className=\"editor-content\"\n        contentEditable\n        suppressContentEditableWarning\n        onInput={(e) => {\n          const text = e.currentTarget.textContent || '';\n          handleTextChange(text);\n        }}\n        onSelect={() => {\n          const selection = window.getSelection();\n          if (selection) {\n            setCursorPosition(selection.anchorOffset);\n          }\n        }}\n      >\n        {value.content.map((node, index) => {\n          if (node.type === 'text') {\n            return <span key={index}>{node.text}</span>;\n          } else {\n            return (\n              <BlockComponent\n                key={index}\n                node={node}\n                selected={selectedBlockIndex === index}\n                onSelect={() => setSelectedBlockIndex(index)}\n                onRemove={() => handleRemoveBlock(index)}\n                onUpdate={(config) => handleUpdateBlock(index, config)}\n              />\n            );\n          }\n        })}\n      </div>\n      \n      {/* Block Config Panel (when block selected) */}\n      {selectedBlockIndex !== null && value.content[selectedBlockIndex].type === 'block' && (\n        <BlockConfigPanel\n          blockNode={value.content[selectedBlockIndex] as BlockNode}\n          onUpdate={(config) => handleUpdateBlock(selectedBlockIndex, config)}\n          onClose={() => setSelectedBlockIndex(null)}\n        />\n      )}\n      \n      {/* Raw Text View (toggle) */}\n      <details className=\"raw-text-view\">\n        <summary>View Raw Text</summary>\n        <textarea \n          value={value.rawText}\n          onChange={(e) => handleTextChange(e.target.value)}\n          className=\"raw-text-editor\"\n        />\n      </details>\n    </div>\n  );\n};\n\nconst BlockComponent: React.FC<{\n  node: BlockNode;\n  selected: boolean;\n  onSelect: () => void;\n  onRemove: () => void;\n  onUpdate: (config: Record<string, any>) => void;\n}> = ({ node, selected, onSelect, onRemove, onUpdate }) => {\n  \n  const getBlockIcon = (type: string) => {\n    switch (type) {\n      case 'field': return '\ud83d\udcc4';\n      case 'filter': return '\ud83d\udd0d';\n      case 'aggregate': return '\u03a3';\n      case 'map': return '\ud83d\uddfa\ufe0f';\n      case 'transform': return '\u2699\ufe0f';\n      default: return '\ud83e\udde9';\n    }\n  };\n  \n  return (\n    <span \n      className={`expression-block ${selected ? 'selected' : ''}`}\n      onClick={onSelect}\n      contentEditable={false}\n    >\n      <span className=\"block-icon\">{getBlockIcon(node.blockType)}</span>\n      <span className=\"block-text\">{node.displayText}</span>\n      <button className=\"block-remove\" onClick={onRemove}>\u00d7</button>\n    </span>\n  );\n};\n\nconst BlockConfigPanel: React.FC<{\n  blockNode: BlockNode;\n  onUpdate: (config: Record<string, any>) => void;\n  onClose: () => void;\n}> = ({ blockNode, onUpdate, onClose }) => {\n  \n  const [config, setConfig] = useState(blockNode.config);\n  \n  const handleSave = () => {\n    onUpdate(config);\n    onClose();\n  };\n  \n  return (\n    <div className=\"block-config-panel\">\n      <div className=\"panel-header\">\n        <h4>Configure {blockNode.blockType}</h4>\n        <button onClick={onClose}>\u00d7</button>\n      </div>\n      \n      <div className=\"panel-body\">\n        {blockNode.blockType === 'field' && (\n          <label>\n            Path:\n            <input \n              type=\"text\"\n              value={config.path || ''}\n              onChange={(e) => setConfig({ ...config, path: e.target.value })}\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'filter' && (\n          <label>\n            Condition:\n            <input \n              type=\"text\"\n              value={config.condition || ''}\n              onChange={(e) => setConfig({ ...config, condition: e.target.value })}\n              placeholder=\"e.g., age > 18\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'aggregate' && (\n          <>\n            <label>\n              Function:\n              <select \n                value={config.function || 'sum'}\n                onChange={(e) => setConfig({ ...config, function: e.target.value })}\n              >\n                <option value=\"sum\">Sum</option>\n                <option value=\"avg\">Average</option>\n                <option value=\"count\">Count</option>\n                <option value=\"min\">Min</option>\n                <option value=\"max\">Max</option>\n              </select>\n            </label>\n            <label>\n              Path:\n              <input \n                type=\"text\"\n                value={config.path || ''}\n                onChange={(e) => setConfig({ ...config, path: e.target.value })}\n              />\n            </label>\n          </>\n        )}\n        \n        {blockNode.blockType === 'map' && (\n          <label>\n            Expression:\n            <input \n              type=\"text\"\n              value={config.expression || ''}\n              onChange={(e) => setConfig({ ...config, expression: e.target.value })}\n              placeholder=\"e.g., name & ' ' & email\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'transform' && (\n          <label>\n            Operation:\n            <select \n              value={config.operation || 'uppercase'}\n              onChange={(e) => setConfig({ ...config, operation: e.target.value })}\n            >\n              <option value=\"uppercase\">Uppercase</option>\n              <option value=\"lowercase\">Lowercase</option>\n              <option value=\"trim\">Trim</option>\n              <option value=\"reverse\">Reverse</option>\n            </select>\n          </label>\n        )}\n      </div>\n      \n      <div className=\"panel-footer\">\n        <button onClick={handleSave}>Save</button>\n        <button onClick={onClose}>Cancel</button>\n      </div>\n    </div>\n  );\n};\n\nfunction getDefaultBlockConfig(blockType: string): Record<string, any> {\n  switch (blockType) {\n    case 'field':\n      return { path: '' };\n    case 'filter':\n      return { condition: '' };\n    case 'aggregate':\n      return { function: 'sum', path: '' };\n    case 'map':\n      return { expression: '' };\n    case 'transform':\n      return { operation: 'uppercase' };\n    default:\n      return {};\n  }\n}\n```\n\n## Sandboxed Compensation Handlers with Trust Levels\n\n### Key Innovation: CSP-Based Sandboxing + Trust Verification\n\n```typescript\ntype TrustLevel = 'system' | 'verified' | 'user' | 'untrusted';\n\ninterface CompensationHandlerConfig {\n  id: string;\n  name: string;\n  trustLevel: TrustLevel;\n  author?: string;\n  verified?: boolean;\n  \n  handlerType: 'builtin' | 'http-api' | 'webhook' | 'custom';\n  \n  matches: {\n    operation?: string[];\n    description?: string[];\n    service?: string[];\n  };\n  \n  compensation: \n    | { type: 'http-api'; config: HttpApiConfig }\n    | { type: 'webhook'; config: WebhookConfig }\n    | { type: 'custom'; workerUrl: string }  // Sandboxed Web Worker\n    | { type: 'manual'; steps: string[] };\n}\n\ninterface HttpApiConfig {\n  method: 'GET' | 'POST' | 'DELETE' | 'PUT';\n  urlTemplate: string;\n  allowedDomains: string[];  // Whitelist of domains this handler can call\n  headers: Record<string, string>;\n  bodyTemplate: string;\n  auth?: {\n    type: 'bearer' | 'basic' | 'api-key';\n    tokenPath: string;\n  };\n}\n\ninterface WebhookConfig {\n  url: string;\n  allowedDomains: string[];\n  method: 'POST' | 'PUT';\n  payloadTemplate: string;\n}\n\nclass SandboxedCompensationHandlerRegistry {\n  private handlers: Map<string, CompensationHandler> = new Map();\n  private configs: Map<string, CompensationHandlerConfig> = new Map();\n  private trustedDomains: Set<string> = new Set(['api.stripe.com', 'hooks.slack.com']);\n  \n  // User approval tracking\n  private approvedHandlers: Set<string> = new Set();\n  \n  constructor() {\n    this.loadApprovedHandlers();\n  }\n  \n  async registerFromConfig(\n    config: CompensationHandlerConfig,\n    requireApproval: boolean = true\n  ): Promise<boolean> {\n    \n    // Check trust level\n    if (requireApproval && config.trustLevel === 'untrusted') {\n      const approved = await this.requestUserApproval(config);\n      if (!approved) {\n        return false;\n      }\n      this.approvedHandlers.add(config.id);\n      this.persistApprovedHandlers();\n    }\n    \n    // Validate config\n    const validation = this.validateConfig(config);\n    if (!validation.valid) {\n      throw new Error(`Invalid handler config: ${validation.error}`);\n    }\n    \n    this.configs.set(config.id, config);\n    \n    const handler = await this.createHandlerFromConfig(config);\n    this.handlers.set(config.id, handler);\n    \n    return true;\n  }\n  \n  private validateConfig(config: CompensationHandlerConfig): { valid: boolean; error?: string } {\n    // Validate HTTP API config\n    if (config.compensation.type === 'http-api') {\n      const httpConfig = config.compensation.config;\n      \n      // Check URL against allowed domains\n      const url = new URL(httpConfig.urlTemplate.replace(/\\{\\{[^}]+\\}\\}/g, 'placeholder'));\n      \n      if (!httpConfig.allowedDomains.includes(url.hostname)) {\n        return {\n          valid: false,\n          error: `Domain ${url.hostname} not in allowedDomains list`\n        };\n      }\n      \n      // For untrusted handlers, require domain to be explicitly trusted\n      if (config.trustLevel === 'untrusted' && !this.trustedDomains.has(url.hostname)) {\n        return {\n          valid: false,\n          error: `Domain ${url.hostname} not in trusted domains list`\n        };\n      }\n    }\n    \n    // Validate webhook config\n    if (config.compensation.type === 'webhook') {\n      const webhookConfig = config.compensation.config;\n      const url = new URL(webhookConfig.url);\n      \n      if (!webhookConfig.allowedDomains.includes(url.hostname)) {\n        return {\n          valid: false,\n          error: `Webhook domain ${url.hostname} not in allowedDomains list`\n        };\n      }\n    }\n    \n    // Validate custom handler\n    if (config.compensation.type === 'custom') {\n      // Custom handlers must use data: URLs or trusted origins\n      const workerUrl = config.compensation.workerUrl;\n      \n      if (!workerUrl.startsWith('data:') && !workerUrl.startsWith('blob:')) {\n        return {\n          valid: false,\n          error: 'Custom handlers must use data: or blob: URLs for security'\n        };\n      }\n    }\n    \n    return { valid: true };\n  }\n  \n  private async requestUserApproval(\n    config: CompensationHandlerConfig\n  ): Promise<boolean> {\n    \n    return new Promise((resolve) => {\n      // Show modal to user\n      const modal = document.createElement('div');\n      modal.className = 'approval-modal';\n      modal.innerHTML = `\n        <div class=\"modal-content\">\n          <h3>\u26a0\ufe0f Handler Approval Required</h3>\n          <p>The handler \"${config.name}\" wants to perform compensations.</p>\n          \n          <div class=\"handler-details\">\n            <p><strong>Trust Level:</strong> ${config.trustLevel}</p>\n            <p><strong>Author:</strong> ${config.author || 'Unknown'}</p>\n            <p><strong>Type:</strong> ${config.compensation.type}</p>\n            \n            ${config.compensation.type === 'http-api' ? `\n              <p><strong>Will call:</strong> ${config.compensation.config.urlTemplate}</p>\n              <p><strong>Allowed domains:</strong> ${config.compensation.config.allowedDomains.join(', ')}</p>\n            ` : ''}\n            \n            ${config.compensation.type === 'webhook' ? `\n              <p><strong>Webhook URL:</strong> ${config.compensation.config.url}</p>\n            ` : ''}\n          </div>\n          \n          <p class=\"warning\">This handler will be able to make HTTP requests on your behalf. Only approve if you trust the source.</p>\n          \n          <div class=\"modal-actions\">\n            <button id=\"approve-btn\">Approve</button>\n            <button id=\"deny-btn\">Deny</button>\n          </div>\n        </div>\n      `;\n      \n      document.body.appendChild(modal);\n      \n      modal.querySelector('#approve-btn')?.addEventListener('click', () => {\n        document.body.removeChild(modal);\n        resolve(true);\n      });\n      \n      modal.querySelector('#deny-btn')?.addEventListener('click', () => {\n        document.body.removeChild(modal);\n        resolve(false);\n      });\n    });\n  }\n  \n  private async createHandlerFromConfig(\n    config: CompensationHandlerConfig\n  ): Promise<CompensationHandler> {\n    \n    return {\n      name: config.name,\n      \n      canHandle: (entry: SideEffectEntry) => {\n        const { matches } = config;\n        \n        if (matches.operation && !matches.operation.includes(entry.operation)) {\n          return false;\n        }\n        \n        if (matches.description) {\n          const descMatch = matches.description.some(pattern => \n            new RegExp(pattern).test(entry.description)\n          );\n          if (!descMatch) return false;\n        }\n        \n        if (matches.service) {\n          const descriptor = entry.compensation as any;\n          if (!matches.service.includes(descriptor.service)) {\n            return false;\n          }\n        }\n        \n        return true;\n      },\n      \n      compensate: async (entry: SideEffectEntry) => {\n        switch (config.compensation.type) {\n          case 'http-api':\n            return this.compensateViaHttpApi(entry, config.compensation.config, config.trustLevel);\n            \n          case 'webhook':\n            return this.compensateViaWebhook(entry, config.compensation.config, config.trustLevel);\n            \n          case 'custom':\n            return this.compensateViaCustomWorker(entry, config.compensation.workerUrl);\n            \n          case 'manual':\n            return {\n              success: false,\n              requiresUserAction: true,\n              manualSteps: config.compensation.steps\n            };\n            \n          default:\n            throw new Error(`Unknown compensation type`);\n        }\n      }\n    };\n  }\n  \n  private async compensateViaHttpApi(\n    entry: SideEffectEntry,\n    config: HttpApiConfig,\n    trustLevel: TrustLevel\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    \n    // Resolve URL\n    const url = this.resolveTemplate(config.urlTemplate, {\n      ...descriptor,\n      effectId: entry.id\n    });\n    \n    // Security check: Verify URL domain\n    const urlObj = new URL(url);\n    if (!config.allowedDomains.includes(urlObj.hostname)) {\n      return {\n        success: false,\n        error: `Security: Domain ${urlObj.hostname} not allowed`,\n        requiresUserAction: true,\n        manualSteps: ['Manually compensate this effect', `URL attempted: ${url}`]\n      };\n    }\n    \n    // Build request\n    const headers = { ...config.headers };\n    \n    if (config.auth) {\n      const token = this.extractValue(descriptor, config.auth.tokenPath);\n      switch (config.auth.type) {\n        case 'bearer':\n          headers['Authorization'] = `Bearer ${token}`;\n          break;\n        case 'api-key':\n          headers['Authorization'] = token;\n          break;\n      }\n    }\n    \n    const body = config.bodyTemplate\n      ? this.resolveTemplate(config.bodyTemplate, descriptor)\n      : undefined;\n    \n    try {\n      const response = await fetch(url, {\n        method: config.method,\n        headers,\n        body: body ? JSON.stringify(JSON.parse(body)) : undefined\n      });\n      \n      if (!response.ok) {\n        throw new Error(`HTTP ${response.status}: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Compensated via ${config.method} ${url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message,\n        requiresUserAction: true,\n        manualSteps: [\n          `Failed to compensate via API: ${error.message}`,\n          `Operation: ${entry.description}`,\n          'Please manually revert'\n        ]\n      };\n    }\n  }\n  \n  private async compensateViaWebhook(\n    entry: SideEffectEntry,\n    config: WebhookConfig,\n    trustLevel: TrustLevel\n  ): Promise<CompensationResult> {\n    \n    const descriptor = entry.compensation as any;\n    \n    // Security check\n    const urlObj = new URL(config.url);\n    if (!config.allowedDomains.includes(urlObj.hostname)) {\n      return {\n        success: false,\n        error: `Security: Webhook domain ${urlObj.hostname} not allowed`\n      };\n    }\n    \n    const payload = this.resolveTemplate(config.payloadTemplate, {\n      ...descriptor,\n      effectId: entry.id,\n      effectDescription: entry.description\n    });\n    \n    try {\n      const response = await fetch(config.url, {\n        method: config.method,\n        headers: { 'Content-Type': 'application/json' },\n        body: payload\n      });\n      \n      if (!response.ok) {\n        throw new Error(`Webhook failed: ${response.statusText}`);\n      }\n      \n      return {\n        success: true,\n        message: `Webhook triggered: ${config.url}`\n      };\n      \n    } catch (error: any) {\n      return {\n        success: false,\n        error: error.message\n      };\n    }\n  }\n  \n  private async compensateViaCustomWorker(\n    entry: SideEffectEntry,\n    workerUrl: string\n  ): Promise<CompensationResult> {\n    \n    return new Promise((resolve) => {\n      const worker = new Worker(workerUrl);\n      \n      const timeout = setTimeout(() => {\n        worker.terminate();\n        resolve({\n          success: false,\n          error: 'Custom handler timeout'\n        });\n      }, 10000);\n      \n      worker.onmessage = (event) => {\n        clearTimeout(timeout);\n        worker.terminate();\n        resolve(event.data);\n      };\n      \n      worker.onerror = (error) => {\n        clearTimeout(timeout);\n        worker.terminate();\n        resolve({\n          success: false,\n          error: `Worker error: ${error.message}`\n        });\n      };\n      \n      worker.postMessage({\n        entry,\n        descriptor: entry.compensation\n      });\n    });\n  }\n  \n  private resolveTemplate(template: string, data: any): string {\n    return template.replace(/\\{\\{([^}]+)\\}\\}/g, (_, path) => {\n      return this.extractValue(data, path) ?? '';\n    });\n  }\n  \n  private extractValue(obj: any, path: string): any {\n    const parts = path.split('.');\n    let result = obj;\n    for (const part of parts) {\n      result = result?.[part];\n    }\n    return result;\n  }\n  \n  private loadApprovedHandlers(): void {\n    const stored = localStorage.getItem('approvedHandlers');\n    if (stored) {\n      this.approvedHandlers = new Set(JSON.parse(stored));\n    }\n  }\n  \n  private persistApprovedHandlers(): void {\n    localStorage.setItem('approvedHandlers', JSON.stringify([...this.approvedHandlers]));\n  }\n  \n  serializeConfigs(): CompensationHandlerConfig[] {\n    return Array.from(this.configs.values());\n  }\n  \n  loadConfigs(configs: CompensationHandlerConfig[]): void {\n    for (const config of configs) {\n      // Check if already approved\n      const requireApproval = !this.approvedHandlers.has(config.id);\n      this.registerFromConfig(config, requireApproval);\n    }\n  }\n  \n  unregister(id: string): void {\n    this.handlers.delete(id);\n    this.configs.delete(id);\n    this.approvedHandlers.delete(id);\n    this.persistApprovedHandlers();\n  }\n  \n  findHandler(entry: SideEffectEntry): CompensationHandler | null {\n    for (const handler of this.handlers.values()) {\n      if (handler.canHandle(entry)) {\n        return handler;\n      }\n    }\n    return null;\n  }\n}\n\ninterface CompensationHandler {\n  name: string;\n  compensate(entry: SideEffectEntry): Promise<CompensationResult>;\n  canHandle(entry: SideEffectEntry): boolean;\n}\n\ninterface SideEffectEntry {\n  id: string;\n  nodeId: string;\n  executionId: string;\n  timestamp: number;\n  operation: string;\n  description: string;\n  compensation: any;\n  status: string;\n}\n\ninterface CompensationResult {\n  success: boolean;\n  error?: string;\n  message?: string;\n  manualSteps?: string[];\n  requiresUserAction?: boolean;\n}\n```\n\n## Adaptive Performance Metrics with IndexedDB Persistence\n\n### Key Innovation: Long-Term Analysis + Sampling Strategy\n\n```typescript\ninterface MetricsConfig {\n  enablePersistence: boolean;        // Store to IndexedDB\n  samplingRate: number;              // 0.1 = 10% of measurements\n  retentionDays: number;             // 7 days\n  enableRecommendations: boolean;\n}\n\ninterface StreamingMetrics {\n  decisions: {\n    teeCount: number;\n    materializeCount: number;\n    batchCount: number;\n  };\n  \n  performance: {\n    avgTeeMemory: number;\n    avgMaterializeSize: number;\n    avgTeeTime: number;\n    avgMaterializeTime: number;\n  };\n  \n  errors: {\n    teeFailures: number;\n    materializeFailures: number;\n    storageFullErrors: number;\n  };\n  \n  recommendations: {\n    suggestedTeeThreshold: number;\n    suggestedMaterializeThreshold: number;\n    suggestedOverflowPolicy: string;\n  };\n}\n\ninterface Measurement {\n  id: string;\n  timestamp: number;\n  strategy: 'tee' | 'materialize';\n  sizeBytes: number;\n  durationMs: number;\n  memoryUsed: number;\n  workflowId: string;\n  nodeId: string;\n}\n\nclass AdaptivePerformanceMonitor {\n  private metrics: StreamingMetrics;\n  private recentMeasurements: Measurement[] = [];\n  private db: IDBDatabase | null = null;\n  private config: MetricsConfig;\n  \n  constructor(config: MetricsConfig) {\n    this.config = config;\n    this.metrics = this.initMetrics();\n    \n    if (config.enablePersistence) {\n      this.initDatabase();\n    }\n  }\n  \n  recordDecision(\n    strategy: 'tee' | 'materialize' | 'batch',\n    sizeBytes: number,\n    durationMs: number,\n    memoryUsed: number,\n    context: {\n      workflowId: string;\n      nodeId: string;\n    }\n  ): void {\n    \n    switch (strategy) {\n      case 'tee':\n        this.metrics.decisions.teeCount++;\n        break;\n      case 'materialize':\n        this.metrics.decisions.materializeCount++;\n        break;\n      case 'batch':\n        this.metrics.decisions.batchCount++;\n        break;\n    }\n    \n    if (strategy === 'tee' || strategy === 'materialize') {\n      // Apply sampling\n      if (Math.random() < this.config.samplingRate) {\n        const measurement: Measurement = {\n          id: crypto.randomUUID(),\n          timestamp: Date.now(),\n          strategy,\n          sizeBytes,\n          durationMs,\n          memoryUsed,\n          workflowId: context.workflowId,\n          nodeId: context.nodeId\n        };\n        \n        this.recentMeasurements.push(measurement);\n        \n        // Keep last 100 in memory\n        if (this.recentMeasurements.length > 100) {\n          this.recentMeasurements.shift();\n        }\n        \n        // Persist to IndexedDB\n        if (this.config.enablePersistence && this.db) {\n          this.persistMeasurement(measurement);\n        }\n      }\n      \n      this.updateAverages();\n      \n      if (this.config.enableRecommendations) {\n        this.generateRecommendations();\n      }\n    }\n  }\n  \n  recordError(type: 'tee' | 'materialize' | 'storage-full'): void {\n    switch (type) {\n      case 'tee':\n        this.metrics.errors.teeFailures++;\n        break;\n      case 'materialize':\n        this.metrics.errors.materializeFailures++;\n        break;\n      case 'storage-full':\n        this.metrics.errors.storageFullErrors++;\n        break;\n    }\n  }\n  \n  private updateAverages(): void {\n    const tees = this.recentMeasurements.filter(m => m.strategy === 'tee');\n    const materializes = this.recentMeasurements.filter(m => m.strategy === 'materialize');\n    \n    if (tees.length > 0) {\n      this.metrics.performance.avgTeeMemory = \n        tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length;\n      this.metrics.performance.avgTeeTime = \n        tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length;\n    }\n    \n    if (materializes.length > 0) {\n      this.metrics.performance.avgMaterializeSize = \n        materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length;\n      this.metrics.performance.avgMaterializeTime = \n        materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length;\n    }\n  }\n  \n  private generateRecommendations(): void {\n    // Tee threshold\n    if (this.metrics.performance.avgTeeMemory > 10 * 1024 * 1024) {\n      this.metrics.recommendations.suggestedTeeThreshold = 3 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedTeeThreshold = 5 * 1024 * 1024;\n    }\n    \n    // Materialize threshold\n    if (this.metrics.performance.avgMaterializeTime < 100) {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 100 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 50 * 1024 * 1024;\n    }\n    \n    // Overflow policy\n    if (this.metrics.errors.storageFullErrors > 5) {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'evict-lru';\n    } else {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'compress';\n    }\n  }\n  \n  async getHistoricalMetrics(\n    workflowId?: string,\n    startTime?: number,\n    endTime?: number\n  ): Promise<Measurement[]> {\n    \n    if (!this.config.enablePersistence || !this.db) {\n      return this.recentMeasurements;\n    }\n    \n    return new Promise((resolve, reject) => {\n      const tx = this.db!.transaction('measurements', 'readonly');\n      const store = tx.objectStore('measurements');\n      const index = store.index('timestamp');\n      \n      const range = IDBKeyRange.bound(\n        startTime || 0,\n        endTime || Date.now()\n      );\n      \n      const results: Measurement[] = [];\n      const request = index.openCursor(range);\n      \n      request.onsuccess = (event) => {\n        const cursor = (event.target as IDBRequest).result;\n        if (cursor) {\n          const measurement: Measurement = cursor.value;\n          \n          if (!workflowId || measurement.workflowId === workflowId) {\n            results.push(measurement);\n          }\n          \n          cursor.continue();\n        } else {\n          resolve(results);\n        }\n      };\n      \n      request.onerror = () => reject(request.error);\n    });\n  }\n  \n  async generateReport(workflowId?: string): Promise<PerformanceReport> {\n    const measurements = await this.getHistoricalMetrics(workflowId);\n    \n    const tees = measurements.filter(m => m.strategy === 'tee');\n    const materializes = measurements.filter(m => m.strategy === 'materialize');\n    \n    return {\n      totalMeasurements: measurements.length,\n      timeRange: {\n        start: Math.min(...measurements.map(m => m.timestamp)),\n        end: Math.max(...measurements.map(m => m.timestamp))\n      },\n      strategies: {\n        tee: {\n          count: tees.length,\n          avgMemory: tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length,\n          avgDuration: tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length,\n          avgSize: tees.reduce((sum, m) => sum + m.sizeBytes, 0) / tees.length\n        },\n        materialize: {\n          count: materializes.length,\n          avgMemory: materializes.reduce((sum, m) => sum + m.memoryUsed, 0) / materializes.length,\n          avgDuration: materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length,\n          avgSize: materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length\n        }\n      },\n      recommendations: this.metrics.recommendations\n    };\n  }\n  \n  getMetrics(): StreamingMetrics {\n    return { ...this.metrics };\n  }\n  \n  getMeasurements(): Measurement[] {\n    return [...this.recentMeasurements];\n  }\n  \n  private async initDatabase(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('PerformanceMetrics', 2);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        this.cleanupOldMeasurements();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('measurements')) {\n          const store = db.createObjectStore('measurements', { keyPath: 'id' });\n          store.createIndex('timestamp', 'timestamp', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n          store.createIndex('strategy', 'strategy', { unique: false });\n        }\n      };\n    });\n  }\n  \n  private async persistMeasurement(measurement: Measurement): Promise<void> {\n    if (!this.db) return;\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    await tx.objectStore('measurements').add(measurement);\n  }\n  \n  private async cleanupOldMeasurements(): Promise<void> {\n    if (!this.db) return;\n    \n    const cutoff = Date.now() - (this.config.retentionDays * 24 * 60 * 60 * 1000);\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    const store = tx.objectStore('measurements');\n    const index = store.index('timestamp');\n    \n    const range = IDBKeyRange.upperBound(cutoff);\n    const request = index.openCursor(range);\n    \n    request.onsuccess = (event) => {\n      const cursor = (event.target as IDBRequest).result;\n      if (cursor) {\n        cursor.delete();\n        cursor.continue();\n      }\n    };\n  }\n  \n  private initMetrics(): StreamingMetrics {\n    return {\n      decisions: {\n        teeCount: 0,\n        materializeCount: 0,\n        batchCount: 0\n      },\n      performance: {\n        avgTeeMemory: 0,\n        avgMaterializeSize: 0,\n        avgTeeTime: 0,\n        avgMaterializeTime: 0\n      },\n      errors: {\n        teeFailures: 0,\n        materializeFailures: 0,\n        storageFullErrors: 0\n      },\n      recommendations: {\n        suggestedTeeThreshold: 5 * 1024 * 1024,\n        suggestedMaterializeThreshold: 50 * 1024 * 1024,\n        suggestedOverflowPolicy: 'evict-lru'\n      }\n    };\n  }\n}\n\ninterface PerformanceReport {\n  totalMeasurements: number;\n  timeRange: {\n    start: number;\n    end: number;\n  };\n  strategies: {\n    tee: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n    };\n    materialize: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n    };\n  };\n  recommendations: StreamingMetrics['recommendations'];\n}\n```\n\n---\n\n## Rationale\n\n### 1. Resilient Stream Manager with Auto-Checkpointing\n**Why**: Solves the page refresh problem without requiring user intervention.\n- **Three materialization strategies**: `full` (all data), `partial` (last N chunks), `regenerate` (re-execute node)\n- **Automatic checkpointing**: Long-lived streams (>60s) are automatically checkpointed every 30s\n- **SessionStorage for metadata**: Checkpoint metadata persists across page refresh\n- **Resurrection logic**: Streams can be restored from checkpoints transparently\n\n### 2. Three-Tier Storage with Per-Workflow Policies\n**Why**: Different workflows have different storage needs. Heavy data workflows need aggressive eviction, lightweight workflows don't.\n- **Memory + IndexedDB + SessionStorage**: Three tiers with clear purposes\n- **Per-workflow policies**: Each workflow can configure its own overflow policy and eviction preferences\n- **Global fallback**: Workflows inherit global policy if not customized\n- **Session tier for critical state**: Workflow definitions and execution state use SessionStorage (survives refresh)\n\n### 3. Visual-Text Hybrid Expression Editor\n**Why**: Best of both worlds - power users can type, visual users can click.\n- **Rich text editor with inline blocks**: Blocks appear as visual chips within text\n- **Click-to-insert palette**: Easy block insertion at cursor position\n- **Configurable blocks**: Click a block to open config panel\n- **Raw text fallback**: Toggle to see/edit the underlying `{{block:type:config}}` syntax\n- **Automatic sync**: Changes in visual mode update raw text and vice versa\n\n### 4. Sandboxed Compensation Handlers with Trust Levels\n**Why**: Security without sacrificing flexibility.\n- **Four trust levels**: `system` (built-in), `verified` (marketplace), `user` (self-created), `untrusted` (external)\n- **Domain whitelisting**: HTTP handlers can only call explicitly allowed domains\n- **User approval for untrusted**: Modal prompts user before registering untrusted handlers\n- **CSP sandboxing for custom**: Custom handlers run in Web Workers with no DOM access\n- **Approval persistence**: Once approved, handlers don't require re-approval (stored in localStorage)\n\n### 5. Adaptive Performance Metrics with IndexedDB\n**Why**: Long-term learning without memory bloat.\n- **10% sampling rate**: Only record 1 in 10 measurements to reduce overhead\n- **IndexedDB persistence**: Metrics survive page refresh for long-term analysis\n- **7-day retention**: Auto-cleanup of old data\n- **Per-workflow reports**: Generate performance reports for specific workflows\n- **Recommendation engine**: Uses historical data to suggest better thresholds\n\n---\n\n## What I Changed\n\n### Stream Persistence Strategy\n**Changed from**: Vague \"materialize on timeout\" to concrete auto-checkpointing system\n- **Added**: Three materialization strategies (full/partial/regenerate)\n- **Added**: Automatic 30s checkpoint interval for long-lived streams\n- **Added**: SessionStorage-based checkpoint metadata persistence\n- **Added**: Transparent stream resurrection after page refresh\n\n### Storage Architecture\n**Changed from**: Global overflow policies to per-workflow policies\n- **Added**: Third storage tier (SessionStorage) for critical workflow state\n- **Added**: Workflow-scoped storage quotas and eviction preferences\n- **Added**: Global policy as fallback for workflows without custom config\n\n### Expression Editor UX\n**Changed from**: Abstract \"inline markers\" to concrete rich text editor with visual blocks\n- **Added**: Click-to-insert block palette\n- **Added**: Visual block components rendered inline with text\n- **Added**: Config panel that opens on block selection\n- **Added**: Raw text toggle for advanced users\n\n### Compensation Handler Security\n**Changed from**: \"Should we sandbox?\" to concrete CSP-based trust model\n- **Added**: Four trust levels with different approval requirements\n- **Added**: Domain whitelisting for HTTP/webhook handlers\n- **Added**: User approval modal for untrusted handlers\n- **Added**: Approval persistence in localStorage\n- **Added**: Web Worker sandboxing for custom handlers\n\n### Performance Metrics\n**Changed from**: In-memory metrics to persistent, sampled metrics\n- **Added**: 10% sampling rate to reduce overhead\n- **Added**: IndexedDB persistence with 7-day retention\n- **Added**: Historical query API for long-term analysis\n- **Added**: Per-workflow performance reports\n\n---\n\n## What I Kept\n\nAll core architectural decisions remain:\n- React Flow canvas\n- Zustand + immer state management\n- Adaptive streaming executor\n- Two-tier storage (Memory + IndexedDB) - now enhanced to three-tier\n- Composable expression system - now with concrete visual editor\n- Append-only side effect log\n- Checkpoint manager\n- Worker pools\n- Compression\n- All performance optimizations\n\n---\n\n## Open Questions\n\n### 1. Stream Regeneration Executor Injection\nThe `ResilientStreamManager` needs access to the executor to regenerate streams. Currently it throws an error.\n- **Option A**: Inject executor via constructor parameter\n- **Option B**: Use callback property `regenerationExecutor` set after initialization\n- **Option C**: Pass executor on each `registerStream` call\n- **Recommendation**: Option B (callback property) for flexibility\n\n### 2. SessionStorage Capacity Limits\nSessionStorage has ~5-10MB limit. For large workflows with many checkpoints:\n- **Issue**: May exceed SessionStorage quota\n- **Mitigation**: Only store lightweight checkpoint metadata, not full data\n- **Alternative**: Use IndexedDB for checkpoint metadata too (but won't survive tab close)\n\n### 3. Sampling Rate Configuration\n10% sampling might be too aggressive for small workflows (< 10 executions):\n- **Option A**: Dynamic sampling (100% for first 20 measurements, then 10%)\n- **Option B**: User-configurable sampling rate per workflow\n- **Option C**: Keep static 10% globally\n- **Recommendation**: Option A (dynamic) for better initial data\n\n### 4. Trust Level Assignment\nHow do we assign trust levels to handlers?\n- **System**: Built-in handlers (Stripe, Slack, etc.)\n- **Verified**: Who verifies? Do we need a marketplace?\n- **User**: User-created handlers (default)\n- **Untrusted**: Imported from external sources\n- **Question**: Should we auto-assign based on handler origin (same-domain = user, cross-domain = untrusted)?\n\n---\n\n## Convergence Status\n\n**PROPOSING_FINAL** - All five critical design questions have concrete, implementable solutions:\n\n\u2705 Stream persistence: Auto-checkpointing with three materialization strategies  \n\u2705 Expression editor UX: Rich text editor with inline visual blocks  \n\u2705 Handler security: Trust levels + domain whitelisting + CSP sandboxing  \n\u2705 Metrics persistence: IndexedDB + 10% sampling + 7-day retention  \n\u2705 Eviction strategy: Per-workflow policies with global fallback  \n\nAll remaining questions are implementation details that can be resolved during coding.\n\n---\n\n## PROMPT_FOR_CRITIC:\n\nThe design is complete and ready for implementation. Please focus final review on:\n\n1. **Stream Resurrection Edge Cases**: Are there scenarios where resurrection would fail catastrophically? What if the node config changed between checkpoint and resurrection?\n\n2. **SessionStorage Overflow**: With checkpoint metadata in SessionStorage, what happens when we hit the 5MB limit? Should we have a fallback strategy?\n\n3. **Visual Block Editor Performance**: Rendering many inline block components could be slow. Should we virtualize the editor content? Or set a max block count per expression?\n\n4. **Trust Level UX**: Is the four-level trust model (system/verified/user/untrusted) too complex for users? Would a simpler \"trusted/untrusted\" binary be better?\n\n5. **Metrics Sampling Bias**: 10% sampling means we might miss rare edge cases or performance spikes. Is there a smarter sampling strategy (e.g., always sample errors, sometimes sample successes)?\n\nIf you have strong objections to any of these solutions, please suggest concrete alternatives. Otherwise, this design is ready to build."
      },
      "agent_b": {
        "content": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v6\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Resilient \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Stream    \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502            \u2502  \u2502  Manager   \u2502  \u2502            \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Tiered   \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  Storage   \u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Manager   \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Enhanced Resilient Stream Manager (Addressing Critical Edge Cases)\n\n### Key Enhancement: Version-Aware Stream Resurrection with Graceful Degradation\n\n```typescript\ninterface StreamCheckpoint {\n  streamId: string;\n  sourceNodeId: string;\n  executionId: string;\n  consumerCount: number;\n  createdAt: number;\n  lastCheckpointAt: number;\n  \n  // VERSION TRACKING - Critical for detecting config changes\n  nodeConfigVersion: string;         // Hash of node config at checkpoint time\n  nodeConfigSnapshot: any;           // Full config snapshot for validation\n  \n  // Resurrection data\n  materializationType: 'full' | 'partial' | 'regenerate';\n  materializedData?: any;\n  partialOffset?: number;\n  regenerationConfig?: {\n    nodeConfig: any;\n    inputs: Record<string, any>;\n    inputHashes: Record<string, string>;  // NEW: Detect input changes\n  };\n  \n  // Resurrection metadata\n  status: 'active' | 'checkpointed' | 'resurrected' | 'expired' | 'stale';\n  resurrectionAttempts: number;\n  resurrectionStrategy: 'exact' | 'compatible' | 'fail';\n  staleness: 'fresh' | 'config-changed' | 'inputs-changed' | 'node-deleted';\n}\n\ninterface ResurrectionResult {\n  success: boolean;\n  stream?: ReadableStream;\n  degraded?: boolean;              // NEW: Indicates partial success\n  warning?: string;                // NEW: User-facing warning message\n  fallbackUsed?: 'cached' | 'regenerated' | 'empty';\n}\n\nclass ResilientStreamManager {\n  private activeStreams: Map<string, ActiveStreamEntry> = new Map();\n  private checkpoints: Map<string, StreamCheckpoint> = new Map();\n  private storage: HybridStorageManager;\n  private config: StreamManagerConfig;\n  private workflowState: WorkflowStateProvider;  // NEW: Validates node configs\n  \n  constructor(\n    storage: HybridStorageManager, \n    config: StreamManagerConfig,\n    workflowState: WorkflowStateProvider\n  ) {\n    this.storage = storage;\n    this.config = config;\n    this.workflowState = workflowState;\n    \n    if (config.enableResurrection) {\n      this.restoreCheckpointsFromStorage();\n    }\n    \n    this.startAutoCheckpointing();\n    this.startTimeoutMonitor();\n    this.startStalenessDetector();  // NEW: Detect stale checkpoints\n  }\n  \n  async registerStream(\n    sourceNodeId: string,\n    executionId: string,\n    stream: ReadableStream,\n    consumerCount: number,\n    options: {\n      estimatedSize: number;\n      materializationType?: 'full' | 'partial' | 'regenerate';\n      regenerationConfig?: any;\n      nodeConfig: any;  // NEW: Required for version tracking\n      inputs: Record<string, any>;  // NEW: Required for version tracking\n    }\n  ): Promise<string> {\n    \n    if (this.activeStreams.size >= this.config.maxConcurrentStreams) {\n      throw new Error(\n        `Max concurrent streams reached (${this.config.maxConcurrentStreams})`\n      );\n    }\n    \n    const streamId = crypto.randomUUID();\n    const teedStreams = this.createMultiTee(stream, consumerCount);\n    \n    // Create version-aware checkpoint\n    const checkpoint: StreamCheckpoint = {\n      streamId,\n      sourceNodeId,\n      executionId,\n      consumerCount,\n      createdAt: Date.now(),\n      lastCheckpointAt: Date.now(),\n      nodeConfigVersion: this.hashConfig(options.nodeConfig),\n      nodeConfigSnapshot: structuredClone(options.nodeConfig),\n      materializationType: options.materializationType ?? 'full',\n      regenerationConfig: options.regenerationConfig ? {\n        nodeConfig: structuredClone(options.nodeConfig),\n        inputs: structuredClone(options.inputs),\n        inputHashes: this.hashInputs(options.inputs)\n      } : undefined,\n      status: 'active',\n      resurrectionAttempts: 0,\n      resurrectionStrategy: 'exact',\n      staleness: 'fresh'\n    };\n    \n    this.checkpoints.set(streamId, checkpoint);\n    \n    this.activeStreams.set(streamId, {\n      checkpoint,\n      streams: teedStreams,\n      readers: new Set(),\n      materializedChunks: options.materializationType !== 'regenerate' ? [] : undefined\n    });\n    \n    return streamId;\n  }\n  \n  async getStream(streamId: string, consumerIndex: number): Promise<ReadableStream> {\n    const active = this.activeStreams.get(streamId);\n    \n    if (active) {\n      active.checkpoint.lastCheckpointAt = Date.now();\n      return active.streams[consumerIndex];\n    }\n    \n    // Try resurrection with validation\n    if (this.config.enableResurrection) {\n      const result = await this.resurrectStreamSafe(streamId, consumerIndex);\n      \n      if (result.success && result.stream) {\n        // Notify user if degraded\n        if (result.degraded && result.warning) {\n          this.notifyDegradedResurrection(streamId, result.warning);\n        }\n        return result.stream;\n      }\n    }\n    \n    throw new Error(`Stream not found and could not be resurrected: ${streamId}`);\n  }\n  \n  private async resurrectStreamSafe(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ResurrectionResult> {\n    \n    const checkpoint = this.checkpoints.get(streamId);\n    if (!checkpoint) {\n      return { success: false };\n    }\n    \n    if (checkpoint.resurrectionAttempts >= this.config.maxResurrectionAttempts) {\n      console.error(`Max resurrection attempts reached for stream ${streamId}`);\n      return { success: false };\n    }\n    \n    checkpoint.resurrectionAttempts++;\n    \n    // CRITICAL: Validate checkpoint freshness\n    const validation = await this.validateCheckpointFreshness(checkpoint);\n    \n    if (validation.stale) {\n      checkpoint.staleness = validation.reason;\n      \n      switch (validation.severity) {\n        case 'critical':\n          // Node deleted or inputs fundamentally changed\n          return {\n            success: false,\n            warning: `Cannot resurrect: ${validation.message}`\n          };\n          \n        case 'major':\n          // Config changed significantly - try regeneration or fail gracefully\n          if (checkpoint.materializationType === 'regenerate') {\n            // Config changed but we can try regenerating with old config\n            return await this.resurrectWithDegradation(checkpoint, {\n              strategy: 'regenerate-old-config',\n              warning: validation.message\n            });\n          } else {\n            // Use cached data with warning\n            return await this.resurrectWithDegradation(checkpoint, {\n              strategy: 'use-cached-with-warning',\n              warning: validation.message\n            });\n          }\n          \n        case 'minor':\n          // Minor changes - proceed with warning\n          const result = await this.resurrectStream(checkpoint);\n          return {\n            ...result,\n            degraded: true,\n            warning: validation.message\n          };\n          \n        default:\n          // Fresh - proceed normally\n          return await this.resurrectStream(checkpoint);\n      }\n    }\n    \n    return await this.resurrectStream(checkpoint);\n  }\n  \n  private async validateCheckpointFreshness(\n    checkpoint: StreamCheckpoint\n  ): Promise<{\n    stale: boolean;\n    reason: StreamCheckpoint['staleness'];\n    severity: 'none' | 'minor' | 'major' | 'critical';\n    message?: string;\n  }> {\n    \n    // Check if node still exists\n    const currentNode = this.workflowState.getNode(checkpoint.sourceNodeId);\n    if (!currentNode) {\n      return {\n        stale: true,\n        reason: 'node-deleted',\n        severity: 'critical',\n        message: 'Source node no longer exists in workflow'\n      };\n    }\n    \n    // Check if node config changed\n    const currentConfigHash = this.hashConfig(currentNode.config);\n    if (currentConfigHash !== checkpoint.nodeConfigVersion) {\n      // Determine severity of config changes\n      const diff = this.diffConfigs(checkpoint.nodeConfigSnapshot, currentNode.config);\n      \n      if (diff.breaking) {\n        return {\n          stale: true,\n          reason: 'config-changed',\n          severity: 'major',\n          message: `Node configuration changed: ${diff.summary}`\n        };\n      } else {\n        return {\n          stale: true,\n          reason: 'config-changed',\n          severity: 'minor',\n          message: `Minor node configuration changes detected`\n        };\n      }\n    }\n    \n    // Check if inputs changed (for regenerate strategy)\n    if (checkpoint.regenerationConfig) {\n      const currentInputs = this.workflowState.getNodeInputs(checkpoint.sourceNodeId);\n      const currentInputHashes = this.hashInputs(currentInputs);\n      \n      const inputsChanged = Object.keys(checkpoint.regenerationConfig.inputHashes).some(\n        key => currentInputHashes[key] !== checkpoint.regenerationConfig!.inputHashes[key]\n      );\n      \n      if (inputsChanged) {\n        return {\n          stale: true,\n          reason: 'inputs-changed',\n          severity: 'major',\n          message: 'Input data has changed since checkpoint'\n        };\n      }\n    }\n    \n    return {\n      stale: false,\n      reason: 'fresh',\n      severity: 'none'\n    };\n  }\n  \n  private async resurrectWithDegradation(\n    checkpoint: StreamCheckpoint,\n    options: {\n      strategy: 'regenerate-old-config' | 'use-cached-with-warning' | 'empty-stream';\n      warning: string;\n    }\n  ): Promise<ResurrectionResult> {\n    \n    switch (options.strategy) {\n      case 'regenerate-old-config':\n        try {\n          // Attempt regeneration using OLD config from checkpoint\n          const stream = await this.resurrectViaRegeneration(checkpoint, {\n            useSnapshotConfig: true\n          });\n          \n          return {\n            success: true,\n            stream,\n            degraded: true,\n            warning: `${options.warning}. Using previous configuration.`,\n            fallbackUsed: 'regenerated'\n          };\n        } catch (error: any) {\n          // Fall back to cached data\n          return await this.resurrectWithDegradation(checkpoint, {\n            strategy: 'use-cached-with-warning',\n            warning: options.warning\n          });\n        }\n        \n      case 'use-cached-with-warning':\n        if (checkpoint.materializationType !== 'regenerate' && checkpoint.materializedData) {\n          const stream = await this.resurrectFromFullMaterialization(checkpoint);\n          \n          return {\n            success: true,\n            stream,\n            degraded: true,\n            warning: `${options.warning}. Using cached data from previous run.`,\n            fallbackUsed: 'cached'\n          };\n        }\n        \n        // No cached data available - return empty stream\n        return await this.resurrectWithDegradation(checkpoint, {\n          strategy: 'empty-stream',\n          warning: options.warning\n        });\n        \n      case 'empty-stream':\n        const emptyStream = new ReadableStream({\n          start(controller) {\n            controller.enqueue({\n              $error: true,\n              $resurrectionFailed: true,\n              message: options.warning\n            });\n            controller.close();\n          }\n        });\n        \n        return {\n          success: true,\n          stream: emptyStream,\n          degraded: true,\n          warning: `${options.warning}. Returning empty stream.`,\n          fallbackUsed: 'empty'\n        };\n    }\n  }\n  \n  private async resurrectStream(\n    checkpoint: StreamCheckpoint\n  ): Promise<ResurrectionResult> {\n    \n    try {\n      let stream: ReadableStream;\n      \n      switch (checkpoint.materializationType) {\n        case 'full':\n          stream = await this.resurrectFromFullMaterialization(checkpoint);\n          break;\n          \n        case 'partial':\n          stream = await this.resurrectFromPartialMaterialization(checkpoint);\n          break;\n          \n        case 'regenerate':\n          stream = await this.resurrectViaRegeneration(checkpoint, {\n            useSnapshotConfig: false\n          });\n          break;\n          \n        default:\n          throw new Error(`Unknown materialization type: ${checkpoint.materializationType}`);\n      }\n      \n      checkpoint.status = 'resurrected';\n      \n      return { success: true, stream };\n      \n    } catch (error: any) {\n      console.error(`Stream resurrection failed: ${error.message}`);\n      return { success: false };\n    }\n  }\n  \n  private async resurrectFromFullMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const materialized = await this.storage.retrieve(storageKey);\n    \n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(materialized)) {\n          for (const chunk of materialized) {\n            controller.enqueue(chunk);\n          }\n        } else {\n          controller.enqueue(materialized);\n        }\n        controller.close();\n      }\n    });\n    \n    return stream;\n  }\n  \n  private async resurrectFromPartialMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const partialData = await this.storage.retrieve(storageKey);\n    \n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(partialData)) {\n          for (const chunk of partialData) {\n            controller.enqueue(chunk);\n          }\n        }\n        \n        controller.enqueue({\n          $incomplete: true,\n          offset: checkpoint.partialOffset,\n          message: 'Stream was interrupted. Partial data only.'\n        });\n        \n        controller.close();\n      }\n    });\n    \n    return stream;\n  }\n  \n  private async resurrectViaRegeneration(\n    checkpoint: StreamCheckpoint,\n    options: {\n      useSnapshotConfig: boolean;\n    }\n  ): Promise<ReadableStream> {\n    \n    if (!checkpoint.regenerationConfig) {\n      throw new Error('No regeneration config available');\n    }\n    \n    const config = options.useSnapshotConfig\n      ? checkpoint.regenerationConfig.nodeConfig\n      : this.workflowState.getNode(checkpoint.sourceNodeId)?.config;\n    \n    if (!config) {\n      throw new Error('Node config not available for regeneration');\n    }\n    \n    const inputs = checkpoint.regenerationConfig.inputs;\n    \n    const regenerated = await this.executeNodeForRegeneration(\n      checkpoint.sourceNodeId,\n      config,\n      inputs\n    );\n    \n    return regenerated;\n  }\n  \n  private async executeNodeForRegeneration(\n    nodeId: string,\n    nodeConfig: any,\n    inputs: Record<string, any>\n  ): Promise<ReadableStream> {\n    \n    if (!this.regenerationExecutor) {\n      throw new Error(\n        'Regeneration requires executor callback. ' +\n        'Set ResilientStreamManager.regenerationExecutor = (node, inputs) => stream'\n      );\n    }\n    \n    return await this.regenerationExecutor(nodeId, nodeConfig, inputs);\n  }\n  \n  private hashConfig(config: any): string {\n    const serialized = JSON.stringify(config, Object.keys(config).sort());\n    return this.simpleHash(serialized);\n  }\n  \n  private hashInputs(inputs: Record<string, any>): Record<string, string> {\n    const hashes: Record<string, string> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      const serialized = JSON.stringify(value);\n      hashes[key] = this.simpleHash(serialized);\n    }\n    \n    return hashes;\n  }\n  \n  private simpleHash(str: string): string {\n    let hash = 0;\n    for (let i = 0; i < str.length; i++) {\n      const char = str.charCodeAt(i);\n      hash = ((hash << 5) - hash) + char;\n      hash = hash & hash;\n    }\n    return hash.toString(36);\n  }\n  \n  private diffConfigs(oldConfig: any, newConfig: any): {\n    breaking: boolean;\n    summary: string;\n  } {\n    const changes: string[] = [];\n    let breaking = false;\n    \n    // Critical fields that indicate breaking changes\n    const criticalFields = ['type', 'operation', 'model', 'endpoint'];\n    \n    for (const field of criticalFields) {\n      if (oldConfig[field] !== newConfig[field]) {\n        breaking = true;\n        changes.push(`${field} changed`);\n      }\n    }\n    \n    // Check all other fields\n    const allFields = new Set([\n      ...Object.keys(oldConfig),\n      ...Object.keys(newConfig)\n    ]);\n    \n    for (const field of allFields) {\n      if (!criticalFields.includes(field) && oldConfig[field] !== newConfig[field]) {\n        changes.push(field);\n      }\n    }\n    \n    return {\n      breaking,\n      summary: changes.length > 0 ? changes.join(', ') : 'no changes'\n    };\n  }\n  \n  private notifyDegradedResurrection(streamId: string, warning: string): void {\n    // Emit event for UI to show warning banner\n    window.dispatchEvent(new CustomEvent('stream-resurrection-degraded', {\n      detail: { streamId, warning }\n    }));\n  }\n  \n  private startStalenessDetector(): void {\n    setInterval(() => {\n      for (const [streamId, checkpoint] of this.checkpoints.entries()) {\n        if (checkpoint.status === 'checkpointed') {\n          this.validateCheckpointFreshness(checkpoint).then(validation => {\n            if (validation.stale && validation.severity === 'critical') {\n              // Mark as expired\n              checkpoint.status = 'expired';\n              checkpoint.staleness = validation.reason;\n            }\n          });\n        }\n      }\n    }, 60000);  // Check every minute\n  }\n  \n  private async checkpointStream(streamId: string): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    const checkpoint = entry.checkpoint;\n    \n    switch (checkpoint.materializationType) {\n      case 'full':\n        if (entry.materializedChunks && entry.materializedChunks.length > 0) {\n          const storageKey = `stream-checkpoint-${streamId}`;\n          checkpoint.materializedData = [...entry.materializedChunks];\n          \n          await this.storage.store(storageKey, checkpoint.materializedData, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'partial':\n        if (entry.materializedChunks) {\n          const keepCount = 10;\n          const partial = entry.materializedChunks.slice(-keepCount);\n          \n          const storageKey = `stream-checkpoint-${streamId}`;\n          await this.storage.store(storageKey, partial, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.partialOffset = entry.materializedChunks.length - keepCount;\n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'regenerate':\n        checkpoint.status = 'checkpointed';\n        checkpoint.lastCheckpointAt = Date.now();\n        break;\n    }\n    \n    await this.persistCheckpoint(checkpoint);\n  }\n  \n  private async persistCheckpoint(checkpoint: StreamCheckpoint): Promise<void> {\n    const key = `stream-checkpoint-meta-${checkpoint.streamId}`;\n    sessionStorage.setItem(key, JSON.stringify(checkpoint));\n  }\n  \n  private async restoreCheckpointsFromStorage(): Promise<void> {\n    const keys = Object.keys(sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('stream-checkpoint-meta-')) {\n        const checkpointData = sessionStorage.getItem(key);\n        if (checkpointData) {\n          const checkpoint: StreamCheckpoint = JSON.parse(checkpointData);\n          this.checkpoints.set(checkpoint.streamId, checkpoint);\n        }\n      }\n    }\n  }\n  \n  private createMultiTee(stream: ReadableStream, count: number): ReadableStream[] {\n    if (count === 1) return [stream];\n    if (count === 2) return stream.tee();\n    \n    const streams: ReadableStream[] = [];\n    let remaining = stream;\n    \n    for (let i = 0; i < count - 1; i++) {\n      const [stream1, stream2] = remaining.tee();\n      streams.push(stream1);\n      remaining = stream2;\n    }\n    streams.push(remaining);\n    \n    return streams;\n  }\n  \n  private startAutoCheckpointing(): void {\n    setInterval(() => {\n      this.checkpointLongLivedStreams();\n    }, this.config.autoCheckpointInterval);\n  }\n  \n  private async checkpointLongLivedStreams(): Promise<void> {\n    const now = Date.now();\n    const threshold = this.config.checkpointThresholdAge;\n    \n    for (const [streamId, entry] of this.activeStreams.entries()) {\n      const age = now - entry.checkpoint.createdAt;\n      \n      if (age > threshold && entry.checkpoint.status === 'active') {\n        await this.checkpointStream(streamId);\n      }\n    }\n  }\n  \n  private startTimeoutMonitor(): void {\n    setInterval(() => {\n      const now = Date.now();\n      const timeout = this.config.streamTimeout;\n      \n      for (const [streamId, entry] of this.activeStreams.entries()) {\n        if (now - entry.checkpoint.lastCheckpointAt > timeout) {\n          console.warn(`Stream timeout: ${streamId}`);\n          this.cleanup(streamId, 'timeout');\n        }\n      }\n    }, 60000);\n  }\n  \n  async acquireReader(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStreamDefaultReader> {\n    const stream = await this.getStream(streamId, consumerIndex);\n    const reader = stream.getReader();\n    \n    const entry = this.activeStreams.get(streamId);\n    if (entry) {\n      entry.readers.add(reader);\n      \n      if (entry.materializedChunks) {\n        return this.wrapReaderForCheckpointing(reader, entry.materializedChunks);\n      }\n    }\n    \n    return reader;\n  }\n  \n  private wrapReaderForCheckpointing(\n    reader: ReadableStreamDefaultReader,\n    chunksBuffer: any[]\n  ): ReadableStreamDefaultReader {\n    \n    const originalRead = reader.read.bind(reader);\n    \n    reader.read = async () => {\n      const result = await originalRead();\n      \n      if (!result.done && result.value !== undefined) {\n        chunksBuffer.push(result.value);\n      }\n      \n      return result;\n    };\n    \n    return reader;\n  }\n  \n  async releaseReader(\n    streamId: string, \n    reader: ReadableStreamDefaultReader\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    try {\n      reader.releaseLock();\n    } catch {\n      // Already released\n    }\n    \n    entry.readers.delete(reader);\n    \n    if (entry.readers.size === 0) {\n      await this.cleanup(streamId, 'completed');\n    }\n  }\n  \n  private async cleanup(\n    streamId: string, \n    reason: 'completed' | 'error' | 'timeout'\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    if (reason === 'completed' && entry.checkpoint.materializationType !== 'regenerate') {\n      await this.checkpointStream(streamId);\n    }\n    \n    entry.checkpoint.status = 'expired';\n    \n    for (const stream of entry.streams) {\n      try {\n        const reader = stream.getReader();\n        await reader.cancel(`Cleanup: ${reason}`);\n        reader.releaseLock();\n      } catch {\n        // Ignore\n      }\n    }\n    \n    this.activeStreams.delete(streamId);\n  }\n  \n  getCheckpoint(streamId: string): StreamCheckpoint | null {\n    return this.checkpoints.get(streamId) ?? null;\n  }\n  \n  listActiveStreams(): StreamCheckpoint[] {\n    return Array.from(this.activeStreams.values())\n      .map(entry => entry.checkpoint);\n  }\n  \n  regenerationExecutor?: (\n    nodeId: string, \n    nodeConfig: any, \n    inputs: Record<string, any>\n  ) => Promise<ReadableStream>;\n}\n\ninterface ActiveStreamEntry {\n  checkpoint: StreamCheckpoint;\n  streams: ReadableStream[];\n  readers: Set<ReadableStreamDefaultReader>;\n  materializedChunks?: any[];\n}\n\ninterface WorkflowStateProvider {\n  getNode(nodeId: string): { config: any } | null;\n  getNodeInputs(nodeId: string): Record<string, any>;\n}\n\ninterface StreamManagerConfig {\n  maxConcurrentStreams: number;\n  streamTimeout: number;\n  autoCheckpointInterval: number;\n  checkpointThresholdAge: number;\n  enableResurrection: boolean;\n  maxResurrectionAttempts: number;\n}\n```\n\n## Enhanced Tiered Storage with Circular Buffer SessionStorage\n\n### Key Enhancement: Circular Buffer Pattern + Compression for SessionStorage\n\n```typescript\ninterface SessionStorageManager {\n  maxSizeBytes: number;              // 4MB (\u7559\u51fa1MB\u7f13\u51b2)\n  currentSizeBytes: number;\n  entries: Map<string, SessionEntry>;\n  compressionThreshold: number;      // 10KB\n}\n\ninterface SessionEntry {\n  key: string;\n  sizeBytes: number;\n  priority: number;                  // Higher = more important\n  createdAt: number;\n  lastAccessedAt: number;\n  compressed: boolean;\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  \n  private workflowPolicies: Map<string, WorkflowStoragePolicy> = new Map();\n  private globalPolicy: WorkflowStoragePolicy;\n  \n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  private externalBlobs: Map<string, string> = new Map();\n  \n  // Enhanced SessionStorage manager\n  private sessionManager: SessionStorageManager = {\n    maxSizeBytes: 4 * 1024 * 1024,  // 4MB\n    currentSizeBytes: 0,\n    entries: new Map(),\n    compressionThreshold: 10 * 1024  // 10KB\n  };\n  \n  constructor(globalPolicy: WorkflowStoragePolicy) {\n    this.globalPolicy = globalPolicy;\n    this.initIndexedDB();\n    this.restoreFromSession();\n    this.calculateSessionStorageSize();\n  }\n  \n  async store(\n    key: string, \n    data: any, \n    hints: StorageHints = {}\n  ): Promise<void> {\n    \n    const policy = this.getPolicy(hints.workflowId);\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    const metadata: ArtifactMetadata = {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      pinned: hints.pinned ?? false,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      workflowId: hints.workflowId,\n      dataType: hints.dataType ?? 'json'\n    };\n    \n    this.artifactIndex.set(key, metadata);\n    \n    // Enhanced SessionStorage logic with circular buffer\n    if (hints.useSessionStorage) {\n      await this.storeInSessionStorage(\n        key, \n        serialized, \n        sizeBytes,\n        hints.priority ?? 5\n      );\n      metadata.tier = 'session';\n      return;\n    }\n    \n    // Memory tier\n    if (sizeBytes < 1024 * 1024) {\n      if (await this.tryStoreInMemory(key, data, sizeBytes, policy)) {\n        metadata.tier = 'memory';\n        return;\n      }\n    }\n    \n    // IndexedDB tier\n    await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n    metadata.tier = 'indexeddb';\n  }\n  \n  private async storeInSessionStorage(\n    key: string,\n    serialized: string,\n    sizeBytes: number,\n    priority: number\n  ): Promise<void> {\n    \n    // Try compression first if size > threshold\n    let toStore = serialized;\n    let compressed = false;\n    let actualSize = sizeBytes;\n    \n    if (sizeBytes > this.sessionManager.compressionThreshold) {\n      try {\n        toStore = await this.compressForSession(serialized);\n        actualSize = new TextEncoder().encode(toStore).length;\n        compressed = true;\n      } catch (error) {\n        console.warn('Compression failed, using original', error);\n      }\n    }\n    \n    // Check if we need to evict\n    while (this.sessionManager.currentSizeBytes + actualSize > this.sessionManager.maxSizeBytes) {\n      const evicted = this.evictFromSessionStorage(priority);\n      if (!evicted) {\n        // Cannot evict - move to IndexedDB instead\n        console.warn('SessionStorage full, moving to IndexedDB');\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'indexeddb';\n        }\n        \n        await this.storeInIndexedDB(key, serialized, sizeBytes, this.globalPolicy);\n        return;\n      }\n    }\n    \n    // Store in SessionStorage\n    const storageKey = `app-storage-${key}`;\n    const envelope = {\n      data: toStore,\n      compressed,\n      sizeBytes: actualSize,\n      priority,\n      createdAt: Date.now()\n    };\n    \n    try {\n      sessionStorage.setItem(storageKey, JSON.stringify(envelope));\n      \n      this.sessionManager.entries.set(key, {\n        key: storageKey,\n        sizeBytes: actualSize,\n        priority,\n        createdAt: Date.now(),\n        lastAccessedAt: Date.now(),\n        compressed\n      });\n      \n      this.sessionManager.currentSizeBytes += actualSize;\n      \n    } catch (error: any) {\n      // Quota exceeded despite our calculations - force evict and retry\n      console.error('SessionStorage quota exceeded:', error);\n      \n      this.evictFromSessionStorage(priority, true);\n      \n      // Move to IndexedDB\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n      }\n      \n      await this.storeInIndexedDB(key, serialized, sizeBytes, this.globalPolicy);\n    }\n  }\n  \n  private evictFromSessionStorage(\n    incomingPriority: number,\n    force: boolean = false\n  ): boolean {\n    \n    const candidates = Array.from(this.sessionManager.entries.values())\n      .filter(entry => force || entry.priority < incomingPriority)\n      .sort((a, b) => {\n        // Sort by priority (ascending), then LRU\n        if (a.priority !== b.priority) {\n          return a.priority - b.priority;\n        }\n        return a.lastAccessedAt - b.lastAccessedAt;\n      });\n    \n    if (candidates.length === 0) {\n      return false;\n    }\n    \n    const toEvict = candidates[0];\n    \n    try {\n      sessionStorage.removeItem(toEvict.key);\n      \n      const artifactKey = toEvict.key.replace('app-storage-', '');\n      this.sessionManager.entries.delete(artifactKey);\n      this.sessionManager.currentSizeBytes -= toEvict.sizeBytes;\n      \n      // Update metadata\n      const metadata = this.artifactIndex.get(artifactKey);\n      if (metadata) {\n        metadata.tier = 'evicted';\n      }\n      \n      return true;\n      \n    } catch (error) {\n      console.error('Failed to evict from SessionStorage:', error);\n      return false;\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    switch (metadata.tier) {\n      case 'memory':\n        return this.memoryCache.get(key)?.data;\n        \n      case 'indexeddb':\n        return await this.retrieveFromIndexedDB(key);\n        \n      case 'session':\n        return await this.retrieveFromSessionStorage(key);\n        \n      case 'external':\n        return await this.retrieveFromExternalBlob(key);\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n  }\n  \n  private async retrieveFromSessionStorage(key: string): Promise<any> {\n    const storageKey = `app-storage-${key}`;\n    const envelopeStr = sessionStorage.getItem(storageKey);\n    \n    if (!envelopeStr) {\n      throw new Error(`Artifact not in SessionStorage: ${key}`);\n    }\n    \n    const envelope = JSON.parse(envelopeStr);\n    \n    // Update access time\n    const entry = this.sessionManager.entries.get(key);\n    if (entry) {\n      entry.lastAccessedAt = Date.now();\n    }\n    \n    // Decompress if needed\n    const serialized = envelope.compressed\n      ? await this.decompressFromSession(envelope.data)\n      : envelope.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async compressForSession(data: string): Promise<string> {\n    // Use LZ-based compression (smaller implementation)\n    // This is a simple placeholder - real implementation would use pako or similar\n    const encoder = new TextEncoder();\n    const uint8 = encoder.encode(data);\n    \n    const stream = new Blob([uint8]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const compressedUint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...compressedUint8));\n  }\n  \n  private async decompressFromSession(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private calculateSessionStorageSize(): void {\n    let totalSize = 0;\n    \n    for (const key of Object.keys(sessionStorage)) {\n      if (key.startsWith('app-storage-')) {\n        const value = sessionStorage.getItem(key);\n        if (value) {\n          totalSize += new TextEncoder().encode(value).length;\n        }\n      }\n    }\n    \n    this.sessionManager.currentSizeBytes = totalSize;\n  }\n  \n  getSessionStorageStats(): {\n    usedBytes: number;\n    maxBytes: number;\n    utilization: number;\n    entriesCount: number;\n  } {\n    return {\n      usedBytes: this.sessionManager.currentSizeBytes,\n      maxBytes: this.sessionManager.maxSizeBytes,\n      utilization: this.sessionManager.currentSizeBytes / this.sessionManager.maxSizeBytes,\n      entriesCount: this.sessionManager.entries.size\n    };\n  }\n  \n  setWorkflowPolicy(workflowId: string, policy: WorkflowStoragePolicy): void {\n    this.workflowPolicies.set(workflowId, policy);\n  }\n  \n  getPolicy(workflowId?: string): WorkflowStoragePolicy {\n    if (workflowId) {\n      return this.workflowPolicies.get(workflowId) ?? this.globalPolicy;\n    }\n    return this.globalPolicy;\n  }\n  \n  private async tryStoreInMemory(\n    key: string, \n    data: any, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    if (this.memoryCacheSize + sizeBytes <= policy.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    if (await this.evictFromMemory(sizeBytes, policy)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(\n    requiredBytes: number, \n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'memory' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        (!policy.evictionPreferences.protectRecent || \n         Date.now() - m.createdAt > 5000)\n      );\n    \n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const metadata of candidates) {\n      toEvict.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes, policy);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n      }\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    if (this.indexedDBSize + sizeBytes > policy.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes, policy);\n      return;\n    }\n    \n    const shouldCompress = sizeBytes > 1024 * 1024;\n    const toStore = shouldCompress \n      ? await this.compress(serialized)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    await tx.objectStore('artifacts').put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    switch (policy.overflowPolicy) {\n      case 'evict-lru':\n      case 'evict-oldest':\n        await this.evictFromIndexedDB(sizeBytes, policy);\n        await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n        break;\n        \n      case 'fail':\n        throw new StorageFullError(\n          `Storage full for workflow: ${policy.workflowId}`,\n          this.getStorageStats()\n        );\n        \n      case 'compress':\n        const compressed = await this.compress(serialized);\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > policy.indexedDBMaxBytes) {\n          await this.evictFromIndexedDB(compressedSize, policy);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        break;\n        \n      case 'external':\n        const blob = new Blob([await this.compress(serialized)]);\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'external';\n        }\n        break;\n    }\n  }\n  \n  private async evictFromIndexedDB(\n    requiredBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'indexeddb' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        m.workflowId === policy.workflowId\n      );\n    \n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const metadata of candidates) {\n      toDelete.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (toDelete.length === 0) {\n      throw new Error('Cannot evict: all items protected or wrong workflow');\n    }\n    \n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const record = await tx.objectStore('artifacts').get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async pin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = true;\n    }\n  }\n  \n  async unpin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = false;\n    }\n  }\n  \n  private async compress(data: string): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 4);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n  \n  private restoreFromSession(): void {\n    const keys = Object.keys(sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('app-storage-')) {\n        const artifactKey = key.replace('app-storage-', '');\n        const envelopeStr = sessionStorage.getItem(key);\n        \n        if (envelopeStr) {\n          const envelope = JSON.parse(envelopeStr);\n          \n          const metadata: ArtifactMetadata = {\n            key: artifactKey,\n            sizeBytes: envelope.sizeBytes,\n            tier: 'session',\n            createdAt: envelope.createdAt,\n            lastAccessedAt: Date.now(),\n            accessCount: 0,\n            temporary: false,\n            pinned: true,\n            dataType: 'json'\n          };\n          \n          this.artifactIndex.set(artifactKey, metadata);\n          \n          this.sessionManager.entries.set(artifactKey, {\n            key,\n            sizeBytes: envelope.sizeBytes,\n            priority: envelope.priority,\n            createdAt: envelope.createdAt,\n            lastAccessedAt: Date.now(),\n            compressed: envelope.compressed\n          });\n        }\n      }\n    }\n  }\n  \n  getStorageStats(workflowId?: string): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      session: { count: 0, bytes: this.sessionManager.currentSizeBytes },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      if (workflowId && metadata.workflowId !== workflowId) continue;\n      \n      stats[metadata.tier as keyof typeof stats].count++;\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n}\n\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  pinned?: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType?: 'stream-ref' | 'materialized' | 'blob' | 'json';\n  useSessionStorage?: boolean;\n  priority?: number;  // 1-10, default 5\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'session' | 'external' | 'determining' | 'evicted';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  pinned: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\ninterface WorkflowStoragePolicy {\n  workflowId: string;\n  memoryCacheMaxBytes: number;\n  indexedDBMaxBytes: number;\n  overflowPolicy: 'evict-lru' | 'evict-oldest' | 'fail' | 'compress' | 'external';\n  evictionPreferences: {\n    protectRecent: boolean;\n    protectFrequent: boolean;\n    protectPinned: boolean;\n  };\n  autoCleanupAge: number;\n  enableAutoCleanup: boolean;\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  session: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n\nclass StorageFullError extends Error {\n  constructor(message: string, public stats: StorageStats) {\n    super(message);\n    this.name = 'StorageFullError';\n  }\n}\n```\n\n## Simplified Visual Block Editor with Virtual Scrolling\n\n### Key Enhancement: Virtualized Rendering + Block Limit + Performance Budget\n\n```typescript\ninterface HybridExpression {\n  mode: 'hybrid';\n  content: EditorContent[];\n  rawText: string;\n  blockCount: number;  // NEW: Track block count\n}\n\nconst EDITOR_CONFIG = {\n  maxBlocks: 50,              // Hard limit on blocks per expression\n  virtualScrollThreshold: 20,  // Start virtualizing if > 20 blocks\n  performanceBudgetMs: 16     // Target 60fps (16ms per frame)\n};\n\nclass HybridExpressionEditor {\n  private engine: ComposableExpressionEngine;\n  \n  constructor() {\n    this.engine = new ComposableExpressionEngine();\n  }\n  \n  parseFromText(text: string): HybridExpression {\n    const content: EditorContent[] = [];\n    const blockMarkerRegex = /\\{\\{block:([^:}]+):([^}]+)\\}\\}/g;\n    \n    let lastIndex = 0;\n    let blockCount = 0;\n    const matches = text.matchAll(blockMarkerRegex);\n    \n    for (const match of matches) {\n      if (blockCount >= EDITOR_CONFIG.maxBlocks) {\n        console.warn(`Block limit reached (${EDITOR_CONFIG.maxBlocks}). Remaining blocks ignored.`);\n        break;\n      }\n      \n      if (match.index! > lastIndex) {\n        const textPart = text.slice(lastIndex, match.index);\n        if (textPart) {\n          content.push({ type: 'text', text: textPart });\n        }\n      }\n      \n      const [_, blockType, configStr] = match;\n      const config = this.parseBlockConfig(blockType, configStr);\n      const displayText = this.generateDisplayText(blockType, config);\n      \n      content.push({\n        type: 'block',\n        blockType: blockType as any,\n        config,\n        displayText\n      });\n      \n      blockCount++;\n      lastIndex = match.index! + match[0].length;\n    }\n    \n    if (lastIndex < text.length) {\n      content.push({ type: 'text', text: text.slice(lastIndex) });\n    }\n    \n    return {\n      mode: 'hybrid',\n      content,\n      rawText: text,\n      blockCount\n    };\n  }\n  \n  serializeToText(expression: HybridExpression): string {\n    return expression.content.map(node => {\n      if (node.type === 'text') {\n        return node.text;\n      } else {\n        const configStr = this.serializeBlockConfig(node.blockType, node.config);\n        return `{{block:${node.blockType}:${configStr}}}`;\n      }\n    }).join('');\n  }\n  \n  insertBlock(\n    expression: HybridExpression, \n    cursorPosition: number,\n    blockType: string,\n    config: Record<string, any>\n  ): HybridExpression {\n    \n    if (expression.blockCount >= EDITOR_CONFIG.maxBlocks) {\n      throw new Error(`Cannot insert block: limit of ${EDITOR_CONFIG.maxBlocks} blocks reached`);\n    }\n    \n    const displayText = this.generateDisplayText(blockType, config);\n    const blockNode: BlockNode = {\n      type: 'block',\n      blockType: blockType as any,\n      config,\n      displayText\n    };\n    \n    let currentPos = 0;\n    let insertIndex = 0;\n    \n    for (let i = 0; i < expression.content.length; i++) {\n      const node = expression.content[i];\n      const nodeLength = node.type === 'text' \n        ? node.text.length \n        : node.displayText.length;\n      \n      if (currentPos + nodeLength >= cursorPosition) {\n        insertIndex = i + 1;\n        break;\n      }\n      \n      currentPos += nodeLength;\n    }\n    \n    const newContent = [\n      ...expression.content.slice(0, insertIndex),\n      blockNode,\n      ...expression.content.slice(insertIndex)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent }),\n      blockCount: expression.blockCount + 1\n    };\n  }\n  \n  removeBlock(expression: HybridExpression, blockIndex: number): HybridExpression {\n    const newContent = expression.content.filter((_, i) => i !== blockIndex);\n    \n    const blockCount = newContent.filter(n => n.type === 'block').length;\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent }),\n      blockCount\n    };\n  }\n  \n  updateBlock(\n    expression: HybridExpression, \n    blockIndex: number,\n    newConfig: Record<string, any>\n  ): HybridExpression {\n    \n    const node = expression.content[blockIndex];\n    if (node.type !== 'block') {\n      throw new Error('Node is not a block');\n    }\n    \n    const updatedNode: BlockNode = {\n      ...node,\n      config: newConfig,\n      displayText: this.generateDisplayText(node.blockType, newConfig)\n    };\n    \n    const newContent = [\n      ...expression.content.slice(0, blockIndex),\n      updatedNode,\n      ...expression.content.slice(blockIndex + 1)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent }),\n      blockCount: expression.blockCount\n    };\n  }\n  \n  private parseBlockConfig(type: string, configStr: string): Record<string, any> {\n    const pairs = configStr.split(':');\n    \n    switch (type) {\n      case 'field':\n        return { path: pairs[0] };\n      case 'filter':\n        return { condition: pairs[0] };\n      case 'aggregate':\n        return { function: pairs[0], path: pairs[1] || '' };\n      case 'map':\n        return { expression: pairs[0] };\n      case 'transform':\n        return { operation: pairs[0] };\n      default:\n        return {};\n    }\n  }\n  \n  private serializeBlockConfig(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return config.path;\n      case 'filter':\n        return config.condition;\n      case 'aggregate':\n        return `${config.function}:${config.path}`;\n      case 'map':\n        return config.expression;\n      case 'transform':\n        return config.operation;\n      default:\n        return '';\n    }\n  }\n  \n  private generateDisplayText(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return `$.${config.path}`;\n      case 'filter':\n        return `[${config.condition}]`;\n      case 'aggregate':\n        return `${config.function}(${config.path})`;\n      case 'map':\n        return `map(${config.expression})`;\n      case 'transform':\n        return config.operation;\n      default:\n        return 'block';\n    }\n  }\n}\n\n// React Component with Virtual Scrolling\nconst HybridExpressionEditorUI: React.FC<{\n  value: HybridExpression;\n  onChange: (value: HybridExpression) => void;\n}> = ({ value, onChange }) => {\n  \n  const editor = useMemo(() => new HybridExpressionEditor(), []);\n  const [cursorPosition, setCursorPosition] = useState(0);\n  const [selectedBlockIndex, setSelectedBlockIndex] = useState<number | null>(null);\n  const editorRef = useRef<HTMLDivElement>(null);\n  \n  // Virtualization state\n  const [viewportStart, setViewportStart] = useState(0);\n  const [viewportEnd, setViewportEnd] = useState(100);\n  \n  const shouldVirtualize = value.blockCount > EDITOR_CONFIG.virtualScrollThreshold;\n  \n  const visibleContent = useMemo(() => {\n    if (!shouldVirtualize) {\n      return value.content;\n    }\n    \n    // Only render blocks in viewport\n    return value.content.slice(viewportStart, viewportEnd);\n  }, [value.content, viewportStart, viewportEnd, shouldVirtualize]);\n  \n  const handleInsertBlock = useCallback((blockType: string) => {\n    if (value.blockCount >= EDITOR_CONFIG.maxBlocks) {\n      alert(`Cannot insert block: maximum of ${EDITOR_CONFIG.maxBlocks} blocks reached.`);\n      return;\n    }\n    \n    const defaultConfig = getDefaultBlockConfig(blockType);\n    const updated = editor.insertBlock(value, cursorPosition, blockType, defaultConfig);\n    onChange(updated);\n  }, [value, cursorPosition, editor, onChange]);\n  \n  const handleRemoveBlock = useCallback((blockIndex: number) => {\n    const updated = editor.removeBlock(value, blockIndex);\n    onChange(updated);\n    setSelectedBlockIndex(null);\n  }, [value, editor, onChange]);\n  \n  const handleUpdateBlock = useCallback((blockIndex: number, newConfig: Record<string, any>) => {\n    const updated = editor.updateBlock(value, blockIndex, newConfig);\n    onChange(updated);\n  }, [value, editor, onChange]);\n  \n  const handleTextChange = useCallback((newText: string) => {\n    try {\n      const parsed = editor.parseFromText(newText);\n      onChange(parsed);\n    } catch (error: any) {\n      console.error('Parse error:', error.message);\n    }\n  }, [editor, onChange]);\n  \n  const handleScroll = useCallback((e: React.UIEvent<HTMLDivElement>) => {\n    if (!shouldVirtualize) return;\n    \n    const container = e.currentTarget;\n    const scrollTop = container.scrollTop;\n    const itemHeight = 30;  // Approximate height per block\n    \n    const start = Math.floor(scrollTop / itemHeight);\n    const end = start + Math.ceil(container.clientHeight / itemHeight) + 10;  // Buffer\n    \n    setViewportStart(Math.max(0, start));\n    setViewportEnd(Math.min(value.content.length, end));\n  }, [shouldVirtualize, value.content.length]);\n  \n  return (\n    <div className=\"hybrid-expression-editor\">\n      {/* Block Palette */}\n      <div className=\"block-palette\">\n        <button onClick={() => handleInsertBlock('field')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\ud83d\udcc4</span> Field\n        </button>\n        <button onClick={() => handleInsertBlock('filter')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\ud83d\udd0d</span> Filter\n        </button>\n        <button onClick={() => handleInsertBlock('aggregate')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\u03a3</span> Aggregate\n        </button>\n        <button onClick={() => handleInsertBlock('map')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\ud83d\uddfa\ufe0f</span> Map\n        </button>\n        <button onClick={() => handleInsertBlock('transform')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\u2699\ufe0f</span> Transform\n        </button>\n        \n        <span className=\"block-count\">\n          {value.blockCount} / {EDITOR_CONFIG.maxBlocks} blocks\n        </span>\n      </div>\n      \n      {/* Rich Text Editor */}\n      <div \n        ref={editorRef}\n        className=\"editor-content\"\n        contentEditable\n        suppressContentEditableWarning\n        onScroll={handleScroll}\n        onInput={(e) => {\n          const text = e.currentTarget.textContent || '';\n          handleTextChange(text);\n        }}\n        onSelect={() => {\n          const selection = window.getSelection();\n          if (selection) {\n            setCursorPosition(selection.anchorOffset);\n          }\n        }}\n      >\n        {visibleContent.map((node, index) => {\n          const actualIndex = shouldVirtualize ? viewportStart + index : index;\n          \n          if (node.type === 'text') {\n            return <span key={actualIndex}>{node.text}</span>;\n          } else {\n            return (\n              <BlockComponent\n                key={actualIndex}\n                node={node}\n                selected={selectedBlockIndex === actualIndex}\n                onSelect={() => setSelectedBlockIndex(actualIndex)}\n                onRemove={() => handleRemoveBlock(actualIndex)}\n                onUpdate={(config) => handleUpdateBlock(actualIndex, config)}\n              />\n            );\n          }\n        })}\n      </div>\n      \n      {/* Block Config Panel */}\n      {selectedBlockIndex !== null && value.content[selectedBlockIndex]?.type === 'block' && (\n        <BlockConfigPanel\n          blockNode={value.content[selectedBlockIndex] as BlockNode}\n          onUpdate={(config) => handleUpdateBlock(selectedBlockIndex, config)}\n          onClose={() => setSelectedBlockIndex(null)}\n        />\n      )}\n      \n      {/* Raw Text View */}\n      <details className=\"raw-text-view\">\n        <summary>View Raw Text</summary>\n        <textarea \n          value={value.rawText}\n          onChange={(e) => handleTextChange(e.target.value)}\n          className=\"raw-text-editor\"\n        />\n      </details>\n    </div>\n  );\n};\n\n// Memoized block component for performance\nconst BlockComponent = React.memo<{\n  node: BlockNode;\n  selected: boolean;\n  onSelect: () => void;\n  onRemove: () => void;\n  onUpdate: (config: Record<string, any>) => void;\n}>(({ node, selected, onSelect, onRemove }) => {\n  \n  const getBlockIcon = (type: string) => {\n    switch (type) {\n      case 'field': return '\ud83d\udcc4';\n      case 'filter': return '\ud83d\udd0d';\n      case 'aggregate': return '\u03a3';\n      case 'map': return '\ud83d\uddfa\ufe0f';\n      case 'transform': return '\u2699\ufe0f';\n      default: return '\ud83e\udde9';\n    }\n  };\n  \n  return (\n    <span \n      className={`expression-block ${selected ? 'selected' : ''}`}\n      onClick={onSelect}\n      contentEditable={false}\n    >\n      <span className=\"block-icon\">{getBlockIcon(node.blockType)}</span>\n      <span className=\"block-text\">{node.displayText}</span>\n      <button className=\"block-remove\" onClick={onRemove}>\u00d7</button>\n    </span>\n  );\n});\n\nconst BlockConfigPanel: React.FC<{\n  blockNode: BlockNode;\n  onUpdate: (config: Record<string, any>) => void;\n  onClose: () => void;\n}> = ({ blockNode, onUpdate, onClose }) => {\n  \n  const [config, setConfig] = useState(blockNode.config);\n  \n  const handleSave = () => {\n    onUpdate(config);\n    onClose();\n  };\n  \n  return (\n    <div className=\"block-config-panel\">\n      <div className=\"panel-header\">\n        <h4>Configure {blockNode.blockType}</h4>\n        <button onClick={onClose}>\u00d7</button>\n      </div>\n      \n      <div className=\"panel-body\">\n        {blockNode.blockType === 'field' && (\n          <label>\n            Path:\n            <input \n              type=\"text\"\n              value={config.path || ''}\n              onChange={(e) => setConfig({ ...config, path: e.target.value })}\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'filter' && (\n          <label>\n            Condition:\n            <input \n              type=\"text\"\n              value={config.condition || ''}\n              onChange={(e) => setConfig({ ...config, condition: e.target.value })}\n              placeholder=\"e.g., age > 18\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'aggregate' && (\n          <>\n            <label>\n              Function:\n              <select \n                value={config.function || 'sum'}\n                onChange={(e) => setConfig({ ...config, function: e.target.value })}\n              >\n                <option value=\"sum\">Sum</option>\n                <option value=\"avg\">Average</option>\n                <option value=\"count\">Count</option>\n                <option value=\"min\">Min</option>\n                <option value=\"max\">Max</option>\n              </select>\n            </label>\n            <label>\n              Path:\n              <input \n                type=\"text\"\n                value={config.path || ''}\n                onChange={(e) => setConfig({ ...config, path: e.target.value })}\n              />\n            </label>\n          </>\n        )}\n        \n        {blockNode.blockType === 'map' && (\n          <label>\n            Expression:\n            <input \n              type=\"text\"\n              value={config.expression || ''}\n              onChange={(e) => setConfig({ ...config, expression: e.target.value })}\n              placeholder=\"e.g., name & ' ' & email\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'transform' && (\n          <label>\n            Operation:\n            <select \n              value={config.operation || 'uppercase'}\n              onChange={(e) => setConfig({ ...config, operation: e.target.value })}\n            >\n              <option value=\"uppercase\">Uppercase</option>\n              <option value=\"lowercase\">Lowercase</option>\n              <option value=\"trim\">Trim</option>\n              <option value=\"reverse\">Reverse</option>\n            </select>\n          </label>\n        )}\n      </div>\n      \n      <div className=\"panel-footer\">\n        <button onClick={handleSave}>Save</button>\n        <button onClick={onClose}>Cancel</button>\n      </div>\n    </div>\n  );\n};\n\nfunction getDefaultBlockConfig(blockType: string): Record<string, any> {\n  switch (blockType) {\n    case 'field':\n      return { path: '' };\n    case 'filter':\n      return { condition: '' };\n    case 'aggregate':\n      return { function: 'sum', path: '' };\n    case 'map':\n      return { expression: '' };\n    case 'transform':\n      return { operation: 'uppercase' };\n    default:\n      return {};\n  }\n}\n\ntype EditorContent = TextNode | BlockNode;\n\ninterface TextNode {\n  type: 'text';\n  text: string;\n}\n\ninterface BlockNode {\n  type: 'block';\n  blockType: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n  displayText: string;\n}\n```\n\n## Simplified Trust Model + Stratified Sampling for Metrics\n\n### Key Enhancement: Binary Trust + Always-Sample-Errors Strategy\n\n```typescript\n// Simplified to binary trust model\ntype TrustLevel = 'trusted' | 'untrusted';\n\ninterface CompensationHandlerConfig {\n  id: string;\n  name: string;\n  trustLevel: TrustLevel;  // Simplified to binary\n  author?: string;\n  \n  handlerType: 'builtin' | 'http-api' | 'webhook' | 'custom';\n  \n  matches: {\n    operation?: string[];\n    description?: string[];\n    service?: string[];\n  };\n  \n  compensation: \n    | { type: 'http-api'; config: HttpApiConfig }\n    | { type: 'webhook'; config: WebhookConfig }\n    | { type: 'custom'; workerUrl: string }\n    | { type: 'manual'; steps: string[] };\n}\n\n// Stratified sampling for metrics\ninterface MetricsConfig {\n  enablePersistence: boolean;\n  \n  // Stratified sampling rates\n  sampling: {\n    errors: number;          // 100% - always sample errors\n    successes: number;       // 5% - rarely sample successes\n    slowOperations: number;  // 50% - sample operations > 1s\n  };\n  \n  retentionDays: number;\n  enableRecommendations: boolean;\n}\n\nclass AdaptivePerformanceMonitor {\n  private metrics: StreamingMetrics;\n  private recentMeasurements: Measurement[] = [];\n  private db: IDBDatabase | null = null;\n  private config: MetricsConfig;\n  \n  constructor(config: MetricsConfig) {\n    this.config = config;\n    this.metrics = this.initMetrics();\n    \n    if (config.enablePersistence) {\n      this.initDatabase();\n    }\n  }\n  \n  recordDecision(\n    strategy: 'tee' | 'materialize' | 'batch',\n    sizeBytes: number,\n    durationMs: number,\n    memoryUsed: number,\n    context: {\n      workflowId: string;\n      nodeId: string;\n      success: boolean;  // NEW: Track success/failure\n    }\n  ): void {\n    \n    switch (strategy) {\n      case 'tee':\n        this.metrics.decisions.teeCount++;\n        break;\n      case 'materialize':\n        this.metrics.decisions.materializeCount++;\n        break;\n      case 'batch':\n        this.metrics.decisions.batchCount++;\n        break;\n    }\n    \n    if (strategy === 'tee' || strategy === 'materialize') {\n      // Stratified sampling\n      const shouldSample = this.shouldSample(context.success, durationMs);\n      \n      if (shouldSample) {\n        const measurement: Measurement = {\n          id: crypto.randomUUID(),\n          timestamp: Date.now(),\n          strategy,\n          sizeBytes,\n          durationMs,\n          memoryUsed,\n          workflowId: context.workflowId,\n          nodeId: context.nodeId,\n          success: context.success\n        };\n        \n        this.recentMeasurements.push(measurement);\n        \n        if (this.recentMeasurements.length > 100) {\n          this.recentMeasurements.shift();\n        }\n        \n        if (this.config.enablePersistence && this.db) {\n          this.persistMeasurement(measurement);\n        }\n      }\n      \n      this.updateAverages();\n      \n      if (this.config.enableRecommendations) {\n        this.generateRecommendations();\n      }\n    }\n  }\n  \n  private shouldSample(success: boolean, durationMs: number): boolean {\n    // Always sample errors\n    if (!success) {\n      return Math.random() < this.config.sampling.errors;\n    }\n    \n    // Sample slow operations more frequently\n    if (durationMs > 1000) {\n      return Math.random() < this.config.sampling.slowOperations;\n    }\n    \n    // Rarely sample fast successes\n    return Math.random() < this.config.sampling.successes;\n  }\n  \n  recordError(type: 'tee' | 'materialize' | 'storage-full'): void {\n    switch (type) {\n      case 'tee':\n        this.metrics.errors.teeFailures++;\n        break;\n      case 'materialize':\n        this.metrics.errors.materializeFailures++;\n        break;\n      case 'storage-full':\n        this.metrics.errors.storageFullErrors++;\n        break;\n    }\n  }\n  \n  private updateAverages(): void {\n    const tees = this.recentMeasurements.filter(m => m.strategy === 'tee');\n    const materializes = this.recentMeasurements.filter(m => m.strategy === 'materialize');\n    \n    if (tees.length > 0) {\n      this.metrics.performance.avgTeeMemory = \n        tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length;\n      this.metrics.performance.avgTeeTime = \n        tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length;\n    }\n    \n    if (materializes.length > 0) {\n      this.metrics.performance.avgMaterializeSize = \n        materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length;\n      this.metrics.performance.avgMaterializeTime = \n        materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length;\n    }\n  }\n  \n  private generateRecommendations(): void {\n    if (this.metrics.performance.avgTeeMemory > 10 * 1024 * 1024) {\n      this.metrics.recommendations.suggestedTeeThreshold = 3 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedTeeThreshold = 5 * 1024 * 1024;\n    }\n    \n    if (this.metrics.performance.avgMaterializeTime < 100) {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 100 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 50 * 1024 * 1024;\n    }\n    \n    if (this.metrics.errors.storageFullErrors > 5) {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'evict-lru';\n    } else {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'compress';\n    }\n  }\n  \n  async getHistoricalMetrics(\n    workflowId?: string,\n    startTime?: number,\n    endTime?: number\n  ): Promise<Measurement[]> {\n    \n    if (!this.config.enablePersistence || !this.db) {\n      return this.recentMeasurements;\n    }\n    \n    return new Promise((resolve, reject) => {\n      const tx = this.db!.transaction('measurements', 'readonly');\n      const store = tx.objectStore('measurements');\n      const index = store.index('timestamp');\n      \n      const range = IDBKeyRange.bound(\n        startTime || 0,\n        endTime || Date.now()\n      );\n      \n      const results: Measurement[] = [];\n      const request = index.openCursor(range);\n      \n      request.onsuccess = (event) => {\n        const cursor = (event.target as IDBRequest).result;\n        if (cursor) {\n          const measurement: Measurement = cursor.value;\n          \n          if (!workflowId || measurement.workflowId === workflowId) {\n            results.push(measurement);\n          }\n          \n          cursor.continue();\n        } else {\n          resolve(results);\n        }\n      };\n      \n      request.onerror = () => reject(request.error);\n    });\n  }\n  \n  async generateReport(workflowId?: string): Promise<PerformanceReport> {\n    const measurements = await this.getHistoricalMetrics(workflowId);\n    \n    const tees = measurements.filter(m => m.strategy === 'tee');\n    const materializes = measurements.filter(m => m.strategy === 'materialize');\n    \n    return {\n      totalMeasurements: measurements.length,\n      timeRange: {\n        start: Math.min(...measurements.map(m => m.timestamp)),\n        end: Math.max(...measurements.map(m => m.timestamp))\n      },\n      strategies: {\n        tee: {\n          count: tees.length,\n          avgMemory: tees.reduce((sum, m) => sum + m.memoryUsed, 0) / (tees.length || 1),\n          avgDuration: tees.reduce((sum, m) => sum + m.durationMs, 0) / (tees.length || 1),\n          avgSize: tees.reduce((sum, m) => sum + m.sizeBytes, 0) / (tees.length || 1),\n          successRate: tees.filter(m => m.success).length / (tees.length || 1)\n        },\n        materialize: {\n          count: materializes.length,\n          avgMemory: materializes.reduce((sum, m) => sum + m.memoryUsed, 0) / (materializes.length || 1),\n          avgDuration: materializes.reduce((sum, m) => sum + m.durationMs, 0) / (materializes.length || 1),\n          avgSize: materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / (materializes.length || 1),\n          successRate: materializes.filter(m => m.success).length / (materializes.length || 1)\n        }\n      },\n      recommendations: this.metrics.recommendations\n    };\n  }\n  \n  getMetrics(): StreamingMetrics {\n    return { ...this.metrics };\n  }\n  \n  getMeasurements(): Measurement[] {\n    return [...this.recentMeasurements];\n  }\n  \n  private async initDatabase(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('PerformanceMetrics', 3);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        this.cleanupOldMeasurements();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('measurements')) {\n          const store = db.createObjectStore('measurements', { keyPath: 'id' });\n          store.createIndex('timestamp', 'timestamp', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n          store.createIndex('strategy', 'strategy', { unique: false });\n          store.createIndex('success', 'success', { unique: false });\n        }\n      };\n    });\n  }\n  \n  private async persistMeasurement(measurement: Measurement): Promise<void> {\n    if (!this.db) return;\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    await tx.objectStore('measurements').add(measurement);\n  }\n  \n  private async cleanupOldMeasurements(): Promise<void> {\n    if (!this.db) return;\n    \n    const cutoff = Date.now() - (this.config.retentionDays * 24 * 60 * 60 * 1000);\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    const store = tx.objectStore('measurements');\n    const index = store.index('timestamp');\n    \n    const range = IDBKeyRange.upperBound(cutoff);\n    const request = index.openCursor(range);\n    \n    request.onsuccess = (event) => {\n      const cursor = (event.target as IDBRequest).result;\n      if (cursor) {\n        cursor.delete();\n        cursor.continue();\n      }\n    };\n  }\n  \n  private initMetrics(): StreamingMetrics {\n    return {\n      decisions: {\n        teeCount: 0,\n        materializeCount: 0,\n        batchCount: 0\n      },\n      performance: {\n        avgTeeMemory: 0,\n        avgMaterializeSize: 0,\n        avgTeeTime: 0,\n        avgMaterializeTime: 0\n      },\n      errors: {\n        teeFailures: 0,\n        materializeFailures: 0,\n        storageFullErrors: 0\n      },\n      recommendations: {\n        suggestedTeeThreshold: 5 * 1024 * 1024,\n        suggestedMaterializeThreshold: 50 * 1024 * 1024,\n        suggestedOverflowPolicy: 'evict-lru'\n      }\n    };\n  }\n}\n\ninterface Measurement {\n  id: string;\n  timestamp: number;\n  strategy: 'tee' | 'materialize';\n  sizeBytes: number;\n  durationMs: number;\n  memoryUsed: number;\n  workflowId: string;\n  nodeId: string;\n  success: boolean;  // NEW\n}\n\ninterface StreamingMetrics {\n  decisions: {\n    teeCount: number;\n    materializeCount: number;\n    batchCount: number;\n  };\n  \n  performance: {\n    avgTeeMemory: number;\n    avgMaterializeSize: number;\n    avgTeeTime: number;\n    avgMaterializeTime: number;\n  };\n  \n  errors: {\n    teeFailures: number;\n    materializeFailures: number;\n    storageFullErrors: number;\n  };\n  \n  recommendations: {\n    suggestedTeeThreshold: number;\n    suggestedMaterializeThreshold: number;\n    suggestedOverflowPolicy: string;\n  };\n}\n\ninterface PerformanceReport {\n  totalMeasurements: number;\n  timeRange: {\n    start: number;\n    end: number;\n  };\n  strategies: {\n    tee: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n      successRate: number;  // NEW\n    };\n    materialize: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n      successRate: number;  // NEW\n    };\n  };\n  recommendations: StreamingMetrics['recommendations'];\n}\n\ninterface HttpApiConfig {\n  method: 'GET' | 'POST' | 'DELETE' | 'PUT';\n  urlTemplate: string;\n  allowedDomains: string[];\n  headers: Record<string, string>;\n  bodyTemplate: string;\n  auth?: {\n    type: 'bearer' | 'basic' | 'api-key';\n    tokenPath: string;\n  };\n}\n\ninterface WebhookConfig {\n  url: string;\n  allowedDomains: string[];\n  method: 'POST' | 'PUT';\n  payloadTemplate: string;\n}\n```\n\n---\n\n## What I Improved\n\n### 1. Stream Resurrection Edge Cases (CRITICAL FIX)\n**Problem**: Original design had no validation for config changes between checkpoint and resurrection.\n**Solution**:\n- **Version tracking**: Hash node config and inputs at checkpoint time\n- **Staleness detection**: Validate checkpoint freshness before resurrection\n- **Graceful degradation**: Three fallback strategies when config changes:\n  1. Regenerate with old config (snapshot)\n  2. Use cached data with warning\n  3. Return empty stream with error message\n- **Severity classification**: `none`/`minor`/`major`/`critical` determines fallback strategy\n- **User notifications**: Event-based warnings for degraded resurrections\n\n### 2. SessionStorage Overflow (CRITICAL FIX)\n**Problem**: Original design would silently fail when hitting 5MB SessionStorage limit.\n**Solution**:\n- **Circular buffer pattern**: Priority-based eviction when approaching limit\n- **Compression**: Auto-compress items > 10KB before storing\n- **Size tracking**: Real-time monitoring of SessionStorage usage\n- **Graceful fallback**: Move to IndexedDB if SessionStorage full\n- **4MB soft limit**: Leave 1MB buffer to prevent quota errors\n\n### 3. Visual Block Editor Performance\n**Problem**: Rendering 50+ inline blocks would cause lag.\n**Solution**:\n- **Hard block limit**: 50 blocks per expression (prevents abuse)\n- **Virtual scrolling**: Only render visible blocks when > 20 blocks\n- **Memoization**: Block components wrapped in `React.memo`\n- **Performance budget**: 16ms target (60fps)\n- **Block counter**: Visible feedback on current usage\n\n### 4. Trust Level UX Simplification\n**Problem**: Four-level trust model (`system`/`verified`/`user`/`untrusted`) too complex.\n**Solution**:\n- **Binary trust**: `trusted` vs `untrusted` only\n- **Clear approval flow**: Untrusted handlers require explicit user approval\n- **Approval persistence**: Once approved, stored in localStorage\n- **Domain whitelisting**: All handlers must declare allowed domains\n- **No \"verified\" tier**: Removes need for marketplace/verification system\n\n### 5. Metrics Sampling Strategy\n**Problem**: 10% uniform sampling misses errors and performance spikes.\n**Solution**:\n- **Stratified sampling**:\n  - **100% errors**: Always sample failures\n  - **50% slow ops**: Sample operations > 1s\n  - **5% fast successes**: Rarely sample quick successes\n- **Success tracking**: Add `success: boolean` to measurements\n- **Success rate in reports**: Show reliability metrics alongside performance\n\n---\n\n## What I Kept\n\nAll core architectural decisions remain intact:\n- React Flow canvas \u2705\n- Zustand + immer state management \u2705\n- Adaptive streaming executor \u2705\n- Three-tier storage (enhanced) \u2705\n- Composable expression system \u2705\n- Append-only side effect log \u2705\n- Checkpoint manager \u2705\n- Worker pools \u2705\n- Compression \u2705\n- Auto-checkpointing streams \u2705\n\n---\n\n## Remaining Concerns\n\n### 1. Version Hashing Collisions\n**Issue**: Simple hash function could have collisions for similar configs.\n**Mitigation**: Use full config snapshot comparison as secondary check.\n**Trade-off**: Slightly larger checkpoint metadata.\n\n### 2. SessionStorage Cross-Tab Behavior\n**Issue**: SessionStorage is per-tab, not shared across browser tabs.\n**Impact**: Opening workflow in new tab loses checkpoints.\n**Mitigation**: Add \"Export/Import Checkpoint\" feature for multi-tab workflows.\n\n### 3. Virtual Scroll with contentEditable\n**Issue**: contentEditable + virtual scrolling is complex to implement correctly.\n**Alternative**: Use separate input for text, render blocks as read-only chips.\n**Recommendation**: Prototype both approaches, measure complexity.\n\n### 4. Compensation Handler Marketplace\n**Issue**: No \"verified\" trust level means users can't discover trusted handlers.\n**Future Work**: Build handler marketplace with community ratings + audits.\n**Short-term**: Ship with built-in handlers only (Stripe, Slack, etc).\n\n---\n\n## Convergence Status\n\n**ITERATING** - Found critical edge cases and improved the design with concrete solutions:\n\n\u2705 **Stream resurrection**: Version-aware with graceful degradation  \n\u2705 **SessionStorage overflow**: Circular buffer + compression  \n\u2705 **Block editor performance**: Virtual scrolling + hard limits  \n\u2705 **Trust model**: Simplified to binary with clear approval flow  \n\u2705 **Metrics sampling**: Stratified sampling captures errors reliably  \n\nThe design is significantly more robust but needs one more iteration to validate the complexity trade-offs.\n\n---\n\n##",
        "prompt_for_other": "Your design was excellent - I've built on it to address the edge cases you identified. Please review my enhancements:\n\n1. **Stream Resurrection with Version Tracking**: Do you see any scenarios where the staleness detection would fail? Should we also track workflow version (not just node config)?\n\n2. **SessionStorage Circular Buffer**: Is priority-based eviction (1-10 scale) sufficient, or should we add more dimensions (access frequency, size, age)?\n\n3. **Virtual Scrolling for contentEditable**: This is complex to implement correctly. Should we simplify to a non-editable block list + separate text input?\n\n4. **Binary Trust Model**: Does removing \"verified\" tier hurt discoverability of safe handlers? Should we add a \"community-trusted\" flag instead?\n\n5. **Stratified Sampling Rates**: Are 100%/50%/5% the right rates, or should they be configurable per workflow?\n\nIf these solutions work, we're ready to build. If not, suggest alternatives and we'll iterate once more.",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Visual AI Agent Workflow Builder - Production-Ready Architecture v6\n\n## Core Architecture\n\n### System Overview\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                     React Application Layer                      \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502\n\u2502  \u2502   Canvas     \u2502  \u2502  Properties  \u2502  \u2502   Toolbar    \u2502         \u2502\n\u2502  \u2502   (React     \u2502  \u2502    Panel     \u2502  \u2502   & Palette  \u2502         \u2502\n\u2502  \u2502    Flow)     \u2502  \u2502              \u2502  \u2502              \u2502         \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502           State Management (Zustand + Immer)                     \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Execution Orchestration Layer                \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Adaptive   \u2502  \u2502  Resilient \u2502  \u2502 Checkpoint \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Executor   \u2502  \u2502  Stream    \u2502  \u2502  Manager   \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502            \u2502  \u2502  Manager   \u2502  \u2502            \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502\n\u2502  \u2502              Data Flow & Storage Layer                    \u2502  \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u2502  \u2502\n\u2502  \u2502  \u2502 Composable \u2502  \u2502   Tiered   \u2502  \u2502 Effect Log \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502 Expression \u2502  \u2502  Storage   \u2502  \u2502  (Append-  \u2502         \u2502  \u2502\n\u2502  \u2502  \u2502  System    \u2502  \u2502  Manager   \u2502  \u2502   Only)    \u2502         \u2502  \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2502  \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Enhanced Resilient Stream Manager (Addressing Critical Edge Cases)\n\n### Key Enhancement: Version-Aware Stream Resurrection with Graceful Degradation\n\n```typescript\ninterface StreamCheckpoint {\n  streamId: string;\n  sourceNodeId: string;\n  executionId: string;\n  consumerCount: number;\n  createdAt: number;\n  lastCheckpointAt: number;\n  \n  // VERSION TRACKING - Critical for detecting config changes\n  nodeConfigVersion: string;         // Hash of node config at checkpoint time\n  nodeConfigSnapshot: any;           // Full config snapshot for validation\n  \n  // Resurrection data\n  materializationType: 'full' | 'partial' | 'regenerate';\n  materializedData?: any;\n  partialOffset?: number;\n  regenerationConfig?: {\n    nodeConfig: any;\n    inputs: Record<string, any>;\n    inputHashes: Record<string, string>;  // NEW: Detect input changes\n  };\n  \n  // Resurrection metadata\n  status: 'active' | 'checkpointed' | 'resurrected' | 'expired' | 'stale';\n  resurrectionAttempts: number;\n  resurrectionStrategy: 'exact' | 'compatible' | 'fail';\n  staleness: 'fresh' | 'config-changed' | 'inputs-changed' | 'node-deleted';\n}\n\ninterface ResurrectionResult {\n  success: boolean;\n  stream?: ReadableStream;\n  degraded?: boolean;              // NEW: Indicates partial success\n  warning?: string;                // NEW: User-facing warning message\n  fallbackUsed?: 'cached' | 'regenerated' | 'empty';\n}\n\nclass ResilientStreamManager {\n  private activeStreams: Map<string, ActiveStreamEntry> = new Map();\n  private checkpoints: Map<string, StreamCheckpoint> = new Map();\n  private storage: HybridStorageManager;\n  private config: StreamManagerConfig;\n  private workflowState: WorkflowStateProvider;  // NEW: Validates node configs\n  \n  constructor(\n    storage: HybridStorageManager, \n    config: StreamManagerConfig,\n    workflowState: WorkflowStateProvider\n  ) {\n    this.storage = storage;\n    this.config = config;\n    this.workflowState = workflowState;\n    \n    if (config.enableResurrection) {\n      this.restoreCheckpointsFromStorage();\n    }\n    \n    this.startAutoCheckpointing();\n    this.startTimeoutMonitor();\n    this.startStalenessDetector();  // NEW: Detect stale checkpoints\n  }\n  \n  async registerStream(\n    sourceNodeId: string,\n    executionId: string,\n    stream: ReadableStream,\n    consumerCount: number,\n    options: {\n      estimatedSize: number;\n      materializationType?: 'full' | 'partial' | 'regenerate';\n      regenerationConfig?: any;\n      nodeConfig: any;  // NEW: Required for version tracking\n      inputs: Record<string, any>;  // NEW: Required for version tracking\n    }\n  ): Promise<string> {\n    \n    if (this.activeStreams.size >= this.config.maxConcurrentStreams) {\n      throw new Error(\n        `Max concurrent streams reached (${this.config.maxConcurrentStreams})`\n      );\n    }\n    \n    const streamId = crypto.randomUUID();\n    const teedStreams = this.createMultiTee(stream, consumerCount);\n    \n    // Create version-aware checkpoint\n    const checkpoint: StreamCheckpoint = {\n      streamId,\n      sourceNodeId,\n      executionId,\n      consumerCount,\n      createdAt: Date.now(),\n      lastCheckpointAt: Date.now(),\n      nodeConfigVersion: this.hashConfig(options.nodeConfig),\n      nodeConfigSnapshot: structuredClone(options.nodeConfig),\n      materializationType: options.materializationType ?? 'full',\n      regenerationConfig: options.regenerationConfig ? {\n        nodeConfig: structuredClone(options.nodeConfig),\n        inputs: structuredClone(options.inputs),\n        inputHashes: this.hashInputs(options.inputs)\n      } : undefined,\n      status: 'active',\n      resurrectionAttempts: 0,\n      resurrectionStrategy: 'exact',\n      staleness: 'fresh'\n    };\n    \n    this.checkpoints.set(streamId, checkpoint);\n    \n    this.activeStreams.set(streamId, {\n      checkpoint,\n      streams: teedStreams,\n      readers: new Set(),\n      materializedChunks: options.materializationType !== 'regenerate' ? [] : undefined\n    });\n    \n    return streamId;\n  }\n  \n  async getStream(streamId: string, consumerIndex: number): Promise<ReadableStream> {\n    const active = this.activeStreams.get(streamId);\n    \n    if (active) {\n      active.checkpoint.lastCheckpointAt = Date.now();\n      return active.streams[consumerIndex];\n    }\n    \n    // Try resurrection with validation\n    if (this.config.enableResurrection) {\n      const result = await this.resurrectStreamSafe(streamId, consumerIndex);\n      \n      if (result.success && result.stream) {\n        // Notify user if degraded\n        if (result.degraded && result.warning) {\n          this.notifyDegradedResurrection(streamId, result.warning);\n        }\n        return result.stream;\n      }\n    }\n    \n    throw new Error(`Stream not found and could not be resurrected: ${streamId}`);\n  }\n  \n  private async resurrectStreamSafe(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ResurrectionResult> {\n    \n    const checkpoint = this.checkpoints.get(streamId);\n    if (!checkpoint) {\n      return { success: false };\n    }\n    \n    if (checkpoint.resurrectionAttempts >= this.config.maxResurrectionAttempts) {\n      console.error(`Max resurrection attempts reached for stream ${streamId}`);\n      return { success: false };\n    }\n    \n    checkpoint.resurrectionAttempts++;\n    \n    // CRITICAL: Validate checkpoint freshness\n    const validation = await this.validateCheckpointFreshness(checkpoint);\n    \n    if (validation.stale) {\n      checkpoint.staleness = validation.reason;\n      \n      switch (validation.severity) {\n        case 'critical':\n          // Node deleted or inputs fundamentally changed\n          return {\n            success: false,\n            warning: `Cannot resurrect: ${validation.message}`\n          };\n          \n        case 'major':\n          // Config changed significantly - try regeneration or fail gracefully\n          if (checkpoint.materializationType === 'regenerate') {\n            // Config changed but we can try regenerating with old config\n            return await this.resurrectWithDegradation(checkpoint, {\n              strategy: 'regenerate-old-config',\n              warning: validation.message\n            });\n          } else {\n            // Use cached data with warning\n            return await this.resurrectWithDegradation(checkpoint, {\n              strategy: 'use-cached-with-warning',\n              warning: validation.message\n            });\n          }\n          \n        case 'minor':\n          // Minor changes - proceed with warning\n          const result = await this.resurrectStream(checkpoint);\n          return {\n            ...result,\n            degraded: true,\n            warning: validation.message\n          };\n          \n        default:\n          // Fresh - proceed normally\n          return await this.resurrectStream(checkpoint);\n      }\n    }\n    \n    return await this.resurrectStream(checkpoint);\n  }\n  \n  private async validateCheckpointFreshness(\n    checkpoint: StreamCheckpoint\n  ): Promise<{\n    stale: boolean;\n    reason: StreamCheckpoint['staleness'];\n    severity: 'none' | 'minor' | 'major' | 'critical';\n    message?: string;\n  }> {\n    \n    // Check if node still exists\n    const currentNode = this.workflowState.getNode(checkpoint.sourceNodeId);\n    if (!currentNode) {\n      return {\n        stale: true,\n        reason: 'node-deleted',\n        severity: 'critical',\n        message: 'Source node no longer exists in workflow'\n      };\n    }\n    \n    // Check if node config changed\n    const currentConfigHash = this.hashConfig(currentNode.config);\n    if (currentConfigHash !== checkpoint.nodeConfigVersion) {\n      // Determine severity of config changes\n      const diff = this.diffConfigs(checkpoint.nodeConfigSnapshot, currentNode.config);\n      \n      if (diff.breaking) {\n        return {\n          stale: true,\n          reason: 'config-changed',\n          severity: 'major',\n          message: `Node configuration changed: ${diff.summary}`\n        };\n      } else {\n        return {\n          stale: true,\n          reason: 'config-changed',\n          severity: 'minor',\n          message: `Minor node configuration changes detected`\n        };\n      }\n    }\n    \n    // Check if inputs changed (for regenerate strategy)\n    if (checkpoint.regenerationConfig) {\n      const currentInputs = this.workflowState.getNodeInputs(checkpoint.sourceNodeId);\n      const currentInputHashes = this.hashInputs(currentInputs);\n      \n      const inputsChanged = Object.keys(checkpoint.regenerationConfig.inputHashes).some(\n        key => currentInputHashes[key] !== checkpoint.regenerationConfig!.inputHashes[key]\n      );\n      \n      if (inputsChanged) {\n        return {\n          stale: true,\n          reason: 'inputs-changed',\n          severity: 'major',\n          message: 'Input data has changed since checkpoint'\n        };\n      }\n    }\n    \n    return {\n      stale: false,\n      reason: 'fresh',\n      severity: 'none'\n    };\n  }\n  \n  private async resurrectWithDegradation(\n    checkpoint: StreamCheckpoint,\n    options: {\n      strategy: 'regenerate-old-config' | 'use-cached-with-warning' | 'empty-stream';\n      warning: string;\n    }\n  ): Promise<ResurrectionResult> {\n    \n    switch (options.strategy) {\n      case 'regenerate-old-config':\n        try {\n          // Attempt regeneration using OLD config from checkpoint\n          const stream = await this.resurrectViaRegeneration(checkpoint, {\n            useSnapshotConfig: true\n          });\n          \n          return {\n            success: true,\n            stream,\n            degraded: true,\n            warning: `${options.warning}. Using previous configuration.`,\n            fallbackUsed: 'regenerated'\n          };\n        } catch (error: any) {\n          // Fall back to cached data\n          return await this.resurrectWithDegradation(checkpoint, {\n            strategy: 'use-cached-with-warning',\n            warning: options.warning\n          });\n        }\n        \n      case 'use-cached-with-warning':\n        if (checkpoint.materializationType !== 'regenerate' && checkpoint.materializedData) {\n          const stream = await this.resurrectFromFullMaterialization(checkpoint);\n          \n          return {\n            success: true,\n            stream,\n            degraded: true,\n            warning: `${options.warning}. Using cached data from previous run.`,\n            fallbackUsed: 'cached'\n          };\n        }\n        \n        // No cached data available - return empty stream\n        return await this.resurrectWithDegradation(checkpoint, {\n          strategy: 'empty-stream',\n          warning: options.warning\n        });\n        \n      case 'empty-stream':\n        const emptyStream = new ReadableStream({\n          start(controller) {\n            controller.enqueue({\n              $error: true,\n              $resurrectionFailed: true,\n              message: options.warning\n            });\n            controller.close();\n          }\n        });\n        \n        return {\n          success: true,\n          stream: emptyStream,\n          degraded: true,\n          warning: `${options.warning}. Returning empty stream.`,\n          fallbackUsed: 'empty'\n        };\n    }\n  }\n  \n  private async resurrectStream(\n    checkpoint: StreamCheckpoint\n  ): Promise<ResurrectionResult> {\n    \n    try {\n      let stream: ReadableStream;\n      \n      switch (checkpoint.materializationType) {\n        case 'full':\n          stream = await this.resurrectFromFullMaterialization(checkpoint);\n          break;\n          \n        case 'partial':\n          stream = await this.resurrectFromPartialMaterialization(checkpoint);\n          break;\n          \n        case 'regenerate':\n          stream = await this.resurrectViaRegeneration(checkpoint, {\n            useSnapshotConfig: false\n          });\n          break;\n          \n        default:\n          throw new Error(`Unknown materialization type: ${checkpoint.materializationType}`);\n      }\n      \n      checkpoint.status = 'resurrected';\n      \n      return { success: true, stream };\n      \n    } catch (error: any) {\n      console.error(`Stream resurrection failed: ${error.message}`);\n      return { success: false };\n    }\n  }\n  \n  private async resurrectFromFullMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const materialized = await this.storage.retrieve(storageKey);\n    \n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(materialized)) {\n          for (const chunk of materialized) {\n            controller.enqueue(chunk);\n          }\n        } else {\n          controller.enqueue(materialized);\n        }\n        controller.close();\n      }\n    });\n    \n    return stream;\n  }\n  \n  private async resurrectFromPartialMaterialization(\n    checkpoint: StreamCheckpoint\n  ): Promise<ReadableStream> {\n    \n    const storageKey = `stream-checkpoint-${checkpoint.streamId}`;\n    const partialData = await this.storage.retrieve(storageKey);\n    \n    const stream = new ReadableStream({\n      start(controller) {\n        if (Array.isArray(partialData)) {\n          for (const chunk of partialData) {\n            controller.enqueue(chunk);\n          }\n        }\n        \n        controller.enqueue({\n          $incomplete: true,\n          offset: checkpoint.partialOffset,\n          message: 'Stream was interrupted. Partial data only.'\n        });\n        \n        controller.close();\n      }\n    });\n    \n    return stream;\n  }\n  \n  private async resurrectViaRegeneration(\n    checkpoint: StreamCheckpoint,\n    options: {\n      useSnapshotConfig: boolean;\n    }\n  ): Promise<ReadableStream> {\n    \n    if (!checkpoint.regenerationConfig) {\n      throw new Error('No regeneration config available');\n    }\n    \n    const config = options.useSnapshotConfig\n      ? checkpoint.regenerationConfig.nodeConfig\n      : this.workflowState.getNode(checkpoint.sourceNodeId)?.config;\n    \n    if (!config) {\n      throw new Error('Node config not available for regeneration');\n    }\n    \n    const inputs = checkpoint.regenerationConfig.inputs;\n    \n    const regenerated = await this.executeNodeForRegeneration(\n      checkpoint.sourceNodeId,\n      config,\n      inputs\n    );\n    \n    return regenerated;\n  }\n  \n  private async executeNodeForRegeneration(\n    nodeId: string,\n    nodeConfig: any,\n    inputs: Record<string, any>\n  ): Promise<ReadableStream> {\n    \n    if (!this.regenerationExecutor) {\n      throw new Error(\n        'Regeneration requires executor callback. ' +\n        'Set ResilientStreamManager.regenerationExecutor = (node, inputs) => stream'\n      );\n    }\n    \n    return await this.regenerationExecutor(nodeId, nodeConfig, inputs);\n  }\n  \n  private hashConfig(config: any): string {\n    const serialized = JSON.stringify(config, Object.keys(config).sort());\n    return this.simpleHash(serialized);\n  }\n  \n  private hashInputs(inputs: Record<string, any>): Record<string, string> {\n    const hashes: Record<string, string> = {};\n    \n    for (const [key, value] of Object.entries(inputs)) {\n      const serialized = JSON.stringify(value);\n      hashes[key] = this.simpleHash(serialized);\n    }\n    \n    return hashes;\n  }\n  \n  private simpleHash(str: string): string {\n    let hash = 0;\n    for (let i = 0; i < str.length; i++) {\n      const char = str.charCodeAt(i);\n      hash = ((hash << 5) - hash) + char;\n      hash = hash & hash;\n    }\n    return hash.toString(36);\n  }\n  \n  private diffConfigs(oldConfig: any, newConfig: any): {\n    breaking: boolean;\n    summary: string;\n  } {\n    const changes: string[] = [];\n    let breaking = false;\n    \n    // Critical fields that indicate breaking changes\n    const criticalFields = ['type', 'operation', 'model', 'endpoint'];\n    \n    for (const field of criticalFields) {\n      if (oldConfig[field] !== newConfig[field]) {\n        breaking = true;\n        changes.push(`${field} changed`);\n      }\n    }\n    \n    // Check all other fields\n    const allFields = new Set([\n      ...Object.keys(oldConfig),\n      ...Object.keys(newConfig)\n    ]);\n    \n    for (const field of allFields) {\n      if (!criticalFields.includes(field) && oldConfig[field] !== newConfig[field]) {\n        changes.push(field);\n      }\n    }\n    \n    return {\n      breaking,\n      summary: changes.length > 0 ? changes.join(', ') : 'no changes'\n    };\n  }\n  \n  private notifyDegradedResurrection(streamId: string, warning: string): void {\n    // Emit event for UI to show warning banner\n    window.dispatchEvent(new CustomEvent('stream-resurrection-degraded', {\n      detail: { streamId, warning }\n    }));\n  }\n  \n  private startStalenessDetector(): void {\n    setInterval(() => {\n      for (const [streamId, checkpoint] of this.checkpoints.entries()) {\n        if (checkpoint.status === 'checkpointed') {\n          this.validateCheckpointFreshness(checkpoint).then(validation => {\n            if (validation.stale && validation.severity === 'critical') {\n              // Mark as expired\n              checkpoint.status = 'expired';\n              checkpoint.staleness = validation.reason;\n            }\n          });\n        }\n      }\n    }, 60000);  // Check every minute\n  }\n  \n  private async checkpointStream(streamId: string): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    const checkpoint = entry.checkpoint;\n    \n    switch (checkpoint.materializationType) {\n      case 'full':\n        if (entry.materializedChunks && entry.materializedChunks.length > 0) {\n          const storageKey = `stream-checkpoint-${streamId}`;\n          checkpoint.materializedData = [...entry.materializedChunks];\n          \n          await this.storage.store(storageKey, checkpoint.materializedData, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'partial':\n        if (entry.materializedChunks) {\n          const keepCount = 10;\n          const partial = entry.materializedChunks.slice(-keepCount);\n          \n          const storageKey = `stream-checkpoint-${streamId}`;\n          await this.storage.store(storageKey, partial, {\n            temporary: false,\n            pinned: true,\n            nodeId: checkpoint.sourceNodeId,\n            dataType: 'materialized'\n          });\n          \n          checkpoint.partialOffset = entry.materializedChunks.length - keepCount;\n          checkpoint.status = 'checkpointed';\n          checkpoint.lastCheckpointAt = Date.now();\n        }\n        break;\n        \n      case 'regenerate':\n        checkpoint.status = 'checkpointed';\n        checkpoint.lastCheckpointAt = Date.now();\n        break;\n    }\n    \n    await this.persistCheckpoint(checkpoint);\n  }\n  \n  private async persistCheckpoint(checkpoint: StreamCheckpoint): Promise<void> {\n    const key = `stream-checkpoint-meta-${checkpoint.streamId}`;\n    sessionStorage.setItem(key, JSON.stringify(checkpoint));\n  }\n  \n  private async restoreCheckpointsFromStorage(): Promise<void> {\n    const keys = Object.keys(sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('stream-checkpoint-meta-')) {\n        const checkpointData = sessionStorage.getItem(key);\n        if (checkpointData) {\n          const checkpoint: StreamCheckpoint = JSON.parse(checkpointData);\n          this.checkpoints.set(checkpoint.streamId, checkpoint);\n        }\n      }\n    }\n  }\n  \n  private createMultiTee(stream: ReadableStream, count: number): ReadableStream[] {\n    if (count === 1) return [stream];\n    if (count === 2) return stream.tee();\n    \n    const streams: ReadableStream[] = [];\n    let remaining = stream;\n    \n    for (let i = 0; i < count - 1; i++) {\n      const [stream1, stream2] = remaining.tee();\n      streams.push(stream1);\n      remaining = stream2;\n    }\n    streams.push(remaining);\n    \n    return streams;\n  }\n  \n  private startAutoCheckpointing(): void {\n    setInterval(() => {\n      this.checkpointLongLivedStreams();\n    }, this.config.autoCheckpointInterval);\n  }\n  \n  private async checkpointLongLivedStreams(): Promise<void> {\n    const now = Date.now();\n    const threshold = this.config.checkpointThresholdAge;\n    \n    for (const [streamId, entry] of this.activeStreams.entries()) {\n      const age = now - entry.checkpoint.createdAt;\n      \n      if (age > threshold && entry.checkpoint.status === 'active') {\n        await this.checkpointStream(streamId);\n      }\n    }\n  }\n  \n  private startTimeoutMonitor(): void {\n    setInterval(() => {\n      const now = Date.now();\n      const timeout = this.config.streamTimeout;\n      \n      for (const [streamId, entry] of this.activeStreams.entries()) {\n        if (now - entry.checkpoint.lastCheckpointAt > timeout) {\n          console.warn(`Stream timeout: ${streamId}`);\n          this.cleanup(streamId, 'timeout');\n        }\n      }\n    }, 60000);\n  }\n  \n  async acquireReader(\n    streamId: string, \n    consumerIndex: number\n  ): Promise<ReadableStreamDefaultReader> {\n    const stream = await this.getStream(streamId, consumerIndex);\n    const reader = stream.getReader();\n    \n    const entry = this.activeStreams.get(streamId);\n    if (entry) {\n      entry.readers.add(reader);\n      \n      if (entry.materializedChunks) {\n        return this.wrapReaderForCheckpointing(reader, entry.materializedChunks);\n      }\n    }\n    \n    return reader;\n  }\n  \n  private wrapReaderForCheckpointing(\n    reader: ReadableStreamDefaultReader,\n    chunksBuffer: any[]\n  ): ReadableStreamDefaultReader {\n    \n    const originalRead = reader.read.bind(reader);\n    \n    reader.read = async () => {\n      const result = await originalRead();\n      \n      if (!result.done && result.value !== undefined) {\n        chunksBuffer.push(result.value);\n      }\n      \n      return result;\n    };\n    \n    return reader;\n  }\n  \n  async releaseReader(\n    streamId: string, \n    reader: ReadableStreamDefaultReader\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    try {\n      reader.releaseLock();\n    } catch {\n      // Already released\n    }\n    \n    entry.readers.delete(reader);\n    \n    if (entry.readers.size === 0) {\n      await this.cleanup(streamId, 'completed');\n    }\n  }\n  \n  private async cleanup(\n    streamId: string, \n    reason: 'completed' | 'error' | 'timeout'\n  ): Promise<void> {\n    const entry = this.activeStreams.get(streamId);\n    if (!entry) return;\n    \n    if (reason === 'completed' && entry.checkpoint.materializationType !== 'regenerate') {\n      await this.checkpointStream(streamId);\n    }\n    \n    entry.checkpoint.status = 'expired';\n    \n    for (const stream of entry.streams) {\n      try {\n        const reader = stream.getReader();\n        await reader.cancel(`Cleanup: ${reason}`);\n        reader.releaseLock();\n      } catch {\n        // Ignore\n      }\n    }\n    \n    this.activeStreams.delete(streamId);\n  }\n  \n  getCheckpoint(streamId: string): StreamCheckpoint | null {\n    return this.checkpoints.get(streamId) ?? null;\n  }\n  \n  listActiveStreams(): StreamCheckpoint[] {\n    return Array.from(this.activeStreams.values())\n      .map(entry => entry.checkpoint);\n  }\n  \n  regenerationExecutor?: (\n    nodeId: string, \n    nodeConfig: any, \n    inputs: Record<string, any>\n  ) => Promise<ReadableStream>;\n}\n\ninterface ActiveStreamEntry {\n  checkpoint: StreamCheckpoint;\n  streams: ReadableStream[];\n  readers: Set<ReadableStreamDefaultReader>;\n  materializedChunks?: any[];\n}\n\ninterface WorkflowStateProvider {\n  getNode(nodeId: string): { config: any } | null;\n  getNodeInputs(nodeId: string): Record<string, any>;\n}\n\ninterface StreamManagerConfig {\n  maxConcurrentStreams: number;\n  streamTimeout: number;\n  autoCheckpointInterval: number;\n  checkpointThresholdAge: number;\n  enableResurrection: boolean;\n  maxResurrectionAttempts: number;\n}\n```\n\n## Enhanced Tiered Storage with Circular Buffer SessionStorage\n\n### Key Enhancement: Circular Buffer Pattern + Compression for SessionStorage\n\n```typescript\ninterface SessionStorageManager {\n  maxSizeBytes: number;              // 4MB (\u7559\u51fa1MB\u7f13\u51b2)\n  currentSizeBytes: number;\n  entries: Map<string, SessionEntry>;\n  compressionThreshold: number;      // 10KB\n}\n\ninterface SessionEntry {\n  key: string;\n  sizeBytes: number;\n  priority: number;                  // Higher = more important\n  createdAt: number;\n  lastAccessedAt: number;\n  compressed: boolean;\n}\n\nclass HybridStorageManager {\n  private memoryCache: Map<string, CacheEntry> = new Map();\n  private memoryCacheSize: number = 0;\n  private indexedDB: IDBDatabase;\n  private indexedDBSize: number = 0;\n  \n  private workflowPolicies: Map<string, WorkflowStoragePolicy> = new Map();\n  private globalPolicy: WorkflowStoragePolicy;\n  \n  private artifactIndex: Map<string, ArtifactMetadata> = new Map();\n  private externalBlobs: Map<string, string> = new Map();\n  \n  // Enhanced SessionStorage manager\n  private sessionManager: SessionStorageManager = {\n    maxSizeBytes: 4 * 1024 * 1024,  // 4MB\n    currentSizeBytes: 0,\n    entries: new Map(),\n    compressionThreshold: 10 * 1024  // 10KB\n  };\n  \n  constructor(globalPolicy: WorkflowStoragePolicy) {\n    this.globalPolicy = globalPolicy;\n    this.initIndexedDB();\n    this.restoreFromSession();\n    this.calculateSessionStorageSize();\n  }\n  \n  async store(\n    key: string, \n    data: any, \n    hints: StorageHints = {}\n  ): Promise<void> {\n    \n    const policy = this.getPolicy(hints.workflowId);\n    const serialized = JSON.stringify(data);\n    const sizeBytes = new TextEncoder().encode(serialized).length;\n    \n    const metadata: ArtifactMetadata = {\n      key,\n      sizeBytes,\n      tier: 'determining',\n      createdAt: Date.now(),\n      lastAccessedAt: Date.now(),\n      accessCount: 0,\n      temporary: hints.temporary ?? true,\n      pinned: hints.pinned ?? false,\n      nodeId: hints.nodeId,\n      executionId: hints.executionId,\n      workflowId: hints.workflowId,\n      dataType: hints.dataType ?? 'json'\n    };\n    \n    this.artifactIndex.set(key, metadata);\n    \n    // Enhanced SessionStorage logic with circular buffer\n    if (hints.useSessionStorage) {\n      await this.storeInSessionStorage(\n        key, \n        serialized, \n        sizeBytes,\n        hints.priority ?? 5\n      );\n      metadata.tier = 'session';\n      return;\n    }\n    \n    // Memory tier\n    if (sizeBytes < 1024 * 1024) {\n      if (await this.tryStoreInMemory(key, data, sizeBytes, policy)) {\n        metadata.tier = 'memory';\n        return;\n      }\n    }\n    \n    // IndexedDB tier\n    await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n    metadata.tier = 'indexeddb';\n  }\n  \n  private async storeInSessionStorage(\n    key: string,\n    serialized: string,\n    sizeBytes: number,\n    priority: number\n  ): Promise<void> {\n    \n    // Try compression first if size > threshold\n    let toStore = serialized;\n    let compressed = false;\n    let actualSize = sizeBytes;\n    \n    if (sizeBytes > this.sessionManager.compressionThreshold) {\n      try {\n        toStore = await this.compressForSession(serialized);\n        actualSize = new TextEncoder().encode(toStore).length;\n        compressed = true;\n      } catch (error) {\n        console.warn('Compression failed, using original', error);\n      }\n    }\n    \n    // Check if we need to evict\n    while (this.sessionManager.currentSizeBytes + actualSize > this.sessionManager.maxSizeBytes) {\n      const evicted = this.evictFromSessionStorage(priority);\n      if (!evicted) {\n        // Cannot evict - move to IndexedDB instead\n        console.warn('SessionStorage full, moving to IndexedDB');\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'indexeddb';\n        }\n        \n        await this.storeInIndexedDB(key, serialized, sizeBytes, this.globalPolicy);\n        return;\n      }\n    }\n    \n    // Store in SessionStorage\n    const storageKey = `app-storage-${key}`;\n    const envelope = {\n      data: toStore,\n      compressed,\n      sizeBytes: actualSize,\n      priority,\n      createdAt: Date.now()\n    };\n    \n    try {\n      sessionStorage.setItem(storageKey, JSON.stringify(envelope));\n      \n      this.sessionManager.entries.set(key, {\n        key: storageKey,\n        sizeBytes: actualSize,\n        priority,\n        createdAt: Date.now(),\n        lastAccessedAt: Date.now(),\n        compressed\n      });\n      \n      this.sessionManager.currentSizeBytes += actualSize;\n      \n    } catch (error: any) {\n      // Quota exceeded despite our calculations - force evict and retry\n      console.error('SessionStorage quota exceeded:', error);\n      \n      this.evictFromSessionStorage(priority, true);\n      \n      // Move to IndexedDB\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n      }\n      \n      await this.storeInIndexedDB(key, serialized, sizeBytes, this.globalPolicy);\n    }\n  }\n  \n  private evictFromSessionStorage(\n    incomingPriority: number,\n    force: boolean = false\n  ): boolean {\n    \n    const candidates = Array.from(this.sessionManager.entries.values())\n      .filter(entry => force || entry.priority < incomingPriority)\n      .sort((a, b) => {\n        // Sort by priority (ascending), then LRU\n        if (a.priority !== b.priority) {\n          return a.priority - b.priority;\n        }\n        return a.lastAccessedAt - b.lastAccessedAt;\n      });\n    \n    if (candidates.length === 0) {\n      return false;\n    }\n    \n    const toEvict = candidates[0];\n    \n    try {\n      sessionStorage.removeItem(toEvict.key);\n      \n      const artifactKey = toEvict.key.replace('app-storage-', '');\n      this.sessionManager.entries.delete(artifactKey);\n      this.sessionManager.currentSizeBytes -= toEvict.sizeBytes;\n      \n      // Update metadata\n      const metadata = this.artifactIndex.get(artifactKey);\n      if (metadata) {\n        metadata.tier = 'evicted';\n      }\n      \n      return true;\n      \n    } catch (error) {\n      console.error('Failed to evict from SessionStorage:', error);\n      return false;\n    }\n  }\n  \n  async retrieve(key: string): Promise<any> {\n    const metadata = this.artifactIndex.get(key);\n    if (!metadata) {\n      throw new Error(`Artifact not found: ${key}`);\n    }\n    \n    metadata.accessCount++;\n    metadata.lastAccessedAt = Date.now();\n    \n    switch (metadata.tier) {\n      case 'memory':\n        return this.memoryCache.get(key)?.data;\n        \n      case 'indexeddb':\n        return await this.retrieveFromIndexedDB(key);\n        \n      case 'session':\n        return await this.retrieveFromSessionStorage(key);\n        \n      case 'external':\n        return await this.retrieveFromExternalBlob(key);\n        \n      default:\n        throw new Error(`Unknown tier: ${metadata.tier}`);\n    }\n  }\n  \n  private async retrieveFromSessionStorage(key: string): Promise<any> {\n    const storageKey = `app-storage-${key}`;\n    const envelopeStr = sessionStorage.getItem(storageKey);\n    \n    if (!envelopeStr) {\n      throw new Error(`Artifact not in SessionStorage: ${key}`);\n    }\n    \n    const envelope = JSON.parse(envelopeStr);\n    \n    // Update access time\n    const entry = this.sessionManager.entries.get(key);\n    if (entry) {\n      entry.lastAccessedAt = Date.now();\n    }\n    \n    // Decompress if needed\n    const serialized = envelope.compressed\n      ? await this.decompressFromSession(envelope.data)\n      : envelope.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async compressForSession(data: string): Promise<string> {\n    // Use LZ-based compression (smaller implementation)\n    // This is a simple placeholder - real implementation would use pako or similar\n    const encoder = new TextEncoder();\n    const uint8 = encoder.encode(data);\n    \n    const stream = new Blob([uint8]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const compressedUint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...compressedUint8));\n  }\n  \n  private async decompressFromSession(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private calculateSessionStorageSize(): void {\n    let totalSize = 0;\n    \n    for (const key of Object.keys(sessionStorage)) {\n      if (key.startsWith('app-storage-')) {\n        const value = sessionStorage.getItem(key);\n        if (value) {\n          totalSize += new TextEncoder().encode(value).length;\n        }\n      }\n    }\n    \n    this.sessionManager.currentSizeBytes = totalSize;\n  }\n  \n  getSessionStorageStats(): {\n    usedBytes: number;\n    maxBytes: number;\n    utilization: number;\n    entriesCount: number;\n  } {\n    return {\n      usedBytes: this.sessionManager.currentSizeBytes,\n      maxBytes: this.sessionManager.maxSizeBytes,\n      utilization: this.sessionManager.currentSizeBytes / this.sessionManager.maxSizeBytes,\n      entriesCount: this.sessionManager.entries.size\n    };\n  }\n  \n  setWorkflowPolicy(workflowId: string, policy: WorkflowStoragePolicy): void {\n    this.workflowPolicies.set(workflowId, policy);\n  }\n  \n  getPolicy(workflowId?: string): WorkflowStoragePolicy {\n    if (workflowId) {\n      return this.workflowPolicies.get(workflowId) ?? this.globalPolicy;\n    }\n    return this.globalPolicy;\n  }\n  \n  private async tryStoreInMemory(\n    key: string, \n    data: any, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    if (this.memoryCacheSize + sizeBytes <= policy.memoryCacheMaxBytes) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    if (await this.evictFromMemory(sizeBytes, policy)) {\n      this.memoryCache.set(key, {\n        data,\n        sizeBytes,\n        storedAt: Date.now()\n      });\n      this.memoryCacheSize += sizeBytes;\n      return true;\n    }\n    \n    return false;\n  }\n  \n  private async evictFromMemory(\n    requiredBytes: number, \n    policy: WorkflowStoragePolicy\n  ): Promise<boolean> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'memory' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        (!policy.evictionPreferences.protectRecent || \n         Date.now() - m.createdAt > 5000)\n      );\n    \n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toEvict: string[] = [];\n    \n    for (const metadata of candidates) {\n      toEvict.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (freedBytes < requiredBytes) return false;\n    \n    for (const key of toEvict) {\n      const entry = this.memoryCache.get(key)!;\n      await this.storeInIndexedDB(key, JSON.stringify(entry.data), entry.sizeBytes, policy);\n      \n      this.memoryCache.delete(key);\n      this.memoryCacheSize -= entry.sizeBytes;\n      \n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        metadata.tier = 'indexeddb';\n      }\n    }\n    \n    return true;\n  }\n  \n  private async storeInIndexedDB(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    if (this.indexedDBSize + sizeBytes > policy.indexedDBMaxBytes) {\n      await this.handleIndexedDBOverflow(key, serialized, sizeBytes, policy);\n      return;\n    }\n    \n    const shouldCompress = sizeBytes > 1024 * 1024;\n    const toStore = shouldCompress \n      ? await this.compress(serialized)\n      : serialized;\n    \n    const actualSize = shouldCompress \n      ? new TextEncoder().encode(toStore).length \n      : sizeBytes;\n    \n    const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n    await tx.objectStore('artifacts').put({\n      key,\n      data: toStore,\n      compressed: shouldCompress,\n      sizeBytes: actualSize,\n      storedAt: Date.now()\n    });\n    \n    this.indexedDBSize += actualSize;\n  }\n  \n  private async handleIndexedDBOverflow(\n    key: string, \n    serialized: string, \n    sizeBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    switch (policy.overflowPolicy) {\n      case 'evict-lru':\n      case 'evict-oldest':\n        await this.evictFromIndexedDB(sizeBytes, policy);\n        await this.storeInIndexedDB(key, serialized, sizeBytes, policy);\n        break;\n        \n      case 'fail':\n        throw new StorageFullError(\n          `Storage full for workflow: ${policy.workflowId}`,\n          this.getStorageStats()\n        );\n        \n      case 'compress':\n        const compressed = await this.compress(serialized);\n        const compressedSize = new TextEncoder().encode(compressed).length;\n        \n        if (this.indexedDBSize + compressedSize > policy.indexedDBMaxBytes) {\n          await this.evictFromIndexedDB(compressedSize, policy);\n        }\n        \n        const tx = this.indexedDB.transaction('artifacts', 'readwrite');\n        await tx.objectStore('artifacts').put({\n          key,\n          data: compressed,\n          compressed: true,\n          sizeBytes: compressedSize,\n          storedAt: Date.now()\n        });\n        \n        this.indexedDBSize += compressedSize;\n        break;\n        \n      case 'external':\n        const blob = new Blob([await this.compress(serialized)]);\n        const blobUrl = URL.createObjectURL(blob);\n        \n        this.externalBlobs.set(key, blobUrl);\n        \n        const tx2 = this.indexedDB.transaction('blobRefs', 'readwrite');\n        await tx2.objectStore('blobRefs').put({\n          key,\n          blobUrl,\n          sizeBytes,\n          storedAt: Date.now()\n        });\n        \n        const metadata = this.artifactIndex.get(key);\n        if (metadata) {\n          metadata.tier = 'external';\n        }\n        break;\n    }\n  }\n  \n  private async evictFromIndexedDB(\n    requiredBytes: number,\n    policy: WorkflowStoragePolicy\n  ): Promise<void> {\n    \n    const candidates = Array.from(this.artifactIndex.values())\n      .filter(m => \n        m.tier === 'indexeddb' &&\n        (!policy.evictionPreferences.protectPinned || !m.pinned) &&\n        m.workflowId === policy.workflowId\n      );\n    \n    candidates.sort((a, b) => {\n      if (policy.evictionPreferences.protectFrequent) {\n        if (a.accessCount !== b.accessCount) {\n          return a.accessCount - b.accessCount;\n        }\n      }\n      \n      if (policy.overflowPolicy === 'evict-oldest') {\n        return a.createdAt - b.createdAt;\n      } else {\n        return a.lastAccessedAt - b.lastAccessedAt;\n      }\n    });\n    \n    let freedBytes = 0;\n    const toDelete: string[] = [];\n    \n    for (const metadata of candidates) {\n      toDelete.push(metadata.key);\n      freedBytes += metadata.sizeBytes;\n      \n      if (freedBytes >= requiredBytes) break;\n    }\n    \n    if (toDelete.length === 0) {\n      throw new Error('Cannot evict: all items protected or wrong workflow');\n    }\n    \n    const txDelete = this.indexedDB.transaction('artifacts', 'readwrite');\n    const storeDelete = txDelete.objectStore('artifacts');\n    \n    for (const key of toDelete) {\n      await storeDelete.delete(key);\n      const metadata = this.artifactIndex.get(key);\n      if (metadata) {\n        this.indexedDBSize -= metadata.sizeBytes;\n      }\n      this.artifactIndex.delete(key);\n    }\n  }\n  \n  private async retrieveFromIndexedDB(key: string): Promise<any> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const record = await tx.objectStore('artifacts').get(key);\n    \n    if (!record) {\n      throw new Error(`Artifact not in IndexedDB: ${key}`);\n    }\n    \n    const serialized = record.compressed\n      ? await this.decompress(record.data)\n      : record.data;\n    \n    return JSON.parse(serialized);\n  }\n  \n  private async retrieveFromExternalBlob(key: string): Promise<any> {\n    const blobUrl = this.externalBlobs.get(key);\n    if (!blobUrl) {\n      throw new Error(`External blob not found: ${key}`);\n    }\n    \n    const response = await fetch(blobUrl);\n    const compressed = await response.text();\n    const serialized = await this.decompress(compressed);\n    return JSON.parse(serialized);\n  }\n  \n  async pin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = true;\n    }\n  }\n  \n  async unpin(key: string): Promise<void> {\n    const metadata = this.artifactIndex.get(key);\n    if (metadata) {\n      metadata.pinned = false;\n    }\n  }\n  \n  private async compress(data: string): Promise<string> {\n    const stream = new Blob([data]).stream();\n    const compressed = stream.pipeThrough(new CompressionStream('gzip'));\n    const buffer = await new Response(compressed).arrayBuffer();\n    \n    const uint8 = new Uint8Array(buffer);\n    return btoa(String.fromCharCode(...uint8));\n  }\n  \n  private async decompress(data: string): Promise<string> {\n    const binary = atob(data);\n    const uint8 = new Uint8Array(binary.length);\n    for (let i = 0; i < binary.length; i++) {\n      uint8[i] = binary.charCodeAt(i);\n    }\n    \n    const stream = new Blob([uint8]).stream();\n    const decompressed = stream.pipeThrough(new DecompressionStream('gzip'));\n    return await new Response(decompressed).text();\n  }\n  \n  private async initIndexedDB(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('WorkflowStorage', 4);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.indexedDB = request.result;\n        this.calculateIndexedDBSize();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('artifacts')) {\n          const store = db.createObjectStore('artifacts', { keyPath: 'key' });\n          store.createIndex('storedAt', 'storedAt', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n        }\n        \n        if (!db.objectStoreNames.contains('blobRefs')) {\n          db.createObjectStore('blobRefs', { keyPath: 'key' });\n        }\n      };\n    });\n  }\n  \n  private async calculateIndexedDBSize(): Promise<void> {\n    const tx = this.indexedDB.transaction('artifacts', 'readonly');\n    const store = tx.objectStore('artifacts');\n    let cursor = await store.openCursor();\n    \n    let totalSize = 0;\n    while (cursor) {\n      totalSize += cursor.value.sizeBytes || 0;\n      cursor = await cursor.continue();\n    }\n    \n    this.indexedDBSize = totalSize;\n  }\n  \n  private restoreFromSession(): void {\n    const keys = Object.keys(sessionStorage);\n    \n    for (const key of keys) {\n      if (key.startsWith('app-storage-')) {\n        const artifactKey = key.replace('app-storage-', '');\n        const envelopeStr = sessionStorage.getItem(key);\n        \n        if (envelopeStr) {\n          const envelope = JSON.parse(envelopeStr);\n          \n          const metadata: ArtifactMetadata = {\n            key: artifactKey,\n            sizeBytes: envelope.sizeBytes,\n            tier: 'session',\n            createdAt: envelope.createdAt,\n            lastAccessedAt: Date.now(),\n            accessCount: 0,\n            temporary: false,\n            pinned: true,\n            dataType: 'json'\n          };\n          \n          this.artifactIndex.set(artifactKey, metadata);\n          \n          this.sessionManager.entries.set(artifactKey, {\n            key,\n            sizeBytes: envelope.sizeBytes,\n            priority: envelope.priority,\n            createdAt: envelope.createdAt,\n            lastAccessedAt: Date.now(),\n            compressed: envelope.compressed\n          });\n        }\n      }\n    }\n  }\n  \n  getStorageStats(workflowId?: string): StorageStats {\n    const stats = {\n      memory: { count: 0, bytes: this.memoryCacheSize },\n      indexeddb: { count: 0, bytes: this.indexedDBSize },\n      session: { count: 0, bytes: this.sessionManager.currentSizeBytes },\n      external: { count: 0, bytes: 0 },\n      total: { count: this.artifactIndex.size, bytes: 0 }\n    };\n    \n    for (const metadata of this.artifactIndex.values()) {\n      if (workflowId && metadata.workflowId !== workflowId) continue;\n      \n      stats[metadata.tier as keyof typeof stats].count++;\n      stats.total.bytes += metadata.sizeBytes;\n    }\n    \n    return stats;\n  }\n}\n\ninterface CacheEntry {\n  data: any;\n  sizeBytes: number;\n  storedAt: number;\n}\n\ninterface StorageHints {\n  temporary?: boolean;\n  pinned?: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType?: 'stream-ref' | 'materialized' | 'blob' | 'json';\n  useSessionStorage?: boolean;\n  priority?: number;  // 1-10, default 5\n}\n\ninterface ArtifactMetadata {\n  key: string;\n  sizeBytes: number;\n  tier: 'memory' | 'indexeddb' | 'session' | 'external' | 'determining' | 'evicted';\n  createdAt: number;\n  lastAccessedAt: number;\n  accessCount: number;\n  temporary: boolean;\n  pinned: boolean;\n  nodeId?: string;\n  executionId?: string;\n  workflowId?: string;\n  dataType: 'stream-ref' | 'materialized' | 'blob' | 'json';\n}\n\ninterface WorkflowStoragePolicy {\n  workflowId: string;\n  memoryCacheMaxBytes: number;\n  indexedDBMaxBytes: number;\n  overflowPolicy: 'evict-lru' | 'evict-oldest' | 'fail' | 'compress' | 'external';\n  evictionPreferences: {\n    protectRecent: boolean;\n    protectFrequent: boolean;\n    protectPinned: boolean;\n  };\n  autoCleanupAge: number;\n  enableAutoCleanup: boolean;\n}\n\ninterface StorageStats {\n  memory: { count: number; bytes: number };\n  indexeddb: { count: number; bytes: number };\n  session: { count: number; bytes: number };\n  external: { count: number; bytes: number };\n  total: { count: number; bytes: number };\n}\n\nclass StorageFullError extends Error {\n  constructor(message: string, public stats: StorageStats) {\n    super(message);\n    this.name = 'StorageFullError';\n  }\n}\n```\n\n## Simplified Visual Block Editor with Virtual Scrolling\n\n### Key Enhancement: Virtualized Rendering + Block Limit + Performance Budget\n\n```typescript\ninterface HybridExpression {\n  mode: 'hybrid';\n  content: EditorContent[];\n  rawText: string;\n  blockCount: number;  // NEW: Track block count\n}\n\nconst EDITOR_CONFIG = {\n  maxBlocks: 50,              // Hard limit on blocks per expression\n  virtualScrollThreshold: 20,  // Start virtualizing if > 20 blocks\n  performanceBudgetMs: 16     // Target 60fps (16ms per frame)\n};\n\nclass HybridExpressionEditor {\n  private engine: ComposableExpressionEngine;\n  \n  constructor() {\n    this.engine = new ComposableExpressionEngine();\n  }\n  \n  parseFromText(text: string): HybridExpression {\n    const content: EditorContent[] = [];\n    const blockMarkerRegex = /\\{\\{block:([^:}]+):([^}]+)\\}\\}/g;\n    \n    let lastIndex = 0;\n    let blockCount = 0;\n    const matches = text.matchAll(blockMarkerRegex);\n    \n    for (const match of matches) {\n      if (blockCount >= EDITOR_CONFIG.maxBlocks) {\n        console.warn(`Block limit reached (${EDITOR_CONFIG.maxBlocks}). Remaining blocks ignored.`);\n        break;\n      }\n      \n      if (match.index! > lastIndex) {\n        const textPart = text.slice(lastIndex, match.index);\n        if (textPart) {\n          content.push({ type: 'text', text: textPart });\n        }\n      }\n      \n      const [_, blockType, configStr] = match;\n      const config = this.parseBlockConfig(blockType, configStr);\n      const displayText = this.generateDisplayText(blockType, config);\n      \n      content.push({\n        type: 'block',\n        blockType: blockType as any,\n        config,\n        displayText\n      });\n      \n      blockCount++;\n      lastIndex = match.index! + match[0].length;\n    }\n    \n    if (lastIndex < text.length) {\n      content.push({ type: 'text', text: text.slice(lastIndex) });\n    }\n    \n    return {\n      mode: 'hybrid',\n      content,\n      rawText: text,\n      blockCount\n    };\n  }\n  \n  serializeToText(expression: HybridExpression): string {\n    return expression.content.map(node => {\n      if (node.type === 'text') {\n        return node.text;\n      } else {\n        const configStr = this.serializeBlockConfig(node.blockType, node.config);\n        return `{{block:${node.blockType}:${configStr}}}`;\n      }\n    }).join('');\n  }\n  \n  insertBlock(\n    expression: HybridExpression, \n    cursorPosition: number,\n    blockType: string,\n    config: Record<string, any>\n  ): HybridExpression {\n    \n    if (expression.blockCount >= EDITOR_CONFIG.maxBlocks) {\n      throw new Error(`Cannot insert block: limit of ${EDITOR_CONFIG.maxBlocks} blocks reached`);\n    }\n    \n    const displayText = this.generateDisplayText(blockType, config);\n    const blockNode: BlockNode = {\n      type: 'block',\n      blockType: blockType as any,\n      config,\n      displayText\n    };\n    \n    let currentPos = 0;\n    let insertIndex = 0;\n    \n    for (let i = 0; i < expression.content.length; i++) {\n      const node = expression.content[i];\n      const nodeLength = node.type === 'text' \n        ? node.text.length \n        : node.displayText.length;\n      \n      if (currentPos + nodeLength >= cursorPosition) {\n        insertIndex = i + 1;\n        break;\n      }\n      \n      currentPos += nodeLength;\n    }\n    \n    const newContent = [\n      ...expression.content.slice(0, insertIndex),\n      blockNode,\n      ...expression.content.slice(insertIndex)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent }),\n      blockCount: expression.blockCount + 1\n    };\n  }\n  \n  removeBlock(expression: HybridExpression, blockIndex: number): HybridExpression {\n    const newContent = expression.content.filter((_, i) => i !== blockIndex);\n    \n    const blockCount = newContent.filter(n => n.type === 'block').length;\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent }),\n      blockCount\n    };\n  }\n  \n  updateBlock(\n    expression: HybridExpression, \n    blockIndex: number,\n    newConfig: Record<string, any>\n  ): HybridExpression {\n    \n    const node = expression.content[blockIndex];\n    if (node.type !== 'block') {\n      throw new Error('Node is not a block');\n    }\n    \n    const updatedNode: BlockNode = {\n      ...node,\n      config: newConfig,\n      displayText: this.generateDisplayText(node.blockType, newConfig)\n    };\n    \n    const newContent = [\n      ...expression.content.slice(0, blockIndex),\n      updatedNode,\n      ...expression.content.slice(blockIndex + 1)\n    ];\n    \n    return {\n      mode: 'hybrid',\n      content: newContent,\n      rawText: this.serializeToText({ ...expression, content: newContent }),\n      blockCount: expression.blockCount\n    };\n  }\n  \n  private parseBlockConfig(type: string, configStr: string): Record<string, any> {\n    const pairs = configStr.split(':');\n    \n    switch (type) {\n      case 'field':\n        return { path: pairs[0] };\n      case 'filter':\n        return { condition: pairs[0] };\n      case 'aggregate':\n        return { function: pairs[0], path: pairs[1] || '' };\n      case 'map':\n        return { expression: pairs[0] };\n      case 'transform':\n        return { operation: pairs[0] };\n      default:\n        return {};\n    }\n  }\n  \n  private serializeBlockConfig(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return config.path;\n      case 'filter':\n        return config.condition;\n      case 'aggregate':\n        return `${config.function}:${config.path}`;\n      case 'map':\n        return config.expression;\n      case 'transform':\n        return config.operation;\n      default:\n        return '';\n    }\n  }\n  \n  private generateDisplayText(type: string, config: Record<string, any>): string {\n    switch (type) {\n      case 'field':\n        return `$.${config.path}`;\n      case 'filter':\n        return `[${config.condition}]`;\n      case 'aggregate':\n        return `${config.function}(${config.path})`;\n      case 'map':\n        return `map(${config.expression})`;\n      case 'transform':\n        return config.operation;\n      default:\n        return 'block';\n    }\n  }\n}\n\n// React Component with Virtual Scrolling\nconst HybridExpressionEditorUI: React.FC<{\n  value: HybridExpression;\n  onChange: (value: HybridExpression) => void;\n}> = ({ value, onChange }) => {\n  \n  const editor = useMemo(() => new HybridExpressionEditor(), []);\n  const [cursorPosition, setCursorPosition] = useState(0);\n  const [selectedBlockIndex, setSelectedBlockIndex] = useState<number | null>(null);\n  const editorRef = useRef<HTMLDivElement>(null);\n  \n  // Virtualization state\n  const [viewportStart, setViewportStart] = useState(0);\n  const [viewportEnd, setViewportEnd] = useState(100);\n  \n  const shouldVirtualize = value.blockCount > EDITOR_CONFIG.virtualScrollThreshold;\n  \n  const visibleContent = useMemo(() => {\n    if (!shouldVirtualize) {\n      return value.content;\n    }\n    \n    // Only render blocks in viewport\n    return value.content.slice(viewportStart, viewportEnd);\n  }, [value.content, viewportStart, viewportEnd, shouldVirtualize]);\n  \n  const handleInsertBlock = useCallback((blockType: string) => {\n    if (value.blockCount >= EDITOR_CONFIG.maxBlocks) {\n      alert(`Cannot insert block: maximum of ${EDITOR_CONFIG.maxBlocks} blocks reached.`);\n      return;\n    }\n    \n    const defaultConfig = getDefaultBlockConfig(blockType);\n    const updated = editor.insertBlock(value, cursorPosition, blockType, defaultConfig);\n    onChange(updated);\n  }, [value, cursorPosition, editor, onChange]);\n  \n  const handleRemoveBlock = useCallback((blockIndex: number) => {\n    const updated = editor.removeBlock(value, blockIndex);\n    onChange(updated);\n    setSelectedBlockIndex(null);\n  }, [value, editor, onChange]);\n  \n  const handleUpdateBlock = useCallback((blockIndex: number, newConfig: Record<string, any>) => {\n    const updated = editor.updateBlock(value, blockIndex, newConfig);\n    onChange(updated);\n  }, [value, editor, onChange]);\n  \n  const handleTextChange = useCallback((newText: string) => {\n    try {\n      const parsed = editor.parseFromText(newText);\n      onChange(parsed);\n    } catch (error: any) {\n      console.error('Parse error:', error.message);\n    }\n  }, [editor, onChange]);\n  \n  const handleScroll = useCallback((e: React.UIEvent<HTMLDivElement>) => {\n    if (!shouldVirtualize) return;\n    \n    const container = e.currentTarget;\n    const scrollTop = container.scrollTop;\n    const itemHeight = 30;  // Approximate height per block\n    \n    const start = Math.floor(scrollTop / itemHeight);\n    const end = start + Math.ceil(container.clientHeight / itemHeight) + 10;  // Buffer\n    \n    setViewportStart(Math.max(0, start));\n    setViewportEnd(Math.min(value.content.length, end));\n  }, [shouldVirtualize, value.content.length]);\n  \n  return (\n    <div className=\"hybrid-expression-editor\">\n      {/* Block Palette */}\n      <div className=\"block-palette\">\n        <button onClick={() => handleInsertBlock('field')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\ud83d\udcc4</span> Field\n        </button>\n        <button onClick={() => handleInsertBlock('filter')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\ud83d\udd0d</span> Filter\n        </button>\n        <button onClick={() => handleInsertBlock('aggregate')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\u03a3</span> Aggregate\n        </button>\n        <button onClick={() => handleInsertBlock('map')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\ud83d\uddfa\ufe0f</span> Map\n        </button>\n        <button onClick={() => handleInsertBlock('transform')} disabled={value.blockCount >= EDITOR_CONFIG.maxBlocks}>\n          <span className=\"icon\">\u2699\ufe0f</span> Transform\n        </button>\n        \n        <span className=\"block-count\">\n          {value.blockCount} / {EDITOR_CONFIG.maxBlocks} blocks\n        </span>\n      </div>\n      \n      {/* Rich Text Editor */}\n      <div \n        ref={editorRef}\n        className=\"editor-content\"\n        contentEditable\n        suppressContentEditableWarning\n        onScroll={handleScroll}\n        onInput={(e) => {\n          const text = e.currentTarget.textContent || '';\n          handleTextChange(text);\n        }}\n        onSelect={() => {\n          const selection = window.getSelection();\n          if (selection) {\n            setCursorPosition(selection.anchorOffset);\n          }\n        }}\n      >\n        {visibleContent.map((node, index) => {\n          const actualIndex = shouldVirtualize ? viewportStart + index : index;\n          \n          if (node.type === 'text') {\n            return <span key={actualIndex}>{node.text}</span>;\n          } else {\n            return (\n              <BlockComponent\n                key={actualIndex}\n                node={node}\n                selected={selectedBlockIndex === actualIndex}\n                onSelect={() => setSelectedBlockIndex(actualIndex)}\n                onRemove={() => handleRemoveBlock(actualIndex)}\n                onUpdate={(config) => handleUpdateBlock(actualIndex, config)}\n              />\n            );\n          }\n        })}\n      </div>\n      \n      {/* Block Config Panel */}\n      {selectedBlockIndex !== null && value.content[selectedBlockIndex]?.type === 'block' && (\n        <BlockConfigPanel\n          blockNode={value.content[selectedBlockIndex] as BlockNode}\n          onUpdate={(config) => handleUpdateBlock(selectedBlockIndex, config)}\n          onClose={() => setSelectedBlockIndex(null)}\n        />\n      )}\n      \n      {/* Raw Text View */}\n      <details className=\"raw-text-view\">\n        <summary>View Raw Text</summary>\n        <textarea \n          value={value.rawText}\n          onChange={(e) => handleTextChange(e.target.value)}\n          className=\"raw-text-editor\"\n        />\n      </details>\n    </div>\n  );\n};\n\n// Memoized block component for performance\nconst BlockComponent = React.memo<{\n  node: BlockNode;\n  selected: boolean;\n  onSelect: () => void;\n  onRemove: () => void;\n  onUpdate: (config: Record<string, any>) => void;\n}>(({ node, selected, onSelect, onRemove }) => {\n  \n  const getBlockIcon = (type: string) => {\n    switch (type) {\n      case 'field': return '\ud83d\udcc4';\n      case 'filter': return '\ud83d\udd0d';\n      case 'aggregate': return '\u03a3';\n      case 'map': return '\ud83d\uddfa\ufe0f';\n      case 'transform': return '\u2699\ufe0f';\n      default: return '\ud83e\udde9';\n    }\n  };\n  \n  return (\n    <span \n      className={`expression-block ${selected ? 'selected' : ''}`}\n      onClick={onSelect}\n      contentEditable={false}\n    >\n      <span className=\"block-icon\">{getBlockIcon(node.blockType)}</span>\n      <span className=\"block-text\">{node.displayText}</span>\n      <button className=\"block-remove\" onClick={onRemove}>\u00d7</button>\n    </span>\n  );\n});\n\nconst BlockConfigPanel: React.FC<{\n  blockNode: BlockNode;\n  onUpdate: (config: Record<string, any>) => void;\n  onClose: () => void;\n}> = ({ blockNode, onUpdate, onClose }) => {\n  \n  const [config, setConfig] = useState(blockNode.config);\n  \n  const handleSave = () => {\n    onUpdate(config);\n    onClose();\n  };\n  \n  return (\n    <div className=\"block-config-panel\">\n      <div className=\"panel-header\">\n        <h4>Configure {blockNode.blockType}</h4>\n        <button onClick={onClose}>\u00d7</button>\n      </div>\n      \n      <div className=\"panel-body\">\n        {blockNode.blockType === 'field' && (\n          <label>\n            Path:\n            <input \n              type=\"text\"\n              value={config.path || ''}\n              onChange={(e) => setConfig({ ...config, path: e.target.value })}\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'filter' && (\n          <label>\n            Condition:\n            <input \n              type=\"text\"\n              value={config.condition || ''}\n              onChange={(e) => setConfig({ ...config, condition: e.target.value })}\n              placeholder=\"e.g., age > 18\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'aggregate' && (\n          <>\n            <label>\n              Function:\n              <select \n                value={config.function || 'sum'}\n                onChange={(e) => setConfig({ ...config, function: e.target.value })}\n              >\n                <option value=\"sum\">Sum</option>\n                <option value=\"avg\">Average</option>\n                <option value=\"count\">Count</option>\n                <option value=\"min\">Min</option>\n                <option value=\"max\">Max</option>\n              </select>\n            </label>\n            <label>\n              Path:\n              <input \n                type=\"text\"\n                value={config.path || ''}\n                onChange={(e) => setConfig({ ...config, path: e.target.value })}\n              />\n            </label>\n          </>\n        )}\n        \n        {blockNode.blockType === 'map' && (\n          <label>\n            Expression:\n            <input \n              type=\"text\"\n              value={config.expression || ''}\n              onChange={(e) => setConfig({ ...config, expression: e.target.value })}\n              placeholder=\"e.g., name & ' ' & email\"\n            />\n          </label>\n        )}\n        \n        {blockNode.blockType === 'transform' && (\n          <label>\n            Operation:\n            <select \n              value={config.operation || 'uppercase'}\n              onChange={(e) => setConfig({ ...config, operation: e.target.value })}\n            >\n              <option value=\"uppercase\">Uppercase</option>\n              <option value=\"lowercase\">Lowercase</option>\n              <option value=\"trim\">Trim</option>\n              <option value=\"reverse\">Reverse</option>\n            </select>\n          </label>\n        )}\n      </div>\n      \n      <div className=\"panel-footer\">\n        <button onClick={handleSave}>Save</button>\n        <button onClick={onClose}>Cancel</button>\n      </div>\n    </div>\n  );\n};\n\nfunction getDefaultBlockConfig(blockType: string): Record<string, any> {\n  switch (blockType) {\n    case 'field':\n      return { path: '' };\n    case 'filter':\n      return { condition: '' };\n    case 'aggregate':\n      return { function: 'sum', path: '' };\n    case 'map':\n      return { expression: '' };\n    case 'transform':\n      return { operation: 'uppercase' };\n    default:\n      return {};\n  }\n}\n\ntype EditorContent = TextNode | BlockNode;\n\ninterface TextNode {\n  type: 'text';\n  text: string;\n}\n\ninterface BlockNode {\n  type: 'block';\n  blockType: 'field' | 'filter' | 'map' | 'aggregate' | 'transform';\n  config: Record<string, any>;\n  displayText: string;\n}\n```\n\n## Simplified Trust Model + Stratified Sampling for Metrics\n\n### Key Enhancement: Binary Trust + Always-Sample-Errors Strategy\n\n```typescript\n// Simplified to binary trust model\ntype TrustLevel = 'trusted' | 'untrusted';\n\ninterface CompensationHandlerConfig {\n  id: string;\n  name: string;\n  trustLevel: TrustLevel;  // Simplified to binary\n  author?: string;\n  \n  handlerType: 'builtin' | 'http-api' | 'webhook' | 'custom';\n  \n  matches: {\n    operation?: string[];\n    description?: string[];\n    service?: string[];\n  };\n  \n  compensation: \n    | { type: 'http-api'; config: HttpApiConfig }\n    | { type: 'webhook'; config: WebhookConfig }\n    | { type: 'custom'; workerUrl: string }\n    | { type: 'manual'; steps: string[] };\n}\n\n// Stratified sampling for metrics\ninterface MetricsConfig {\n  enablePersistence: boolean;\n  \n  // Stratified sampling rates\n  sampling: {\n    errors: number;          // 100% - always sample errors\n    successes: number;       // 5% - rarely sample successes\n    slowOperations: number;  // 50% - sample operations > 1s\n  };\n  \n  retentionDays: number;\n  enableRecommendations: boolean;\n}\n\nclass AdaptivePerformanceMonitor {\n  private metrics: StreamingMetrics;\n  private recentMeasurements: Measurement[] = [];\n  private db: IDBDatabase | null = null;\n  private config: MetricsConfig;\n  \n  constructor(config: MetricsConfig) {\n    this.config = config;\n    this.metrics = this.initMetrics();\n    \n    if (config.enablePersistence) {\n      this.initDatabase();\n    }\n  }\n  \n  recordDecision(\n    strategy: 'tee' | 'materialize' | 'batch',\n    sizeBytes: number,\n    durationMs: number,\n    memoryUsed: number,\n    context: {\n      workflowId: string;\n      nodeId: string;\n      success: boolean;  // NEW: Track success/failure\n    }\n  ): void {\n    \n    switch (strategy) {\n      case 'tee':\n        this.metrics.decisions.teeCount++;\n        break;\n      case 'materialize':\n        this.metrics.decisions.materializeCount++;\n        break;\n      case 'batch':\n        this.metrics.decisions.batchCount++;\n        break;\n    }\n    \n    if (strategy === 'tee' || strategy === 'materialize') {\n      // Stratified sampling\n      const shouldSample = this.shouldSample(context.success, durationMs);\n      \n      if (shouldSample) {\n        const measurement: Measurement = {\n          id: crypto.randomUUID(),\n          timestamp: Date.now(),\n          strategy,\n          sizeBytes,\n          durationMs,\n          memoryUsed,\n          workflowId: context.workflowId,\n          nodeId: context.nodeId,\n          success: context.success\n        };\n        \n        this.recentMeasurements.push(measurement);\n        \n        if (this.recentMeasurements.length > 100) {\n          this.recentMeasurements.shift();\n        }\n        \n        if (this.config.enablePersistence && this.db) {\n          this.persistMeasurement(measurement);\n        }\n      }\n      \n      this.updateAverages();\n      \n      if (this.config.enableRecommendations) {\n        this.generateRecommendations();\n      }\n    }\n  }\n  \n  private shouldSample(success: boolean, durationMs: number): boolean {\n    // Always sample errors\n    if (!success) {\n      return Math.random() < this.config.sampling.errors;\n    }\n    \n    // Sample slow operations more frequently\n    if (durationMs > 1000) {\n      return Math.random() < this.config.sampling.slowOperations;\n    }\n    \n    // Rarely sample fast successes\n    return Math.random() < this.config.sampling.successes;\n  }\n  \n  recordError(type: 'tee' | 'materialize' | 'storage-full'): void {\n    switch (type) {\n      case 'tee':\n        this.metrics.errors.teeFailures++;\n        break;\n      case 'materialize':\n        this.metrics.errors.materializeFailures++;\n        break;\n      case 'storage-full':\n        this.metrics.errors.storageFullErrors++;\n        break;\n    }\n  }\n  \n  private updateAverages(): void {\n    const tees = this.recentMeasurements.filter(m => m.strategy === 'tee');\n    const materializes = this.recentMeasurements.filter(m => m.strategy === 'materialize');\n    \n    if (tees.length > 0) {\n      this.metrics.performance.avgTeeMemory = \n        tees.reduce((sum, m) => sum + m.memoryUsed, 0) / tees.length;\n      this.metrics.performance.avgTeeTime = \n        tees.reduce((sum, m) => sum + m.durationMs, 0) / tees.length;\n    }\n    \n    if (materializes.length > 0) {\n      this.metrics.performance.avgMaterializeSize = \n        materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / materializes.length;\n      this.metrics.performance.avgMaterializeTime = \n        materializes.reduce((sum, m) => sum + m.durationMs, 0) / materializes.length;\n    }\n  }\n  \n  private generateRecommendations(): void {\n    if (this.metrics.performance.avgTeeMemory > 10 * 1024 * 1024) {\n      this.metrics.recommendations.suggestedTeeThreshold = 3 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedTeeThreshold = 5 * 1024 * 1024;\n    }\n    \n    if (this.metrics.performance.avgMaterializeTime < 100) {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 100 * 1024 * 1024;\n    } else {\n      this.metrics.recommendations.suggestedMaterializeThreshold = 50 * 1024 * 1024;\n    }\n    \n    if (this.metrics.errors.storageFullErrors > 5) {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'evict-lru';\n    } else {\n      this.metrics.recommendations.suggestedOverflowPolicy = 'compress';\n    }\n  }\n  \n  async getHistoricalMetrics(\n    workflowId?: string,\n    startTime?: number,\n    endTime?: number\n  ): Promise<Measurement[]> {\n    \n    if (!this.config.enablePersistence || !this.db) {\n      return this.recentMeasurements;\n    }\n    \n    return new Promise((resolve, reject) => {\n      const tx = this.db!.transaction('measurements', 'readonly');\n      const store = tx.objectStore('measurements');\n      const index = store.index('timestamp');\n      \n      const range = IDBKeyRange.bound(\n        startTime || 0,\n        endTime || Date.now()\n      );\n      \n      const results: Measurement[] = [];\n      const request = index.openCursor(range);\n      \n      request.onsuccess = (event) => {\n        const cursor = (event.target as IDBRequest).result;\n        if (cursor) {\n          const measurement: Measurement = cursor.value;\n          \n          if (!workflowId || measurement.workflowId === workflowId) {\n            results.push(measurement);\n          }\n          \n          cursor.continue();\n        } else {\n          resolve(results);\n        }\n      };\n      \n      request.onerror = () => reject(request.error);\n    });\n  }\n  \n  async generateReport(workflowId?: string): Promise<PerformanceReport> {\n    const measurements = await this.getHistoricalMetrics(workflowId);\n    \n    const tees = measurements.filter(m => m.strategy === 'tee');\n    const materializes = measurements.filter(m => m.strategy === 'materialize');\n    \n    return {\n      totalMeasurements: measurements.length,\n      timeRange: {\n        start: Math.min(...measurements.map(m => m.timestamp)),\n        end: Math.max(...measurements.map(m => m.timestamp))\n      },\n      strategies: {\n        tee: {\n          count: tees.length,\n          avgMemory: tees.reduce((sum, m) => sum + m.memoryUsed, 0) / (tees.length || 1),\n          avgDuration: tees.reduce((sum, m) => sum + m.durationMs, 0) / (tees.length || 1),\n          avgSize: tees.reduce((sum, m) => sum + m.sizeBytes, 0) / (tees.length || 1),\n          successRate: tees.filter(m => m.success).length / (tees.length || 1)\n        },\n        materialize: {\n          count: materializes.length,\n          avgMemory: materializes.reduce((sum, m) => sum + m.memoryUsed, 0) / (materializes.length || 1),\n          avgDuration: materializes.reduce((sum, m) => sum + m.durationMs, 0) / (materializes.length || 1),\n          avgSize: materializes.reduce((sum, m) => sum + m.sizeBytes, 0) / (materializes.length || 1),\n          successRate: materializes.filter(m => m.success).length / (materializes.length || 1)\n        }\n      },\n      recommendations: this.metrics.recommendations\n    };\n  }\n  \n  getMetrics(): StreamingMetrics {\n    return { ...this.metrics };\n  }\n  \n  getMeasurements(): Measurement[] {\n    return [...this.recentMeasurements];\n  }\n  \n  private async initDatabase(): Promise<void> {\n    return new Promise((resolve, reject) => {\n      const request = indexedDB.open('PerformanceMetrics', 3);\n      \n      request.onerror = () => reject(request.error);\n      request.onsuccess = () => {\n        this.db = request.result;\n        this.cleanupOldMeasurements();\n        resolve();\n      };\n      \n      request.onupgradeneeded = (event) => {\n        const db = (event.target as IDBOpenDBRequest).result;\n        \n        if (!db.objectStoreNames.contains('measurements')) {\n          const store = db.createObjectStore('measurements', { keyPath: 'id' });\n          store.createIndex('timestamp', 'timestamp', { unique: false });\n          store.createIndex('workflowId', 'workflowId', { unique: false });\n          store.createIndex('strategy', 'strategy', { unique: false });\n          store.createIndex('success', 'success', { unique: false });\n        }\n      };\n    });\n  }\n  \n  private async persistMeasurement(measurement: Measurement): Promise<void> {\n    if (!this.db) return;\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    await tx.objectStore('measurements').add(measurement);\n  }\n  \n  private async cleanupOldMeasurements(): Promise<void> {\n    if (!this.db) return;\n    \n    const cutoff = Date.now() - (this.config.retentionDays * 24 * 60 * 60 * 1000);\n    \n    const tx = this.db.transaction('measurements', 'readwrite');\n    const store = tx.objectStore('measurements');\n    const index = store.index('timestamp');\n    \n    const range = IDBKeyRange.upperBound(cutoff);\n    const request = index.openCursor(range);\n    \n    request.onsuccess = (event) => {\n      const cursor = (event.target as IDBRequest).result;\n      if (cursor) {\n        cursor.delete();\n        cursor.continue();\n      }\n    };\n  }\n  \n  private initMetrics(): StreamingMetrics {\n    return {\n      decisions: {\n        teeCount: 0,\n        materializeCount: 0,\n        batchCount: 0\n      },\n      performance: {\n        avgTeeMemory: 0,\n        avgMaterializeSize: 0,\n        avgTeeTime: 0,\n        avgMaterializeTime: 0\n      },\n      errors: {\n        teeFailures: 0,\n        materializeFailures: 0,\n        storageFullErrors: 0\n      },\n      recommendations: {\n        suggestedTeeThreshold: 5 * 1024 * 1024,\n        suggestedMaterializeThreshold: 50 * 1024 * 1024,\n        suggestedOverflowPolicy: 'evict-lru'\n      }\n    };\n  }\n}\n\ninterface Measurement {\n  id: string;\n  timestamp: number;\n  strategy: 'tee' | 'materialize';\n  sizeBytes: number;\n  durationMs: number;\n  memoryUsed: number;\n  workflowId: string;\n  nodeId: string;\n  success: boolean;  // NEW\n}\n\ninterface StreamingMetrics {\n  decisions: {\n    teeCount: number;\n    materializeCount: number;\n    batchCount: number;\n  };\n  \n  performance: {\n    avgTeeMemory: number;\n    avgMaterializeSize: number;\n    avgTeeTime: number;\n    avgMaterializeTime: number;\n  };\n  \n  errors: {\n    teeFailures: number;\n    materializeFailures: number;\n    storageFullErrors: number;\n  };\n  \n  recommendations: {\n    suggestedTeeThreshold: number;\n    suggestedMaterializeThreshold: number;\n    suggestedOverflowPolicy: string;\n  };\n}\n\ninterface PerformanceReport {\n  totalMeasurements: number;\n  timeRange: {\n    start: number;\n    end: number;\n  };\n  strategies: {\n    tee: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n      successRate: number;  // NEW\n    };\n    materialize: {\n      count: number;\n      avgMemory: number;\n      avgDuration: number;\n      avgSize: number;\n      successRate: number;  // NEW\n    };\n  };\n  recommendations: StreamingMetrics['recommendations'];\n}\n\ninterface HttpApiConfig {\n  method: 'GET' | 'POST' | 'DELETE' | 'PUT';\n  urlTemplate: string;\n  allowedDomains: string[];\n  headers: Record<string, string>;\n  bodyTemplate: string;\n  auth?: {\n    type: 'bearer' | 'basic' | 'api-key';\n    tokenPath: string;\n  };\n}\n\ninterface WebhookConfig {\n  url: string;\n  allowedDomains: string[];\n  method: 'POST' | 'PUT';\n  payloadTemplate: string;\n}\n```\n\n---\n\n## What I Improved\n\n### 1. Stream Resurrection Edge Cases (CRITICAL FIX)\n**Problem**: Original design had no validation for config changes between checkpoint and resurrection.\n**Solution**:\n- **Version tracking**: Hash node config and inputs at checkpoint time\n- **Staleness detection**: Validate checkpoint freshness before resurrection\n- **Graceful degradation**: Three fallback strategies when config changes:\n  1. Regenerate with old config (snapshot)\n  2. Use cached data with warning\n  3. Return empty stream with error message\n- **Severity classification**: `none`/`minor`/`major`/`critical` determines fallback strategy\n- **User notifications**: Event-based warnings for degraded resurrections\n\n### 2. SessionStorage Overflow (CRITICAL FIX)\n**Problem**: Original design would silently fail when hitting 5MB SessionStorage limit.\n**Solution**:\n- **Circular buffer pattern**: Priority-based eviction when approaching limit\n- **Compression**: Auto-compress items > 10KB before storing\n- **Size tracking**: Real-time monitoring of SessionStorage usage\n- **Graceful fallback**: Move to IndexedDB if SessionStorage full\n- **4MB soft limit**: Leave 1MB buffer to prevent quota errors\n\n### 3. Visual Block Editor Performance\n**Problem**: Rendering 50+ inline blocks would cause lag.\n**Solution**:\n- **Hard block limit**: 50 blocks per expression (prevents abuse)\n- **Virtual scrolling**: Only render visible blocks when > 20 blocks\n- **Memoization**: Block components wrapped in `React.memo`\n- **Performance budget**: 16ms target (60fps)\n- **Block counter**: Visible feedback on current usage\n\n### 4. Trust Level UX Simplification\n**Problem**: Four-level trust model (`system`/`verified`/`user`/`untrusted`) too complex.\n**Solution**:\n- **Binary trust**: `trusted` vs `untrusted` only\n- **Clear approval flow**: Untrusted handlers require explicit user approval\n- **Approval persistence**: Once approved, stored in localStorage\n- **Domain whitelisting**: All handlers must declare allowed domains\n- **No \"verified\" tier**: Removes need for marketplace/verification system\n\n### 5. Metrics Sampling Strategy\n**Problem**: 10% uniform sampling misses errors and performance spikes.\n**Solution**:\n- **Stratified sampling**:\n  - **100% errors**: Always sample failures\n  - **50% slow ops**: Sample operations > 1s\n  - **5% fast successes**: Rarely sample quick successes\n- **Success tracking**: Add `success: boolean` to measurements\n- **Success rate in reports**: Show reliability metrics alongside performance\n\n---\n\n## What I Kept\n\nAll core architectural decisions remain intact:\n- React Flow canvas \u2705\n- Zustand + immer state management \u2705\n- Adaptive streaming executor \u2705\n- Three-tier storage (enhanced) \u2705\n- Composable expression system \u2705\n- Append-only side effect log \u2705\n- Checkpoint manager \u2705\n- Worker pools \u2705\n- Compression \u2705\n- Auto-checkpointing streams \u2705\n\n---\n\n## Remaining Concerns\n\n### 1. Version Hashing Collisions\n**Issue**: Simple hash function could have collisions for similar configs.\n**Mitigation**: Use full config snapshot comparison as secondary check.\n**Trade-off**: Slightly larger checkpoint metadata.\n\n### 2. SessionStorage Cross-Tab Behavior\n**Issue**: SessionStorage is per-tab, not shared across browser tabs.\n**Impact**: Opening workflow in new tab loses checkpoints.\n**Mitigation**: Add \"Export/Import Checkpoint\" feature for multi-tab workflows.\n\n### 3. Virtual Scroll with contentEditable\n**Issue**: contentEditable + virtual scrolling is complex to implement correctly.\n**Alternative**: Use separate input for text, render blocks as read-only chips.\n**Recommendation**: Prototype both approaches, measure complexity.\n\n### 4. Compensation Handler Marketplace\n**Issue**: No \"verified\" trust level means users can't discover trusted handlers.\n**Future Work**: Build handler marketplace with community ratings + audits.\n**Short-term**: Ship with built-in handlers only (Stripe, Slack, etc).\n\n---\n\n## Convergence Status\n\n**ITERATING** - Found critical edge cases and improved the design with concrete solutions:\n\n\u2705 **Stream resurrection**: Version-aware with graceful degradation  \n\u2705 **SessionStorage overflow**: Circular buffer + compression  \n\u2705 **Block editor performance**: Virtual scrolling + hard limits  \n\u2705 **Trust model**: Simplified to binary with clear approval flow  \n\u2705 **Metrics sampling**: Stratified sampling captures errors reliably  \n\nThe design is significantly more robust but needs one more iteration to validate the complexity trade-offs.\n\n---\n\n## PROMPT_FOR_ARCHITECT:\n\nYour design was excellent - I've built on it to address the edge cases you identified. Please review my enhancements:\n\n1. **Stream Resurrection with Version Tracking**: Do you see any scenarios where the staleness detection would fail? Should we also track workflow version (not just node config)?\n\n2. **SessionStorage Circular Buffer**: Is priority-based eviction (1-10 scale) sufficient, or should we add more dimensions (access frequency, size, age)?\n\n3. **Virtual Scrolling for contentEditable**: This is complex to implement correctly. Should we simplify to a non-editable block list + separate text input?\n\n4. **Binary Trust Model**: Does removing \"verified\" tier hurt discoverability of safe handlers? Should we add a \"community-trusted\" flag instead?\n\n5. **Stratified Sampling Rates**: Are 100%/50%/5% the right rates, or should they be configurable per workflow?\n\nIf these solutions work, we're ready to build. If not, suggest alternatives and we'll iterate once more."
      }
    }
  ],
  "final_design": "",
  "status": "debating",
  "started_at": "2026-01-17T20:52:15.016785",
  "ended_at": "",
  "max_rounds": 4,
  "working_dir": ".",
  "current_round": 4,
  "round_times": [
    304.9472894668579,
    341.6121127605438,
    424.33400535583496,
    492.8848657608032
  ]
}