{
  "initial_prompt": "Design authentication system with JWT refresh tokens\n\nRequirements:\n- Secure login flow with JWT tokens\n- Refresh token mechanism for session extension\n- Token revocation capability\n- Protection against common attacks (CSRF, XSS, replay)\n- Support for multiple devices/sessions",
  "rounds": [
    {
      "round_number": 1,
      "agent_a": {
        "content": "## Design\n\n# JWT Authentication System with Refresh Tokens\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502  (Redis)    \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                                  \u25bc\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502  Database   \u2502\n                                           \u2502  (Postgres) \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Core Components\n\n### 1. Token Types\n\n**Access Token (JWT)**\n- **Lifetime**: 15 minutes\n- **Storage**: Memory only (never localStorage)\n- **Payload**:\n```json\n{\n  \"sub\": \"user_id\",\n  \"email\": \"user@example.com\",\n  \"roles\": [\"user\", \"admin\"],\n  \"permissions\": [\"read:profile\", \"write:posts\"],\n  \"device_id\": \"uuid-v4\",\n  \"session_id\": \"uuid-v4\",\n  \"iat\": 1234567890,\n  \"exp\": 1234568790,\n  \"jti\": \"unique-token-id\"\n}\n```\n\n**Refresh Token**\n- **Lifetime**: 30 days (configurable)\n- **Storage**: Secure, httpOnly cookie with SameSite=Strict\n- **Format**: Opaque token (cryptographically random, 256 bits)\n- **One-time use**: Invalidated after refresh, new token issued\n\n### 2. Authentication Service API\n\n```typescript\ninterface AuthService {\n  // Primary flows\n  login(credentials: LoginRequest): Promise<AuthResponse>\n  logout(refreshToken: string, allDevices: boolean): Promise<void>\n  refreshAccessToken(refreshToken: string): Promise<AuthResponse>\n  \n  // Token management\n  revokeToken(tokenId: string): Promise<void>\n  revokeAllUserTokens(userId: string): Promise<void>\n  revokeDeviceTokens(userId: string, deviceId: string): Promise<void>\n  \n  // Session management\n  listActiveSessions(userId: string): Promise<Session[]>\n  validateAccessToken(token: string): Promise<TokenValidation>\n}\n\ninterface LoginRequest {\n  email: string\n  password: string\n  deviceInfo: {\n    deviceId?: string  // Client-generated, stored in localStorage\n    deviceName: string // \"Chrome on MacOS\"\n    deviceType: 'mobile' | 'desktop' | 'tablet'\n    userAgent: string\n  }\n  mfaCode?: string\n}\n\ninterface AuthResponse {\n  accessToken: string  // Returned in JSON body\n  refreshToken: string // Set as httpOnly cookie\n  expiresIn: number   // Access token lifetime in seconds\n  user: {\n    id: string\n    email: string\n    roles: string[]\n  }\n}\n\ninterface Session {\n  sessionId: string\n  deviceId: string\n  deviceName: string\n  deviceType: string\n  createdAt: Date\n  lastUsedAt: Date\n  expiresAt: Date\n  ipAddress: string\n  current: boolean\n}\n```\n\n### 3. Data Models\n\n**Users Table (Postgres)**\n```sql\nCREATE TABLE users (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email VARCHAR(255) UNIQUE NOT NULL,\n  password_hash TEXT NOT NULL,\n  roles TEXT[] NOT NULL DEFAULT '{\"user\"}',\n  mfa_enabled BOOLEAN DEFAULT false,\n  mfa_secret TEXT,\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW(),\n  last_login_at TIMESTAMP,\n  password_changed_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_users_email ON users(email);\n```\n\n**Refresh Tokens Table (Postgres)**\n```sql\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n  token_hash TEXT NOT NULL UNIQUE,\n  session_id UUID NOT NULL,\n  device_id UUID NOT NULL,\n  device_name VARCHAR(255),\n  device_type VARCHAR(50),\n  user_agent TEXT,\n  ip_address INET,\n  created_at TIMESTAMP DEFAULT NOW(),\n  last_used_at TIMESTAMP DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  revoked BOOLEAN DEFAULT false,\n  revoked_at TIMESTAMP,\n  parent_token_id UUID REFERENCES refresh_tokens(id)\n);\n\nCREATE INDEX idx_refresh_tokens_user_id ON refresh_tokens(user_id);\nCREATE INDEX idx_refresh_tokens_token_hash ON refresh_tokens(token_hash);\nCREATE INDEX idx_refresh_tokens_session_id ON refresh_tokens(session_id);\nCREATE INDEX idx_refresh_tokens_expires_at ON refresh_tokens(expires_at);\n```\n\n**Token Blacklist (Redis)**\n- Key pattern: `blacklist:jti:{jti}`\n- Value: `{userId, revokedAt, reason}`\n- TTL: Access token lifetime (15 minutes)\n- Used for immediate access token revocation\n\n**Refresh Token Cache (Redis)**\n```\nKey: refresh:{token_hash}\nValue: {\n  userId: string,\n  sessionId: string,\n  deviceId: string,\n  expiresAt: timestamp,\n  used: boolean\n}\nTTL: 30 days\n```\n\n## Security Mechanisms\n\n### 1. CSRF Protection\n\n**Token Binding**\n- Refresh tokens bound to specific session/device via cookies\n- SameSite=Strict prevents cross-site cookie transmission\n- Access tokens transmitted via Authorization header only\n\n**Double-Submit Cookie Pattern (Optional Layer)**\n- Generate CSRF token on login, store in httpOnly cookie\n- Client reads from separate non-httpOnly cookie, sends in X-CSRF-Token header\n- Server validates both match\n\n### 2. XSS Protection\n\n**Access Token Storage**\n- Never store in localStorage or sessionStorage\n- Keep in memory only (React state, Vuex, Redux)\n- Automatically lost on page refresh (forces token refresh)\n\n**Cookie Security**\n- httpOnly flag prevents JavaScript access\n- Secure flag ensures HTTPS-only transmission\n- SameSite=Strict prevents CSRF\n\n### 3. Replay Attack Prevention\n\n**Access Tokens**\n- Short lifetime (15 minutes) limits replay window\n- JTI (JWT ID) tracked in blacklist for revocation\n- Optional: Include nonce in critical operations\n\n**Refresh Tokens**\n- One-time use with token rotation\n- Automatic revocation chain if reuse detected\n- Parent token tracking for fraud detection\n\n**Reuse Detection Algorithm**\n```\nIf refresh token already used:\n  1. Mark entire token family as compromised\n  2. Revoke all tokens in the chain (via parent_token_id)\n  3. Revoke all sessions for that device\n  4. Notify user via email\n  5. Require re-authentication\n```\n\n### 4. Token Rotation on Refresh\n\n```\nClient sends: Refresh Token A\nServer:\n  1. Validates token A is unused and not expired\n  2. Marks token A as used in Redis\n  3. Generates new access token B\n  4. Generates new refresh token C\n  5. Stores token C with parent_token_id = A\n  6. Returns access token B and refresh token C\n  7. Token A becomes invalid for future use\n```\n\n## Authentication Flows\n\n### Login Flow\n\n```\n1. Client \u2192 POST /auth/login\n   Body: { email, password, deviceInfo }\n\n2. Server validates credentials\n   - Check password hash (bcrypt/argon2)\n   - Rate limit: 5 attempts per 15 min per IP\n   - Check account status (locked, suspended)\n\n3. If MFA enabled:\n   - Return { requiresMfa: true, tempToken }\n   - Client \u2192 POST /auth/mfa/verify\n   - Body: { tempToken, mfaCode }\n\n4. Generate tokens:\n   - Create session_id (UUID)\n   - Generate access token (JWT, 15 min)\n   - Generate refresh token (random 256-bit)\n   - Hash refresh token before storage\n   - Store in Postgres + Redis\n\n5. Response:\n   - Set-Cookie: refresh_token (httpOnly, Secure, SameSite=Strict)\n   - Set-Cookie: csrf_token (httpOnly, Secure, SameSite=Strict)\n   - Body: { accessToken, expiresIn, user }\n\n6. Client stores:\n   - Access token in memory\n   - Device ID in localStorage (if new)\n```\n\n### Token Refresh Flow\n\n```\n1. Client detects access token expiring soon\n   - Proactive refresh at 80% lifetime (12 min)\n   - Or reactive on 401 Unauthorized\n\n2. Client \u2192 POST /auth/refresh\n   Headers: Cookie: refresh_token={token}\n   Body: { deviceId }\n\n3. Server validates:\n   - Extract refresh token from cookie\n   - Hash and lookup in Redis/Postgres\n   - Check not expired, not revoked, not used\n   - Verify device_id matches\n   - Verify IP hasn't changed dramatically (optional)\n\n4. Detect reuse:\n   - If token.used === true \u2192 trigger fraud response\n   - Revoke entire token family\n   - Return 401 with { error: 'token_reuse' }\n\n5. Generate new tokens:\n   - Mark old refresh token as used\n   - Create new access + refresh tokens\n   - Link via parent_token_id\n   - Update last_used_at\n\n6. Response:\n   - New refresh token in httpOnly cookie\n   - New access token in body\n```\n\n### Logout Flow\n\n```\nSingle Device Logout:\n1. Client \u2192 POST /auth/logout\n   Headers: \n     - Authorization: Bearer {accessToken}\n     - Cookie: refresh_token={token}\n\n2. Server:\n   - Extract JTI from access token\n   - Add to Redis blacklist (TTL = token expiry)\n   - Mark refresh token as revoked\n   - Delete from Redis cache\n   - Clear cookies in response\n\nAll Devices Logout:\n1. Client \u2192 POST /auth/logout?allDevices=true\n\n2. Server:\n   - Revoke all refresh tokens for user\n   - Add all active JTIs to blacklist\n   - Clear all Redis cache entries\n   - Update revoked flag in Postgres\n```\n\n### Token Validation Middleware\n\n```typescript\nasync function validateAccessToken(req, res, next) {\n  try {\n    // Extract token\n    const authHeader = req.headers.authorization;\n    if (!authHeader?.startsWith('Bearer ')) {\n      return res.status(401).json({ error: 'missing_token' });\n    }\n    \n    const token = authHeader.substring(7);\n    \n    // Verify JWT signature and expiration\n    const payload = jwt.verify(token, PUBLIC_KEY, {\n      algorithms: ['RS256'],\n      issuer: 'auth.example.com',\n      audience: 'api.example.com'\n    });\n    \n    // Check blacklist (Redis)\n    const isBlacklisted = await redis.exists(`blacklist:jti:${payload.jti}`);\n    if (isBlacklisted) {\n      return res.status(401).json({ error: 'token_revoked' });\n    }\n    \n    // Optional: Check if password changed after token issued\n    const user = await getUserById(payload.sub);\n    if (user.password_changed_at > payload.iat) {\n      return res.status(401).json({ error: 'token_invalidated' });\n    }\n    \n    // Attach user context\n    req.user = {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions,\n      sessionId: payload.session_id,\n      deviceId: payload.device_id\n    };\n    \n    next();\n  } catch (error) {\n    if (error.name === 'TokenExpiredError') {\n      return res.status(401).json({ \n        error: 'token_expired',\n        message: 'Please refresh your token'\n      });\n    }\n    return res.status(401).json({ error: 'invalid_token' });\n  }\n}\n```\n\n## Error Handling\n\n### Error Codes\n\n```typescript\nenum AuthErrorCode {\n  // Authentication errors\n  INVALID_CREDENTIALS = 'invalid_credentials',\n  ACCOUNT_LOCKED = 'account_locked',\n  ACCOUNT_SUSPENDED = 'account_suspended',\n  MFA_REQUIRED = 'mfa_required',\n  INVALID_MFA_CODE = 'invalid_mfa_code',\n  \n  // Token errors\n  TOKEN_EXPIRED = 'token_expired',\n  TOKEN_REVOKED = 'token_revoked',\n  TOKEN_REUSE_DETECTED = 'token_reuse_detected',\n  INVALID_TOKEN = 'invalid_token',\n  MISSING_TOKEN = 'missing_token',\n  \n  // Session errors\n  SESSION_EXPIRED = 'session_expired',\n  DEVICE_MISMATCH = 'device_mismatch',\n  SUSPICIOUS_ACTIVITY = 'suspicious_activity',\n  \n  // Rate limiting\n  TOO_MANY_ATTEMPTS = 'too_many_attempts',\n  RATE_LIMIT_EXCEEDED = 'rate_limit_exceeded'\n}\n\ninterface AuthError {\n  code: AuthErrorCode,\n  message: string,\n  details?: Record<string, any>,\n  retryAfter?: number, // For rate limiting\n  requiresAction?: 'login' | 'mfa' | 'unlock_account'\n}\n```\n\n### Client Error Handling\n\n```typescript\n// Global error interceptor\naxios.interceptors.response.use(\n  response => response,\n  async error => {\n    const originalRequest = error.config;\n    \n    if (error.response?.status === 401) {\n      const errorCode = error.response.data?.error;\n      \n      // Token expired - try refresh\n      if (errorCode === 'token_expired' && !originalRequest._retry) {\n        originalRequest._retry = true;\n        \n        try {\n          const { accessToken } = await refreshAccessToken();\n          setAccessToken(accessToken);\n          originalRequest.headers.Authorization = `Bearer ${accessToken}`;\n          return axios(originalRequest);\n        } catch (refreshError) {\n          // Refresh failed - redirect to login\n          clearAuth();\n          window.location.href = '/login';\n          return Promise.reject(refreshError);\n        }\n      }\n      \n      // Token reuse detected - force logout\n      if (errorCode === 'token_reuse_detected') {\n        clearAuth();\n        alert('Security alert: Multiple devices detected. Please login again.');\n        window.location.href = '/login';\n      }\n      \n      // Other 401 errors - redirect to login\n      clearAuth();\n      window.location.href = '/login';\n    }\n    \n    return Promise.reject(error);\n  }\n);\n```\n\n## Security Best Practices\n\n### 1. Key Management\n- Use RS256 (asymmetric) for JWTs\n- Rotate signing keys every 90 days\n- Store private keys in HSM or secrets manager\n- Support multiple active public keys (kid in JWT header)\n\n### 2. Password Security\n- Use Argon2id or bcrypt (cost factor \u226512)\n- Enforce strong password policy\n- Implement breach detection (HaveIBeenPwned API)\n- Force password change on compromise detection\n\n### 3. Rate Limiting\n- Login attempts: 5 per 15 minutes per IP\n- Token refresh: 10 per minute per session\n- Failed MFA: 3 per 5 minutes per session\n- Progressive delays on repeated failures\n\n### 4. Monitoring & Alerts\n- Log all authentication events\n- Alert on: token reuse, unusual IP changes, mass revocations\n- Track failed login attempts per user/IP\n- Monitor token refresh frequency anomalies\n\n### 5. Session Management\n- Limit concurrent sessions per user (configurable, default 5)\n- Automatic session cleanup (daily cron job)\n- Allow users to view and revoke active sessions\n- Notify users of new device logins\n\n## Configuration\n\n```typescript\ninterface AuthConfig {\n  accessToken: {\n    lifetime: number;        // 15 * 60 (seconds)\n    algorithm: 'RS256';\n    issuer: string;\n    audience: string;\n  };\n  \n  refreshToken: {\n    lifetime: number;        // 30 * 24 * 60 * 60 (seconds)\n    rotationEnabled: boolean; // true\n    reuseDetection: boolean;  // true\n  };\n  \n  security: {\n    bcryptRounds: number;     // 12\n    maxSessions: number;      // 5\n    requireMfa: boolean;      // false (per-user setting)\n    allowedOrigins: string[];\n  };\n  \n  rateLimit: {\n    loginAttempts: { max: 5, window: 900 };      // 15 min\n    refreshAttempts: { max: 10, window: 60 };    // 1 min\n    mfaAttempts: { max: 3, window: 300 };        // 5 min\n  };\n  \n  cookies: {\n    domain: string;\n    secure: boolean;          // true in production\n    sameSite: 'Strict';\n    httpOnly: boolean;        // true\n  };\n}\n```\n\n## Rationale\n\n**Short-lived access tokens (15 min)**: Minimizes the impact of token theft while maintaining good UX. Shorter would increase refresh frequency unnecessarily.\n\n**Opaque refresh tokens**: Unlike JWTs, these can be revoked immediately without blacklist lookup. Using random tokens prevents information leakage.\n\n**Token rotation on refresh**: Prevents refresh token replay attacks. One-time use ensures that if a refresh token is stolen and used, the legitimate user's next refresh will trigger fraud detection.\n\n**Redis + Postgres hybrid**: Redis for fast lookup and blacklist TTL management; Postgres for durable storage and audit trail. Redis acts as write-through cache.\n\n**httpOnly cookies for refresh tokens**: Prevents XSS attacks from stealing the long-lived token. SameSite=Strict prevents CSRF.\n\n**Memory-only access token storage**: Balances security (lost on XSS) with UX. Automatic page refresh triggers token refresh flow.\n\n**Device binding**: Prevents token theft across devices. Device ID stored client-side provides identity; server validates consistency.\n\n**Parent token tracking**: Enables detection of token family compromise. When reuse is detected, entire chain can be revoked.\n\n## What I Changed\n\nThis is the initial design, so there are no changes yet. Key design decisions made:\n\n- Chose RS256 over HS256 for better key distribution in microservices\n- Selected 15-minute access token lifetime as balance between security and UX\n- Implemented token family tracking for sophisticated replay detection\n- Used Redis + Postgres hybrid for performance + durability\n- Added device binding to prevent cross-device token theft\n\n## What I Kept\n\nN/A - Initial design\n\n## Open Questions\n\n1. **Token lifetime tuning**: Should access token lifetime be configurable per-client (mobile vs web)?\n\n2. **IP validation strictness**: How aggressive should IP change detection be? Geographic changes vs ISP changes?\n\n3. **Offline support**: How should mobile apps handle extended offline periods where refresh tokens might expire?\n\n4. **Token payload size**: Are the included permissions making the JWT too large? Should we fetch permissions separately?\n\n5. **Concurrent refresh handling**: If multiple requests trigger refresh simultaneously, should we deduplicate or allow race conditions?\n\n## Convergence Status\n\n**ITERATING** - This is the first iteration. The design needs review for:\n- Security vulnerabilities\n- Implementation complexity\n- Edge cases in token rotation\n- Performance bottlenecks\n- Missing error scenarios",
        "prompt_for_other": "Focus on security vulnerabilities, particularly in the token rotation mechanism and replay attack prevention. Are there any attack vectors I've missed? Is the token reuse detection robust enough?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# JWT Authentication System with Refresh Tokens\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502  (Redis)    \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                                  \u25bc\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502  Database   \u2502\n                                           \u2502  (Postgres) \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Core Components\n\n### 1. Token Types\n\n**Access Token (JWT)**\n- **Lifetime**: 15 minutes\n- **Storage**: Memory only (never localStorage)\n- **Payload**:\n```json\n{\n  \"sub\": \"user_id\",\n  \"email\": \"user@example.com\",\n  \"roles\": [\"user\", \"admin\"],\n  \"permissions\": [\"read:profile\", \"write:posts\"],\n  \"device_id\": \"uuid-v4\",\n  \"session_id\": \"uuid-v4\",\n  \"iat\": 1234567890,\n  \"exp\": 1234568790,\n  \"jti\": \"unique-token-id\"\n}\n```\n\n**Refresh Token**\n- **Lifetime**: 30 days (configurable)\n- **Storage**: Secure, httpOnly cookie with SameSite=Strict\n- **Format**: Opaque token (cryptographically random, 256 bits)\n- **One-time use**: Invalidated after refresh, new token issued\n\n### 2. Authentication Service API\n\n```typescript\ninterface AuthService {\n  // Primary flows\n  login(credentials: LoginRequest): Promise<AuthResponse>\n  logout(refreshToken: string, allDevices: boolean): Promise<void>\n  refreshAccessToken(refreshToken: string): Promise<AuthResponse>\n  \n  // Token management\n  revokeToken(tokenId: string): Promise<void>\n  revokeAllUserTokens(userId: string): Promise<void>\n  revokeDeviceTokens(userId: string, deviceId: string): Promise<void>\n  \n  // Session management\n  listActiveSessions(userId: string): Promise<Session[]>\n  validateAccessToken(token: string): Promise<TokenValidation>\n}\n\ninterface LoginRequest {\n  email: string\n  password: string\n  deviceInfo: {\n    deviceId?: string  // Client-generated, stored in localStorage\n    deviceName: string // \"Chrome on MacOS\"\n    deviceType: 'mobile' | 'desktop' | 'tablet'\n    userAgent: string\n  }\n  mfaCode?: string\n}\n\ninterface AuthResponse {\n  accessToken: string  // Returned in JSON body\n  refreshToken: string // Set as httpOnly cookie\n  expiresIn: number   // Access token lifetime in seconds\n  user: {\n    id: string\n    email: string\n    roles: string[]\n  }\n}\n\ninterface Session {\n  sessionId: string\n  deviceId: string\n  deviceName: string\n  deviceType: string\n  createdAt: Date\n  lastUsedAt: Date\n  expiresAt: Date\n  ipAddress: string\n  current: boolean\n}\n```\n\n### 3. Data Models\n\n**Users Table (Postgres)**\n```sql\nCREATE TABLE users (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email VARCHAR(255) UNIQUE NOT NULL,\n  password_hash TEXT NOT NULL,\n  roles TEXT[] NOT NULL DEFAULT '{\"user\"}',\n  mfa_enabled BOOLEAN DEFAULT false,\n  mfa_secret TEXT,\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW(),\n  last_login_at TIMESTAMP,\n  password_changed_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_users_email ON users(email);\n```\n\n**Refresh Tokens Table (Postgres)**\n```sql\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n  token_hash TEXT NOT NULL UNIQUE,\n  session_id UUID NOT NULL,\n  device_id UUID NOT NULL,\n  device_name VARCHAR(255),\n  device_type VARCHAR(50),\n  user_agent TEXT,\n  ip_address INET,\n  created_at TIMESTAMP DEFAULT NOW(),\n  last_used_at TIMESTAMP DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  revoked BOOLEAN DEFAULT false,\n  revoked_at TIMESTAMP,\n  parent_token_id UUID REFERENCES refresh_tokens(id)\n);\n\nCREATE INDEX idx_refresh_tokens_user_id ON refresh_tokens(user_id);\nCREATE INDEX idx_refresh_tokens_token_hash ON refresh_tokens(token_hash);\nCREATE INDEX idx_refresh_tokens_session_id ON refresh_tokens(session_id);\nCREATE INDEX idx_refresh_tokens_expires_at ON refresh_tokens(expires_at);\n```\n\n**Token Blacklist (Redis)**\n- Key pattern: `blacklist:jti:{jti}`\n- Value: `{userId, revokedAt, reason}`\n- TTL: Access token lifetime (15 minutes)\n- Used for immediate access token revocation\n\n**Refresh Token Cache (Redis)**\n```\nKey: refresh:{token_hash}\nValue: {\n  userId: string,\n  sessionId: string,\n  deviceId: string,\n  expiresAt: timestamp,\n  used: boolean\n}\nTTL: 30 days\n```\n\n## Security Mechanisms\n\n### 1. CSRF Protection\n\n**Token Binding**\n- Refresh tokens bound to specific session/device via cookies\n- SameSite=Strict prevents cross-site cookie transmission\n- Access tokens transmitted via Authorization header only\n\n**Double-Submit Cookie Pattern (Optional Layer)**\n- Generate CSRF token on login, store in httpOnly cookie\n- Client reads from separate non-httpOnly cookie, sends in X-CSRF-Token header\n- Server validates both match\n\n### 2. XSS Protection\n\n**Access Token Storage**\n- Never store in localStorage or sessionStorage\n- Keep in memory only (React state, Vuex, Redux)\n- Automatically lost on page refresh (forces token refresh)\n\n**Cookie Security**\n- httpOnly flag prevents JavaScript access\n- Secure flag ensures HTTPS-only transmission\n- SameSite=Strict prevents CSRF\n\n### 3. Replay Attack Prevention\n\n**Access Tokens**\n- Short lifetime (15 minutes) limits replay window\n- JTI (JWT ID) tracked in blacklist for revocation\n- Optional: Include nonce in critical operations\n\n**Refresh Tokens**\n- One-time use with token rotation\n- Automatic revocation chain if reuse detected\n- Parent token tracking for fraud detection\n\n**Reuse Detection Algorithm**\n```\nIf refresh token already used:\n  1. Mark entire token family as compromised\n  2. Revoke all tokens in the chain (via parent_token_id)\n  3. Revoke all sessions for that device\n  4. Notify user via email\n  5. Require re-authentication\n```\n\n### 4. Token Rotation on Refresh\n\n```\nClient sends: Refresh Token A\nServer:\n  1. Validates token A is unused and not expired\n  2. Marks token A as used in Redis\n  3. Generates new access token B\n  4. Generates new refresh token C\n  5. Stores token C with parent_token_id = A\n  6. Returns access token B and refresh token C\n  7. Token A becomes invalid for future use\n```\n\n## Authentication Flows\n\n### Login Flow\n\n```\n1. Client \u2192 POST /auth/login\n   Body: { email, password, deviceInfo }\n\n2. Server validates credentials\n   - Check password hash (bcrypt/argon2)\n   - Rate limit: 5 attempts per 15 min per IP\n   - Check account status (locked, suspended)\n\n3. If MFA enabled:\n   - Return { requiresMfa: true, tempToken }\n   - Client \u2192 POST /auth/mfa/verify\n   - Body: { tempToken, mfaCode }\n\n4. Generate tokens:\n   - Create session_id (UUID)\n   - Generate access token (JWT, 15 min)\n   - Generate refresh token (random 256-bit)\n   - Hash refresh token before storage\n   - Store in Postgres + Redis\n\n5. Response:\n   - Set-Cookie: refresh_token (httpOnly, Secure, SameSite=Strict)\n   - Set-Cookie: csrf_token (httpOnly, Secure, SameSite=Strict)\n   - Body: { accessToken, expiresIn, user }\n\n6. Client stores:\n   - Access token in memory\n   - Device ID in localStorage (if new)\n```\n\n### Token Refresh Flow\n\n```\n1. Client detects access token expiring soon\n   - Proactive refresh at 80% lifetime (12 min)\n   - Or reactive on 401 Unauthorized\n\n2. Client \u2192 POST /auth/refresh\n   Headers: Cookie: refresh_token={token}\n   Body: { deviceId }\n\n3. Server validates:\n   - Extract refresh token from cookie\n   - Hash and lookup in Redis/Postgres\n   - Check not expired, not revoked, not used\n   - Verify device_id matches\n   - Verify IP hasn't changed dramatically (optional)\n\n4. Detect reuse:\n   - If token.used === true \u2192 trigger fraud response\n   - Revoke entire token family\n   - Return 401 with { error: 'token_reuse' }\n\n5. Generate new tokens:\n   - Mark old refresh token as used\n   - Create new access + refresh tokens\n   - Link via parent_token_id\n   - Update last_used_at\n\n6. Response:\n   - New refresh token in httpOnly cookie\n   - New access token in body\n```\n\n### Logout Flow\n\n```\nSingle Device Logout:\n1. Client \u2192 POST /auth/logout\n   Headers: \n     - Authorization: Bearer {accessToken}\n     - Cookie: refresh_token={token}\n\n2. Server:\n   - Extract JTI from access token\n   - Add to Redis blacklist (TTL = token expiry)\n   - Mark refresh token as revoked\n   - Delete from Redis cache\n   - Clear cookies in response\n\nAll Devices Logout:\n1. Client \u2192 POST /auth/logout?allDevices=true\n\n2. Server:\n   - Revoke all refresh tokens for user\n   - Add all active JTIs to blacklist\n   - Clear all Redis cache entries\n   - Update revoked flag in Postgres\n```\n\n### Token Validation Middleware\n\n```typescript\nasync function validateAccessToken(req, res, next) {\n  try {\n    // Extract token\n    const authHeader = req.headers.authorization;\n    if (!authHeader?.startsWith('Bearer ')) {\n      return res.status(401).json({ error: 'missing_token' });\n    }\n    \n    const token = authHeader.substring(7);\n    \n    // Verify JWT signature and expiration\n    const payload = jwt.verify(token, PUBLIC_KEY, {\n      algorithms: ['RS256'],\n      issuer: 'auth.example.com',\n      audience: 'api.example.com'\n    });\n    \n    // Check blacklist (Redis)\n    const isBlacklisted = await redis.exists(`blacklist:jti:${payload.jti}`);\n    if (isBlacklisted) {\n      return res.status(401).json({ error: 'token_revoked' });\n    }\n    \n    // Optional: Check if password changed after token issued\n    const user = await getUserById(payload.sub);\n    if (user.password_changed_at > payload.iat) {\n      return res.status(401).json({ error: 'token_invalidated' });\n    }\n    \n    // Attach user context\n    req.user = {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions,\n      sessionId: payload.session_id,\n      deviceId: payload.device_id\n    };\n    \n    next();\n  } catch (error) {\n    if (error.name === 'TokenExpiredError') {\n      return res.status(401).json({ \n        error: 'token_expired',\n        message: 'Please refresh your token'\n      });\n    }\n    return res.status(401).json({ error: 'invalid_token' });\n  }\n}\n```\n\n## Error Handling\n\n### Error Codes\n\n```typescript\nenum AuthErrorCode {\n  // Authentication errors\n  INVALID_CREDENTIALS = 'invalid_credentials',\n  ACCOUNT_LOCKED = 'account_locked',\n  ACCOUNT_SUSPENDED = 'account_suspended',\n  MFA_REQUIRED = 'mfa_required',\n  INVALID_MFA_CODE = 'invalid_mfa_code',\n  \n  // Token errors\n  TOKEN_EXPIRED = 'token_expired',\n  TOKEN_REVOKED = 'token_revoked',\n  TOKEN_REUSE_DETECTED = 'token_reuse_detected',\n  INVALID_TOKEN = 'invalid_token',\n  MISSING_TOKEN = 'missing_token',\n  \n  // Session errors\n  SESSION_EXPIRED = 'session_expired',\n  DEVICE_MISMATCH = 'device_mismatch',\n  SUSPICIOUS_ACTIVITY = 'suspicious_activity',\n  \n  // Rate limiting\n  TOO_MANY_ATTEMPTS = 'too_many_attempts',\n  RATE_LIMIT_EXCEEDED = 'rate_limit_exceeded'\n}\n\ninterface AuthError {\n  code: AuthErrorCode,\n  message: string,\n  details?: Record<string, any>,\n  retryAfter?: number, // For rate limiting\n  requiresAction?: 'login' | 'mfa' | 'unlock_account'\n}\n```\n\n### Client Error Handling\n\n```typescript\n// Global error interceptor\naxios.interceptors.response.use(\n  response => response,\n  async error => {\n    const originalRequest = error.config;\n    \n    if (error.response?.status === 401) {\n      const errorCode = error.response.data?.error;\n      \n      // Token expired - try refresh\n      if (errorCode === 'token_expired' && !originalRequest._retry) {\n        originalRequest._retry = true;\n        \n        try {\n          const { accessToken } = await refreshAccessToken();\n          setAccessToken(accessToken);\n          originalRequest.headers.Authorization = `Bearer ${accessToken}`;\n          return axios(originalRequest);\n        } catch (refreshError) {\n          // Refresh failed - redirect to login\n          clearAuth();\n          window.location.href = '/login';\n          return Promise.reject(refreshError);\n        }\n      }\n      \n      // Token reuse detected - force logout\n      if (errorCode === 'token_reuse_detected') {\n        clearAuth();\n        alert('Security alert: Multiple devices detected. Please login again.');\n        window.location.href = '/login';\n      }\n      \n      // Other 401 errors - redirect to login\n      clearAuth();\n      window.location.href = '/login';\n    }\n    \n    return Promise.reject(error);\n  }\n);\n```\n\n## Security Best Practices\n\n### 1. Key Management\n- Use RS256 (asymmetric) for JWTs\n- Rotate signing keys every 90 days\n- Store private keys in HSM or secrets manager\n- Support multiple active public keys (kid in JWT header)\n\n### 2. Password Security\n- Use Argon2id or bcrypt (cost factor \u226512)\n- Enforce strong password policy\n- Implement breach detection (HaveIBeenPwned API)\n- Force password change on compromise detection\n\n### 3. Rate Limiting\n- Login attempts: 5 per 15 minutes per IP\n- Token refresh: 10 per minute per session\n- Failed MFA: 3 per 5 minutes per session\n- Progressive delays on repeated failures\n\n### 4. Monitoring & Alerts\n- Log all authentication events\n- Alert on: token reuse, unusual IP changes, mass revocations\n- Track failed login attempts per user/IP\n- Monitor token refresh frequency anomalies\n\n### 5. Session Management\n- Limit concurrent sessions per user (configurable, default 5)\n- Automatic session cleanup (daily cron job)\n- Allow users to view and revoke active sessions\n- Notify users of new device logins\n\n## Configuration\n\n```typescript\ninterface AuthConfig {\n  accessToken: {\n    lifetime: number;        // 15 * 60 (seconds)\n    algorithm: 'RS256';\n    issuer: string;\n    audience: string;\n  };\n  \n  refreshToken: {\n    lifetime: number;        // 30 * 24 * 60 * 60 (seconds)\n    rotationEnabled: boolean; // true\n    reuseDetection: boolean;  // true\n  };\n  \n  security: {\n    bcryptRounds: number;     // 12\n    maxSessions: number;      // 5\n    requireMfa: boolean;      // false (per-user setting)\n    allowedOrigins: string[];\n  };\n  \n  rateLimit: {\n    loginAttempts: { max: 5, window: 900 };      // 15 min\n    refreshAttempts: { max: 10, window: 60 };    // 1 min\n    mfaAttempts: { max: 3, window: 300 };        // 5 min\n  };\n  \n  cookies: {\n    domain: string;\n    secure: boolean;          // true in production\n    sameSite: 'Strict';\n    httpOnly: boolean;        // true\n  };\n}\n```\n\n## Rationale\n\n**Short-lived access tokens (15 min)**: Minimizes the impact of token theft while maintaining good UX. Shorter would increase refresh frequency unnecessarily.\n\n**Opaque refresh tokens**: Unlike JWTs, these can be revoked immediately without blacklist lookup. Using random tokens prevents information leakage.\n\n**Token rotation on refresh**: Prevents refresh token replay attacks. One-time use ensures that if a refresh token is stolen and used, the legitimate user's next refresh will trigger fraud detection.\n\n**Redis + Postgres hybrid**: Redis for fast lookup and blacklist TTL management; Postgres for durable storage and audit trail. Redis acts as write-through cache.\n\n**httpOnly cookies for refresh tokens**: Prevents XSS attacks from stealing the long-lived token. SameSite=Strict prevents CSRF.\n\n**Memory-only access token storage**: Balances security (lost on XSS) with UX. Automatic page refresh triggers token refresh flow.\n\n**Device binding**: Prevents token theft across devices. Device ID stored client-side provides identity; server validates consistency.\n\n**Parent token tracking**: Enables detection of token family compromise. When reuse is detected, entire chain can be revoked.\n\n## What I Changed\n\nThis is the initial design, so there are no changes yet. Key design decisions made:\n\n- Chose RS256 over HS256 for better key distribution in microservices\n- Selected 15-minute access token lifetime as balance between security and UX\n- Implemented token family tracking for sophisticated replay detection\n- Used Redis + Postgres hybrid for performance + durability\n- Added device binding to prevent cross-device token theft\n\n## What I Kept\n\nN/A - Initial design\n\n## Open Questions\n\n1. **Token lifetime tuning**: Should access token lifetime be configurable per-client (mobile vs web)?\n\n2. **IP validation strictness**: How aggressive should IP change detection be? Geographic changes vs ISP changes?\n\n3. **Offline support**: How should mobile apps handle extended offline periods where refresh tokens might expire?\n\n4. **Token payload size**: Are the included permissions making the JWT too large? Should we fetch permissions separately?\n\n5. **Concurrent refresh handling**: If multiple requests trigger refresh simultaneously, should we deduplicate or allow race conditions?\n\n## Convergence Status\n\n**ITERATING** - This is the first iteration. The design needs review for:\n- Security vulnerabilities\n- Implementation complexity\n- Edge cases in token rotation\n- Performance bottlenecks\n- Missing error scenarios\n\nPROMPT_FOR_CRITIC:\nFocus on security vulnerabilities, particularly in the token rotation mechanism and replay attack prevention. Are there any attack vectors I've missed? Is the token reuse detection robust enough?"
      },
      "agent_b": {
        "content": "## Design\n\n# JWT Authentication System with Refresh Tokens\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502  (Redis)    \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                                  \u25bc\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502  Database   \u2502\n                                           \u2502  (Postgres) \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Core Components\n\n### 1. Token Types\n\n**Access Token (JWT)**\n- **Lifetime**: 15 minutes\n- **Storage**: Memory only (never localStorage)\n- **Payload**:\n```json\n{\n  \"sub\": \"user_id\",\n  \"email\": \"user@example.com\",\n  \"roles\": [\"user\", \"admin\"],\n  \"permissions\": [\"read:profile\", \"write:posts\"],\n  \"device_id\": \"uuid-v4\",\n  \"session_id\": \"uuid-v4\",\n  \"token_version\": 1,\n  \"iat\": 1234567890,\n  \"exp\": 1234568790,\n  \"jti\": \"unique-token-id\",\n  \"aud\": \"api.example.com\",\n  \"iss\": \"auth.example.com\"\n}\n```\n\n**Refresh Token**\n- **Lifetime**: 30 days (configurable)\n- **Storage**: Secure, httpOnly cookie with SameSite=Strict\n- **Format**: Opaque token (cryptographically random, 256 bits)\n- **One-time use**: Invalidated after refresh, new token issued\n- **Structure**: `{version}.{randomBytes}.{checksum}` for format evolution\n\n### 2. Authentication Service API\n\n```typescript\ninterface AuthService {\n  // Primary flows\n  login(credentials: LoginRequest): Promise<AuthResponse>\n  logout(refreshToken: string, allDevices: boolean): Promise<void>\n  refreshAccessToken(refreshToken: string, fingerprint: string): Promise<AuthResponse>\n  \n  // Token management\n  revokeToken(tokenId: string, reason: RevocationReason): Promise<void>\n  revokeAllUserTokens(userId: string, exceptSessionId?: string): Promise<void>\n  revokeDeviceTokens(userId: string, deviceId: string): Promise<void>\n  \n  // Session management\n  listActiveSessions(userId: string): Promise<Session[]>\n  validateAccessToken(token: string): Promise<TokenValidation>\n  \n  // Security operations\n  reportSuspiciousActivity(userId: string, activity: SecurityEvent): Promise<void>\n  checkTokenHealth(tokenId: string): Promise<TokenHealth>\n}\n\ninterface LoginRequest {\n  email: string\n  password: string\n  deviceInfo: {\n    deviceId?: string  // Client-generated, stored in localStorage\n    deviceName: string // \"Chrome on MacOS\"\n    deviceType: 'mobile' | 'desktop' | 'tablet'\n    userAgent: string\n    fingerprint: string // Browser/device fingerprint for binding\n  }\n  mfaCode?: string\n  captchaToken?: string // Required after failed attempts\n}\n\ninterface AuthResponse {\n  accessToken: string  // Returned in JSON body\n  refreshToken: string // Set as httpOnly cookie\n  expiresIn: number   // Access token lifetime in seconds\n  tokenType: 'Bearer'\n  refreshExpiresIn: number // When refresh token expires\n  user: {\n    id: string\n    email: string\n    roles: string[]\n    mfaEnabled: boolean\n  }\n}\n\ninterface Session {\n  sessionId: string\n  deviceId: string\n  deviceName: string\n  deviceType: string\n  createdAt: Date\n  lastUsedAt: Date\n  expiresAt: Date\n  ipAddress: string\n  location?: {\n    country: string\n    city: string\n  }\n  current: boolean\n  trustLevel: 'trusted' | 'suspicious' | 'compromised'\n}\n\ninterface TokenHealth {\n  valid: boolean\n  timesRefreshed: number\n  ageInDays: number\n  riskScore: number // 0-100\n  issues: string[]\n}\n\nenum RevocationReason {\n  USER_LOGOUT = 'user_logout',\n  ADMIN_REVOKE = 'admin_revoke',\n  TOKEN_REUSE = 'token_reuse',\n  SUSPICIOUS_ACTIVITY = 'suspicious_activity',\n  PASSWORD_CHANGE = 'password_change',\n  ACCOUNT_DELETION = 'account_deletion',\n  SESSION_TIMEOUT = 'session_timeout'\n}\n```\n\n### 3. Data Models\n\n**Users Table (Postgres)**\n```sql\nCREATE TABLE users (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email VARCHAR(255) UNIQUE NOT NULL,\n  password_hash TEXT NOT NULL,\n  roles TEXT[] NOT NULL DEFAULT '{\"user\"}',\n  mfa_enabled BOOLEAN DEFAULT false,\n  mfa_secret TEXT,\n  token_version INTEGER NOT NULL DEFAULT 1,\n  failed_login_attempts INTEGER DEFAULT 0,\n  locked_until TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW(),\n  last_login_at TIMESTAMP,\n  last_login_ip INET,\n  password_changed_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_users_locked_until ON users(locked_until) WHERE locked_until IS NOT NULL;\n```\n\n**Refresh Tokens Table (Postgres)**\n```sql\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n  token_hash TEXT NOT NULL UNIQUE,\n  token_family_id UUID NOT NULL, -- Groups tokens from same initial login\n  session_id UUID NOT NULL,\n  device_id UUID NOT NULL,\n  device_name VARCHAR(255),\n  device_type VARCHAR(50),\n  device_fingerprint_hash TEXT NOT NULL, -- For additional binding\n  user_agent TEXT,\n  ip_address INET,\n  location JSONB, -- {country, city, region}\n  created_at TIMESTAMP DEFAULT NOW(),\n  last_used_at TIMESTAMP DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  times_refreshed INTEGER DEFAULT 0,\n  revoked BOOLEAN DEFAULT false,\n  revoked_at TIMESTAMP,\n  revoked_reason VARCHAR(100),\n  parent_token_id UUID REFERENCES refresh_tokens(id),\n  CONSTRAINT check_not_self_parent CHECK (id != parent_token_id)\n);\n\nCREATE INDEX idx_refresh_tokens_user_id ON refresh_tokens(user_id);\nCREATE INDEX idx_refresh_tokens_token_hash ON refresh_tokens(token_hash);\nCREATE INDEX idx_refresh_tokens_session_id ON refresh_tokens(session_id);\nCREATE INDEX idx_refresh_tokens_token_family_id ON refresh_tokens(token_family_id);\nCREATE INDEX idx_refresh_tokens_expires_at ON refresh_tokens(expires_at) WHERE revoked = false;\nCREATE INDEX idx_refresh_tokens_device_id ON refresh_tokens(device_id, user_id);\n```\n\n**Security Events Table (Postgres)**\n```sql\nCREATE TABLE security_events (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID REFERENCES users(id) ON DELETE CASCADE,\n  event_type VARCHAR(100) NOT NULL,\n  severity VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'\n  details JSONB NOT NULL,\n  ip_address INET,\n  user_agent TEXT,\n  session_id UUID,\n  device_id UUID,\n  created_at TIMESTAMP DEFAULT NOW(),\n  resolved BOOLEAN DEFAULT false,\n  resolved_at TIMESTAMP\n);\n\nCREATE INDEX idx_security_events_user_id ON security_events(user_id, created_at DESC);\nCREATE INDEX idx_security_events_type ON security_events(event_type, created_at DESC);\nCREATE INDEX idx_security_events_severity ON security_events(severity) WHERE resolved = false;\n```\n\n**Token Blacklist (Redis)**\n```\nKey: blacklist:jti:{jti}\nValue: {\n  userId: string,\n  revokedAt: timestamp,\n  reason: string,\n  sessionId: string\n}\nTTL: Access token remaining lifetime\n```\n\n**Refresh Token Cache (Redis)**\n```\nKey: refresh:{token_hash}\nValue: {\n  userId: string,\n  sessionId: string,\n  deviceId: string,\n  fingerprint: string,\n  expiresAt: timestamp,\n  used: boolean,\n  usedAt?: timestamp,\n  familyId: string\n}\nTTL: 30 days\n```\n\n**Concurrent Refresh Lock (Redis)**\n```\nKey: refresh_lock:{token_hash}\nValue: {requestId: string, timestamp: number}\nTTL: 5 seconds\n```\n\n**Rate Limiting (Redis)**\n```\nKey patterns:\n  - ratelimit:login:{ip}:{email}\n  - ratelimit:refresh:{session_id}\n  - ratelimit:mfa:{session_id}\nValue: counter\nTTL: window duration\n```\n\n## Security Mechanisms\n\n### 1. CSRF Protection\n\n**Token Binding**\n- Refresh tokens bound to specific session/device via cookies\n- SameSite=Strict prevents cross-site cookie transmission\n- Access tokens transmitted via Authorization header only\n- Device fingerprint validated on each refresh\n\n**Cookie Security Attributes**\n```http\nSet-Cookie: refresh_token={token}; \n  HttpOnly; \n  Secure; \n  SameSite=Strict; \n  Path=/auth/refresh; \n  Max-Age=2592000;\n  Domain=.example.com\n```\n\n### 2. XSS Protection\n\n**Access Token Storage**\n- Never store in localStorage or sessionStorage\n- Keep in memory only (React state, Zustand, Redux)\n- Automatically lost on page refresh (forces token refresh)\n- Consider Web Workers for additional isolation\n\n**Cookie Security**\n- httpOnly flag prevents JavaScript access\n- Secure flag ensures HTTPS-only transmission\n- SameSite=Strict prevents CSRF\n- Path restriction to /auth/refresh endpoint only\n\n**Content Security Policy**\n```http\nContent-Security-Policy: \n  default-src 'self'; \n  script-src 'self' 'strict-dynamic'; \n  object-src 'none'; \n  base-uri 'self';\n```\n\n### 3. Replay Attack Prevention\n\n**Access Tokens**\n- Short lifetime (15 minutes) limits replay window\n- JTI (JWT ID) tracked in blacklist for immediate revocation\n- Token version in payload (incremented on password change)\n- Optional: Include request-specific nonce for critical operations\n\n**Refresh Tokens**\n- One-time use with automatic rotation\n- Immediate revocation chain if reuse detected\n- Parent token tracking for fraud detection\n- Device fingerprint validation prevents theft across devices\n- Concurrent refresh protection with distributed locks\n\n**Enhanced Reuse Detection Algorithm**\n```\nOn refresh token use:\n  1. Acquire distributed lock (Redis) with 5s timeout\n  2. Check token not already marked as used (atomic check-and-set)\n  3. If already used:\n     a. Get token family (all tokens with same family_id)\n     b. Mark entire family as compromised\n     c. Revoke all tokens in family\n     d. Add all access tokens to blacklist\n     e. Create security event (CRITICAL severity)\n     f. Notify user via email + push notification\n     g. Return 401 with specific error code\n     h. Optionally: Lock account temporarily\n  4. Mark token as used with timestamp\n  5. Generate new token pair linked to family\n  6. Release lock\n```\n\n### 4. Token Rotation on Refresh\n\n```\nClient sends: Refresh Token A + Device Fingerprint\nServer:\n  1. Acquire refresh lock (Redis, 5s TTL)\n  2. Validate token A exists and not expired\n  3. Check token A not already used (Redis cache first, then DB)\n  4. Verify device fingerprint matches stored value\n  5. Validate IP address hasn't drastically changed (geo-location check)\n  6. If suspicious: require step-up authentication\n  7. Atomically mark token A as used in Redis\n  8. Generate new access token B (JWT)\n  9. Generate new refresh token C (random)\n  10. Store token C with:\n      - parent_token_id = A.id\n      - token_family_id = A.token_family_id\n      - times_refreshed = A.times_refreshed + 1\n  11. Update last_used_at for token A\n  12. Release refresh lock\n  13. Return access token B and refresh token C\n  14. Token A becomes permanently invalid\n```\n\n### 5. Token Family Compromise Detection\n\n**Indicators of Compromise**\n- Refresh token reuse detected\n- Token used from dramatically different location (different country)\n- Token used from different device fingerprint\n- Excessive refresh frequency (>10 per minute)\n- Token family age exceeds maximum (e.g., 90 days of continuous refresh)\n\n**Compromise Response**\n```typescript\nasync function handleTokenFamilyCompromise(\n  familyId: string, \n  reason: string\n): Promise<void> {\n  // 1. Get all tokens in family\n  const family = await getTokenFamily(familyId);\n  const userId = family[0].user_id;\n  \n  // 2. Revoke entire family\n  await db.query(\n    'UPDATE refresh_tokens SET revoked = true, revoked_at = NOW(), ' +\n    'revoked_reason = $1 WHERE token_family_id = $2',\n    [reason, familyId]\n  );\n  \n  // 3. Blacklist all current access tokens for this session\n  const sessions = family.map(t => t.session_id);\n  await blacklistActiveTokensForSessions(sessions);\n  \n  // 4. Create security event\n  await createSecurityEvent({\n    userId,\n    eventType: 'TOKEN_FAMILY_COMPROMISED',\n    severity: 'CRITICAL',\n    details: {\n      familyId,\n      reason,\n      tokensRevoked: family.length,\n      sessions: sessions.length\n    }\n  });\n  \n  // 5. Notify user\n  await notifyUser(userId, {\n    type: 'SECURITY_ALERT',\n    title: 'Security Alert: Suspicious Activity Detected',\n    message: 'Your session was terminated due to suspicious activity. ' +\n             'Please log in again and review your recent sessions.',\n    action: 'REVIEW_SESSIONS'\n  });\n  \n  // 6. Optionally lock account for brief period\n  if (reason === 'TOKEN_REUSE') {\n    await lockAccountTemporarily(userId, 300); // 5 minutes\n  }\n}\n```\n\n### 6. Device Fingerprinting\n\n**Fingerprint Generation (Client)**\n```typescript\nasync function generateDeviceFingerprint(): Promise<string> {\n  const components = {\n    userAgent: navigator.userAgent,\n    language: navigator.language,\n    colorDepth: screen.colorDepth,\n    deviceMemory: navigator.deviceMemory,\n    hardwareConcurrency: navigator.hardwareConcurrency,\n    screenResolution: `${screen.width}x${screen.height}`,\n    timezone: Intl.DateTimeFormat().resolvedOptions().timeZone,\n    platform: navigator.platform,\n    // Canvas fingerprint (for desktop browsers)\n    canvas: await getCanvasFingerprint(),\n    // WebGL fingerprint\n    webgl: getWebGLFingerprint(),\n    // Audio context fingerprint\n    audio: await getAudioFingerprint()\n  };\n  \n  const fingerprintString = JSON.stringify(components);\n  const hash = await crypto.subtle.digest(\n    'SHA-256',\n    new TextEncoder().encode(fingerprintString)\n  );\n  \n  return Array.from(new Uint8Array(hash))\n    .map(b => b.toString(16).padStart(2, '0'))\n    .join('');\n}\n```\n\n**Fingerprint Validation (Server)**\n```typescript\nfunction validateFingerprint(\n  storedHash: string, \n  providedFingerprint: string\n): ValidationResult {\n  // Exact match required for refresh\n  if (storedHash === providedFingerprint) {\n    return { valid: true, confidence: 'high' };\n  }\n  \n  // Fingerprint changed - potential security issue\n  return { \n    valid: false, \n    confidence: 'low',\n    action: 'require_reauth'\n  };\n}\n```\n\n### 7. Concurrent Refresh Protection\n\n**Problem**: Multiple tabs/requests refresh simultaneously, causing race conditions\n\n**Solution**: Distributed locking with grace period\n\n```typescript\nasync function handleConcurrentRefresh(\n  refreshToken: string,\n  requestId: string\n): Promise<AuthResponse> {\n  const lockKey = `refresh_lock:${hashToken(refreshToken)}`;\n  const graceKey = `refresh_grace:${hashToken(refreshToken)}`;\n  \n  // Try to acquire lock\n  const lockAcquired = await redis.set(\n    lockKey,\n    JSON.stringify({ requestId, timestamp: Date.now() }),\n    'EX', 5, // 5 second expiry\n    'NX'    // Only set if not exists\n  );\n  \n  if (lockAcquired) {\n    // This request won the race - perform refresh\n    try {\n      const result = await performTokenRefresh(refreshToken);\n      \n      // Store result in grace period cache (5s)\n      await redis.setex(\n        graceKey,\n        5,\n        JSON.stringify(result)\n      );\n      \n      return result;\n    } finally {\n      await redis.del(lockKey);\n    }\n  } else {\n    // Another request is refreshing - wait and check grace cache\n    await sleep(100); // Brief wait\n    \n    const cachedResult = await redis.get(graceKey);\n    if (cachedResult) {\n      // Another tab already refreshed - use same result\n      return JSON.parse(cachedResult);\n    }\n    \n    // Lock expired or grace period passed - retry\n    if (retryCount < 3) {\n      return handleConcurrentRefresh(refreshToken, requestId);\n    }\n    \n    throw new Error('CONCURRENT_REFRESH_TIMEOUT');\n  }\n}\n```\n\n## Authentication Flows\n\n### Login Flow\n\n```\n1. Client \u2192 POST /auth/login\n   Body: { email, password, deviceInfo, captchaToken? }\n   \n2. Server preprocessing:\n   - Check rate limit: 5 attempts per 15 min per (IP + email)\n   - Validate CAPTCHA if required (after 3 failed attempts)\n   - Check if account locked (locked_until > NOW())\n\n3. Credential validation:\n   - Lookup user by email (constant-time to prevent enumeration)\n   - Verify password hash (Argon2id)\n   - On failure:\n     * Increment failed_login_attempts\n     * If attempts >= 5: Set locked_until (15 min)\n     * Return generic error (constant delay)\n   - On success:\n     * Reset failed_login_attempts to 0\n     * Clear locked_until\n\n4. If MFA enabled:\n   - Generate temporary token (5 min expiry)\n   - Return { requiresMfa: true, tempToken }\n   - Client \u2192 POST /auth/mfa/verify\n   - Body: { tempToken, mfaCode }\n   - Validate TOTP code (with time-window tolerance)\n   - Rate limit: 3 attempts per 5 min\n\n5. Generate tokens:\n   - Create token_family_id (UUID) for this login session\n   - Create session_id (UUID)\n   - Generate access token (JWT, RS256, 15 min)\n   - Generate refresh token (crypto.randomBytes(32))\n   - Hash refresh token (SHA-256) before storage\n   - Store in Postgres + Redis (write-through)\n   - Hash device fingerprint for storage\n\n6. Response:\n   - Set-Cookie: refresh_token (httpOnly, Secure, SameSite=Strict)\n   - Body: { \n       accessToken, \n       expiresIn: 900,\n       refreshExpiresIn: 2592000,\n       tokenType: 'Bearer',\n       user: { id, email, roles, mfaEnabled }\n     }\n\n7. Update user record:\n   - Set last_login_at = NOW()\n   - Set last_login_ip = client IP\n\n8. Create security event (audit log):\n   - Type: 'USER_LOGIN'\n   - Details: device info, IP, location\n\n9. Client stores:\n   - Access token in memory (React state)\n   - Device ID in localStorage\n   - Device fingerprint in memory (regenerated on load)\n```\n\n### Token Refresh Flow with Locking\n\n```\n1. Client detects access token expiring soon\n   - Proactive refresh at 80% lifetime (12 min)\n   - Or reactive on 401 Unauthorized\n   - Generate request ID for deduplication\n\n2. Client \u2192 POST /auth/refresh\n   Headers: Cookie: refresh_token={token}\n   Body: { deviceId, fingerprint, requestId }\n\n3. Server validation (with distributed lock):\n   - Acquire refresh lock (5s timeout)\n   - Extract refresh token from cookie\n   - Hash token and lookup in Redis cache\n   - If Redis miss: Query Postgres (cache miss)\n   - Validate:\n     * Token exists\n     * Not expired (expires_at > NOW())\n     * Not revoked (revoked = false)\n     * Not already used (used = false)\n     * Device ID matches stored device_id\n     * Fingerprint matches stored fingerprint_hash\n     * IP geo-location hasn't changed drastically\n\n4. Suspicious activity checks:\n   - If IP country changed: Require step-up auth\n   - If fingerprint mismatch: Reject + alert\n   - If times_refreshed > 5000: Force re-login\n   - If token age > 90 days: Force re-login\n\n5. Detect reuse (atomic check):\n   - Check Redis: refresh:{token_hash}.used\n   - If true \u2192 trigger compromise response:\n     * Release lock immediately\n     * Revoke token family\n     * Blacklist active tokens\n     * Send alert\n     * Return 401 with TOKEN_REUSE error\n\n6. Generate new tokens (atomic operation):\n   - Mark old refresh token as used (Redis + DB)\n   - Create new access token (JWT)\n   - Create new refresh token (random)\n   - Link via parent_token_id and token_family_id\n   - Increment times_refreshed counter\n   - Update last_used_at timestamp\n   - Store in Redis + Postgres\n\n7. Release distributed lock\n\n8. Response:\n   - Set-Cookie: new refresh_token\n   - Body: { \n       accessToken, \n       expiresIn: 900,\n       tokenType: 'Bearer'\n     }\n\n9. Background tasks (async):\n   - Update session last_used_at\n   - Clean up expired tokens (if not done recently)\n   - Check token health metrics\n```\n\n### Logout Flow\n\n```\nSingle Device Logout:\n1. Client \u2192 POST /auth/logout\n   Headers: \n     - Authorization: Bearer {accessToken}\n     - Cookie: refresh_token={token}\n\n2. Server:\n   - Extract JTI from access token\n   - Calculate remaining TTL\n   - Add to Redis blacklist with TTL\n   - Extract refresh token from cookie\n   - Mark refresh token as revoked (reason: 'user_logout')\n   - Delete from Redis cache\n   - Clear cookies in response (Max-Age=0)\n   - Create security event (audit log)\n\n3. Response:\n   - 200 OK\n   - Clear-Site-Data header: \"cache\", \"cookies\", \"storage\"\n\nAll Devices Logout:\n1. Client \u2192 POST /auth/logout?allDevices=true\n   Headers: \n     - Authorization: Bearer {accessToken}\n\n2. Server:\n   - Get user ID from access token\n   - Query all active refresh tokens for user\n   - Get current session from token (to exclude if desired)\n   - Revoke all tokens (revoked = true, reason = 'user_logout_all')\n   - Extract all active JTIs from sessions\n   - Add all JTIs to Redis blacklist\n   - Clear all Redis cache entries for user\n   - Create security event\n\n3. Response:\n   - 200 OK with count of sessions terminated\n   - Body: { sessionsClosed: 5 }\n\n4. Notify other devices (via push notification if available)\n```\n\n### Token Validation Middleware\n\n```typescript\nasync function validateAccessToken(req, res, next) {\n  try {\n    // 1. Extract token\n    const authHeader = req.headers.authorization;\n    if (!authHeader?.startsWith('Bearer ')) {\n      return res.status(401).json({ \n        error: 'missing_token',\n        message: 'Authorization header required' \n      });\n    }\n    \n    const token = authHeader.substring(7);\n    \n    // 2. Verify JWT signature and expiration\n    const payload = jwt.verify(token, getPublicKey(token), {\n      algorithms: ['RS256'],\n      issuer: 'auth.example.com',\n      audience: 'api.example.com',\n      clockTolerance: 10 // 10 second tolerance\n    });\n    \n    // 3. Check blacklist (Redis - fast path)\n    const blacklistKey = `blacklist:jti:${payload.jti}`;\n    const isBlacklisted = await redis.exists(blacklistKey);\n    if (isBlacklisted) {\n      const reason = await redis.hget(blacklistKey, 'reason');\n      return res.status(401).json({ \n        error: 'token_revoked',\n        reason: reason,\n        message: 'Token has been revoked'\n      });\n    }\n    \n    // 4. Fetch user and validate token version\n    const user = await getUserById(payload.sub);\n    if (!user) {\n      return res.status(401).json({ error: 'user_not_found' });\n    }\n    \n    // Check if token version matches (incremented on password change)\n    if (payload.token_version !== user.token_version) {\n      return res.status(401).json({ \n        error: 'token_invalidated',\n        message: 'Token version mismatch - password may have changed'\n      });\n    }\n    \n    // 5. Check if password changed after token issued\n    if (user.password_changed_at > new Date(payload.iat * 1000)) {\n      return res.status(401).json({ \n        error: 'token_invalidated',\n        message: 'Password changed - please login again'\n      });\n    }\n    \n    // 6. Attach user context to request\n    req.user = {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions,\n      sessionId: payload.session_id,\n      deviceId: payload.device_id,\n      tokenVersion: payload.token_version\n    };\n    \n    // 7. Optional: Record token usage for analytics\n    await recordTokenUsage(payload.jti, req.ip);\n    \n    next();\n  } catch (error) {\n    if (error.name === 'TokenExpiredError') {\n      return res.status(401).json({ \n        error: 'token_expired',\n        message: 'Access token expired',\n        expiredAt: error.expiredAt\n      });\n    }\n    \n    if (error.name === 'JsonWebTokenError') {\n      return res.status(401).json({ \n        error: 'invalid_token',\n        message: 'Token signature invalid'\n      });\n    }\n    \n    // Log unexpected errors\n    logger.error('Token validation error', { error, token: token.substring(0, 20) });\n    \n    return res.status(401).json({ \n      error: 'invalid_token',\n      message: 'Token validation failed'\n    });\n  }\n}\n\n// Helper to get correct public key based on kid in JWT header\nfunction getPublicKey(token: string): string {\n  const decoded = jwt.decode(token, { complete: true });\n  const kid = decoded?.header?.kid;\n  \n  if (!kid) {\n    return process.env.JWT_PUBLIC_KEY; // Default key\n  }\n  \n  // Fetch from key store (support key rotation)\n  return keyStore.getPublicKey(kid);\n}\n```\n\n## Error Handling\n\n### Error Codes\n\n```typescript\nenum AuthErrorCode {\n  // Authentication errors\n  INVALID_CREDENTIALS = 'invalid_credentials',\n  ACCOUNT_LOCKED = 'account_locked',\n  ACCOUNT_SUSPENDED = 'account_suspended',\n  ACCOUNT_DELETED = 'account_deleted',\n  MFA_REQUIRED = 'mfa_required',\n  INVALID_MFA_CODE = 'invalid_mfa_code',\n  MFA_RATE_LIMITED = 'mfa_rate_limited',\n  \n  // Token errors\n  TOKEN_EXPIRED = 'token_expired',\n  TOKEN_REVOKED = 'token_revoked',\n  TOKEN_REUSE_DETECTED = 'token_reuse_detected',\n  TOKEN_INVALIDATED = 'token_invalidated', // Password change, etc.\n  INVALID_TOKEN = 'invalid_token',\n  MISSING_TOKEN = 'missing_token',\n  TOKEN_VERSION_MISMATCH = 'token_version_mismatch',\n  \n  // Session errors\n  SESSION_EXPIRED = 'session_expired',\n  DEVICE_MISMATCH = 'device_mismatch',\n  FINGERPRINT_MISMATCH = 'fingerprint_mismatch',\n  SUSPICIOUS_ACTIVITY = 'suspicious_activity',\n  SESSION_LIMIT_EXCEEDED = 'session_limit_exceeded',\n  CONCURRENT_REFRESH_TIMEOUT = 'concurrent_refresh_timeout',\n  \n  // Rate limiting\n  TOO_MANY_ATTEMPTS = 'too_many_attempts',\n  RATE_LIMIT_EXCEEDED = 'rate_limit_exceeded',\n  \n  // Geo/security\n  LOCATION_CHANGE_DETECTED = 'location_change_detected',\n  STEP_UP_AUTH_REQUIRED = 'step_up_auth_required',\n  \n  // System errors\n  SERVICE_UNAVAILABLE = 'service_unavailable',\n  INTERNAL_ERROR = 'internal_error'\n}\n\ninterface AuthError {\n  code: AuthErrorCode\n  message: string\n  details?: Record<string, any>\n  retryAfter?: number // For rate limiting (seconds)\n  requiresAction?: 'login' | 'mfa' | 'unlock_account' | 'verify_device'\n  timestamp: string\n  requestId: string // For support/debugging\n}\n```\n\n### Client Error Handling Strategy\n\n```typescript\n// Axios interceptor with retry and refresh logic\nlet isRefreshing = false;\nlet refreshSubscribers: Array<(token: string) => void> = [];\n\nfunction subscribeTokenRefresh(cb: (token: string) => void) {\n  refreshSubscribers.push(cb);\n}\n\nfunction onRefreshComplete(token: string) {\n  refreshSubscribers.forEach(cb => cb(token));\n  refreshSubscribers = [];\n}\n\naxios.interceptors.response.use(\n  response => response,\n  async error => {\n    const originalRequest = error.config;\n    const errorCode = error.response?.data?.error;\n    \n    // Handle 401 errors\n    if (error.response?.status === 401 && !originalRequest._retry) {\n      \n      // Token expired - attempt refresh\n      if (errorCode === 'token_expired') {\n        if (isRefreshing) {\n          // Wait for ongoing refresh to complete\n          return new Promise(resolve => {\n            subscribeTokenRefresh((token: string) => {\n              originalRequest.headers.Authorization = `Bearer ${token}`;\n              resolve(axios(originalRequest));\n            });\n          });\n        }\n        \n        originalRequest._retry = true;\n        isRefreshing = true;\n        \n        try {\n          const fingerprint = await generateDeviceFingerprint();\n          const deviceId = localStorage.getItem('device_id');\n          const requestId = generateRequestId();\n          \n          const { data } = await axios.post('/auth/refresh', {\n            deviceId,\n            fingerprint,\n            requestId\n          }, {\n            withCredentials: true // Include cookies\n          });\n          \n          const { accessToken } = data;\n          setAccessToken(accessToken);\n          \n          // Notify all waiting requests\n          onRefreshComplete(accessToken);\n          isRefreshing = false;\n          \n          // Retry original request\n          originalRequest.headers.Authorization = `Bearer ${accessToken}`;\n          return axios(originalRequest);\n          \n        } catch (refreshError) {\n          isRefreshing = false;\n          refreshSubscribers = [];\n          \n          // Refresh failed - redirect to login\n          handleAuthFailure(refreshError);\n          return Promise.reject(refreshError);\n        }\n      }\n      \n      // Token reuse detected - critical security event\n      if (errorCode === 'token_reuse_detected') {\n        clearAuth();\n        showSecurityAlert(\n          'Security Alert',\n          'Your session was terminated due to suspicious activity. ' +\n          'This may indicate your account was accessed from another device. ' +\n          'Please login again and review your recent sessions.'\n        );\n        window.location.href = '/login?reason=security';\n        return Promise.reject(error);\n      }\n      \n      // Token invalidated (password change, etc.)\n      if (errorCode === 'token_invalidated' || \n          errorCode === 'token_version_mismatch') {\n        clearAuth();\n        showAlert(\n          'Session Expired',\n          'Your password was changed. Please login again.'\n        );\n        window.location.href = '/login?reason=password_change';\n        return Promise.reject(error);\n      }\n      \n      // Device or fingerprint mismatch\n      if (errorCode === 'device_mismatch' || \n          errorCode === 'fingerprint_mismatch') {\n        clearAuth();\n        window.location.href = '/login/verify-device';\n        return Promise.reject(error);\n      }\n      \n      // Location change detected - step-up auth required\n      if (errorCode === 'location_change_detected' ||\n          errorCode === 'step_up_auth_required') {\n        window.location.href = '/auth/verify-location';\n        return Promise.reject(error);\n      }\n      \n      // Other 401 errors - redirect to login\n      clearAuth();\n      window.location.href = '/login';\n    }\n    \n    // Handle 429 rate limiting\n    if (error.response?.status === 429) {\n      const retryAfter = error.response.data?.retryAfter || 60;\n      \n      showAlert(\n        'Too Many Requests',\n        `Please wait ${retryAfter} seconds before trying again.`\n      );\n      \n      // Optional: Implement exponential backoff retry\n      if (originalRequest.retryCount < 3) {\n        await sleep(retryAfter * 1000);\n        originalRequest.retryCount = (originalRequest.retryCount || 0) + 1;\n        return axios(originalRequest);\n      }\n    }\n    \n    return Promise.reject(error);\n  }\n);\n\nfunction handleAuthFailure(error: any) {\n  const errorCode = error.response?.data?.error;\n  \n  clearAuth();\n  \n  // Show appropriate message based on error\n  const messages: Record<string, string> = {\n    'session_expired': 'Your session has expired. Please login again.',\n    'account_locked': 'Your account has been locked. Please contact support.',\n    'account_suspended': 'Your account has been suspended.',\n    'service_unavailable': 'Authentication service is temporarily unavailable.'\n  };\n  \n  const message = messages[errorCode] || 'Please login again.';\n  \n  window.location.href = `/login?reason=${errorCode}&message=${encodeURIComponent(message)}`;\n}\n\nfunction clearAuth() {\n  // Clear in-memory token\n  setAccessToken(null);\n  \n  // Note: refresh token in httpOnly cookie is cleared by server\n  // We can't access it from JavaScript\n  \n  // Clear any other auth-related data\n  sessionStorage.clear();\n}\n\nfunction generateRequestId(): string {\n  return `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n}\n```\n\n## Security Best Practices\n\n### 1. Key Management\n\n**Asymmetric Keys (RS256)**\n- Use RSA 4096-bit or ECDSA P-384 for signing\n- Store private keys in HSM, AWS KMS, or HashiCorp Vault\n- Never commit keys to version control\n- Separate keys per environment (dev, staging, prod)\n\n**Key Rotation**\n```typescript\ninterface KeyRotationStrategy {\n  // Rotate signing keys every 90 days\n  rotationInterval: 90 * 24 * 60 * 60 * 1000; // milliseconds\n  \n  // Support multiple active public keys simultaneously\n  activeKeys: {\n    kid: string;        // Key ID\n    publicKey: string;\n    createdAt: Date;\n    expiresAt: Date;\n    status: 'active' | 'retiring' | 'expired';\n  }[];\n  \n  // Grace period for old keys (7 days)\n  gracePeriod: 7 * 24 * 60 * 60 * 1000;\n}\n\n// Token includes kid in header for key selection\nconst token = jwt.sign(payload, privateKey, {\n  algorithm: 'RS256',\n  keyid: currentKey.kid,\n  issuer: 'auth.example.com',\n  audience: 'api.example.com'\n});\n```\n\n**Rotation Process**\n1. Generate new key pair\n2. Add public key to active key set with new `kid`\n3. Update signing service to use new private key\n4. Wait grace period (7 days)\n5. Remove old public key from active set\n6. Archive old key pair for audit purposes\n\n### 2. Password Security\n\n**Hashing Algorithm**\n```typescript\n// Use Argon2id (recommended) or bcrypt as fallback\nimport argon2 from 'argon2';\n\nasync function hashPassword(password: string): Promise<string> {\n  return argon2.hash(password, {\n    type: argon2.argon2id,\n    memoryCost: 65536,    // 64 MB\n    timeCost: 3,          // 3 iterations\n    parallelism: 4        // 4 threads\n  });\n}\n\nasync function verifyPassword(password: string, hash: string): Promise<boolean> {\n  try {\n    return await argon2.verify(hash, password);\n  } catch {\n    return false;\n  }\n}\n```\n\n**Password Policy**\n```typescript\ninterface PasswordPolicy {\n  minLength: 12;\n  requireUppercase: true;\n  requireLowercase: true;\n  requireNumbers: true;\n  requireSpecialChars: true;\n  preventCommonPasswords: true;\n  preventBreachedPasswords: true; // Check against HIBP\n  preventPasswordReuse: 5; // Last 5 passwords\n  maxAge: 90 * 24 * 60 * 60 * 1000; // 90 days\n}\n```\n\n**Breach Detection**\n```typescript\nimport { pwnedPassword } from 'hibp';\n\nasync function checkPasswordBreach(password: string): Promise<boolean> {\n  try {\n    const breachCount = await pwnedPassword(password);\n    return breachCount > 0;\n  } catch {\n    // Fail open - don't block user if API unavailable\n    return false;\n  }\n}\n```\n\n### 3. Rate Limiting Strategy\n\n**Multi-Layer Rate Limiting**\n\n```typescript\n// Layer 1: IP-based (stops brute force from single source)\nconst ipRateLimit = {\n  login: { max: 10, window: 900 },      // 10 per 15 min\n  refresh: { max: 50, window: 60 },     // 50 per min\n  mfa: { max: 5, window: 300 }          // 5 per 5 min\n};\n\n// Layer 2: Account-based (stops distributed attacks)\nconst accountRateLimit = {\n  login: { max: 5, window: 900 },       // 5 per 15 min\n  passwordReset: { max: 3, window: 3600 } // 3 per hour\n};\n\n// Layer 3: Session-based (stops token abuse)\nconst sessionRateLimit = {\n  refresh: { max: 10, window: 60 },     // 10 per min\n  apiCalls: { max: 100, window: 60 }    // 100 per min\n};\n\n// Implementation with Redis\nasync function checkRateLimit(\n  key: string, \n  max: number, \n  windowSec: number\n): Promise<{ allowed: boolean; retryAfter?: number }> {\n  const count = await redis.incr(key);\n  \n  if (count === 1) {\n    await redis.expire(key, windowSec);\n  }\n  \n  if (count > max) {\n    const ttl = await redis.ttl(key);\n    return { allowed: false, retryAfter: ttl };\n  }\n  \n  return { allowed: true };\n}\n```\n\n**Progressive Delays**\n```typescript\nfunction getLoginDelay(attemptCount: number): number {\n  // Exponential backoff: 1s, 2s, 4s, 8s, 16s, ...\n  return Math.min(Math.pow(2, attemptCount - 1) * 1000, 30000);\n}\n\nasync function handleFailedLogin(email: string, ip: string) {\n  const attempts = await incrementFailedAttempts(email, ip);\n  \n  if (attempts >= 3) {\n    // Require CAPTCHA\n    await setRequiresCaptcha(email, ip);\n  }\n  \n  if (attempts >= 5) {\n    // Lock account temporarily\n    await lockAccount(email, 900); // 15 minutes\n  }\n  \n  // Progressive delay before response\n  const delay = getLoginDelay(attempts);\n  await sleep(delay);\n}\n```\n\n### 4. Monitoring & Alerting\n\n**Security Events to Monitor**\n\n```typescript\nenum SecurityEventType {\n  // Authentication\n  LOGIN_SUCCESS = 'login_success',\n  LOGIN_FAILURE = 'login_failure',\n  LOGIN_LOCKED = 'login_locked',\n  LOGOUT = 'logout',\n  \n  // Token management\n  TOKEN_ISSUED = 'token_issued',\n  TOKEN_REFRESHED = 'token_refreshed',\n  TOKEN_REVOKED = 'token_revoked',\n  TOKEN_REUSE = 'token_reuse',\n  TOKEN_FAMILY_COMPROMISED = 'token_family_compromised',\n  \n  // Suspicious activity\n  LOCATION_CHANGE = 'location_change',\n  DEVICE_CHANGE = 'device_change',\n  FINGERPRINT_MISMATCH = 'fingerprint_mismatch',\n  EXCESSIVE_REFRESH = 'excessive_refresh',\n  CONCURRENT_SESSIONS_EXCEEDED = 'concurrent_sessions_exceeded',\n  \n  // Account changes\n  PASSWORD_CHANGED = 'password_changed',\n  MFA_ENABLED = 'mfa_enabled',\n  MFA_DISABLED = 'mfa_disabled',\n  EMAIL_CHANGED = 'email_changed',\n  \n  // Admin actions\n  ADMIN_PASSWORD_RESET = 'admin_password_reset',\n  ADMIN_ACCOUNT_LOCK = 'admin_account_lock',\n  ADMIN_TOKEN_REVOKE = 'admin_token_revoke'\n}\n\ninterface AlertRule {\n  eventType: SecurityEventType;\n  condition: (event: SecurityEvent) => boolean;\n  severity: 'low' | 'medium' | 'high' | 'critical';\n  actions: AlertAction[];\n}\n\n// Example alert rules\nconst alertRules: AlertRule[] = [\n  {\n    eventType: SecurityEventType.TOKEN_REUSE,\n    condition: () => true, // Always alert\n    severity: 'critical',\n    actions: ['email', 'slack', 'pagerduty']\n  },\n  {\n    eventType: SecurityEventType.LOGIN_FAILURE,\n    condition: (e) => e.details.attemptCount >= 5,\n    severity: 'high',\n    actions: ['email', 'log']\n  },\n  {\n    eventType: SecurityEventType.LOCATION_CHANGE,\n    condition: (e) => {\n      const { oldCountry, newCountry } = e.details;\n      return oldCountry !== newCountry;\n    },\n    severity: 'medium',\n    actions: ['email', 'log']\n  }\n];\n```\n\n**Metrics to Track**\n\n```typescript\ninterface AuthMetrics {\n  // Login metrics\n  loginAttempts: number;\n  loginSuccesses: number;\n  loginFailures: number;\n  mfaSuccesses: number;\n  mfaFailures: number;\n  \n  // Token metrics\n  tokensIssued: number;\n  tokensRefreshed: number;\n  tokensRevoked: number;\n  tokenReuseDetected: number;\n  \n  // Performance metrics\n  avgLoginDuration: number;\n  avgRefreshDuration: number;\n  avgTokenValidationDuration: number;\n  \n  // Security metrics\n  accountsLocked: number;\n  suspiciousActivityDetected: number;\n  concurrentSessionsExceeded: number;\n  \n  // Rate limiting\n  rateLimitHits: number;\n}\n\n// Export to monitoring system (Prometheus, DataDog, etc.)\n```\n\n### 5. Session Management\n\n**Concurrent Session Limits**\n\n```typescript\nconst MAX_CONCURRENT_SESSIONS = 5;\n\nasync function enforceSessionLimit(userId: string): Promise<void> {\n  // Get all active sessions for user\n  const sessions = await db.query(\n    'SELECT * FROM refresh_tokens ' +\n    'WHERE user_id = $1 AND revoked = false AND expires_at > NOW() ' +\n    'ORDER BY last_used_at DESC',\n    [userId]\n  );\n  \n  if (sessions.length >= MAX_CONCURRENT_SESSIONS) {\n    // Revoke oldest session(s)\n    const toRevoke = sessions.slice(MAX_CONCURRENT_SESSIONS - 1);\n    \n    for (const session of toRevoke) {\n      await revokeToken(session.id, 'SESSION_LIMIT_EXCEEDED');\n    }\n    \n    // Notify user\n    await notifyUser(userId, {\n      type: 'SESSION_LIMIT',\n      message: `Session limit reached. Oldest session(s) were terminated.`\n    });\n  }\n}\n```\n\n**Session Cleanup**\n\n```typescript\n// Daily cron job to clean up expired tokens\nasync function cleanupExpiredTokens(): Promise<void> {\n  const result = await db.query(\n    'DELETE FROM refresh_tokens ' +\n    'WHERE expires_at < NOW() - INTERVAL \\'7 days\\' ' +\n    'RETURNING id'\n  );\n  \n  logger.info(`Cleaned up ${result.rowCount} expired tokens`);\n  \n  // Also clean up old security events (keep 90 days)\n  await db.query(\n    'DELETE FROM security_events ' +\n    'WHERE created_at < NOW() - INTERVAL \\'90 days\\''\n  );\n}\n\n// Run every 24 hours at 3 AM\ncron.schedule('0 3 * * *', cleanupExpiredTokens);\n```\n\n**Session Health Monitoring**\n\n```typescript\nasync function monitorSessionHealth(sessionId: string): Promise<void> {\n  const session = await getSession(sessionId);\n  \n  const healthChecks = {\n    // Check if session is being refreshed too frequently\n    excessiveRefresh: session.times_refreshed > 5000,\n    \n    // Check if session is too old\n    tooOld: Date.now() - session.created_at.getTime() > 90 * 24 * 60 * 60 * 1000,\n    \n    // Check if location changed dramatically\n    locationChange: await checkLocationChange(sessionId),\n    \n    // Check if device fingerprint changed\n    fingerprintChange: await checkFingerprintChange(sessionId)\n  };\n  \n  if (Object.values(healthChecks).some(check => check)) {\n    await createSecurityEvent({\n      userId: session.user_id,\n      eventType: 'SESSION_HEALTH_ISSUE',\n      severity: 'MEDIUM',\n      details: healthChecks\n    });\n    \n    // Optionally force re-authentication\n    if (healthChecks.tooOld || healthChecks.fingerprintChange) {\n      await revokeToken(session.id, 'HEALTH_CHECK_FAILED');\n    }\n  }\n}\n```\n\n## Configuration\n\n```typescript\ninterface AuthConfig {\n  accessToken: {\n    lifetime: number;        // 900 seconds (15 min)\n    algorithm: 'RS256';\n    issuer: string;          // 'auth.example.com'\n    audience: string;        // 'api.example.com'\n    clockTolerance: number;  // 10 seconds\n  };\n  \n  refreshToken: {\n    lifetime: number;            // 2592000 seconds (30 days)\n    rotationEnabled: boolean;    // true\n    reuseDetection: boolean;     // true\n    maxAge: number;              // 90 days (force reauth after)\n    maxRefreshCount: number;     // 5000 (force reauth after)\n  };\n  \n  security: {\n    passwordHasher: 'argon2id' | 'bcrypt';\n    argon2MemoryCost: number;    // 65536 (64 MB)\n    argon2TimeCost: number;      // 3\n    argon2Parallelism: number;   // 4\n    bcryptRounds: number;        // 12 (fallback)\n    \n    maxSessions: number;         // 5\n    requireMfa: boolean;         // false (per-user)\n    allowedOrigins: string[];\n    \n    fingerprintValidation: boolean; // true\n    ipValidation: 'strict' | 'moderate' | 'none'; // 'moderate'\n    geoLocationTracking: boolean;   // true\n    \n    tokenVersioning: boolean;    // true\n  };\n  \n  rateLimit: {\n    login: { max: 5, window: 900 };        // 15 min\n    loginPerIp: { max: 10, window: 900 };  // 15 min\n    refresh: { max: 10, window: 60 };      // 1 min\n    refreshPerSession: { max: 10, window: 60 };\n    mfa: { max: 3, window: 300 };          // 5 min\n    passwordReset: { max: 3, window: 3600 }; // 1 hour\n  };\n  \n  cookies: {\n    domain: string;              // '.example.com'\n    secure: boolean;             // true in production\n    sameSite: 'Strict' | 'Lax';  // 'Strict'\n    httpOnly: boolean;           // true\n    path: string;                // '/auth/refresh'\n  };\n  \n  notifications: {\n    emailOnNewDevice: boolean;   // true\n    emailOnLocationChange: boolean; // true\n    emailOnPasswordChange: boolean; // true\n    emailOnMfaChange: boolean;   // true\n    pushNotificationOnLogout: boolean; // true\n  };\n  \n  monitoring: {\n    logLevel: 'debug' | 'info' | 'warn' | 'error';\n    logAuthEvents: boolean;      // true\n    logSecurityEvents: boolean;  // true\n    metricsEnabled: boolean;     // true\n    metricsInterval: number;     // 60 seconds\n  };\n}\n```\n\n## What I Improved\n\n### 1. **Token Reuse Detection Robustness**\n- Added **distributed locking** with Redis to prevent race conditions during concurrent refresh attempts\n- Implemented **atomic check-and-set** operations to ensure token usage state is consistent\n- Added **grace period caching** to handle legitimate concurrent refreshes from multiple tabs\n- Included **token family ID** to group related tokens and enable full chain revocation\n\n### 2. **Device Fingerprinting**\n- Added comprehensive device fingerprinting (canvas, WebGL, audio) beyond just device ID\n- Fingerprint is **validated on every refresh** to detect device theft\n- Stored as hash for privacy protection\n- Provides stronger binding than device ID alone\n\n### 3. **Token Versioning**\n- Added `token_version` field to users table and JWT payload\n- Incremented on password change, allowing instant invalidation of all tokens\n- More efficient than checking `password_changed_at` timestamp\n\n### 4. **Enhanced Security Event Tracking**\n- Added dedicated `security_events` table with severity levels\n- Comprehensive event types covering all security scenarios\n- Automated alerting rules based on event patterns\n- Better audit trail for compliance and forensics\n\n### 5. **Concurrent Refresh Protection**\n- Solved the \"multiple tabs refreshing simultaneously\" race condition\n- Uses distributed locks with requestId for deduplication\n- Grace period cache allows sharing refresh results across concurrent requests\n- Prevents token reuse false positives\n\n### 6. **Session Health Monitoring**\n- Track `times_refreshed` counter to detect excessive refresh patterns\n- Monitor session age and force re-auth after 90 days\n- Geographic location tracking for suspicious movement detection\n- Proactive security checks before issues escalate\n\n### 7. **Improved Rate Limiting**\n- Multi-layer rate limiting (IP, account, session)\n- Progressive delays on failed attempts\n- CAPTCHA requirement after threshold\n- Prevents both single-source and distributed attacks\n\n### 8. **Token Family Compromise Handling**\n- Full token family revocation on reuse detection\n- Tracks parent-child relationships via `parent_token_id`\n- Prevents attacker from using any token in compromised chain\n- Automatic account lock option for high-severity events\n\n### 9. **Client-Side Error Handling**\n- Comprehensive error code taxonomy\n- Specific handling for each error type (reuse, invalidation, location change)\n- Request deduplication to prevent refresh storms\n- Clear user messaging for security events\n\n### 10. **Key Rotation Strategy**\n- Support for multiple active keys simultaneously (kid-based selection)\n- 90-day rotation schedule with 7-day grace period\n- Documented rotation process for zero-downtime updates\n- HSM/KMS integration guidance\n\n## What I Kept\n\nThe architect's design had several excellent foundational elements:\n\n1. **15-minute access token lifetime** - Perfect balance of security and UX\n2. **Opaque refresh tokens** - Correct choice over JWT for revocability\n3. **httpOnly cookies with SameSite=Strict** - Industry best practice\n4. **Memory-only access token storage** - Proper XSS protection\n5. **Redis + Postgres hybrid** - Smart caching strategy\n6. **RS256 asymmetric signing** - Good for microservices architecture\n7. **Parent token tracking** - Foundation for token family detection\n8. **Comprehensive data model** - Well-structured tables with proper indexes\n9. **Device binding concept** - Good security layer\n10. **Clear separation of concerns** - Auth service, token store, application services\n\n## Remaining Concerns\n\n### 1. **Token Payload Size**\nThe JWT includes roles, permissions, device_id, session_id, and token_version. For users with many permissions, this could exceed typical header size limits (8KB). Consider:\n- Fetching permissions separately after token validation\n- Using role-based access control (RBAC) with permission lookup\n- Compressing permission strings\n\n### 2. **Offline Mobile Apps**\nIf a mobile app is offline for >30 days, the refresh token expires and user must re-authenticate. This could be frustrating for apps used intermittently. Potential solutions:\n- Longer refresh token lifetime for mobile (configurable)\n- Background token refresh when device comes online\n- Biometric re-auth instead of full login\n\n### 3. **Cross-Domain Authentication**\nThe design uses cookies with SameSite=Strict, which prevents cross-domain authentication. For multi-domain scenarios (e.g., app.example.com and admin.example.com), consider:\n- Shared authentication domain (auth.example.com)\n- Token exchange mechanism for cross-domain\n- SameSite=Lax with additional CSRF protection\n\n### 4. **Redis Failure Scenarios**\nThe system relies heavily on Redis for:\n- Token blacklist lookups\n- Refresh token cache\n- Rate limiting\n- Distributed locks\n\nIf Redis becomes unavailable:\n- Access token validation fails (blacklist check)\n- Refresh operations may fail (lock acquisition)\n- Rate limiting disabled\n\nConsider fallback strategies:\n- Fallback to Postgres for critical operations (slower but available)\n- Circuit breaker pattern for Redis operations\n- Graceful degradation (temporarily accept slightly elevated risk)\n\n### 5. **Token Fingerprint Privacy**\nDevice fingerprinting can be privacy-invasive. Browser fingerprinting techniques may be blocked by privacy-focused browsers (Brave, Firefox with resistFingerprinting). Consider:\n- Make fingerprinting opt-in for privacy-conscious users\n- Fallback to weaker binding (IP + User-Agent) when fingerprinting unavailable\n- Document fingerprinting in privacy policy\n\n### 6. **Database Write Amplification**\nEvery token refresh writes to:\n- Postgres (new token row, update old token)\n- Redis (cache new token, mark old as used)\n\nAt scale with millions of users, this creates significant write load. Optimizations:\n- Batch token updates during off-peak hours\n- Use write-behind caching for non-critical updates (last_used_at)\n- Consider token refresh throttling (don't allow refresh more than once per minute)\n\n### 7. **Geographic Location Detection**\nIP-based geolocation has limitations:\n- VPN users appear in different countries frequently\n- Mobile users on cellular networks may have inconsistent IPs\n- CDN/proxy services can mask true location\n\nNeed clear policy on when to flag location changes:\n- Same country, different city: Log but allow\n- Different country: Challenge or notify\n- Multiple countries in short time: Block and require verification\n\n## Convergence Status\n\n**ITERATING** - This is the first review. The design has strong foundations but would benefit from another iteration to address:\n- Scaling concerns (Redis/DB write load)\n- Edge cases (offline mobile, Redis failure, fingerprint blocking)\n- Privacy considerations (fingerprinting disclosure)\n- Cross-domain authentication requirements\n\nThe token reuse detection is now robust with distributed locking, but we should validate the performance characteristics under high concurrency.",
        "prompt_for_other": "Please address the **Redis failure scenarios** and propose a fallback strategy that maintains security guarantees when Redis is unavailable. Specifically:\n\n1. How should token validation proceed if the blacklist (Redis) is unreachable?\n2. What's the fallback for distributed locking during refresh operations?\n3. How do we handle rate limiting when Redis counters aren't accessible?\n\nAlso, clarify the **geographic location policy**: What specific location changes should trigger user notification vs. blocking vs. requiring step-up authentication? Consider VPN users and mobile users with dynamic IPs.",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# JWT Authentication System with Refresh Tokens\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502  (Redis)    \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                                  \u25bc\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502  Database   \u2502\n                                           \u2502  (Postgres) \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Core Components\n\n### 1. Token Types\n\n**Access Token (JWT)**\n- **Lifetime**: 15 minutes\n- **Storage**: Memory only (never localStorage)\n- **Payload**:\n```json\n{\n  \"sub\": \"user_id\",\n  \"email\": \"user@example.com\",\n  \"roles\": [\"user\", \"admin\"],\n  \"permissions\": [\"read:profile\", \"write:posts\"],\n  \"device_id\": \"uuid-v4\",\n  \"session_id\": \"uuid-v4\",\n  \"token_version\": 1,\n  \"iat\": 1234567890,\n  \"exp\": 1234568790,\n  \"jti\": \"unique-token-id\",\n  \"aud\": \"api.example.com\",\n  \"iss\": \"auth.example.com\"\n}\n```\n\n**Refresh Token**\n- **Lifetime**: 30 days (configurable)\n- **Storage**: Secure, httpOnly cookie with SameSite=Strict\n- **Format**: Opaque token (cryptographically random, 256 bits)\n- **One-time use**: Invalidated after refresh, new token issued\n- **Structure**: `{version}.{randomBytes}.{checksum}` for format evolution\n\n### 2. Authentication Service API\n\n```typescript\ninterface AuthService {\n  // Primary flows\n  login(credentials: LoginRequest): Promise<AuthResponse>\n  logout(refreshToken: string, allDevices: boolean): Promise<void>\n  refreshAccessToken(refreshToken: string, fingerprint: string): Promise<AuthResponse>\n  \n  // Token management\n  revokeToken(tokenId: string, reason: RevocationReason): Promise<void>\n  revokeAllUserTokens(userId: string, exceptSessionId?: string): Promise<void>\n  revokeDeviceTokens(userId: string, deviceId: string): Promise<void>\n  \n  // Session management\n  listActiveSessions(userId: string): Promise<Session[]>\n  validateAccessToken(token: string): Promise<TokenValidation>\n  \n  // Security operations\n  reportSuspiciousActivity(userId: string, activity: SecurityEvent): Promise<void>\n  checkTokenHealth(tokenId: string): Promise<TokenHealth>\n}\n\ninterface LoginRequest {\n  email: string\n  password: string\n  deviceInfo: {\n    deviceId?: string  // Client-generated, stored in localStorage\n    deviceName: string // \"Chrome on MacOS\"\n    deviceType: 'mobile' | 'desktop' | 'tablet'\n    userAgent: string\n    fingerprint: string // Browser/device fingerprint for binding\n  }\n  mfaCode?: string\n  captchaToken?: string // Required after failed attempts\n}\n\ninterface AuthResponse {\n  accessToken: string  // Returned in JSON body\n  refreshToken: string // Set as httpOnly cookie\n  expiresIn: number   // Access token lifetime in seconds\n  tokenType: 'Bearer'\n  refreshExpiresIn: number // When refresh token expires\n  user: {\n    id: string\n    email: string\n    roles: string[]\n    mfaEnabled: boolean\n  }\n}\n\ninterface Session {\n  sessionId: string\n  deviceId: string\n  deviceName: string\n  deviceType: string\n  createdAt: Date\n  lastUsedAt: Date\n  expiresAt: Date\n  ipAddress: string\n  location?: {\n    country: string\n    city: string\n  }\n  current: boolean\n  trustLevel: 'trusted' | 'suspicious' | 'compromised'\n}\n\ninterface TokenHealth {\n  valid: boolean\n  timesRefreshed: number\n  ageInDays: number\n  riskScore: number // 0-100\n  issues: string[]\n}\n\nenum RevocationReason {\n  USER_LOGOUT = 'user_logout',\n  ADMIN_REVOKE = 'admin_revoke',\n  TOKEN_REUSE = 'token_reuse',\n  SUSPICIOUS_ACTIVITY = 'suspicious_activity',\n  PASSWORD_CHANGE = 'password_change',\n  ACCOUNT_DELETION = 'account_deletion',\n  SESSION_TIMEOUT = 'session_timeout'\n}\n```\n\n### 3. Data Models\n\n**Users Table (Postgres)**\n```sql\nCREATE TABLE users (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  email VARCHAR(255) UNIQUE NOT NULL,\n  password_hash TEXT NOT NULL,\n  roles TEXT[] NOT NULL DEFAULT '{\"user\"}',\n  mfa_enabled BOOLEAN DEFAULT false,\n  mfa_secret TEXT,\n  token_version INTEGER NOT NULL DEFAULT 1,\n  failed_login_attempts INTEGER DEFAULT 0,\n  locked_until TIMESTAMP,\n  created_at TIMESTAMP DEFAULT NOW(),\n  updated_at TIMESTAMP DEFAULT NOW(),\n  last_login_at TIMESTAMP,\n  last_login_ip INET,\n  password_changed_at TIMESTAMP DEFAULT NOW()\n);\n\nCREATE INDEX idx_users_email ON users(email);\nCREATE INDEX idx_users_locked_until ON users(locked_until) WHERE locked_until IS NOT NULL;\n```\n\n**Refresh Tokens Table (Postgres)**\n```sql\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id) ON DELETE CASCADE,\n  token_hash TEXT NOT NULL UNIQUE,\n  token_family_id UUID NOT NULL, -- Groups tokens from same initial login\n  session_id UUID NOT NULL,\n  device_id UUID NOT NULL,\n  device_name VARCHAR(255),\n  device_type VARCHAR(50),\n  device_fingerprint_hash TEXT NOT NULL, -- For additional binding\n  user_agent TEXT,\n  ip_address INET,\n  location JSONB, -- {country, city, region}\n  created_at TIMESTAMP DEFAULT NOW(),\n  last_used_at TIMESTAMP DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  times_refreshed INTEGER DEFAULT 0,\n  revoked BOOLEAN DEFAULT false,\n  revoked_at TIMESTAMP,\n  revoked_reason VARCHAR(100),\n  parent_token_id UUID REFERENCES refresh_tokens(id),\n  CONSTRAINT check_not_self_parent CHECK (id != parent_token_id)\n);\n\nCREATE INDEX idx_refresh_tokens_user_id ON refresh_tokens(user_id);\nCREATE INDEX idx_refresh_tokens_token_hash ON refresh_tokens(token_hash);\nCREATE INDEX idx_refresh_tokens_session_id ON refresh_tokens(session_id);\nCREATE INDEX idx_refresh_tokens_token_family_id ON refresh_tokens(token_family_id);\nCREATE INDEX idx_refresh_tokens_expires_at ON refresh_tokens(expires_at) WHERE revoked = false;\nCREATE INDEX idx_refresh_tokens_device_id ON refresh_tokens(device_id, user_id);\n```\n\n**Security Events Table (Postgres)**\n```sql\nCREATE TABLE security_events (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID REFERENCES users(id) ON DELETE CASCADE,\n  event_type VARCHAR(100) NOT NULL,\n  severity VARCHAR(20) NOT NULL, -- 'low', 'medium', 'high', 'critical'\n  details JSONB NOT NULL,\n  ip_address INET,\n  user_agent TEXT,\n  session_id UUID,\n  device_id UUID,\n  created_at TIMESTAMP DEFAULT NOW(),\n  resolved BOOLEAN DEFAULT false,\n  resolved_at TIMESTAMP\n);\n\nCREATE INDEX idx_security_events_user_id ON security_events(user_id, created_at DESC);\nCREATE INDEX idx_security_events_type ON security_events(event_type, created_at DESC);\nCREATE INDEX idx_security_events_severity ON security_events(severity) WHERE resolved = false;\n```\n\n**Token Blacklist (Redis)**\n```\nKey: blacklist:jti:{jti}\nValue: {\n  userId: string,\n  revokedAt: timestamp,\n  reason: string,\n  sessionId: string\n}\nTTL: Access token remaining lifetime\n```\n\n**Refresh Token Cache (Redis)**\n```\nKey: refresh:{token_hash}\nValue: {\n  userId: string,\n  sessionId: string,\n  deviceId: string,\n  fingerprint: string,\n  expiresAt: timestamp,\n  used: boolean,\n  usedAt?: timestamp,\n  familyId: string\n}\nTTL: 30 days\n```\n\n**Concurrent Refresh Lock (Redis)**\n```\nKey: refresh_lock:{token_hash}\nValue: {requestId: string, timestamp: number}\nTTL: 5 seconds\n```\n\n**Rate Limiting (Redis)**\n```\nKey patterns:\n  - ratelimit:login:{ip}:{email}\n  - ratelimit:refresh:{session_id}\n  - ratelimit:mfa:{session_id}\nValue: counter\nTTL: window duration\n```\n\n## Security Mechanisms\n\n### 1. CSRF Protection\n\n**Token Binding**\n- Refresh tokens bound to specific session/device via cookies\n- SameSite=Strict prevents cross-site cookie transmission\n- Access tokens transmitted via Authorization header only\n- Device fingerprint validated on each refresh\n\n**Cookie Security Attributes**\n```http\nSet-Cookie: refresh_token={token}; \n  HttpOnly; \n  Secure; \n  SameSite=Strict; \n  Path=/auth/refresh; \n  Max-Age=2592000;\n  Domain=.example.com\n```\n\n### 2. XSS Protection\n\n**Access Token Storage**\n- Never store in localStorage or sessionStorage\n- Keep in memory only (React state, Zustand, Redux)\n- Automatically lost on page refresh (forces token refresh)\n- Consider Web Workers for additional isolation\n\n**Cookie Security**\n- httpOnly flag prevents JavaScript access\n- Secure flag ensures HTTPS-only transmission\n- SameSite=Strict prevents CSRF\n- Path restriction to /auth/refresh endpoint only\n\n**Content Security Policy**\n```http\nContent-Security-Policy: \n  default-src 'self'; \n  script-src 'self' 'strict-dynamic'; \n  object-src 'none'; \n  base-uri 'self';\n```\n\n### 3. Replay Attack Prevention\n\n**Access Tokens**\n- Short lifetime (15 minutes) limits replay window\n- JTI (JWT ID) tracked in blacklist for immediate revocation\n- Token version in payload (incremented on password change)\n- Optional: Include request-specific nonce for critical operations\n\n**Refresh Tokens**\n- One-time use with automatic rotation\n- Immediate revocation chain if reuse detected\n- Parent token tracking for fraud detection\n- Device fingerprint validation prevents theft across devices\n- Concurrent refresh protection with distributed locks\n\n**Enhanced Reuse Detection Algorithm**\n```\nOn refresh token use:\n  1. Acquire distributed lock (Redis) with 5s timeout\n  2. Check token not already marked as used (atomic check-and-set)\n  3. If already used:\n     a. Get token family (all tokens with same family_id)\n     b. Mark entire family as compromised\n     c. Revoke all tokens in family\n     d. Add all access tokens to blacklist\n     e. Create security event (CRITICAL severity)\n     f. Notify user via email + push notification\n     g. Return 401 with specific error code\n     h. Optionally: Lock account temporarily\n  4. Mark token as used with timestamp\n  5. Generate new token pair linked to family\n  6. Release lock\n```\n\n### 4. Token Rotation on Refresh\n\n```\nClient sends: Refresh Token A + Device Fingerprint\nServer:\n  1. Acquire refresh lock (Redis, 5s TTL)\n  2. Validate token A exists and not expired\n  3. Check token A not already used (Redis cache first, then DB)\n  4. Verify device fingerprint matches stored value\n  5. Validate IP address hasn't drastically changed (geo-location check)\n  6. If suspicious: require step-up authentication\n  7. Atomically mark token A as used in Redis\n  8. Generate new access token B (JWT)\n  9. Generate new refresh token C (random)\n  10. Store token C with:\n      - parent_token_id = A.id\n      - token_family_id = A.token_family_id\n      - times_refreshed = A.times_refreshed + 1\n  11. Update last_used_at for token A\n  12. Release refresh lock\n  13. Return access token B and refresh token C\n  14. Token A becomes permanently invalid\n```\n\n### 5. Token Family Compromise Detection\n\n**Indicators of Compromise**\n- Refresh token reuse detected\n- Token used from dramatically different location (different country)\n- Token used from different device fingerprint\n- Excessive refresh frequency (>10 per minute)\n- Token family age exceeds maximum (e.g., 90 days of continuous refresh)\n\n**Compromise Response**\n```typescript\nasync function handleTokenFamilyCompromise(\n  familyId: string, \n  reason: string\n): Promise<void> {\n  // 1. Get all tokens in family\n  const family = await getTokenFamily(familyId);\n  const userId = family[0].user_id;\n  \n  // 2. Revoke entire family\n  await db.query(\n    'UPDATE refresh_tokens SET revoked = true, revoked_at = NOW(), ' +\n    'revoked_reason = $1 WHERE token_family_id = $2',\n    [reason, familyId]\n  );\n  \n  // 3. Blacklist all current access tokens for this session\n  const sessions = family.map(t => t.session_id);\n  await blacklistActiveTokensForSessions(sessions);\n  \n  // 4. Create security event\n  await createSecurityEvent({\n    userId,\n    eventType: 'TOKEN_FAMILY_COMPROMISED',\n    severity: 'CRITICAL',\n    details: {\n      familyId,\n      reason,\n      tokensRevoked: family.length,\n      sessions: sessions.length\n    }\n  });\n  \n  // 5. Notify user\n  await notifyUser(userId, {\n    type: 'SECURITY_ALERT',\n    title: 'Security Alert: Suspicious Activity Detected',\n    message: 'Your session was terminated due to suspicious activity. ' +\n             'Please log in again and review your recent sessions.',\n    action: 'REVIEW_SESSIONS'\n  });\n  \n  // 6. Optionally lock account for brief period\n  if (reason === 'TOKEN_REUSE') {\n    await lockAccountTemporarily(userId, 300); // 5 minutes\n  }\n}\n```\n\n### 6. Device Fingerprinting\n\n**Fingerprint Generation (Client)**\n```typescript\nasync function generateDeviceFingerprint(): Promise<string> {\n  const components = {\n    userAgent: navigator.userAgent,\n    language: navigator.language,\n    colorDepth: screen.colorDepth,\n    deviceMemory: navigator.deviceMemory,\n    hardwareConcurrency: navigator.hardwareConcurrency,\n    screenResolution: `${screen.width}x${screen.height}`,\n    timezone: Intl.DateTimeFormat().resolvedOptions().timeZone,\n    platform: navigator.platform,\n    // Canvas fingerprint (for desktop browsers)\n    canvas: await getCanvasFingerprint(),\n    // WebGL fingerprint\n    webgl: getWebGLFingerprint(),\n    // Audio context fingerprint\n    audio: await getAudioFingerprint()\n  };\n  \n  const fingerprintString = JSON.stringify(components);\n  const hash = await crypto.subtle.digest(\n    'SHA-256',\n    new TextEncoder().encode(fingerprintString)\n  );\n  \n  return Array.from(new Uint8Array(hash))\n    .map(b => b.toString(16).padStart(2, '0'))\n    .join('');\n}\n```\n\n**Fingerprint Validation (Server)**\n```typescript\nfunction validateFingerprint(\n  storedHash: string, \n  providedFingerprint: string\n): ValidationResult {\n  // Exact match required for refresh\n  if (storedHash === providedFingerprint) {\n    return { valid: true, confidence: 'high' };\n  }\n  \n  // Fingerprint changed - potential security issue\n  return { \n    valid: false, \n    confidence: 'low',\n    action: 'require_reauth'\n  };\n}\n```\n\n### 7. Concurrent Refresh Protection\n\n**Problem**: Multiple tabs/requests refresh simultaneously, causing race conditions\n\n**Solution**: Distributed locking with grace period\n\n```typescript\nasync function handleConcurrentRefresh(\n  refreshToken: string,\n  requestId: string\n): Promise<AuthResponse> {\n  const lockKey = `refresh_lock:${hashToken(refreshToken)}`;\n  const graceKey = `refresh_grace:${hashToken(refreshToken)}`;\n  \n  // Try to acquire lock\n  const lockAcquired = await redis.set(\n    lockKey,\n    JSON.stringify({ requestId, timestamp: Date.now() }),\n    'EX', 5, // 5 second expiry\n    'NX'    // Only set if not exists\n  );\n  \n  if (lockAcquired) {\n    // This request won the race - perform refresh\n    try {\n      const result = await performTokenRefresh(refreshToken);\n      \n      // Store result in grace period cache (5s)\n      await redis.setex(\n        graceKey,\n        5,\n        JSON.stringify(result)\n      );\n      \n      return result;\n    } finally {\n      await redis.del(lockKey);\n    }\n  } else {\n    // Another request is refreshing - wait and check grace cache\n    await sleep(100); // Brief wait\n    \n    const cachedResult = await redis.get(graceKey);\n    if (cachedResult) {\n      // Another tab already refreshed - use same result\n      return JSON.parse(cachedResult);\n    }\n    \n    // Lock expired or grace period passed - retry\n    if (retryCount < 3) {\n      return handleConcurrentRefresh(refreshToken, requestId);\n    }\n    \n    throw new Error('CONCURRENT_REFRESH_TIMEOUT');\n  }\n}\n```\n\n## Authentication Flows\n\n### Login Flow\n\n```\n1. Client \u2192 POST /auth/login\n   Body: { email, password, deviceInfo, captchaToken? }\n   \n2. Server preprocessing:\n   - Check rate limit: 5 attempts per 15 min per (IP + email)\n   - Validate CAPTCHA if required (after 3 failed attempts)\n   - Check if account locked (locked_until > NOW())\n\n3. Credential validation:\n   - Lookup user by email (constant-time to prevent enumeration)\n   - Verify password hash (Argon2id)\n   - On failure:\n     * Increment failed_login_attempts\n     * If attempts >= 5: Set locked_until (15 min)\n     * Return generic error (constant delay)\n   - On success:\n     * Reset failed_login_attempts to 0\n     * Clear locked_until\n\n4. If MFA enabled:\n   - Generate temporary token (5 min expiry)\n   - Return { requiresMfa: true, tempToken }\n   - Client \u2192 POST /auth/mfa/verify\n   - Body: { tempToken, mfaCode }\n   - Validate TOTP code (with time-window tolerance)\n   - Rate limit: 3 attempts per 5 min\n\n5. Generate tokens:\n   - Create token_family_id (UUID) for this login session\n   - Create session_id (UUID)\n   - Generate access token (JWT, RS256, 15 min)\n   - Generate refresh token (crypto.randomBytes(32))\n   - Hash refresh token (SHA-256) before storage\n   - Store in Postgres + Redis (write-through)\n   - Hash device fingerprint for storage\n\n6. Response:\n   - Set-Cookie: refresh_token (httpOnly, Secure, SameSite=Strict)\n   - Body: { \n       accessToken, \n       expiresIn: 900,\n       refreshExpiresIn: 2592000,\n       tokenType: 'Bearer',\n       user: { id, email, roles, mfaEnabled }\n     }\n\n7. Update user record:\n   - Set last_login_at = NOW()\n   - Set last_login_ip = client IP\n\n8. Create security event (audit log):\n   - Type: 'USER_LOGIN'\n   - Details: device info, IP, location\n\n9. Client stores:\n   - Access token in memory (React state)\n   - Device ID in localStorage\n   - Device fingerprint in memory (regenerated on load)\n```\n\n### Token Refresh Flow with Locking\n\n```\n1. Client detects access token expiring soon\n   - Proactive refresh at 80% lifetime (12 min)\n   - Or reactive on 401 Unauthorized\n   - Generate request ID for deduplication\n\n2. Client \u2192 POST /auth/refresh\n   Headers: Cookie: refresh_token={token}\n   Body: { deviceId, fingerprint, requestId }\n\n3. Server validation (with distributed lock):\n   - Acquire refresh lock (5s timeout)\n   - Extract refresh token from cookie\n   - Hash token and lookup in Redis cache\n   - If Redis miss: Query Postgres (cache miss)\n   - Validate:\n     * Token exists\n     * Not expired (expires_at > NOW())\n     * Not revoked (revoked = false)\n     * Not already used (used = false)\n     * Device ID matches stored device_id\n     * Fingerprint matches stored fingerprint_hash\n     * IP geo-location hasn't changed drastically\n\n4. Suspicious activity checks:\n   - If IP country changed: Require step-up auth\n   - If fingerprint mismatch: Reject + alert\n   - If times_refreshed > 5000: Force re-login\n   - If token age > 90 days: Force re-login\n\n5. Detect reuse (atomic check):\n   - Check Redis: refresh:{token_hash}.used\n   - If true \u2192 trigger compromise response:\n     * Release lock immediately\n     * Revoke token family\n     * Blacklist active tokens\n     * Send alert\n     * Return 401 with TOKEN_REUSE error\n\n6. Generate new tokens (atomic operation):\n   - Mark old refresh token as used (Redis + DB)\n   - Create new access token (JWT)\n   - Create new refresh token (random)\n   - Link via parent_token_id and token_family_id\n   - Increment times_refreshed counter\n   - Update last_used_at timestamp\n   - Store in Redis + Postgres\n\n7. Release distributed lock\n\n8. Response:\n   - Set-Cookie: new refresh_token\n   - Body: { \n       accessToken, \n       expiresIn: 900,\n       tokenType: 'Bearer'\n     }\n\n9. Background tasks (async):\n   - Update session last_used_at\n   - Clean up expired tokens (if not done recently)\n   - Check token health metrics\n```\n\n### Logout Flow\n\n```\nSingle Device Logout:\n1. Client \u2192 POST /auth/logout\n   Headers: \n     - Authorization: Bearer {accessToken}\n     - Cookie: refresh_token={token}\n\n2. Server:\n   - Extract JTI from access token\n   - Calculate remaining TTL\n   - Add to Redis blacklist with TTL\n   - Extract refresh token from cookie\n   - Mark refresh token as revoked (reason: 'user_logout')\n   - Delete from Redis cache\n   - Clear cookies in response (Max-Age=0)\n   - Create security event (audit log)\n\n3. Response:\n   - 200 OK\n   - Clear-Site-Data header: \"cache\", \"cookies\", \"storage\"\n\nAll Devices Logout:\n1. Client \u2192 POST /auth/logout?allDevices=true\n   Headers: \n     - Authorization: Bearer {accessToken}\n\n2. Server:\n   - Get user ID from access token\n   - Query all active refresh tokens for user\n   - Get current session from token (to exclude if desired)\n   - Revoke all tokens (revoked = true, reason = 'user_logout_all')\n   - Extract all active JTIs from sessions\n   - Add all JTIs to Redis blacklist\n   - Clear all Redis cache entries for user\n   - Create security event\n\n3. Response:\n   - 200 OK with count of sessions terminated\n   - Body: { sessionsClosed: 5 }\n\n4. Notify other devices (via push notification if available)\n```\n\n### Token Validation Middleware\n\n```typescript\nasync function validateAccessToken(req, res, next) {\n  try {\n    // 1. Extract token\n    const authHeader = req.headers.authorization;\n    if (!authHeader?.startsWith('Bearer ')) {\n      return res.status(401).json({ \n        error: 'missing_token',\n        message: 'Authorization header required' \n      });\n    }\n    \n    const token = authHeader.substring(7);\n    \n    // 2. Verify JWT signature and expiration\n    const payload = jwt.verify(token, getPublicKey(token), {\n      algorithms: ['RS256'],\n      issuer: 'auth.example.com',\n      audience: 'api.example.com',\n      clockTolerance: 10 // 10 second tolerance\n    });\n    \n    // 3. Check blacklist (Redis - fast path)\n    const blacklistKey = `blacklist:jti:${payload.jti}`;\n    const isBlacklisted = await redis.exists(blacklistKey);\n    if (isBlacklisted) {\n      const reason = await redis.hget(blacklistKey, 'reason');\n      return res.status(401).json({ \n        error: 'token_revoked',\n        reason: reason,\n        message: 'Token has been revoked'\n      });\n    }\n    \n    // 4. Fetch user and validate token version\n    const user = await getUserById(payload.sub);\n    if (!user) {\n      return res.status(401).json({ error: 'user_not_found' });\n    }\n    \n    // Check if token version matches (incremented on password change)\n    if (payload.token_version !== user.token_version) {\n      return res.status(401).json({ \n        error: 'token_invalidated',\n        message: 'Token version mismatch - password may have changed'\n      });\n    }\n    \n    // 5. Check if password changed after token issued\n    if (user.password_changed_at > new Date(payload.iat * 1000)) {\n      return res.status(401).json({ \n        error: 'token_invalidated',\n        message: 'Password changed - please login again'\n      });\n    }\n    \n    // 6. Attach user context to request\n    req.user = {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions,\n      sessionId: payload.session_id,\n      deviceId: payload.device_id,\n      tokenVersion: payload.token_version\n    };\n    \n    // 7. Optional: Record token usage for analytics\n    await recordTokenUsage(payload.jti, req.ip);\n    \n    next();\n  } catch (error) {\n    if (error.name === 'TokenExpiredError') {\n      return res.status(401).json({ \n        error: 'token_expired',\n        message: 'Access token expired',\n        expiredAt: error.expiredAt\n      });\n    }\n    \n    if (error.name === 'JsonWebTokenError') {\n      return res.status(401).json({ \n        error: 'invalid_token',\n        message: 'Token signature invalid'\n      });\n    }\n    \n    // Log unexpected errors\n    logger.error('Token validation error', { error, token: token.substring(0, 20) });\n    \n    return res.status(401).json({ \n      error: 'invalid_token',\n      message: 'Token validation failed'\n    });\n  }\n}\n\n// Helper to get correct public key based on kid in JWT header\nfunction getPublicKey(token: string): string {\n  const decoded = jwt.decode(token, { complete: true });\n  const kid = decoded?.header?.kid;\n  \n  if (!kid) {\n    return process.env.JWT_PUBLIC_KEY; // Default key\n  }\n  \n  // Fetch from key store (support key rotation)\n  return keyStore.getPublicKey(kid);\n}\n```\n\n## Error Handling\n\n### Error Codes\n\n```typescript\nenum AuthErrorCode {\n  // Authentication errors\n  INVALID_CREDENTIALS = 'invalid_credentials',\n  ACCOUNT_LOCKED = 'account_locked',\n  ACCOUNT_SUSPENDED = 'account_suspended',\n  ACCOUNT_DELETED = 'account_deleted',\n  MFA_REQUIRED = 'mfa_required',\n  INVALID_MFA_CODE = 'invalid_mfa_code',\n  MFA_RATE_LIMITED = 'mfa_rate_limited',\n  \n  // Token errors\n  TOKEN_EXPIRED = 'token_expired',\n  TOKEN_REVOKED = 'token_revoked',\n  TOKEN_REUSE_DETECTED = 'token_reuse_detected',\n  TOKEN_INVALIDATED = 'token_invalidated', // Password change, etc.\n  INVALID_TOKEN = 'invalid_token',\n  MISSING_TOKEN = 'missing_token',\n  TOKEN_VERSION_MISMATCH = 'token_version_mismatch',\n  \n  // Session errors\n  SESSION_EXPIRED = 'session_expired',\n  DEVICE_MISMATCH = 'device_mismatch',\n  FINGERPRINT_MISMATCH = 'fingerprint_mismatch',\n  SUSPICIOUS_ACTIVITY = 'suspicious_activity',\n  SESSION_LIMIT_EXCEEDED = 'session_limit_exceeded',\n  CONCURRENT_REFRESH_TIMEOUT = 'concurrent_refresh_timeout',\n  \n  // Rate limiting\n  TOO_MANY_ATTEMPTS = 'too_many_attempts',\n  RATE_LIMIT_EXCEEDED = 'rate_limit_exceeded',\n  \n  // Geo/security\n  LOCATION_CHANGE_DETECTED = 'location_change_detected',\n  STEP_UP_AUTH_REQUIRED = 'step_up_auth_required',\n  \n  // System errors\n  SERVICE_UNAVAILABLE = 'service_unavailable',\n  INTERNAL_ERROR = 'internal_error'\n}\n\ninterface AuthError {\n  code: AuthErrorCode\n  message: string\n  details?: Record<string, any>\n  retryAfter?: number // For rate limiting (seconds)\n  requiresAction?: 'login' | 'mfa' | 'unlock_account' | 'verify_device'\n  timestamp: string\n  requestId: string // For support/debugging\n}\n```\n\n### Client Error Handling Strategy\n\n```typescript\n// Axios interceptor with retry and refresh logic\nlet isRefreshing = false;\nlet refreshSubscribers: Array<(token: string) => void> = [];\n\nfunction subscribeTokenRefresh(cb: (token: string) => void) {\n  refreshSubscribers.push(cb);\n}\n\nfunction onRefreshComplete(token: string) {\n  refreshSubscribers.forEach(cb => cb(token));\n  refreshSubscribers = [];\n}\n\naxios.interceptors.response.use(\n  response => response,\n  async error => {\n    const originalRequest = error.config;\n    const errorCode = error.response?.data?.error;\n    \n    // Handle 401 errors\n    if (error.response?.status === 401 && !originalRequest._retry) {\n      \n      // Token expired - attempt refresh\n      if (errorCode === 'token_expired') {\n        if (isRefreshing) {\n          // Wait for ongoing refresh to complete\n          return new Promise(resolve => {\n            subscribeTokenRefresh((token: string) => {\n              originalRequest.headers.Authorization = `Bearer ${token}`;\n              resolve(axios(originalRequest));\n            });\n          });\n        }\n        \n        originalRequest._retry = true;\n        isRefreshing = true;\n        \n        try {\n          const fingerprint = await generateDeviceFingerprint();\n          const deviceId = localStorage.getItem('device_id');\n          const requestId = generateRequestId();\n          \n          const { data } = await axios.post('/auth/refresh', {\n            deviceId,\n            fingerprint,\n            requestId\n          }, {\n            withCredentials: true // Include cookies\n          });\n          \n          const { accessToken } = data;\n          setAccessToken(accessToken);\n          \n          // Notify all waiting requests\n          onRefreshComplete(accessToken);\n          isRefreshing = false;\n          \n          // Retry original request\n          originalRequest.headers.Authorization = `Bearer ${accessToken}`;\n          return axios(originalRequest);\n          \n        } catch (refreshError) {\n          isRefreshing = false;\n          refreshSubscribers = [];\n          \n          // Refresh failed - redirect to login\n          handleAuthFailure(refreshError);\n          return Promise.reject(refreshError);\n        }\n      }\n      \n      // Token reuse detected - critical security event\n      if (errorCode === 'token_reuse_detected') {\n        clearAuth();\n        showSecurityAlert(\n          'Security Alert',\n          'Your session was terminated due to suspicious activity. ' +\n          'This may indicate your account was accessed from another device. ' +\n          'Please login again and review your recent sessions.'\n        );\n        window.location.href = '/login?reason=security';\n        return Promise.reject(error);\n      }\n      \n      // Token invalidated (password change, etc.)\n      if (errorCode === 'token_invalidated' || \n          errorCode === 'token_version_mismatch') {\n        clearAuth();\n        showAlert(\n          'Session Expired',\n          'Your password was changed. Please login again.'\n        );\n        window.location.href = '/login?reason=password_change';\n        return Promise.reject(error);\n      }\n      \n      // Device or fingerprint mismatch\n      if (errorCode === 'device_mismatch' || \n          errorCode === 'fingerprint_mismatch') {\n        clearAuth();\n        window.location.href = '/login/verify-device';\n        return Promise.reject(error);\n      }\n      \n      // Location change detected - step-up auth required\n      if (errorCode === 'location_change_detected' ||\n          errorCode === 'step_up_auth_required') {\n        window.location.href = '/auth/verify-location';\n        return Promise.reject(error);\n      }\n      \n      // Other 401 errors - redirect to login\n      clearAuth();\n      window.location.href = '/login';\n    }\n    \n    // Handle 429 rate limiting\n    if (error.response?.status === 429) {\n      const retryAfter = error.response.data?.retryAfter || 60;\n      \n      showAlert(\n        'Too Many Requests',\n        `Please wait ${retryAfter} seconds before trying again.`\n      );\n      \n      // Optional: Implement exponential backoff retry\n      if (originalRequest.retryCount < 3) {\n        await sleep(retryAfter * 1000);\n        originalRequest.retryCount = (originalRequest.retryCount || 0) + 1;\n        return axios(originalRequest);\n      }\n    }\n    \n    return Promise.reject(error);\n  }\n);\n\nfunction handleAuthFailure(error: any) {\n  const errorCode = error.response?.data?.error;\n  \n  clearAuth();\n  \n  // Show appropriate message based on error\n  const messages: Record<string, string> = {\n    'session_expired': 'Your session has expired. Please login again.',\n    'account_locked': 'Your account has been locked. Please contact support.',\n    'account_suspended': 'Your account has been suspended.',\n    'service_unavailable': 'Authentication service is temporarily unavailable.'\n  };\n  \n  const message = messages[errorCode] || 'Please login again.';\n  \n  window.location.href = `/login?reason=${errorCode}&message=${encodeURIComponent(message)}`;\n}\n\nfunction clearAuth() {\n  // Clear in-memory token\n  setAccessToken(null);\n  \n  // Note: refresh token in httpOnly cookie is cleared by server\n  // We can't access it from JavaScript\n  \n  // Clear any other auth-related data\n  sessionStorage.clear();\n}\n\nfunction generateRequestId(): string {\n  return `${Date.now()}-${Math.random().toString(36).substr(2, 9)}`;\n}\n```\n\n## Security Best Practices\n\n### 1. Key Management\n\n**Asymmetric Keys (RS256)**\n- Use RSA 4096-bit or ECDSA P-384 for signing\n- Store private keys in HSM, AWS KMS, or HashiCorp Vault\n- Never commit keys to version control\n- Separate keys per environment (dev, staging, prod)\n\n**Key Rotation**\n```typescript\ninterface KeyRotationStrategy {\n  // Rotate signing keys every 90 days\n  rotationInterval: 90 * 24 * 60 * 60 * 1000; // milliseconds\n  \n  // Support multiple active public keys simultaneously\n  activeKeys: {\n    kid: string;        // Key ID\n    publicKey: string;\n    createdAt: Date;\n    expiresAt: Date;\n    status: 'active' | 'retiring' | 'expired';\n  }[];\n  \n  // Grace period for old keys (7 days)\n  gracePeriod: 7 * 24 * 60 * 60 * 1000;\n}\n\n// Token includes kid in header for key selection\nconst token = jwt.sign(payload, privateKey, {\n  algorithm: 'RS256',\n  keyid: currentKey.kid,\n  issuer: 'auth.example.com',\n  audience: 'api.example.com'\n});\n```\n\n**Rotation Process**\n1. Generate new key pair\n2. Add public key to active key set with new `kid`\n3. Update signing service to use new private key\n4. Wait grace period (7 days)\n5. Remove old public key from active set\n6. Archive old key pair for audit purposes\n\n### 2. Password Security\n\n**Hashing Algorithm**\n```typescript\n// Use Argon2id (recommended) or bcrypt as fallback\nimport argon2 from 'argon2';\n\nasync function hashPassword(password: string): Promise<string> {\n  return argon2.hash(password, {\n    type: argon2.argon2id,\n    memoryCost: 65536,    // 64 MB\n    timeCost: 3,          // 3 iterations\n    parallelism: 4        // 4 threads\n  });\n}\n\nasync function verifyPassword(password: string, hash: string): Promise<boolean> {\n  try {\n    return await argon2.verify(hash, password);\n  } catch {\n    return false;\n  }\n}\n```\n\n**Password Policy**\n```typescript\ninterface PasswordPolicy {\n  minLength: 12;\n  requireUppercase: true;\n  requireLowercase: true;\n  requireNumbers: true;\n  requireSpecialChars: true;\n  preventCommonPasswords: true;\n  preventBreachedPasswords: true; // Check against HIBP\n  preventPasswordReuse: 5; // Last 5 passwords\n  maxAge: 90 * 24 * 60 * 60 * 1000; // 90 days\n}\n```\n\n**Breach Detection**\n```typescript\nimport { pwnedPassword } from 'hibp';\n\nasync function checkPasswordBreach(password: string): Promise<boolean> {\n  try {\n    const breachCount = await pwnedPassword(password);\n    return breachCount > 0;\n  } catch {\n    // Fail open - don't block user if API unavailable\n    return false;\n  }\n}\n```\n\n### 3. Rate Limiting Strategy\n\n**Multi-Layer Rate Limiting**\n\n```typescript\n// Layer 1: IP-based (stops brute force from single source)\nconst ipRateLimit = {\n  login: { max: 10, window: 900 },      // 10 per 15 min\n  refresh: { max: 50, window: 60 },     // 50 per min\n  mfa: { max: 5, window: 300 }          // 5 per 5 min\n};\n\n// Layer 2: Account-based (stops distributed attacks)\nconst accountRateLimit = {\n  login: { max: 5, window: 900 },       // 5 per 15 min\n  passwordReset: { max: 3, window: 3600 } // 3 per hour\n};\n\n// Layer 3: Session-based (stops token abuse)\nconst sessionRateLimit = {\n  refresh: { max: 10, window: 60 },     // 10 per min\n  apiCalls: { max: 100, window: 60 }    // 100 per min\n};\n\n// Implementation with Redis\nasync function checkRateLimit(\n  key: string, \n  max: number, \n  windowSec: number\n): Promise<{ allowed: boolean; retryAfter?: number }> {\n  const count = await redis.incr(key);\n  \n  if (count === 1) {\n    await redis.expire(key, windowSec);\n  }\n  \n  if (count > max) {\n    const ttl = await redis.ttl(key);\n    return { allowed: false, retryAfter: ttl };\n  }\n  \n  return { allowed: true };\n}\n```\n\n**Progressive Delays**\n```typescript\nfunction getLoginDelay(attemptCount: number): number {\n  // Exponential backoff: 1s, 2s, 4s, 8s, 16s, ...\n  return Math.min(Math.pow(2, attemptCount - 1) * 1000, 30000);\n}\n\nasync function handleFailedLogin(email: string, ip: string) {\n  const attempts = await incrementFailedAttempts(email, ip);\n  \n  if (attempts >= 3) {\n    // Require CAPTCHA\n    await setRequiresCaptcha(email, ip);\n  }\n  \n  if (attempts >= 5) {\n    // Lock account temporarily\n    await lockAccount(email, 900); // 15 minutes\n  }\n  \n  // Progressive delay before response\n  const delay = getLoginDelay(attempts);\n  await sleep(delay);\n}\n```\n\n### 4. Monitoring & Alerting\n\n**Security Events to Monitor**\n\n```typescript\nenum SecurityEventType {\n  // Authentication\n  LOGIN_SUCCESS = 'login_success',\n  LOGIN_FAILURE = 'login_failure',\n  LOGIN_LOCKED = 'login_locked',\n  LOGOUT = 'logout',\n  \n  // Token management\n  TOKEN_ISSUED = 'token_issued',\n  TOKEN_REFRESHED = 'token_refreshed',\n  TOKEN_REVOKED = 'token_revoked',\n  TOKEN_REUSE = 'token_reuse',\n  TOKEN_FAMILY_COMPROMISED = 'token_family_compromised',\n  \n  // Suspicious activity\n  LOCATION_CHANGE = 'location_change',\n  DEVICE_CHANGE = 'device_change',\n  FINGERPRINT_MISMATCH = 'fingerprint_mismatch',\n  EXCESSIVE_REFRESH = 'excessive_refresh',\n  CONCURRENT_SESSIONS_EXCEEDED = 'concurrent_sessions_exceeded',\n  \n  // Account changes\n  PASSWORD_CHANGED = 'password_changed',\n  MFA_ENABLED = 'mfa_enabled',\n  MFA_DISABLED = 'mfa_disabled',\n  EMAIL_CHANGED = 'email_changed',\n  \n  // Admin actions\n  ADMIN_PASSWORD_RESET = 'admin_password_reset',\n  ADMIN_ACCOUNT_LOCK = 'admin_account_lock',\n  ADMIN_TOKEN_REVOKE = 'admin_token_revoke'\n}\n\ninterface AlertRule {\n  eventType: SecurityEventType;\n  condition: (event: SecurityEvent) => boolean;\n  severity: 'low' | 'medium' | 'high' | 'critical';\n  actions: AlertAction[];\n}\n\n// Example alert rules\nconst alertRules: AlertRule[] = [\n  {\n    eventType: SecurityEventType.TOKEN_REUSE,\n    condition: () => true, // Always alert\n    severity: 'critical',\n    actions: ['email', 'slack', 'pagerduty']\n  },\n  {\n    eventType: SecurityEventType.LOGIN_FAILURE,\n    condition: (e) => e.details.attemptCount >= 5,\n    severity: 'high',\n    actions: ['email', 'log']\n  },\n  {\n    eventType: SecurityEventType.LOCATION_CHANGE,\n    condition: (e) => {\n      const { oldCountry, newCountry } = e.details;\n      return oldCountry !== newCountry;\n    },\n    severity: 'medium',\n    actions: ['email', 'log']\n  }\n];\n```\n\n**Metrics to Track**\n\n```typescript\ninterface AuthMetrics {\n  // Login metrics\n  loginAttempts: number;\n  loginSuccesses: number;\n  loginFailures: number;\n  mfaSuccesses: number;\n  mfaFailures: number;\n  \n  // Token metrics\n  tokensIssued: number;\n  tokensRefreshed: number;\n  tokensRevoked: number;\n  tokenReuseDetected: number;\n  \n  // Performance metrics\n  avgLoginDuration: number;\n  avgRefreshDuration: number;\n  avgTokenValidationDuration: number;\n  \n  // Security metrics\n  accountsLocked: number;\n  suspiciousActivityDetected: number;\n  concurrentSessionsExceeded: number;\n  \n  // Rate limiting\n  rateLimitHits: number;\n}\n\n// Export to monitoring system (Prometheus, DataDog, etc.)\n```\n\n### 5. Session Management\n\n**Concurrent Session Limits**\n\n```typescript\nconst MAX_CONCURRENT_SESSIONS = 5;\n\nasync function enforceSessionLimit(userId: string): Promise<void> {\n  // Get all active sessions for user\n  const sessions = await db.query(\n    'SELECT * FROM refresh_tokens ' +\n    'WHERE user_id = $1 AND revoked = false AND expires_at > NOW() ' +\n    'ORDER BY last_used_at DESC',\n    [userId]\n  );\n  \n  if (sessions.length >= MAX_CONCURRENT_SESSIONS) {\n    // Revoke oldest session(s)\n    const toRevoke = sessions.slice(MAX_CONCURRENT_SESSIONS - 1);\n    \n    for (const session of toRevoke) {\n      await revokeToken(session.id, 'SESSION_LIMIT_EXCEEDED');\n    }\n    \n    // Notify user\n    await notifyUser(userId, {\n      type: 'SESSION_LIMIT',\n      message: `Session limit reached. Oldest session(s) were terminated.`\n    });\n  }\n}\n```\n\n**Session Cleanup**\n\n```typescript\n// Daily cron job to clean up expired tokens\nasync function cleanupExpiredTokens(): Promise<void> {\n  const result = await db.query(\n    'DELETE FROM refresh_tokens ' +\n    'WHERE expires_at < NOW() - INTERVAL \\'7 days\\' ' +\n    'RETURNING id'\n  );\n  \n  logger.info(`Cleaned up ${result.rowCount} expired tokens`);\n  \n  // Also clean up old security events (keep 90 days)\n  await db.query(\n    'DELETE FROM security_events ' +\n    'WHERE created_at < NOW() - INTERVAL \\'90 days\\''\n  );\n}\n\n// Run every 24 hours at 3 AM\ncron.schedule('0 3 * * *', cleanupExpiredTokens);\n```\n\n**Session Health Monitoring**\n\n```typescript\nasync function monitorSessionHealth(sessionId: string): Promise<void> {\n  const session = await getSession(sessionId);\n  \n  const healthChecks = {\n    // Check if session is being refreshed too frequently\n    excessiveRefresh: session.times_refreshed > 5000,\n    \n    // Check if session is too old\n    tooOld: Date.now() - session.created_at.getTime() > 90 * 24 * 60 * 60 * 1000,\n    \n    // Check if location changed dramatically\n    locationChange: await checkLocationChange(sessionId),\n    \n    // Check if device fingerprint changed\n    fingerprintChange: await checkFingerprintChange(sessionId)\n  };\n  \n  if (Object.values(healthChecks).some(check => check)) {\n    await createSecurityEvent({\n      userId: session.user_id,\n      eventType: 'SESSION_HEALTH_ISSUE',\n      severity: 'MEDIUM',\n      details: healthChecks\n    });\n    \n    // Optionally force re-authentication\n    if (healthChecks.tooOld || healthChecks.fingerprintChange) {\n      await revokeToken(session.id, 'HEALTH_CHECK_FAILED');\n    }\n  }\n}\n```\n\n## Configuration\n\n```typescript\ninterface AuthConfig {\n  accessToken: {\n    lifetime: number;        // 900 seconds (15 min)\n    algorithm: 'RS256';\n    issuer: string;          // 'auth.example.com'\n    audience: string;        // 'api.example.com'\n    clockTolerance: number;  // 10 seconds\n  };\n  \n  refreshToken: {\n    lifetime: number;            // 2592000 seconds (30 days)\n    rotationEnabled: boolean;    // true\n    reuseDetection: boolean;     // true\n    maxAge: number;              // 90 days (force reauth after)\n    maxRefreshCount: number;     // 5000 (force reauth after)\n  };\n  \n  security: {\n    passwordHasher: 'argon2id' | 'bcrypt';\n    argon2MemoryCost: number;    // 65536 (64 MB)\n    argon2TimeCost: number;      // 3\n    argon2Parallelism: number;   // 4\n    bcryptRounds: number;        // 12 (fallback)\n    \n    maxSessions: number;         // 5\n    requireMfa: boolean;         // false (per-user)\n    allowedOrigins: string[];\n    \n    fingerprintValidation: boolean; // true\n    ipValidation: 'strict' | 'moderate' | 'none'; // 'moderate'\n    geoLocationTracking: boolean;   // true\n    \n    tokenVersioning: boolean;    // true\n  };\n  \n  rateLimit: {\n    login: { max: 5, window: 900 };        // 15 min\n    loginPerIp: { max: 10, window: 900 };  // 15 min\n    refresh: { max: 10, window: 60 };      // 1 min\n    refreshPerSession: { max: 10, window: 60 };\n    mfa: { max: 3, window: 300 };          // 5 min\n    passwordReset: { max: 3, window: 3600 }; // 1 hour\n  };\n  \n  cookies: {\n    domain: string;              // '.example.com'\n    secure: boolean;             // true in production\n    sameSite: 'Strict' | 'Lax';  // 'Strict'\n    httpOnly: boolean;           // true\n    path: string;                // '/auth/refresh'\n  };\n  \n  notifications: {\n    emailOnNewDevice: boolean;   // true\n    emailOnLocationChange: boolean; // true\n    emailOnPasswordChange: boolean; // true\n    emailOnMfaChange: boolean;   // true\n    pushNotificationOnLogout: boolean; // true\n  };\n  \n  monitoring: {\n    logLevel: 'debug' | 'info' | 'warn' | 'error';\n    logAuthEvents: boolean;      // true\n    logSecurityEvents: boolean;  // true\n    metricsEnabled: boolean;     // true\n    metricsInterval: number;     // 60 seconds\n  };\n}\n```\n\n## What I Improved\n\n### 1. **Token Reuse Detection Robustness**\n- Added **distributed locking** with Redis to prevent race conditions during concurrent refresh attempts\n- Implemented **atomic check-and-set** operations to ensure token usage state is consistent\n- Added **grace period caching** to handle legitimate concurrent refreshes from multiple tabs\n- Included **token family ID** to group related tokens and enable full chain revocation\n\n### 2. **Device Fingerprinting**\n- Added comprehensive device fingerprinting (canvas, WebGL, audio) beyond just device ID\n- Fingerprint is **validated on every refresh** to detect device theft\n- Stored as hash for privacy protection\n- Provides stronger binding than device ID alone\n\n### 3. **Token Versioning**\n- Added `token_version` field to users table and JWT payload\n- Incremented on password change, allowing instant invalidation of all tokens\n- More efficient than checking `password_changed_at` timestamp\n\n### 4. **Enhanced Security Event Tracking**\n- Added dedicated `security_events` table with severity levels\n- Comprehensive event types covering all security scenarios\n- Automated alerting rules based on event patterns\n- Better audit trail for compliance and forensics\n\n### 5. **Concurrent Refresh Protection**\n- Solved the \"multiple tabs refreshing simultaneously\" race condition\n- Uses distributed locks with requestId for deduplication\n- Grace period cache allows sharing refresh results across concurrent requests\n- Prevents token reuse false positives\n\n### 6. **Session Health Monitoring**\n- Track `times_refreshed` counter to detect excessive refresh patterns\n- Monitor session age and force re-auth after 90 days\n- Geographic location tracking for suspicious movement detection\n- Proactive security checks before issues escalate\n\n### 7. **Improved Rate Limiting**\n- Multi-layer rate limiting (IP, account, session)\n- Progressive delays on failed attempts\n- CAPTCHA requirement after threshold\n- Prevents both single-source and distributed attacks\n\n### 8. **Token Family Compromise Handling**\n- Full token family revocation on reuse detection\n- Tracks parent-child relationships via `parent_token_id`\n- Prevents attacker from using any token in compromised chain\n- Automatic account lock option for high-severity events\n\n### 9. **Client-Side Error Handling**\n- Comprehensive error code taxonomy\n- Specific handling for each error type (reuse, invalidation, location change)\n- Request deduplication to prevent refresh storms\n- Clear user messaging for security events\n\n### 10. **Key Rotation Strategy**\n- Support for multiple active keys simultaneously (kid-based selection)\n- 90-day rotation schedule with 7-day grace period\n- Documented rotation process for zero-downtime updates\n- HSM/KMS integration guidance\n\n## What I Kept\n\nThe architect's design had several excellent foundational elements:\n\n1. **15-minute access token lifetime** - Perfect balance of security and UX\n2. **Opaque refresh tokens** - Correct choice over JWT for revocability\n3. **httpOnly cookies with SameSite=Strict** - Industry best practice\n4. **Memory-only access token storage** - Proper XSS protection\n5. **Redis + Postgres hybrid** - Smart caching strategy\n6. **RS256 asymmetric signing** - Good for microservices architecture\n7. **Parent token tracking** - Foundation for token family detection\n8. **Comprehensive data model** - Well-structured tables with proper indexes\n9. **Device binding concept** - Good security layer\n10. **Clear separation of concerns** - Auth service, token store, application services\n\n## Remaining Concerns\n\n### 1. **Token Payload Size**\nThe JWT includes roles, permissions, device_id, session_id, and token_version. For users with many permissions, this could exceed typical header size limits (8KB). Consider:\n- Fetching permissions separately after token validation\n- Using role-based access control (RBAC) with permission lookup\n- Compressing permission strings\n\n### 2. **Offline Mobile Apps**\nIf a mobile app is offline for >30 days, the refresh token expires and user must re-authenticate. This could be frustrating for apps used intermittently. Potential solutions:\n- Longer refresh token lifetime for mobile (configurable)\n- Background token refresh when device comes online\n- Biometric re-auth instead of full login\n\n### 3. **Cross-Domain Authentication**\nThe design uses cookies with SameSite=Strict, which prevents cross-domain authentication. For multi-domain scenarios (e.g., app.example.com and admin.example.com), consider:\n- Shared authentication domain (auth.example.com)\n- Token exchange mechanism for cross-domain\n- SameSite=Lax with additional CSRF protection\n\n### 4. **Redis Failure Scenarios**\nThe system relies heavily on Redis for:\n- Token blacklist lookups\n- Refresh token cache\n- Rate limiting\n- Distributed locks\n\nIf Redis becomes unavailable:\n- Access token validation fails (blacklist check)\n- Refresh operations may fail (lock acquisition)\n- Rate limiting disabled\n\nConsider fallback strategies:\n- Fallback to Postgres for critical operations (slower but available)\n- Circuit breaker pattern for Redis operations\n- Graceful degradation (temporarily accept slightly elevated risk)\n\n### 5. **Token Fingerprint Privacy**\nDevice fingerprinting can be privacy-invasive. Browser fingerprinting techniques may be blocked by privacy-focused browsers (Brave, Firefox with resistFingerprinting). Consider:\n- Make fingerprinting opt-in for privacy-conscious users\n- Fallback to weaker binding (IP + User-Agent) when fingerprinting unavailable\n- Document fingerprinting in privacy policy\n\n### 6. **Database Write Amplification**\nEvery token refresh writes to:\n- Postgres (new token row, update old token)\n- Redis (cache new token, mark old as used)\n\nAt scale with millions of users, this creates significant write load. Optimizations:\n- Batch token updates during off-peak hours\n- Use write-behind caching for non-critical updates (last_used_at)\n- Consider token refresh throttling (don't allow refresh more than once per minute)\n\n### 7. **Geographic Location Detection**\nIP-based geolocation has limitations:\n- VPN users appear in different countries frequently\n- Mobile users on cellular networks may have inconsistent IPs\n- CDN/proxy services can mask true location\n\nNeed clear policy on when to flag location changes:\n- Same country, different city: Log but allow\n- Different country: Challenge or notify\n- Multiple countries in short time: Block and require verification\n\n## Convergence Status\n\n**ITERATING** - This is the first review. The design has strong foundations but would benefit from another iteration to address:\n- Scaling concerns (Redis/DB write load)\n- Edge cases (offline mobile, Redis failure, fingerprint blocking)\n- Privacy considerations (fingerprinting disclosure)\n- Cross-domain authentication requirements\n\nThe token reuse detection is now robust with distributed locking, but we should validate the performance characteristics under high concurrency.\n\nPROMPT_FOR_ARCHITECT:\n\nPlease address the **Redis failure scenarios** and propose a fallback strategy that maintains security guarantees when Redis is unavailable. Specifically:\n\n1. How should token validation proceed if the blacklist (Redis) is unreachable?\n2. What's the fallback for distributed locking during refresh operations?\n3. How do we handle rate limiting when Redis counters aren't accessible?\n\nAlso, clarify the **geographic location policy**: What specific location changes should trigger user notification vs. blocking vs. requiring step-up authentication? Consider VPN users and mobile users with dynamic IPs."
      }
    },
    {
      "round_number": 2,
      "agent_a": {
        "content": "## Design\n\n# JWT Authentication System with Redis Failover & Geo-Location Policy\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 (Redis HA)  \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502             \u2502\n                                           \u25bc             \u25bc\n                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                    \u2502  Database   \u2502 \u2502  Fallback   \u2502\n                                    \u2502  (Postgres) \u2502 \u2502  Cache      \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  (In-Mem)   \u2502\n                                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Redis High Availability & Failover Strategy\n\n### 1. Redis Deployment Architecture\n\n**Primary Configuration: Redis Sentinel**\n```yaml\n# Redis Sentinel for automatic failover\nredis-sentinel:\n  master:\n    name: auth-master\n    host: redis-master.internal\n    port: 6379\n  replicas:\n    - redis-replica-1.internal:6379\n    - redis-replica-2.internal:6379\n  sentinels:\n    - sentinel-1.internal:26379\n    - sentinel-2.internal:26379\n    - sentinel-3.internal:26379\n  quorum: 2  # Minimum sentinels needed for failover decision\n  \n  # Connection from application\n  connection_strategy: sentinel_aware\n  sentinel_failover_timeout: 10000  # 10 seconds\n  max_retry_attempts: 3\n```\n\n**Circuit Breaker Configuration**\n```typescript\ninterface CircuitBreakerConfig {\n  // Trip circuit if 50% of requests fail in window\n  failureThreshold: 0.5;\n  \n  // Consider last 100 requests\n  requestVolumeThreshold: 100;\n  \n  // Check health every 10 seconds when open\n  resetTimeout: 10000;\n  \n  // Operations timeout\n  timeout: 2000; // 2 seconds\n  \n  // What qualifies as failure\n  errorThresholdPercentage: 50;\n}\n\nenum CircuitState {\n  CLOSED = 'closed',    // Normal operation\n  OPEN = 'open',        // Redis unavailable, use fallback\n  HALF_OPEN = 'half_open' // Testing if Redis recovered\n}\n```\n\n### 2. Multi-Layer Caching Strategy\n\n**Layer 1: Redis (Primary)**\n- Fastest access path\n- Distributed across application instances\n- Handles 99.9% of requests under normal conditions\n\n**Layer 2: In-Process Cache (Fallback)**\n- LRU cache in application memory\n- Size-limited (e.g., 10,000 entries)\n- Synchronizes with Redis when available\n- Used when Redis is unreachable\n\n**Layer 3: Postgres (Source of Truth)**\n- Authoritative data store\n- Slower but always available\n- Used for cache misses and rebuilding\n\n```typescript\ninterface CacheLayer {\n  get(key: string): Promise<any | null>;\n  set(key: string, value: any, ttl: number): Promise<void>;\n  delete(key: string): Promise<void>;\n  isAvailable(): Promise<boolean>;\n}\n\nclass MultiLayerCache {\n  private redis: RedisClient;\n  private localCache: LRUCache;\n  private postgres: PostgresClient;\n  private circuitBreaker: CircuitBreaker;\n  \n  constructor() {\n    this.redis = new RedisClient({\n      sentinels: [...],\n      name: 'auth-master',\n      retryStrategy: (times) => Math.min(times * 50, 2000)\n    });\n    \n    this.localCache = new LRUCache({\n      max: 10000, // Maximum entries\n      ttl: 300000, // 5 minutes default TTL\n      updateAgeOnGet: true,\n      updateAgeOnHas: true\n    });\n    \n    this.circuitBreaker = new CircuitBreaker(\n      async (operation) => this.redis[operation.method](...operation.args),\n      circuitBreakerConfig\n    );\n  }\n  \n  async get(key: string): Promise<any | null> {\n    // Try Redis first (through circuit breaker)\n    try {\n      const redisResult = await this.circuitBreaker.execute({\n        method: 'get',\n        args: [key]\n      });\n      \n      if (redisResult !== null) {\n        // Update local cache\n        this.localCache.set(key, redisResult);\n        return redisResult;\n      }\n    } catch (error) {\n      logger.warn('Redis unavailable, using fallback', { key, error });\n      metrics.increment('cache.redis.failure');\n    }\n    \n    // Try local cache\n    const localResult = this.localCache.get(key);\n    if (localResult !== undefined) {\n      metrics.increment('cache.local.hit');\n      return localResult;\n    }\n    \n    // Fall back to Postgres\n    metrics.increment('cache.postgres.fallback');\n    const dbResult = await this.getFromPostgres(key);\n    \n    // Populate caches for future requests\n    if (dbResult !== null) {\n      this.localCache.set(key, dbResult);\n      // Try to update Redis in background (best effort)\n      this.updateRedisAsync(key, dbResult).catch(() => {});\n    }\n    \n    return dbResult;\n  }\n  \n  async set(key: string, value: any, ttl: number): Promise<void> {\n    // Write to Postgres first (source of truth)\n    await this.writeToPostgres(key, value, ttl);\n    \n    // Update local cache immediately\n    this.localCache.set(key, value, { ttl });\n    \n    // Try to update Redis (best effort)\n    try {\n      await this.circuitBreaker.execute({\n        method: 'setex',\n        args: [key, ttl, JSON.stringify(value)]\n      });\n    } catch (error) {\n      logger.warn('Redis write failed, continuing with local cache', { key });\n      metrics.increment('cache.redis.write_failure');\n      // Not a critical failure - data is in Postgres and local cache\n    }\n  }\n  \n  async delete(key: string): Promise<void> {\n    // Delete from all layers\n    await Promise.allSettled([\n      this.deleteFromPostgres(key),\n      this.circuitBreaker.execute({ method: 'del', args: [key] }),\n      Promise.resolve(this.localCache.delete(key))\n    ]);\n  }\n  \n  private async getFromPostgres(key: string): Promise<any | null> {\n    const keyParts = this.parseKey(key);\n    \n    switch (keyParts.type) {\n      case 'blacklist':\n        return this.getBlacklistedToken(keyParts.jti);\n      case 'refresh':\n        return this.getRefreshToken(keyParts.tokenHash);\n      case 'ratelimit':\n        return this.getRateLimitCounter(keyParts.identifier);\n      default:\n        return null;\n    }\n  }\n  \n  private async writeToPostgres(key: string, value: any, ttl: number): Promise<void> {\n    const keyParts = this.parseKey(key);\n    const expiresAt = new Date(Date.now() + ttl * 1000);\n    \n    switch (keyParts.type) {\n      case 'blacklist':\n        await this.blacklistToken(keyParts.jti, value, expiresAt);\n        break;\n      case 'refresh':\n        await this.storeRefreshToken(keyParts.tokenHash, value, expiresAt);\n        break;\n      case 'ratelimit':\n        await this.updateRateLimitCounter(keyParts.identifier, value, expiresAt);\n        break;\n    }\n  }\n  \n  private parseKey(key: string): { type: string; [key: string]: string } {\n    // Parse keys like \"blacklist:jti:{jti}\", \"refresh:{hash}\", \"ratelimit:login:{ip}:{email}\"\n    const parts = key.split(':');\n    return {\n      type: parts[0],\n      ...this.extractKeyComponents(parts)\n    };\n  }\n}\n```\n\n### 3. Token Blacklist Failover Strategy\n\n**Problem**: Access token validation needs to check blacklist. If Redis is down, we risk:\n- Option A: Reject all requests (poor availability)\n- Option B: Accept all tokens (security risk)\n\n**Solution**: Graceful Degradation with Risk-Based Decision\n\n```typescript\ninterface TokenValidationStrategy {\n  // How critical is blacklist checking for this token?\n  riskLevel: 'low' | 'medium' | 'high';\n  \n  // What to do if Redis unavailable?\n  fallbackBehavior: 'accept' | 'reject' | 'accept_with_logging';\n  \n  // Additional checks to perform when Redis down\n  compensatingControls: string[];\n}\n\nasync function validateAccessTokenWithFailover(\n  token: string,\n  req: Request\n): Promise<TokenValidation> {\n  // 1. Verify JWT signature and basic claims\n  const payload = jwt.verify(token, getPublicKey(token), {\n    algorithms: ['RS256'],\n    issuer: 'auth.example.com',\n    audience: 'api.example.com',\n    clockTolerance: 10\n  });\n  \n  // 2. Determine risk level for this request\n  const riskLevel = calculateRiskLevel(payload, req);\n  \n  // 3. Try to check blacklist (with circuit breaker)\n  let isBlacklisted: boolean | null = null;\n  \n  try {\n    isBlacklisted = await cache.get(`blacklist:jti:${payload.jti}`) !== null;\n  } catch (error) {\n    // Redis unavailable - proceed based on risk level\n    logger.warn('Blacklist check failed, using fallback strategy', {\n      jti: payload.jti,\n      riskLevel,\n      error: error.message\n    });\n    \n    metrics.increment('auth.blacklist_check.failure', {\n      risk_level: riskLevel\n    });\n    \n    // Apply fallback strategy\n    return await handleBlacklistUnavailable(payload, req, riskLevel);\n  }\n  \n  if (isBlacklisted) {\n    return { valid: false, reason: 'TOKEN_REVOKED' };\n  }\n  \n  // 4. Additional validation\n  const user = await getUserById(payload.sub);\n  \n  if (!user) {\n    return { valid: false, reason: 'USER_NOT_FOUND' };\n  }\n  \n  if (payload.token_version !== user.token_version) {\n    return { valid: false, reason: 'TOKEN_VERSION_MISMATCH' };\n  }\n  \n  if (user.password_changed_at > new Date(payload.iat * 1000)) {\n    return { valid: false, reason: 'TOKEN_INVALIDATED' };\n  }\n  \n  return { \n    valid: true, \n    user: {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions\n    }\n  };\n}\n\nfunction calculateRiskLevel(\n  payload: JWTPayload,\n  req: Request\n): 'low' | 'medium' | 'high' {\n  const factors = {\n    // Token age (older = lower risk of revocation)\n    tokenAge: Date.now() - payload.iat * 1000,\n    \n    // Request type\n    isReadOnly: ['GET', 'HEAD', 'OPTIONS'].includes(req.method),\n    \n    // Endpoint sensitivity\n    endpoint: req.path,\n    \n    // User role\n    hasAdminRole: payload.roles?.includes('admin'),\n    \n    // Recent activity\n    sessionAge: Date.now() - (payload.session_started_at || payload.iat) * 1000\n  };\n  \n  // High risk: Admin actions, write operations, new sessions\n  if (factors.hasAdminRole || \n      !factors.isReadOnly || \n      factors.sessionAge < 60000) { // < 1 minute old\n    return 'high';\n  }\n  \n  // Low risk: Read-only, old token, mature session\n  if (factors.isReadOnly && \n      factors.tokenAge > 300000 && // > 5 minutes old\n      factors.sessionAge > 3600000) { // > 1 hour old\n    return 'low';\n  }\n  \n  return 'medium';\n}\n\nasync function handleBlacklistUnavailable(\n  payload: JWTPayload,\n  req: Request,\n  riskLevel: 'low' | 'medium' | 'high'\n): Promise<TokenValidation> {\n  \n  switch (riskLevel) {\n    case 'low':\n      // Accept request with logging\n      logger.info('Accepting low-risk request despite blacklist unavailable', {\n        jti: payload.jti,\n        userId: payload.sub,\n        endpoint: req.path,\n        method: req.method\n      });\n      \n      metrics.increment('auth.blacklist_bypass.low_risk');\n      \n      // Perform compensating controls\n      await performCompensatingControls(payload, req);\n      \n      return { \n        valid: true, \n        fallbackMode: true,\n        riskLevel: 'low',\n        user: extractUserFromPayload(payload)\n      };\n    \n    case 'medium':\n      // Check Postgres directly (slower but authoritative)\n      logger.warn('Medium-risk request, checking Postgres blacklist', {\n        jti: payload.jti,\n        userId: payload.sub\n      });\n      \n      const isBlacklistedInDb = await checkBlacklistInPostgres(payload.jti);\n      \n      if (isBlacklistedInDb) {\n        return { valid: false, reason: 'TOKEN_REVOKED' };\n      }\n      \n      metrics.increment('auth.blacklist_bypass.medium_risk_postgres');\n      \n      return { \n        valid: true, \n        fallbackMode: true,\n        riskLevel: 'medium',\n        user: extractUserFromPayload(payload)\n      };\n    \n    case 'high':\n      // Reject high-risk requests when blacklist unavailable\n      logger.error('Rejecting high-risk request due to blacklist unavailable', {\n        jti: payload.jti,\n        userId: payload.sub,\n        endpoint: req.path,\n        method: req.method\n      });\n      \n      metrics.increment('auth.blacklist_bypass.high_risk_rejected');\n      \n      return { \n        valid: false, \n        reason: 'BLACKLIST_UNAVAILABLE',\n        message: 'Service temporarily unavailable for this operation'\n      };\n  }\n}\n\nasync function performCompensatingControls(\n  payload: JWTPayload,\n  req: Request\n): Promise<void> {\n  // 1. Check if user still exists and is active\n  const user = await getUserById(payload.sub);\n  if (!user || user.locked_until > new Date()) {\n    throw new Error('USER_INACTIVE');\n  }\n  \n  // 2. Verify token version matches (password not changed)\n  if (payload.token_version !== user.token_version) {\n    throw new Error('TOKEN_VERSION_MISMATCH');\n  }\n  \n  // 3. Check if session still exists in database\n  const session = await getRefreshTokenBySession(payload.session_id);\n  if (!session || session.revoked) {\n    throw new Error('SESSION_REVOKED');\n  }\n  \n  // 4. Log for audit trail\n  await createSecurityEvent({\n    userId: payload.sub,\n    eventType: 'TOKEN_VALIDATED_FALLBACK_MODE',\n    severity: 'MEDIUM',\n    details: {\n      jti: payload.jti,\n      endpoint: req.path,\n      method: req.method,\n      reason: 'redis_unavailable'\n    }\n  });\n}\n\nasync function checkBlacklistInPostgres(jti: string): Promise<boolean> {\n  const result = await db.query(\n    'SELECT 1 FROM token_blacklist WHERE jti = $1 AND expires_at > NOW()',\n    [jti]\n  );\n  \n  return result.rowCount > 0;\n}\n```\n\n**Token Blacklist Table (Postgres)**\n```sql\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL, -- Same as original token expiry\n  reason VARCHAR(100),\n  session_id UUID,\n  ip_address INET,\n  user_agent TEXT\n);\n\nCREATE INDEX idx_token_blacklist_expires ON token_blacklist(expires_at) \n  WHERE expires_at > NOW();\nCREATE INDEX idx_token_blacklist_user ON token_blacklist(user_id, revoked_at DESC);\n\n-- Automatic cleanup of expired entries\nCREATE OR REPLACE FUNCTION cleanup_expired_blacklist()\nRETURNS void AS $$\nBEGIN\n  DELETE FROM token_blacklist WHERE expires_at < NOW() - INTERVAL '1 day';\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Run cleanup daily\nSELECT cron.schedule('cleanup-blacklist', '0 4 * * *', 'SELECT cleanup_expired_blacklist()');\n```\n\n### 4. Refresh Token Locking Fallback\n\n**Problem**: Distributed locks prevent concurrent refresh. If Redis is down, we risk:\n- Race conditions causing token reuse false positives\n- Complete inability to refresh tokens\n\n**Solution**: Postgres Advisory Locks with Timeout\n\n```typescript\nasync function performTokenRefreshWithFailover(\n  refreshToken: string,\n  fingerprint: string,\n  requestId: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // Try Redis distributed lock first\n  let lockAcquired = false;\n  let useFallbackLock = false;\n  \n  try {\n    lockAcquired = await acquireRedisLock(tokenHash, requestId, 5000);\n  } catch (error) {\n    logger.warn('Redis lock unavailable, using Postgres advisory lock', {\n      tokenHash: tokenHash.substring(0, 16),\n      error: error.message\n    });\n    \n    metrics.increment('auth.refresh_lock.fallback_to_postgres');\n    useFallbackLock = true;\n  }\n  \n  if (useFallbackLock) {\n    // Use Postgres advisory lock as fallback\n    lockAcquired = await acquirePostgresAdvisoryLock(tokenHash, 5000);\n  }\n  \n  if (!lockAcquired) {\n    // Another process is refreshing - wait and check for result\n    return await waitForConcurrentRefresh(tokenHash, requestId, useFallbackLock);\n  }\n  \n  try {\n    // Perform refresh operation\n    const result = await executeTokenRefresh(refreshToken, fingerprint);\n    \n    // Store result for concurrent requests (best effort)\n    if (!useFallbackLock) {\n      try {\n        await cache.set(\n          `refresh_result:${tokenHash}`,\n          result,\n          5 // 5 seconds\n        );\n      } catch {\n        // Not critical if this fails\n      }\n    }\n    \n    return result;\n    \n  } finally {\n    // Release lock\n    if (useFallbackLock) {\n      await releasePostgresAdvisoryLock(tokenHash);\n    } else {\n      await releaseRedisLock(tokenHash, requestId);\n    }\n  }\n}\n\nasync function acquirePostgresAdvisoryLock(\n  tokenHash: string,\n  timeoutMs: number\n): Promise<boolean> {\n  // Convert token hash to numeric lock ID (Postgres advisory locks use bigint)\n  const lockId = hashToLockId(tokenHash);\n  \n  const startTime = Date.now();\n  const maxAttempts = Math.ceil(timeoutMs / 100); // Check every 100ms\n  \n  for (let attempt = 0; attempt < maxAttempts; attempt++) {\n    const result = await db.query(\n      'SELECT pg_try_advisory_lock($1) as acquired',\n      [lockId]\n    );\n    \n    if (result.rows[0].acquired) {\n      logger.debug('Acquired Postgres advisory lock', { lockId, attempt });\n      return true;\n    }\n    \n    // Wait before retry\n    await sleep(100);\n    \n    if (Date.now() - startTime > timeoutMs) {\n      break;\n    }\n  }\n  \n  logger.warn('Failed to acquire Postgres advisory lock', {\n    lockId,\n    timeoutMs,\n    attempts: maxAttempts\n  });\n  \n  return false;\n}\n\nasync function releasePostgresAdvisoryLock(tokenHash: string): Promise<void> {\n  const lockId = hashToLockId(tokenHash);\n  \n  await db.query('SELECT pg_advisory_unlock($1)', [lockId]);\n  \n  logger.debug('Released Postgres advisory lock', { lockId });\n}\n\nfunction hashToLockId(tokenHash: string): bigint {\n  // Convert hash to 64-bit integer for Postgres advisory lock\n  const buffer = Buffer.from(tokenHash.substring(0, 16), 'hex');\n  return buffer.readBigInt64BE(0);\n}\n\nasync function waitForConcurrentRefresh(\n  tokenHash: string,\n  requestId: string,\n  postgresMode: boolean\n): Promise<AuthResponse> {\n  \n  if (postgresMode) {\n    // In Postgres fallback mode, we can't share results easily\n    // Wait longer and retry\n    await sleep(1000); // Wait 1 second\n    \n    // Retry the refresh operation\n    return await performTokenRefreshWithFailover(\n      tokenHash,\n      fingerprint,\n      requestId\n    );\n  }\n  \n  // Try to get cached result from concurrent refresh (Redis available)\n  const maxWaitTime = 5000; // 5 seconds\n  const checkInterval = 100; // Check every 100ms\n  const maxAttempts = maxWaitTime / checkInterval;\n  \n  for (let attempt = 0; attempt < maxAttempts; attempt++) {\n    await sleep(checkInterval);\n    \n    try {\n      const cachedResult = await cache.get(`refresh_result:${tokenHash}`);\n      if (cachedResult) {\n        metrics.increment('auth.refresh.concurrent_reuse');\n        return cachedResult;\n      }\n    } catch {\n      // Cache unavailable, continue waiting\n    }\n  }\n  \n  throw new Error('CONCURRENT_REFRESH_TIMEOUT');\n}\n\nasync function executeTokenRefresh(\n  refreshToken: string,\n  fingerprint: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // 1. Retrieve token from cache (multi-layer)\n  let tokenData = await cache.get(`refresh:${tokenHash}`);\n  \n  if (!tokenData) {\n    // Load from Postgres if cache miss\n    tokenData = await getRefreshTokenFromPostgres(tokenHash);\n    \n    if (!tokenData) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n  }\n  \n  // 2. Validate token\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Check if already used (CRITICAL for security)\n  if (tokenData.used) {\n    // Token reuse detected - trigger compromise response\n    await handleTokenFamilyCompromise(\n      tokenData.token_family_id,\n      'TOKEN_REUSE'\n    );\n    throw new AuthError('TOKEN_REUSE_DETECTED');\n  }\n  \n  // 4. Validate fingerprint\n  if (tokenData.device_fingerprint_hash !== hashFingerprint(fingerprint)) {\n    throw new AuthError('FINGERPRINT_MISMATCH');\n  }\n  \n  // 5. Mark token as used (atomic operation in Postgres)\n  await db.query(\n    'UPDATE refresh_tokens SET used = true, used_at = NOW() ' +\n    'WHERE token_hash = $1 AND used = false',\n    [tokenHash]\n  );\n  \n  // Also mark in cache (best effort)\n  try {\n    await cache.set(\n      `refresh:${tokenHash}:used`,\n      true,\n      tokenData.ttl_remaining\n    );\n  } catch {\n    // Not critical - Postgres is source of truth\n  }\n  \n  // 6. Generate new token pair\n  const user = await getUserById(tokenData.user_id);\n  \n  const newAccessToken = generateAccessToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    tokenVersion: user.token_version\n  });\n  \n  const newRefreshToken = await generateRefreshToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    deviceFingerprint: fingerprint,\n    parentTokenId: tokenData.id,\n    tokenFamilyId: tokenData.token_family_id,\n    timesRefreshed: tokenData.times_refreshed + 1\n  });\n  \n  return {\n    accessToken: newAccessToken.token,\n    refreshToken: newRefreshToken.token,\n    expiresIn: 900,\n    tokenType: 'Bearer',\n    refreshExpiresIn: 2592000\n  };\n}\n```\n\n### 5. Rate Limiting Fallback\n\n**Problem**: Rate limiting requires counters in Redis. If Redis is down:\n- Option A: Disable rate limiting (security risk)\n- Option B: Reject all requests (poor availability)\n\n**Solution**: In-Memory Rate Limiting with Postgres Sync\n\n```typescript\nclass RateLimiterWithFailover {\n  private redis: RedisClient;\n  private localRateLimiter: Map<string, RateLimitCounter>;\n  private circuitBreaker: CircuitBreaker;\n  private syncInterval: NodeJS.Timeout;\n  \n  constructor() {\n    this.localRateLimiter = new Map();\n    \n    // Sync local counters to Postgres every 10 seconds\n    this.syncInterval = setInterval(\n      () => this.syncCountersToPostgres(),\n      10000\n    );\n  }\n  \n  async checkRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    // Try Redis first\n    try {\n      const result = await this.checkRedisRateLimit(key, max, windowSec);\n      return result;\n    } catch (error) {\n      logger.warn('Redis rate limit check failed, using local fallback', {\n        key: this.sanitizeKey(key),\n        error: error.message\n      });\n      \n      metrics.increment('ratelimit.fallback_to_local');\n    }\n    \n    // Use in-memory fallback\n    return this.checkLocalRateLimit(key, max, windowSec);\n  }\n  \n  private async checkRedisRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    const count = await this.circuitBreaker.execute({\n      method: 'incr',\n      args: [key]\n    });\n    \n    if (count === 1) {\n      await this.redis.expire(key, windowSec);\n    }\n    \n    if (count > max) {\n      const ttl = await this.redis.ttl(key);\n      return { allowed: false, retryAfter: ttl };\n    }\n    \n    return { allowed: true };\n  }\n  \n  private checkLocalRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): { allowed: boolean; retryAfter?: number } {\n    \n    const now = Date.now();\n    const windowMs = windowSec * 1000;\n    \n    // Get or create counter\n    let counter = this.localRateLimiter.get(key);\n    \n    if (!counter || now > counter.windowEnd) {\n      // Start new window\n      counter = {\n        count: 1,\n        windowStart: now,\n        windowEnd: now + windowMs,\n        key: key\n      };\n      this.localRateLimiter.set(key, counter);\n      return { allowed: true };\n    }\n    \n    // Increment counter\n    counter.count++;\n    \n    if (counter.count > max) {\n      const retryAfter = Math.ceil((counter.windowEnd - now) / 1000);\n      return { allowed: false, retryAfter };\n    }\n    \n    return { allowed: true };\n  }\n  \n  private async syncCountersToPostgres(): Promise<void> {\n    // Sync local counters to Postgres for cross-instance awareness\n    const counters = Array.from(this.localRateLimiter.entries());\n    \n    if (counters.length === 0) return;\n    \n    try {\n      const values = counters.map(([key, counter]) => ({\n        key: key,\n        count: counter.count,\n        window_start: new Date(counter.windowStart),\n        window_end: new Date(counter.windowEnd),\n        synced_at: new Date()\n      }));\n      \n      await db.query(\n        'INSERT INTO rate_limit_counters (key, count, window_start, window_end, synced_at) ' +\n        'VALUES ' + values.map((_, i) => \n          `($${i*5+1}, $${i*5+2}, $${i*5+3}, $${i*5+4}, $${i*5+5})`\n        ).join(', ') +\n        'ON CONFLICT (key) DO UPDATE SET ' +\n        'count = GREATEST(rate_limit_counters.count, EXCLUDED.count), ' +\n        'synced_at = EXCLUDED.synced_at',\n        values.flatMap(v => [v.key, v.count, v.window_start, v.window_end, v.synced_at])\n      );\n      \n      logger.debug('Synced rate limit counters to Postgres', {\n        count: counters.length\n      });\n      \n    } catch (error) {\n      logger.error('Failed to sync rate limit counters', { error });\n    }\n    \n    // Clean up expired counters\n    const now = Date.now();\n    for (const [key, counter] of counters) {\n      if (now > counter.windowEnd + 60000) { // 1 minute grace\n        this.localRateLimiter.delete(key);\n      }\n    }\n  }\n  \n  async loadCountersFromPostgres(): Promise<void> {\n    // On startup, load recent counters from Postgres\n    // This provides awareness of rate limits from other instances\n    \n    const result = await db.query(\n      'SELECT key, count, window_start, window_end FROM rate_limit_counters ' +\n      'WHERE window_end > NOW() - INTERVAL \\'1 minute\\''\n    );\n    \n    for (const row of result.rows) {\n      this.localRateLimiter.set(row.key, {\n        count: row.count,\n        windowStart: row.window_start.getTime(),\n        windowEnd: row.window_end.getTime(),\n        key: row.key\n      });\n    }\n    \n    logger.info('Loaded rate limit counters from Postgres', {\n      count: result.rowCount\n    });\n  }\n  \n  private sanitizeKey(key: string): string {\n    // Remove sensitive data (emails, IPs) from logs\n    return key.replace(/:[^:]+@/, ':***@').replace(/:\\d+\\.\\d+\\.\\d+\\.\\d+/, ':***');\n  }\n}\n\ninterface RateLimitCounter {\n  count: number;\n  windowStart: number;\n  windowEnd: number;\n  key: string;\n}\n```\n\n**Rate Limit Counters Table (Postgres)**\n```sql\nCREATE TABLE rate_limit_counters (\n  key VARCHAR(500) PRIMARY KEY,\n  count INTEGER NOT NULL,\n  window_start TIMESTAMP NOT NULL,\n  window_end TIMESTAMP NOT NULL,\n  synced_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\nCREATE INDEX idx_rate_limit_window_end ON rate_limit_counters(window_end) \n  WHERE window_end > NOW();\n\n-- Cleanup expired counters\nCREATE OR REPLACE FUNCTION cleanup_rate_limit_counters()\nRETURNS void AS $$\nBEGIN\n  DELETE FROM rate_limit_counters \n  WHERE window_end < NOW() - INTERVAL '1 hour';\nEND;\n$$ LANGUAGE plpgsql;\n\nSELECT cron.schedule('cleanup-rate-limits', '*/15 * * * *', \n  'SELECT cleanup_rate_limit_counters()');\n```\n\n### 6. Circuit Breaker Implementation\n\n```typescript\nclass CircuitBreaker {\n  private state: CircuitState = CircuitState.CLOSED;\n  private failureCount: number = 0;\n  private successCount: number = 0;\n  private lastFailureTime: number = 0;\n  private nextAttemptTime: number = 0;\n  \n  constructor(\n    private operation: (args: any) => Promise<any>,\n    private config: CircuitBreakerConfig\n  ) {}\n  \n  async execute(args: any): Promise<any> {\n    // Check if circuit should transition to half-open\n    if (this.state === CircuitState.OPEN && \n        Date.now() >= this.nextAttemptTime) {\n      this.state = CircuitState.HALF_OPEN;\n      logger.info('Circuit breaker entering HALF_OPEN state');\n      metrics.gauge('circuit_breaker.state', 0.5); // 0=closed, 0.5=half-open, 1=open\n    }\n    \n    // Reject immediately if circuit is open\n    if (this.state === CircuitState.OPEN) {\n      const error = new Error('Circuit breaker is OPEN');\n      error.name = 'CircuitBreakerOpen';\n      metrics.increment('circuit_breaker.rejected');\n      throw error;\n    }\n    \n    try {\n      // Execute with timeout\n      const result = await Promise.race([\n        this.operation(args),\n        this.timeoutPromise(this.config.timeout)\n      ]);\n      \n      // Success\n      this.onSuccess();\n      return result;\n      \n    } catch (error) {\n      // Failure\n      this.onFailure();\n      throw error;\n    }\n  }\n  \n  private onSuccess(): void {\n    this.failureCount = 0;\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      this.successCount++;\n      \n      // After 10 successful requests, close circuit\n      if (this.successCount >= 10) {\n        this.state = CircuitState.CLOSED;\n        this.successCount = 0;\n        logger.info('Circuit breaker closed after successful recovery');\n        metrics.gauge('circuit_breaker.state', 0);\n      }\n    }\n  }\n  \n  private onFailure(): void {\n    this.failureCount++;\n    this.lastFailureTime = Date.now();\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      // Failure in half-open state - immediately re-open circuit\n      this.openCircuit();\n      return;\n    }\n    \n    // Check if we should open the circuit\n    const failureRate = this.failureCount / this.config.requestVolumeThreshold;\n    \n    if (this.failureCount >= this.config.requestVolumeThreshold &&\n        failureRate >= this.config.failureThreshold) {\n      this.openCircuit();\n    }\n  }\n  \n  private openCircuit(): void {\n    this.state = CircuitState.OPEN;\n    this.nextAttemptTime = Date.now() + this.config.resetTimeout;\n    this.successCount = 0;\n    \n    logger.error('Circuit breaker opened', {\n      failureCount: this.failureCount,\n      lastFailureTime: new Date(this.lastFailureTime),\n      nextAttemptTime: new Date(this.nextAttemptTime)\n    });\n    \n    metrics.gauge('circuit_breaker.state', 1);\n    metrics.increment('circuit_breaker.opened');\n  }\n  \n  private timeoutPromise(ms: number): Promise<never> {\n    return new Promise((_, reject) => {\n      setTimeout(() => {\n        const error = new Error(`Operation timed out after ${ms}ms`);\n        error.name = 'TimeoutError';\n        reject(error);\n      }, ms);\n    });\n  }\n  \n  getState(): CircuitState {\n    return this.state;\n  }\n}\n```\n\n## Geographic Location Policy\n\n### 1. Location Change Risk Matrix\n\n```typescript\nenum LocationChangeType {\n  NO_CHANGE = 'no_change',           // Same city\n  CITY_CHANGE = 'city_change',       // Different city, same region\n  REGION_CHANGE = 'region_change',   // Different region, same country\n  COUNTRY_CHANGE = 'country_change', // Different country\n  CONTINENT_CHANGE = 'continent_change' // Different continent\n}\n\nenum ResponseAction {\n  ALLOW = 'allow',                   // Proceed normally\n  LOG = 'log',                       // Allow but log event\n  NOTIFY = 'notify',                 // Allow, log, and notify user\n  CHALLENGE = 'challenge',           // Require additional verification\n  BLOCK = 'block'                    // Deny access, require re-authentication\n}\n\ninterface LocationPolicy {\n  changeType: LocationChangeType;\n  timeWindow: number; // Milliseconds since last activity\n  userTrustLevel: 'new' | 'established' | 'trusted';\n  deviceTrustLevel: 'new' | 'recognized' | 'trusted';\n  action: ResponseAction;\n  notificationDelay?: number; // Delay before notifying (for post-travel)\n}\n\nconst locationPolicies: LocationPolicy[] = [\n  // === ALLOW: Low-risk scenarios ===\n  {\n    changeType: LocationChangeType.NO_CHANGE,\n    timeWindow: Infinity,\n    userTrustLevel: 'new',\n    deviceTrustLevel: 'new',\n    action: ResponseAction.ALLOW\n  },\n  {\n    changeType: LocationChangeType.CITY_CHANGE,\n    timeWindow: 24 * 60 * 60 * 1000, // 24 hours\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.ALLOW\n  },\n  \n  // === LOG: Medium-risk, expected scenarios ===\n  {\n    changeType: LocationChangeType.REGION_CHANGE,\n    timeWindow: 12 * 60 * 60 * 1000, // 12 hours (e.g., domestic travel)\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.LOG\n  },\n  \n  // === NOTIFY: Significant change, likely legitimate ===\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 6 * 60 * 60 * 1000, // 6 hours (flight time)\n    userTrustLevel: 'trusted',\n    deviceTrustLevel: 'trusted',\n    action: ResponseAction.NOTIFY,\n    notificationDelay: 5 * 60 * 1000 // Wait 5 minutes (user may be traveling)\n  },\n  {\n    changeType: LocationChangeType.REGION_CHANGE,\n    timeWindow: 2 * 60 * 60 * 1000, // 2 hours\n    userTrustLevel: 'trusted',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.NOTIFY,\n    notificationDelay: 2 * 60 * 1000\n  },\n  \n  // === CHALLENGE: Suspicious patterns ===\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 2 * 60 * 60 * 1000, // 2 hours (too fast for legitimate travel)\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.CHALLENGE\n  },\n  {\n    changeType: LocationChangeType.CONTINENT_CHANGE,\n    timeWindow: 8 * 60 * 60 * 1000, // 8 hours\n    userTrustLevel: 'new',\n    deviceTrustLevel: 'new',\n    action: ResponseAction.CHALLENGE\n  },\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 24 * 60 * 60 * 1000, // Any country change within 24h\n    userTrustLevel: 'new',\n    deviceTrustLevel: 'new',\n    action: ResponseAction.CHALLENGE\n  },\n  \n  // === BLOCK: High-risk scenarios ===\n  {\n    changeType: LocationChangeType.CONTINENT_CHANGE,\n    timeWindow: 2 * 60 * 60 * 1000, // 2 hours (impossible travel speed)\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.BLOCK\n  },\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 30 * 60 * 1000, // 30 minutes (impossible)\n    userTrustLevel: 'trusted',\n    deviceTrustLevel: 'trusted',\n    action: ResponseAction.BLOCK\n  }\n];\n```\n\n### 2. VPN and Proxy Detection\n\n```typescript\ninterface IpIntelligence {\n  isVpn: boolean;\n  isProxy: boolean;\n  isTor: boolean;\n  isDatacenter: boolean;\n  isHosting: boolean;\n  riskScore: number; // 0-100\n  country: string;\n  region: string;\n  city: string;\n  carrier?: string; // For mobile IPs\n  asn?: string; // Autonomous System Number\n}\n\nasync function getIpIntelligence(ipAddress: string): Promise<IpIntelligence> {\n  // Use service like IPHub, IPQualityScore, or MaxMind\n  const response = await fetch(\n    `https://ipqualityscore.com/api/json/ip/${API_KEY}/${ipAddress}`,\n    { timeout: 2000 }\n  );\n  \n  const data = await response.json();\n  \n  return {\n    isVpn: data.vpn,\n    isProxy: data.proxy,\n    isTor: data.tor,\n    isDatacenter: data.is_crawler || data.recent_abuse,\n    isHosting: data.host === 'hosting',\n    riskScore: data.fraud_score,\n    country: data.country_code,\n    region: data.region,\n    city: data.city,\n    carrier: data.mobile ? data.ISP : undefined,\n    asn: data.ASN\n  };\n}\n\ninterface VpnPolicy {\n  allowVpn: boolean;\n  allowTor: boolean;\n  allowDatacenter: boolean;\n  requireVerificationFor: ('vpn' | 'tor' | 'datacenter')[];\n  blockHighRiskIps: boolean;\n  riskScoreThreshold: number;\n}\n\nconst vpnPolicy: VpnPolicy = {\n  allowVpn: true,           // Generally allow VPNs (common for privacy/work)\n  allowTor: false,          // Block Tor (high anonymity risk)\n  allowDatacenter: false,   // Block datacenter IPs (common for bots)\n  requireVerificationFor: ['vpn'], // Challenge VPN users on location change\n  blockHighRiskIps: true,\n  riskScoreThreshold: 85    // Block IPs with risk score > 85\n};\n\nasync function evaluateIpRisk(\n  ipAddress: string,\n  userTrustLevel: 'new' | 'established' | 'trusted'\n): Promise<{ allowed: boolean; action: ResponseAction; reason?: string }> {\n  \n  let ipInfo: IpIntelligence;\n  \n  try {\n    ipInfo = await getIpIntelligence(ipAddress);\n  } catch (error) {\n    // If IP intelligence service is unavailable, fail open for established users\n    logger.warn('IP intelligence check failed', { ipAddress, error });\n    \n    if (userTrustLevel === 'new') {\n      // Be more cautious with new users\n      return { \n        allowed: true, \n        action: ResponseAction.CHALLENGE,\n        reason: 'ip_intelligence_unavailable' \n      };\n    }\n    \n    return { allowed: true, action: ResponseAction.LOG };\n  }\n  \n  // Block Tor\n  if (ipInfo.isTor && !vpnPolicy.allowTor) {\n    return { \n      allowed: false, \n      action: ResponseAction.BLOCK,\n      reason: 'tor_not_allowed' \n    };\n  }\n  \n  // Block datacenter IPs\n  if (ipInfo.isDatacenter && !vpnPolicy.allowDatacenter) {\n    return { \n      allowed: false, \n      action: ResponseAction.BLOCK,\n      reason: 'datacenter_ip_not_allowed' \n    };\n  }\n  \n  // Block high-risk IPs\n  if (vpnPolicy.blockHighRiskIps && \n      ipInfo.riskScore > vpnPolicy.riskScoreThreshold) {\n    return { \n      allowed: false, \n      action: ResponseAction.BLOCK,\n      reason: `high_risk_score_${ipInfo.riskScore}` \n    };\n  }\n  \n  // Challenge VPN users (especially on location change)\n  if (ipInfo.isVpn && \n      vpnPolicy.requireVerificationFor.includes('vpn') &&\n      userTrustLevel !== 'trusted') {\n    return { \n      allowed: true, \n      action: ResponseAction.CHALLENGE,\n      reason: 'vpn_detected' \n    };\n  }\n  \n  // Allow VPN for trusted users\n  if (ipInfo.isVpn && userTrustLevel === 'trusted') {\n    return { \n      allowed: true, \n      action: ResponseAction.LOG,\n      reason: 'vpn_detected_trusted_user' \n    };\n  }\n  \n  return { allowed: true, action: ResponseAction.ALLOW };\n}\n```\n\n### 3. User and Device Trust Levels\n\n```typescript\ninterface TrustFactors {\n  accountAge: number;           // Days since account creation\n  totalLogins: number;          // Lifetime login count\n  consistentLocations: number;  // Number of distinct locations\n  devicesUsed: number;          // Number of distinct devices\n  securityIncidents: number;    // Past security events\n  mfaEnabled: boolean;\n  recentLocationChanges: number; // Location changes in last 30 days\n  verifiedDevices: number;      // Explicitly verified devices\n}\n\nfunction calculateUserTrustLevel(factors: TrustFactors): 'new' | 'established' | 'trusted' {\n  // New user: < 7 days, < 10 logins\n  if (factors.accountAge < 7 || factors.totalLogins < 10) {\n    return 'new';\n  }\n  \n  // Trusted user: Multiple criteria\n  const trustedCriteria = [\n    factors.accountAge >= 90,              // 90+ days old\n    factors.totalLogins >= 50,             // 50+ logins\n    factors.consistentLocations <= 5,      // Consistent behavior\n    factors.securityIncidents === 0,       // No security issues\n    factors.mfaEnabled,                    // MFA enabled\n    factors.verifiedDevices >= 2           // Multiple verified devices\n  ];\n  \n  const trustedCount = trustedCriteria.filter(Boolean).length;\n  \n  if (trustedCount >= 4) {\n    return 'trusted';\n  }\n  \n  // Established: Between new and trusted\n  return 'established';\n}\n\nfunction calculateDeviceTrustLevel(\n  device: Device,\n  userTrustLevel: 'new' | 'established' | 'trusted'\n): 'new' | 'recognized' | 'trusted' {\n  \n  const deviceAge = Date.now() - device.first_seen_at.getTime();\n  const deviceAgeDays = deviceAge / (24 * 60 * 60 * 1000);\n  \n  // New device: < 1 day, < 5 uses\n  if (deviceAgeDays < 1 || device.times_used < 5) {\n    return 'new';\n  }\n  \n  // Trusted device: User is trusted + device is verified + consistent use\n  if (userTrustLevel === 'trusted' &&\n      device.verified &&\n      deviceAgeDays >= 30 &&\n      device.times_used >= 20 &&\n      device.security_incidents === 0) {\n    return 'trusted';\n  }\n  \n  // Recognized: Between new and trusted\n  return 'recognized';\n}\n\ninterface Device {\n  id: string;\n  device_id: string;\n  device_name: string;\n  device_type: string;\n  first_seen_at: Date;\n  last_seen_at: Date;\n  times_used: number;\n  verified: boolean; // User explicitly verified this device\n  security_incidents: number;\n}\n```\n\n### 4. Location Change Evaluation\n\n```typescript\nasync function evaluateLocationChange(\n  userId: string,\n  sessionId: string,\n  newIpAddress: string\n): Promise<LocationChangeDecision> {\n  \n  // 1. Get current and previous location\n  const session = await getSessionById(sessionId);\n  const prevIpInfo = await getCachedIpInfo(session.ip_address);\n  const newIpInfo = await getIpIntelligence(newIpAddress);\n  \n  // 2. Determine change type\n  const changeType = determineLocationChangeType(prevIpInfo, newIpInfo);\n  \n  // 3. Calculate time since last activity\n  const timeSinceLastActivity = Date.now() - session.last_used_at.getTime();\n  \n  // 4. Get user and device trust levels\n  const user = await getUserById(userId);\n  const trustFactors = await calculateUserTrustFactors(userId);\n  const userTrustLevel = calculateUserTrustLevel(trustFactors);\n  \n  const device = await getDeviceBySession(sessionId);\n  const deviceTrustLevel = calculateDeviceTrustLevel(device, userTrustLevel);\n  \n  // 5. Check IP risk (VPN, Tor, datacenter)\n  const ipRisk = await evaluateIpRisk(newIpAddress, userTrustLevel);\n  \n  if (!ipRisk.allowed) {\n    return {\n      allowed: false,\n      action: ipRisk.action,\n      reason: ipRisk.reason,\n      requiresStepUp: false\n    };\n  }\n  \n  // 6. Find matching policy\n  const policy = findMatchingLocationPolicy(\n    changeType,\n    timeSinceLastActivity,\n    userTrustLevel,\n    deviceTrustLevel\n  );\n  \n  // 7. Special handling for VPN users\n  if (newIpInfo.isVpn || prevIpInfo?.isVpn) {\n    return handleVpnLocationChange(\n      policy,\n      newIpInfo,\n      prevIpInfo,\n      userTrustLevel,\n      timeSinceLastActivity\n    );\n  }\n  \n  // 8. Apply policy\n  const decision = applyLocationPolicy(\n    policy,\n    userId,\n    sessionId,\n    prevIpInfo,\n    newIpInfo,\n    timeSinceLastActivity\n  );\n  \n  return decision;\n}\n\nfunction determineLocationChangeType(\n  prevLocation: IpIntelligence | null,\n  newLocation: IpIntelligence\n): LocationChangeType {\n  \n  if (!prevLocation) {\n    return LocationChangeType.NO_CHANGE;\n  }\n  \n  // Same city\n  if (prevLocation.city === newLocation.city) {\n    return LocationChangeType.NO_CHANGE;\n  }\n  \n  // Same region, different city\n  if (prevLocation.region === newLocation.region) {\n    return LocationChangeType.CITY_CHANGE;\n  }\n  \n  // Same country, different region\n  if (prevLocation.country === newLocation.country) {\n    return LocationChangeType.REGION_CHANGE;\n  }\n  \n  // Different country\n  const prevContinent = getContinent(prevLocation.country);\n  const newContinent = getContinent(newLocation.country);\n  \n  if (prevContinent === newContinent) {\n    return LocationChangeType.COUNTRY_CHANGE;\n  }\n  \n  // Different continent\n  return LocationChangeType.CONTINENT_CHANGE;\n}\n\nfunction handleVpnLocationChange(\n  policy: LocationPolicy,\n  newIpInfo: IpIntelligence,\n  prevIpInfo: IpIntelligence | null,\n  userTrustLevel: 'new' | 'established' | 'trusted',\n  timeSinceLastActivity: number\n): LocationChangeDecision {\n  \n  // Both IPs are VPN - can't reliably detect location change\n  if (prevIpInfo?.isVpn && newIpInfo.isVpn) {\n    \n    // Check if same VPN provider (via ASN)\n    if (prevIpInfo.asn === newIpInfo.asn) {\n      // Same VPN provider - likely legitimate\n      return {\n        allowed: true,\n        action: ResponseAction.LOG,\n        reason: 'vpn_location_change_same_provider',\n        requiresStepUp: false\n      };\n    }\n    \n    // Different VPN providers\n    if (userTrustLevel === 'trusted') {\n      // Trusted user switching VPNs - allow with notification\n      return {\n        allowed: true,\n        action: ResponseAction.NOTIFY,\n        reason: 'vpn_provider_change',\n        requiresStepUp: false,\n        notificationDelay: 5 * 60 * 1000 // 5 minutes\n      };\n    }\n    \n    // Non-trusted user switching VPNs - challenge\n    return {\n      allowed: true,\n      action: ResponseAction.CHALLENGE,\n      reason: 'vpn_provider_change_untrusted',\n      requiresStepUp: true\n    };\n  }\n  \n  // Switching from non-VPN to VPN or vice versa\n  if (prevIpInfo && !prevIpInfo.isVpn && newIpInfo.isVpn) {\n    // User enabled VPN\n    if (userTrustLevel === 'trusted') {\n      return {\n        allowed: true,\n        action: ResponseAction.LOG,\n        reason: 'vpn_enabled',\n        requiresStepUp: false\n      };\n    }\n    \n    // New/established user enabling VPN - challenge\n    return {\n      allowed: true,\n      action: ResponseAction.CHALLENGE,\n      reason: 'vpn_enabled_untrusted',\n      requiresStepUp: true\n    };\n  }\n  \n  if (prevIpInfo?.isVpn && !newIpInfo.isVpn) {\n    // User disabled VPN - now we see real location\n    // Check if the revealed location makes sense\n    const changeType = determineLocationChangeType(\n      { ...prevIpInfo, isVpn: false }, // Treat as if previous was non-VPN\n      newIpInfo\n    );\n    \n    // Apply normal policy to revealed location\n    const revealedPolicy = findMatchingLocationPolicy(\n      changeType,\n      timeSinceLastActivity,\n      userTrustLevel,\n      'recognized' // Treat device as recognized\n    );\n    \n    return {\n      allowed: true,\n      action: revealedPolicy.action,\n      reason: 'vpn_disabled_location_revealed',\n      requiresStepUp: revealedPolicy.action === ResponseAction.CHALLENGE\n    };\n  }\n  \n  // Fallback to original policy\n  return {\n    allowed: true,\n    action: policy.action,\n    reason: 'location_change_with_vpn',\n    requiresStepUp: policy.action === ResponseAction.CHALLENGE\n  };\n}\n\nfunction findMatchingLocationPolicy(\n  changeType: LocationChangeType,\n  timeSinceLastActivity: number,\n  userTrustLevel: 'new' | 'established' | 'trusted',\n  deviceTrustLevel: 'new' | 'recognized' | 'trusted'\n): LocationPolicy {\n  \n  // Find most specific matching policy\n  const matches = locationPolicies.filter(policy =>\n    policy.changeType === changeType &&\n    timeSinceLastActivity <= policy.timeWindow &&\n    (policy.userTrustLevel === userTrustLevel || !policy.userTrustLevel) &&\n    (policy.deviceTrustLevel === deviceTrustLevel || !policy.deviceTrustLevel)\n  );\n  \n  if (matches.length === 0) {\n    // Default fallback: CHALLENGE on significant change\n    return {\n      changeType,\n      timeWindow: Infinity,\n      userTrustLevel,\n      deviceTrustLevel,\n      action: changeType >= LocationChangeType.COUNTRY_CHANGE \n        ? ResponseAction.CHALLENGE \n        : ResponseAction.LOG\n    };\n  }\n  \n  // Return most restrictive policy if multiple matches\n  return matches.sort((a, b) => {\n    const actionPriority = {\n      [ResponseAction.BLOCK]: 5,\n      [ResponseAction.CHALLENGE]: 4,\n      [ResponseAction.NOTIFY]: 3,\n      [ResponseAction.LOG]: 2,\n      [ResponseAction.ALLOW]: 1\n    };\n    \n    return actionPriority[b.action] - actionPriority[a.action];\n  })[0];\n}\n\nasync function applyLocationPolicy(\n  policy: LocationPolicy,\n  userId: string,\n  sessionId: string,\n  prevLocation: IpIntelligence | null,\n  newLocation: IpIntelligence,\n  timeSinceLastActivity: number\n): Promise<LocationChangeDecision> {\n  \n  const changeDetails = {\n    userId,\n    sessionId,\n    changeType: policy.changeType,\n    fromCountry: prevLocation?.country,\n    toCountry: newLocation.country,\n    fromCity: prevLocation?.city,\n    toCity: newLocation.city,\n    timeSinceLastActivity,\n    isVpn: newLocation.isVpn\n  };\n  \n  switch (policy.action) {\n    case ResponseAction.ALLOW:\n      // No action needed\n      return {\n        allowed: true,\n        action: ResponseAction.ALLOW,\n        requiresStepUp: false\n      };\n    \n    case ResponseAction.LOG:\n      // Log event for audit trail\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE',\n        severity: 'LOW',\n        details: changeDetails\n      });\n      \n      return {\n        allowed: true,\n        action: ResponseAction.LOG,\n        requiresStepUp: false\n      };\n    \n    case ResponseAction.NOTIFY:\n      // Log event\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE',\n        severity: 'MEDIUM',\n        details: changeDetails\n      });\n      \n      // Schedule notification (delayed to avoid false positives during travel)\n      if (policy.notificationDelay) {\n        setTimeout(async () => {\n          // Check if user is still in new location before notifying\n          const currentSession = await getSessionById(sessionId);\n          if (currentSession.ip_address === newLocation) {\n            await notifyUserLocationChange(userId, changeDetails);\n          }\n        }, policy.notificationDelay);\n      } else {\n        await notifyUserLocationChange(userId, changeDetails);\n      }\n      \n      return {\n        allowed: true,\n        action: ResponseAction.NOTIFY,\n        requiresStepUp: false\n      };\n    \n    case ResponseAction.CHALLENGE:\n      // Log high-severity event\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE_CHALLENGE',\n        severity: 'HIGH',\n        details: changeDetails\n      });\n      \n      // Require step-up authentication\n      return {\n        allowed: true,\n        action: ResponseAction.CHALLENGE,\n        requiresStepUp: true,\n        challengeType: 'location_verification',\n        challengeData: {\n          newLocation: `${newLocation.city}, ${newLocation.country}`,\n          prevLocation: prevLocation \n            ? `${prevLocation.city}, ${prevLocation.country}` \n            : 'Unknown'\n        }\n      };\n    \n    case ResponseAction.BLOCK:\n      // Log critical event\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE_BLOCKED',\n        severity: 'CRITICAL',\n        details: changeDetails\n      });\n      \n      // Revoke session\n      await revokeToken(sessionId, 'IMPOSSIBLE_TRAVEL_DETECTED');\n      \n      // Notify user immediately\n      await notifyUserSuspiciousActivity(userId, {\n        type: 'IMPOSSIBLE_TRAVEL',\n        details: changeDetails\n      });\n      \n      return {\n        allowed: false,\n        action: ResponseAction.BLOCK,\n        requiresStepUp: false,\n        reason: 'impossible_travel_detected'\n      };\n  }\n}\n\ninterface LocationChangeDecision {\n  allowed: boolean;\n  action: ResponseAction;\n  requiresStepUp: boolean;\n  challengeType?: 'location_verification' | 'mfa' | 'device_verification';\n  challengeData?: any;\n  reason?: string;\n  notificationDelay?: number;\n}\n```\n\n### 5. Mobile User Considerations\n\n```typescript\nasync function handleMobileLocationChange(\n  userId: string,\n  sessionId: string,\n  newIpInfo: IpIntelligence,\n  prevIpInfo: IpIntelligence | null\n): Promise<LocationChangeDecision> {\n  \n  // Mobile IPs change frequently due to cell tower switching\n  // Be more lenient with location changes for mobile users\n  \n  // Check if both IPs are from same mobile carrier\n  if (newIpInfo.carrier && prevIpInfo?.carrier && \n      newIpInfo.carrier === prevIpInfo.carrier) {\n    \n    // Same carrier - likely just cell tower switching\n    if (newIpInfo.region === prevIpInfo.region) {\n      // Within same region - definitely normal\n      return {\n        allowed: true,\n        action: ResponseAction.ALLOW,\n        requiresStepUp: false,\n        reason: 'mobile_carrier_ip_rotation'\n      };\n    }\n    \n    // Different region but same carrier - still likely legitimate\n    return {\n      allowed: true,\n      action: ResponseAction.LOG,\n      requiresStepUp: false,\n      reason: 'mobile_carrier_region_change'\n    };\n  }\n  \n  // Carrier changed or not mobile - apply normal location policy\n  // but with more lenient thresholds\n  return evaluateLocationChange(userId, sessionId, newIpInfo.toString());\n}\n```\n\n## Rationale\n\n### Redis Failover Strategy\n\n1. **Multi-layer caching**: Provides defense in depth. If Redis fails, we have in-memory cache and Postgres as fallbacks.\n\n2. **Circuit breaker**: Prevents cascading failures. If Redis is struggling, we stop sending requests and fail fast, allowing the system to recover.\n\n3. **Risk-based token validation**: Not all tokens need the same level of scrutiny. Low-risk read operations can tolerate slightly elevated risk when Redis is down. High-risk operations (admin actions, writes) require authoritative checks.\n\n4. **Postgres as source of truth**: All critical data (blacklist, refresh tokens, rate limits) is persisted to Postgres. Redis is a performance optimization, not a single point of failure.\n\n5. **Graceful degradation**: System remains available (albeit slower) when Redis fails, rather than complete outage.\n\n### Geographic Location Policy\n\n1. **Trust-based approach**: Trusted users with established patterns get more flexibility. New users face stricter scrutiny.\n\n2. **Context-aware decisions**: Time since last activity matters. A country change after 8 hours (flight time) is different from 30 minutes (impossible).\n\n3. **VPN-aware**: Recognizes that VPN users will have unpredictable location changes. Same VPN provider (ASN) suggests legitimate use.\n\n4. **Mobile-friendly**: Mobile carrier IPs change frequently. Policy accounts for this without compromising security.\n\n5. **Balanced notifications**: Delayed notifications for travel scenarios prevent alarm fatigue. Immediate alerts for suspicious activity.\n\n6. **Progressive responses**: Start with logging, escalate to notification, then challenge, finally block. Matches severity to risk.\n\n## What I Changed\n\n### From Previous Design\n\n1. **Added comprehensive Redis failover**: Previous design relied heavily on Redis without fallback strategy. Now we have circuit breaker, multi-layer cache, and Postgres fallback.\n\n2. **Implemented risk-based token validation**: Instead of binary \"check blacklist or fail\", we now assess risk level and apply appropriate fallback (Postgres check for medium risk, allow for low risk, block for high risk).\n\n3. **Added Postgres advisory locks**: Fallback for distributed locking when Redis unavailable. Ensures token refresh remains safe even during Redis outage.\n\n4. **Created in-memory rate limiting**: With Postgres sync every 10 seconds. Prevents complete rate limiting failure when Redis down.\n\n5. **Detailed location change policy matrix**: Moved from vague \"check location\" to explicit matrix of change types, time windows, trust levels, and actions.\n\n6. **VPN-specific handling**: Recognizes VPN usage patterns, distinguishes between same/different VPN providers, handles VPN enable/disable scenarios.\n\n7. **Mobile carrier awareness**: Treats mobile carrier IP changes more leniently, as they're frequent and expected.\n\n8. **IP intelligence integration**: Added VPN/Tor/datacenter detection with risk scoring. Blocks high-risk IPs proactively.\n\n9. **Trust level calculations**: Quantified user and device trust based on objective factors (account age, login count, MFA, etc.).\n\n10. **Progressive notification delays**: For travel scenarios, delay notifications to avoid false alarms. Immediate alerts for suspicious patterns.\n\n## What I Kept\n\nFrom the previous design's strong foundation:\n\n1. **15-minute access token lifetime** with **30-day refresh tokens**\n2. **Opaque refresh tokens** with cryptographic randomness\n3. **httpOnly cookies** with SameSite=Strict\n4. **Token family tracking** for compromise detection\n5. **Device fingerprinting** for additional binding\n6. **Comprehensive data models** (users, refresh_tokens, security_events)\n7. **Token rotation on refresh** with one-time use\n8. **Multi-device session management** with limits\n9. **Rate limiting** across multiple dimensions\n10. **Security event logging** with severity levels\n\n## Open Questions\n\n### 1. Redis HA Configuration Complexity\n\n**Question**: Should we mandate Redis Sentinel (3+ nodes) or allow simpler Redis configurations for smaller deployments?\n\n**Trade-off**: \n- Sentinel provides automatic failover but adds operational complexity\n- Smaller teams may struggle with Sentinel configuration\n- Fallback strategy makes Redis HA less critical than before\n\n**Recommendation**: Make Sentinel optional but strongly recommended. Document both paths clearly.\n\n### 2. In-Memory Cache Size Limits\n\n**Question**: What's the appropriate size limit for in-memory cache (LRU)?\n\n**Considerations**:\n- Larger cache = more memory usage per instance\n- Smaller cache = more Postgres fallback queries\n- Cache contains sensitive data (token hashes, user IDs)\n- Need to balance coverage vs. memory constraints\n\n**Current proposal**: 10,000 entries. Needs load testing to validate.\n\n### 3. Postgres Write Amplification at Scale\n\n**Concern**: Every token refresh writes to:\n- `refresh_tokens` table (INSERT + UPDATE parent)\n- `token_blacklist` table (when old token has active access token)\n- `rate_limit_counters` table (sync every 10s)\n\nAt 1M active users with 1 refresh/hour = 278 writes/second to Postgres.\n\n**Potential optimizations**:\n- Batch token updates (trade-off: slight delay in revocation)\n- Use write-behind caching for non-critical fields (last_used_at)\n- Partition refresh_tokens table by created_at\n- Consider separate write-optimized database for audit logs\n\n**Question**: At what scale does this become a problem? Need load testing.\n\n### 4. IP Intelligence Service Dependency\n\n**Concern**: IP intelligence (VPN detection, geolocation, risk scoring) adds external dependency.\n\n**Failure scenarios**:\n- Service outage\n- Rate limit exceeded\n- Slow response times\n\n**Current handling**: Fail open for established users, fail closed (challenge) for new users.\n\n**Question**: Should we cache IP intelligence results longer? Build internal IP intelligence database?\n\n### 5. Cross-Region Redis Replication\n\n**Question**: For global deployments, should we use:\n- Single Redis cluster (latency for distant regions)\n- Regional Redis clusters (consistency challenges)\n- Redis Enterprise with active-active geo-replication (expensive)\n\n**Current design**: Assumes single-region deployment. Needs adaptation for global scale.\n\n### 6. Token Blacklist Growth\n\n**Concern**: Token blacklist grows with every logout, password change, and revocation.\n\n**Math**: \n- 1M users\n- Average 2 logouts/day\n- 15-minute token lifetime\n- Blacklist entry TTL = 15 minutes\n\n= 1M * 2 / (24 * 60 / 15) = ~21K concurrent blacklist entries\n\nManageable, but growth is linear with user activity.\n\n**Question**: Should we implement bloom filter for blacklist checks (space-efficient, some false positives)?\n\n## Convergence Status\n\n**ITERATING** - This is round 2. The design now addresses:\n- \u2705 Redis failure scenarios with comprehensive fallback strategy\n- \u2705 Geographic location policy with detailed matrix\n- \u2705 VPN and mobile user considerations\n- \u2705 Risk-based decision making\n\nHowever, we need another iteration to validate:\n- Scaling characteristics (write amplification, cache sizing)\n- Operational complexity (Redis Sentinel setup, monitoring requirements)\n- Cross-region deployment strategy\n- Performance under load (especially Postgres fallback paths)\n\nThe architecture is now resilient to Redis failures, but we should stress-test the fallback paths to ensure performance is acceptable.\n\n##",
        "prompt_for_other": "Please evaluate the **Postgres fallback performance** under Redis failure scenarios:\n\n1. **Token validation fallback**: When checking blacklist in Postgres instead of Redis, what's the expected latency impact? Will the `token_blacklist` table index (`idx_token_blacklist_expires`) be sufficient for fast lookups at scale?\n\n2. **Refresh token locking**: Postgres advisory locks are used as fallback for distributed locking. What's the contention risk when many users refresh simultaneously? Should we implement lock sharding?\n\n3. **Rate limiting sync**: Syncing in-memory counters to Postgres every 10 seconds creates burst write load. How should we batch these updates efficiently? Should we use UPSERT with ON CONFLICT or separate INSERT/UPDATE?\n\n4. **Circuit breaker thresholds**: I proposed 50% failure rate over 100 requests. Is this too aggressive (trips too easily) or too lenient (allows too much failure)?\n\nAdditionally, spot any **security vulnerabilities** introduced by the fallback mechanisms. Does graceful degradation create new attack surfaces?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# JWT Authentication System with Redis Failover & Geo-Location Policy\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 (Redis HA)  \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502             \u2502\n                                           \u25bc             \u25bc\n                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                    \u2502  Database   \u2502 \u2502  Fallback   \u2502\n                                    \u2502  (Postgres) \u2502 \u2502  Cache      \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  (In-Mem)   \u2502\n                                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Redis High Availability & Failover Strategy\n\n### 1. Redis Deployment Architecture\n\n**Primary Configuration: Redis Sentinel**\n```yaml\n# Redis Sentinel for automatic failover\nredis-sentinel:\n  master:\n    name: auth-master\n    host: redis-master.internal\n    port: 6379\n  replicas:\n    - redis-replica-1.internal:6379\n    - redis-replica-2.internal:6379\n  sentinels:\n    - sentinel-1.internal:26379\n    - sentinel-2.internal:26379\n    - sentinel-3.internal:26379\n  quorum: 2  # Minimum sentinels needed for failover decision\n  \n  # Connection from application\n  connection_strategy: sentinel_aware\n  sentinel_failover_timeout: 10000  # 10 seconds\n  max_retry_attempts: 3\n```\n\n**Circuit Breaker Configuration**\n```typescript\ninterface CircuitBreakerConfig {\n  // Trip circuit if 50% of requests fail in window\n  failureThreshold: 0.5;\n  \n  // Consider last 100 requests\n  requestVolumeThreshold: 100;\n  \n  // Check health every 10 seconds when open\n  resetTimeout: 10000;\n  \n  // Operations timeout\n  timeout: 2000; // 2 seconds\n  \n  // What qualifies as failure\n  errorThresholdPercentage: 50;\n}\n\nenum CircuitState {\n  CLOSED = 'closed',    // Normal operation\n  OPEN = 'open',        // Redis unavailable, use fallback\n  HALF_OPEN = 'half_open' // Testing if Redis recovered\n}\n```\n\n### 2. Multi-Layer Caching Strategy\n\n**Layer 1: Redis (Primary)**\n- Fastest access path\n- Distributed across application instances\n- Handles 99.9% of requests under normal conditions\n\n**Layer 2: In-Process Cache (Fallback)**\n- LRU cache in application memory\n- Size-limited (e.g., 10,000 entries)\n- Synchronizes with Redis when available\n- Used when Redis is unreachable\n\n**Layer 3: Postgres (Source of Truth)**\n- Authoritative data store\n- Slower but always available\n- Used for cache misses and rebuilding\n\n```typescript\ninterface CacheLayer {\n  get(key: string): Promise<any | null>;\n  set(key: string, value: any, ttl: number): Promise<void>;\n  delete(key: string): Promise<void>;\n  isAvailable(): Promise<boolean>;\n}\n\nclass MultiLayerCache {\n  private redis: RedisClient;\n  private localCache: LRUCache;\n  private postgres: PostgresClient;\n  private circuitBreaker: CircuitBreaker;\n  \n  constructor() {\n    this.redis = new RedisClient({\n      sentinels: [...],\n      name: 'auth-master',\n      retryStrategy: (times) => Math.min(times * 50, 2000)\n    });\n    \n    this.localCache = new LRUCache({\n      max: 10000, // Maximum entries\n      ttl: 300000, // 5 minutes default TTL\n      updateAgeOnGet: true,\n      updateAgeOnHas: true\n    });\n    \n    this.circuitBreaker = new CircuitBreaker(\n      async (operation) => this.redis[operation.method](...operation.args),\n      circuitBreakerConfig\n    );\n  }\n  \n  async get(key: string): Promise<any | null> {\n    // Try Redis first (through circuit breaker)\n    try {\n      const redisResult = await this.circuitBreaker.execute({\n        method: 'get',\n        args: [key]\n      });\n      \n      if (redisResult !== null) {\n        // Update local cache\n        this.localCache.set(key, redisResult);\n        return redisResult;\n      }\n    } catch (error) {\n      logger.warn('Redis unavailable, using fallback', { key, error });\n      metrics.increment('cache.redis.failure');\n    }\n    \n    // Try local cache\n    const localResult = this.localCache.get(key);\n    if (localResult !== undefined) {\n      metrics.increment('cache.local.hit');\n      return localResult;\n    }\n    \n    // Fall back to Postgres\n    metrics.increment('cache.postgres.fallback');\n    const dbResult = await this.getFromPostgres(key);\n    \n    // Populate caches for future requests\n    if (dbResult !== null) {\n      this.localCache.set(key, dbResult);\n      // Try to update Redis in background (best effort)\n      this.updateRedisAsync(key, dbResult).catch(() => {});\n    }\n    \n    return dbResult;\n  }\n  \n  async set(key: string, value: any, ttl: number): Promise<void> {\n    // Write to Postgres first (source of truth)\n    await this.writeToPostgres(key, value, ttl);\n    \n    // Update local cache immediately\n    this.localCache.set(key, value, { ttl });\n    \n    // Try to update Redis (best effort)\n    try {\n      await this.circuitBreaker.execute({\n        method: 'setex',\n        args: [key, ttl, JSON.stringify(value)]\n      });\n    } catch (error) {\n      logger.warn('Redis write failed, continuing with local cache', { key });\n      metrics.increment('cache.redis.write_failure');\n      // Not a critical failure - data is in Postgres and local cache\n    }\n  }\n  \n  async delete(key: string): Promise<void> {\n    // Delete from all layers\n    await Promise.allSettled([\n      this.deleteFromPostgres(key),\n      this.circuitBreaker.execute({ method: 'del', args: [key] }),\n      Promise.resolve(this.localCache.delete(key))\n    ]);\n  }\n  \n  private async getFromPostgres(key: string): Promise<any | null> {\n    const keyParts = this.parseKey(key);\n    \n    switch (keyParts.type) {\n      case 'blacklist':\n        return this.getBlacklistedToken(keyParts.jti);\n      case 'refresh':\n        return this.getRefreshToken(keyParts.tokenHash);\n      case 'ratelimit':\n        return this.getRateLimitCounter(keyParts.identifier);\n      default:\n        return null;\n    }\n  }\n  \n  private async writeToPostgres(key: string, value: any, ttl: number): Promise<void> {\n    const keyParts = this.parseKey(key);\n    const expiresAt = new Date(Date.now() + ttl * 1000);\n    \n    switch (keyParts.type) {\n      case 'blacklist':\n        await this.blacklistToken(keyParts.jti, value, expiresAt);\n        break;\n      case 'refresh':\n        await this.storeRefreshToken(keyParts.tokenHash, value, expiresAt);\n        break;\n      case 'ratelimit':\n        await this.updateRateLimitCounter(keyParts.identifier, value, expiresAt);\n        break;\n    }\n  }\n  \n  private parseKey(key: string): { type: string; [key: string]: string } {\n    // Parse keys like \"blacklist:jti:{jti}\", \"refresh:{hash}\", \"ratelimit:login:{ip}:{email}\"\n    const parts = key.split(':');\n    return {\n      type: parts[0],\n      ...this.extractKeyComponents(parts)\n    };\n  }\n}\n```\n\n### 3. Token Blacklist Failover Strategy\n\n**Problem**: Access token validation needs to check blacklist. If Redis is down, we risk:\n- Option A: Reject all requests (poor availability)\n- Option B: Accept all tokens (security risk)\n\n**Solution**: Graceful Degradation with Risk-Based Decision\n\n```typescript\ninterface TokenValidationStrategy {\n  // How critical is blacklist checking for this token?\n  riskLevel: 'low' | 'medium' | 'high';\n  \n  // What to do if Redis unavailable?\n  fallbackBehavior: 'accept' | 'reject' | 'accept_with_logging';\n  \n  // Additional checks to perform when Redis down\n  compensatingControls: string[];\n}\n\nasync function validateAccessTokenWithFailover(\n  token: string,\n  req: Request\n): Promise<TokenValidation> {\n  // 1. Verify JWT signature and basic claims\n  const payload = jwt.verify(token, getPublicKey(token), {\n    algorithms: ['RS256'],\n    issuer: 'auth.example.com',\n    audience: 'api.example.com',\n    clockTolerance: 10\n  });\n  \n  // 2. Determine risk level for this request\n  const riskLevel = calculateRiskLevel(payload, req);\n  \n  // 3. Try to check blacklist (with circuit breaker)\n  let isBlacklisted: boolean | null = null;\n  \n  try {\n    isBlacklisted = await cache.get(`blacklist:jti:${payload.jti}`) !== null;\n  } catch (error) {\n    // Redis unavailable - proceed based on risk level\n    logger.warn('Blacklist check failed, using fallback strategy', {\n      jti: payload.jti,\n      riskLevel,\n      error: error.message\n    });\n    \n    metrics.increment('auth.blacklist_check.failure', {\n      risk_level: riskLevel\n    });\n    \n    // Apply fallback strategy\n    return await handleBlacklistUnavailable(payload, req, riskLevel);\n  }\n  \n  if (isBlacklisted) {\n    return { valid: false, reason: 'TOKEN_REVOKED' };\n  }\n  \n  // 4. Additional validation\n  const user = await getUserById(payload.sub);\n  \n  if (!user) {\n    return { valid: false, reason: 'USER_NOT_FOUND' };\n  }\n  \n  if (payload.token_version !== user.token_version) {\n    return { valid: false, reason: 'TOKEN_VERSION_MISMATCH' };\n  }\n  \n  if (user.password_changed_at > new Date(payload.iat * 1000)) {\n    return { valid: false, reason: 'TOKEN_INVALIDATED' };\n  }\n  \n  return { \n    valid: true, \n    user: {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions\n    }\n  };\n}\n\nfunction calculateRiskLevel(\n  payload: JWTPayload,\n  req: Request\n): 'low' | 'medium' | 'high' {\n  const factors = {\n    // Token age (older = lower risk of revocation)\n    tokenAge: Date.now() - payload.iat * 1000,\n    \n    // Request type\n    isReadOnly: ['GET', 'HEAD', 'OPTIONS'].includes(req.method),\n    \n    // Endpoint sensitivity\n    endpoint: req.path,\n    \n    // User role\n    hasAdminRole: payload.roles?.includes('admin'),\n    \n    // Recent activity\n    sessionAge: Date.now() - (payload.session_started_at || payload.iat) * 1000\n  };\n  \n  // High risk: Admin actions, write operations, new sessions\n  if (factors.hasAdminRole || \n      !factors.isReadOnly || \n      factors.sessionAge < 60000) { // < 1 minute old\n    return 'high';\n  }\n  \n  // Low risk: Read-only, old token, mature session\n  if (factors.isReadOnly && \n      factors.tokenAge > 300000 && // > 5 minutes old\n      factors.sessionAge > 3600000) { // > 1 hour old\n    return 'low';\n  }\n  \n  return 'medium';\n}\n\nasync function handleBlacklistUnavailable(\n  payload: JWTPayload,\n  req: Request,\n  riskLevel: 'low' | 'medium' | 'high'\n): Promise<TokenValidation> {\n  \n  switch (riskLevel) {\n    case 'low':\n      // Accept request with logging\n      logger.info('Accepting low-risk request despite blacklist unavailable', {\n        jti: payload.jti,\n        userId: payload.sub,\n        endpoint: req.path,\n        method: req.method\n      });\n      \n      metrics.increment('auth.blacklist_bypass.low_risk');\n      \n      // Perform compensating controls\n      await performCompensatingControls(payload, req);\n      \n      return { \n        valid: true, \n        fallbackMode: true,\n        riskLevel: 'low',\n        user: extractUserFromPayload(payload)\n      };\n    \n    case 'medium':\n      // Check Postgres directly (slower but authoritative)\n      logger.warn('Medium-risk request, checking Postgres blacklist', {\n        jti: payload.jti,\n        userId: payload.sub\n      });\n      \n      const isBlacklistedInDb = await checkBlacklistInPostgres(payload.jti);\n      \n      if (isBlacklistedInDb) {\n        return { valid: false, reason: 'TOKEN_REVOKED' };\n      }\n      \n      metrics.increment('auth.blacklist_bypass.medium_risk_postgres');\n      \n      return { \n        valid: true, \n        fallbackMode: true,\n        riskLevel: 'medium',\n        user: extractUserFromPayload(payload)\n      };\n    \n    case 'high':\n      // Reject high-risk requests when blacklist unavailable\n      logger.error('Rejecting high-risk request due to blacklist unavailable', {\n        jti: payload.jti,\n        userId: payload.sub,\n        endpoint: req.path,\n        method: req.method\n      });\n      \n      metrics.increment('auth.blacklist_bypass.high_risk_rejected');\n      \n      return { \n        valid: false, \n        reason: 'BLACKLIST_UNAVAILABLE',\n        message: 'Service temporarily unavailable for this operation'\n      };\n  }\n}\n\nasync function performCompensatingControls(\n  payload: JWTPayload,\n  req: Request\n): Promise<void> {\n  // 1. Check if user still exists and is active\n  const user = await getUserById(payload.sub);\n  if (!user || user.locked_until > new Date()) {\n    throw new Error('USER_INACTIVE');\n  }\n  \n  // 2. Verify token version matches (password not changed)\n  if (payload.token_version !== user.token_version) {\n    throw new Error('TOKEN_VERSION_MISMATCH');\n  }\n  \n  // 3. Check if session still exists in database\n  const session = await getRefreshTokenBySession(payload.session_id);\n  if (!session || session.revoked) {\n    throw new Error('SESSION_REVOKED');\n  }\n  \n  // 4. Log for audit trail\n  await createSecurityEvent({\n    userId: payload.sub,\n    eventType: 'TOKEN_VALIDATED_FALLBACK_MODE',\n    severity: 'MEDIUM',\n    details: {\n      jti: payload.jti,\n      endpoint: req.path,\n      method: req.method,\n      reason: 'redis_unavailable'\n    }\n  });\n}\n\nasync function checkBlacklistInPostgres(jti: string): Promise<boolean> {\n  const result = await db.query(\n    'SELECT 1 FROM token_blacklist WHERE jti = $1 AND expires_at > NOW()',\n    [jti]\n  );\n  \n  return result.rowCount > 0;\n}\n```\n\n**Token Blacklist Table (Postgres)**\n```sql\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL, -- Same as original token expiry\n  reason VARCHAR(100),\n  session_id UUID,\n  ip_address INET,\n  user_agent TEXT\n);\n\nCREATE INDEX idx_token_blacklist_expires ON token_blacklist(expires_at) \n  WHERE expires_at > NOW();\nCREATE INDEX idx_token_blacklist_user ON token_blacklist(user_id, revoked_at DESC);\n\n-- Automatic cleanup of expired entries\nCREATE OR REPLACE FUNCTION cleanup_expired_blacklist()\nRETURNS void AS $$\nBEGIN\n  DELETE FROM token_blacklist WHERE expires_at < NOW() - INTERVAL '1 day';\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Run cleanup daily\nSELECT cron.schedule('cleanup-blacklist', '0 4 * * *', 'SELECT cleanup_expired_blacklist()');\n```\n\n### 4. Refresh Token Locking Fallback\n\n**Problem**: Distributed locks prevent concurrent refresh. If Redis is down, we risk:\n- Race conditions causing token reuse false positives\n- Complete inability to refresh tokens\n\n**Solution**: Postgres Advisory Locks with Timeout\n\n```typescript\nasync function performTokenRefreshWithFailover(\n  refreshToken: string,\n  fingerprint: string,\n  requestId: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // Try Redis distributed lock first\n  let lockAcquired = false;\n  let useFallbackLock = false;\n  \n  try {\n    lockAcquired = await acquireRedisLock(tokenHash, requestId, 5000);\n  } catch (error) {\n    logger.warn('Redis lock unavailable, using Postgres advisory lock', {\n      tokenHash: tokenHash.substring(0, 16),\n      error: error.message\n    });\n    \n    metrics.increment('auth.refresh_lock.fallback_to_postgres');\n    useFallbackLock = true;\n  }\n  \n  if (useFallbackLock) {\n    // Use Postgres advisory lock as fallback\n    lockAcquired = await acquirePostgresAdvisoryLock(tokenHash, 5000);\n  }\n  \n  if (!lockAcquired) {\n    // Another process is refreshing - wait and check for result\n    return await waitForConcurrentRefresh(tokenHash, requestId, useFallbackLock);\n  }\n  \n  try {\n    // Perform refresh operation\n    const result = await executeTokenRefresh(refreshToken, fingerprint);\n    \n    // Store result for concurrent requests (best effort)\n    if (!useFallbackLock) {\n      try {\n        await cache.set(\n          `refresh_result:${tokenHash}`,\n          result,\n          5 // 5 seconds\n        );\n      } catch {\n        // Not critical if this fails\n      }\n    }\n    \n    return result;\n    \n  } finally {\n    // Release lock\n    if (useFallbackLock) {\n      await releasePostgresAdvisoryLock(tokenHash);\n    } else {\n      await releaseRedisLock(tokenHash, requestId);\n    }\n  }\n}\n\nasync function acquirePostgresAdvisoryLock(\n  tokenHash: string,\n  timeoutMs: number\n): Promise<boolean> {\n  // Convert token hash to numeric lock ID (Postgres advisory locks use bigint)\n  const lockId = hashToLockId(tokenHash);\n  \n  const startTime = Date.now();\n  const maxAttempts = Math.ceil(timeoutMs / 100); // Check every 100ms\n  \n  for (let attempt = 0; attempt < maxAttempts; attempt++) {\n    const result = await db.query(\n      'SELECT pg_try_advisory_lock($1) as acquired',\n      [lockId]\n    );\n    \n    if (result.rows[0].acquired) {\n      logger.debug('Acquired Postgres advisory lock', { lockId, attempt });\n      return true;\n    }\n    \n    // Wait before retry\n    await sleep(100);\n    \n    if (Date.now() - startTime > timeoutMs) {\n      break;\n    }\n  }\n  \n  logger.warn('Failed to acquire Postgres advisory lock', {\n    lockId,\n    timeoutMs,\n    attempts: maxAttempts\n  });\n  \n  return false;\n}\n\nasync function releasePostgresAdvisoryLock(tokenHash: string): Promise<void> {\n  const lockId = hashToLockId(tokenHash);\n  \n  await db.query('SELECT pg_advisory_unlock($1)', [lockId]);\n  \n  logger.debug('Released Postgres advisory lock', { lockId });\n}\n\nfunction hashToLockId(tokenHash: string): bigint {\n  // Convert hash to 64-bit integer for Postgres advisory lock\n  const buffer = Buffer.from(tokenHash.substring(0, 16), 'hex');\n  return buffer.readBigInt64BE(0);\n}\n\nasync function waitForConcurrentRefresh(\n  tokenHash: string,\n  requestId: string,\n  postgresMode: boolean\n): Promise<AuthResponse> {\n  \n  if (postgresMode) {\n    // In Postgres fallback mode, we can't share results easily\n    // Wait longer and retry\n    await sleep(1000); // Wait 1 second\n    \n    // Retry the refresh operation\n    return await performTokenRefreshWithFailover(\n      tokenHash,\n      fingerprint,\n      requestId\n    );\n  }\n  \n  // Try to get cached result from concurrent refresh (Redis available)\n  const maxWaitTime = 5000; // 5 seconds\n  const checkInterval = 100; // Check every 100ms\n  const maxAttempts = maxWaitTime / checkInterval;\n  \n  for (let attempt = 0; attempt < maxAttempts; attempt++) {\n    await sleep(checkInterval);\n    \n    try {\n      const cachedResult = await cache.get(`refresh_result:${tokenHash}`);\n      if (cachedResult) {\n        metrics.increment('auth.refresh.concurrent_reuse');\n        return cachedResult;\n      }\n    } catch {\n      // Cache unavailable, continue waiting\n    }\n  }\n  \n  throw new Error('CONCURRENT_REFRESH_TIMEOUT');\n}\n\nasync function executeTokenRefresh(\n  refreshToken: string,\n  fingerprint: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // 1. Retrieve token from cache (multi-layer)\n  let tokenData = await cache.get(`refresh:${tokenHash}`);\n  \n  if (!tokenData) {\n    // Load from Postgres if cache miss\n    tokenData = await getRefreshTokenFromPostgres(tokenHash);\n    \n    if (!tokenData) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n  }\n  \n  // 2. Validate token\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Check if already used (CRITICAL for security)\n  if (tokenData.used) {\n    // Token reuse detected - trigger compromise response\n    await handleTokenFamilyCompromise(\n      tokenData.token_family_id,\n      'TOKEN_REUSE'\n    );\n    throw new AuthError('TOKEN_REUSE_DETECTED');\n  }\n  \n  // 4. Validate fingerprint\n  if (tokenData.device_fingerprint_hash !== hashFingerprint(fingerprint)) {\n    throw new AuthError('FINGERPRINT_MISMATCH');\n  }\n  \n  // 5. Mark token as used (atomic operation in Postgres)\n  await db.query(\n    'UPDATE refresh_tokens SET used = true, used_at = NOW() ' +\n    'WHERE token_hash = $1 AND used = false',\n    [tokenHash]\n  );\n  \n  // Also mark in cache (best effort)\n  try {\n    await cache.set(\n      `refresh:${tokenHash}:used`,\n      true,\n      tokenData.ttl_remaining\n    );\n  } catch {\n    // Not critical - Postgres is source of truth\n  }\n  \n  // 6. Generate new token pair\n  const user = await getUserById(tokenData.user_id);\n  \n  const newAccessToken = generateAccessToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    tokenVersion: user.token_version\n  });\n  \n  const newRefreshToken = await generateRefreshToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    deviceFingerprint: fingerprint,\n    parentTokenId: tokenData.id,\n    tokenFamilyId: tokenData.token_family_id,\n    timesRefreshed: tokenData.times_refreshed + 1\n  });\n  \n  return {\n    accessToken: newAccessToken.token,\n    refreshToken: newRefreshToken.token,\n    expiresIn: 900,\n    tokenType: 'Bearer',\n    refreshExpiresIn: 2592000\n  };\n}\n```\n\n### 5. Rate Limiting Fallback\n\n**Problem**: Rate limiting requires counters in Redis. If Redis is down:\n- Option A: Disable rate limiting (security risk)\n- Option B: Reject all requests (poor availability)\n\n**Solution**: In-Memory Rate Limiting with Postgres Sync\n\n```typescript\nclass RateLimiterWithFailover {\n  private redis: RedisClient;\n  private localRateLimiter: Map<string, RateLimitCounter>;\n  private circuitBreaker: CircuitBreaker;\n  private syncInterval: NodeJS.Timeout;\n  \n  constructor() {\n    this.localRateLimiter = new Map();\n    \n    // Sync local counters to Postgres every 10 seconds\n    this.syncInterval = setInterval(\n      () => this.syncCountersToPostgres(),\n      10000\n    );\n  }\n  \n  async checkRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    // Try Redis first\n    try {\n      const result = await this.checkRedisRateLimit(key, max, windowSec);\n      return result;\n    } catch (error) {\n      logger.warn('Redis rate limit check failed, using local fallback', {\n        key: this.sanitizeKey(key),\n        error: error.message\n      });\n      \n      metrics.increment('ratelimit.fallback_to_local');\n    }\n    \n    // Use in-memory fallback\n    return this.checkLocalRateLimit(key, max, windowSec);\n  }\n  \n  private async checkRedisRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    const count = await this.circuitBreaker.execute({\n      method: 'incr',\n      args: [key]\n    });\n    \n    if (count === 1) {\n      await this.redis.expire(key, windowSec);\n    }\n    \n    if (count > max) {\n      const ttl = await this.redis.ttl(key);\n      return { allowed: false, retryAfter: ttl };\n    }\n    \n    return { allowed: true };\n  }\n  \n  private checkLocalRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): { allowed: boolean; retryAfter?: number } {\n    \n    const now = Date.now();\n    const windowMs = windowSec * 1000;\n    \n    // Get or create counter\n    let counter = this.localRateLimiter.get(key);\n    \n    if (!counter || now > counter.windowEnd) {\n      // Start new window\n      counter = {\n        count: 1,\n        windowStart: now,\n        windowEnd: now + windowMs,\n        key: key\n      };\n      this.localRateLimiter.set(key, counter);\n      return { allowed: true };\n    }\n    \n    // Increment counter\n    counter.count++;\n    \n    if (counter.count > max) {\n      const retryAfter = Math.ceil((counter.windowEnd - now) / 1000);\n      return { allowed: false, retryAfter };\n    }\n    \n    return { allowed: true };\n  }\n  \n  private async syncCountersToPostgres(): Promise<void> {\n    // Sync local counters to Postgres for cross-instance awareness\n    const counters = Array.from(this.localRateLimiter.entries());\n    \n    if (counters.length === 0) return;\n    \n    try {\n      const values = counters.map(([key, counter]) => ({\n        key: key,\n        count: counter.count,\n        window_start: new Date(counter.windowStart),\n        window_end: new Date(counter.windowEnd),\n        synced_at: new Date()\n      }));\n      \n      await db.query(\n        'INSERT INTO rate_limit_counters (key, count, window_start, window_end, synced_at) ' +\n        'VALUES ' + values.map((_, i) => \n          `($${i*5+1}, $${i*5+2}, $${i*5+3}, $${i*5+4}, $${i*5+5})`\n        ).join(', ') +\n        'ON CONFLICT (key) DO UPDATE SET ' +\n        'count = GREATEST(rate_limit_counters.count, EXCLUDED.count), ' +\n        'synced_at = EXCLUDED.synced_at',\n        values.flatMap(v => [v.key, v.count, v.window_start, v.window_end, v.synced_at])\n      );\n      \n      logger.debug('Synced rate limit counters to Postgres', {\n        count: counters.length\n      });\n      \n    } catch (error) {\n      logger.error('Failed to sync rate limit counters', { error });\n    }\n    \n    // Clean up expired counters\n    const now = Date.now();\n    for (const [key, counter] of counters) {\n      if (now > counter.windowEnd + 60000) { // 1 minute grace\n        this.localRateLimiter.delete(key);\n      }\n    }\n  }\n  \n  async loadCountersFromPostgres(): Promise<void> {\n    // On startup, load recent counters from Postgres\n    // This provides awareness of rate limits from other instances\n    \n    const result = await db.query(\n      'SELECT key, count, window_start, window_end FROM rate_limit_counters ' +\n      'WHERE window_end > NOW() - INTERVAL \\'1 minute\\''\n    );\n    \n    for (const row of result.rows) {\n      this.localRateLimiter.set(row.key, {\n        count: row.count,\n        windowStart: row.window_start.getTime(),\n        windowEnd: row.window_end.getTime(),\n        key: row.key\n      });\n    }\n    \n    logger.info('Loaded rate limit counters from Postgres', {\n      count: result.rowCount\n    });\n  }\n  \n  private sanitizeKey(key: string): string {\n    // Remove sensitive data (emails, IPs) from logs\n    return key.replace(/:[^:]+@/, ':***@').replace(/:\\d+\\.\\d+\\.\\d+\\.\\d+/, ':***');\n  }\n}\n\ninterface RateLimitCounter {\n  count: number;\n  windowStart: number;\n  windowEnd: number;\n  key: string;\n}\n```\n\n**Rate Limit Counters Table (Postgres)**\n```sql\nCREATE TABLE rate_limit_counters (\n  key VARCHAR(500) PRIMARY KEY,\n  count INTEGER NOT NULL,\n  window_start TIMESTAMP NOT NULL,\n  window_end TIMESTAMP NOT NULL,\n  synced_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\nCREATE INDEX idx_rate_limit_window_end ON rate_limit_counters(window_end) \n  WHERE window_end > NOW();\n\n-- Cleanup expired counters\nCREATE OR REPLACE FUNCTION cleanup_rate_limit_counters()\nRETURNS void AS $$\nBEGIN\n  DELETE FROM rate_limit_counters \n  WHERE window_end < NOW() - INTERVAL '1 hour';\nEND;\n$$ LANGUAGE plpgsql;\n\nSELECT cron.schedule('cleanup-rate-limits', '*/15 * * * *', \n  'SELECT cleanup_rate_limit_counters()');\n```\n\n### 6. Circuit Breaker Implementation\n\n```typescript\nclass CircuitBreaker {\n  private state: CircuitState = CircuitState.CLOSED;\n  private failureCount: number = 0;\n  private successCount: number = 0;\n  private lastFailureTime: number = 0;\n  private nextAttemptTime: number = 0;\n  \n  constructor(\n    private operation: (args: any) => Promise<any>,\n    private config: CircuitBreakerConfig\n  ) {}\n  \n  async execute(args: any): Promise<any> {\n    // Check if circuit should transition to half-open\n    if (this.state === CircuitState.OPEN && \n        Date.now() >= this.nextAttemptTime) {\n      this.state = CircuitState.HALF_OPEN;\n      logger.info('Circuit breaker entering HALF_OPEN state');\n      metrics.gauge('circuit_breaker.state', 0.5); // 0=closed, 0.5=half-open, 1=open\n    }\n    \n    // Reject immediately if circuit is open\n    if (this.state === CircuitState.OPEN) {\n      const error = new Error('Circuit breaker is OPEN');\n      error.name = 'CircuitBreakerOpen';\n      metrics.increment('circuit_breaker.rejected');\n      throw error;\n    }\n    \n    try {\n      // Execute with timeout\n      const result = await Promise.race([\n        this.operation(args),\n        this.timeoutPromise(this.config.timeout)\n      ]);\n      \n      // Success\n      this.onSuccess();\n      return result;\n      \n    } catch (error) {\n      // Failure\n      this.onFailure();\n      throw error;\n    }\n  }\n  \n  private onSuccess(): void {\n    this.failureCount = 0;\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      this.successCount++;\n      \n      // After 10 successful requests, close circuit\n      if (this.successCount >= 10) {\n        this.state = CircuitState.CLOSED;\n        this.successCount = 0;\n        logger.info('Circuit breaker closed after successful recovery');\n        metrics.gauge('circuit_breaker.state', 0);\n      }\n    }\n  }\n  \n  private onFailure(): void {\n    this.failureCount++;\n    this.lastFailureTime = Date.now();\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      // Failure in half-open state - immediately re-open circuit\n      this.openCircuit();\n      return;\n    }\n    \n    // Check if we should open the circuit\n    const failureRate = this.failureCount / this.config.requestVolumeThreshold;\n    \n    if (this.failureCount >= this.config.requestVolumeThreshold &&\n        failureRate >= this.config.failureThreshold) {\n      this.openCircuit();\n    }\n  }\n  \n  private openCircuit(): void {\n    this.state = CircuitState.OPEN;\n    this.nextAttemptTime = Date.now() + this.config.resetTimeout;\n    this.successCount = 0;\n    \n    logger.error('Circuit breaker opened', {\n      failureCount: this.failureCount,\n      lastFailureTime: new Date(this.lastFailureTime),\n      nextAttemptTime: new Date(this.nextAttemptTime)\n    });\n    \n    metrics.gauge('circuit_breaker.state', 1);\n    metrics.increment('circuit_breaker.opened');\n  }\n  \n  private timeoutPromise(ms: number): Promise<never> {\n    return new Promise((_, reject) => {\n      setTimeout(() => {\n        const error = new Error(`Operation timed out after ${ms}ms`);\n        error.name = 'TimeoutError';\n        reject(error);\n      }, ms);\n    });\n  }\n  \n  getState(): CircuitState {\n    return this.state;\n  }\n}\n```\n\n## Geographic Location Policy\n\n### 1. Location Change Risk Matrix\n\n```typescript\nenum LocationChangeType {\n  NO_CHANGE = 'no_change',           // Same city\n  CITY_CHANGE = 'city_change',       // Different city, same region\n  REGION_CHANGE = 'region_change',   // Different region, same country\n  COUNTRY_CHANGE = 'country_change', // Different country\n  CONTINENT_CHANGE = 'continent_change' // Different continent\n}\n\nenum ResponseAction {\n  ALLOW = 'allow',                   // Proceed normally\n  LOG = 'log',                       // Allow but log event\n  NOTIFY = 'notify',                 // Allow, log, and notify user\n  CHALLENGE = 'challenge',           // Require additional verification\n  BLOCK = 'block'                    // Deny access, require re-authentication\n}\n\ninterface LocationPolicy {\n  changeType: LocationChangeType;\n  timeWindow: number; // Milliseconds since last activity\n  userTrustLevel: 'new' | 'established' | 'trusted';\n  deviceTrustLevel: 'new' | 'recognized' | 'trusted';\n  action: ResponseAction;\n  notificationDelay?: number; // Delay before notifying (for post-travel)\n}\n\nconst locationPolicies: LocationPolicy[] = [\n  // === ALLOW: Low-risk scenarios ===\n  {\n    changeType: LocationChangeType.NO_CHANGE,\n    timeWindow: Infinity,\n    userTrustLevel: 'new',\n    deviceTrustLevel: 'new',\n    action: ResponseAction.ALLOW\n  },\n  {\n    changeType: LocationChangeType.CITY_CHANGE,\n    timeWindow: 24 * 60 * 60 * 1000, // 24 hours\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.ALLOW\n  },\n  \n  // === LOG: Medium-risk, expected scenarios ===\n  {\n    changeType: LocationChangeType.REGION_CHANGE,\n    timeWindow: 12 * 60 * 60 * 1000, // 12 hours (e.g., domestic travel)\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.LOG\n  },\n  \n  // === NOTIFY: Significant change, likely legitimate ===\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 6 * 60 * 60 * 1000, // 6 hours (flight time)\n    userTrustLevel: 'trusted',\n    deviceTrustLevel: 'trusted',\n    action: ResponseAction.NOTIFY,\n    notificationDelay: 5 * 60 * 1000 // Wait 5 minutes (user may be traveling)\n  },\n  {\n    changeType: LocationChangeType.REGION_CHANGE,\n    timeWindow: 2 * 60 * 60 * 1000, // 2 hours\n    userTrustLevel: 'trusted',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.NOTIFY,\n    notificationDelay: 2 * 60 * 1000\n  },\n  \n  // === CHALLENGE: Suspicious patterns ===\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 2 * 60 * 60 * 1000, // 2 hours (too fast for legitimate travel)\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.CHALLENGE\n  },\n  {\n    changeType: LocationChangeType.CONTINENT_CHANGE,\n    timeWindow: 8 * 60 * 60 * 1000, // 8 hours\n    userTrustLevel: 'new',\n    deviceTrustLevel: 'new',\n    action: ResponseAction.CHALLENGE\n  },\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 24 * 60 * 60 * 1000, // Any country change within 24h\n    userTrustLevel: 'new',\n    deviceTrustLevel: 'new',\n    action: ResponseAction.CHALLENGE\n  },\n  \n  // === BLOCK: High-risk scenarios ===\n  {\n    changeType: LocationChangeType.CONTINENT_CHANGE,\n    timeWindow: 2 * 60 * 60 * 1000, // 2 hours (impossible travel speed)\n    userTrustLevel: 'established',\n    deviceTrustLevel: 'recognized',\n    action: ResponseAction.BLOCK\n  },\n  {\n    changeType: LocationChangeType.COUNTRY_CHANGE,\n    timeWindow: 30 * 60 * 1000, // 30 minutes (impossible)\n    userTrustLevel: 'trusted',\n    deviceTrustLevel: 'trusted',\n    action: ResponseAction.BLOCK\n  }\n];\n```\n\n### 2. VPN and Proxy Detection\n\n```typescript\ninterface IpIntelligence {\n  isVpn: boolean;\n  isProxy: boolean;\n  isTor: boolean;\n  isDatacenter: boolean;\n  isHosting: boolean;\n  riskScore: number; // 0-100\n  country: string;\n  region: string;\n  city: string;\n  carrier?: string; // For mobile IPs\n  asn?: string; // Autonomous System Number\n}\n\nasync function getIpIntelligence(ipAddress: string): Promise<IpIntelligence> {\n  // Use service like IPHub, IPQualityScore, or MaxMind\n  const response = await fetch(\n    `https://ipqualityscore.com/api/json/ip/${API_KEY}/${ipAddress}`,\n    { timeout: 2000 }\n  );\n  \n  const data = await response.json();\n  \n  return {\n    isVpn: data.vpn,\n    isProxy: data.proxy,\n    isTor: data.tor,\n    isDatacenter: data.is_crawler || data.recent_abuse,\n    isHosting: data.host === 'hosting',\n    riskScore: data.fraud_score,\n    country: data.country_code,\n    region: data.region,\n    city: data.city,\n    carrier: data.mobile ? data.ISP : undefined,\n    asn: data.ASN\n  };\n}\n\ninterface VpnPolicy {\n  allowVpn: boolean;\n  allowTor: boolean;\n  allowDatacenter: boolean;\n  requireVerificationFor: ('vpn' | 'tor' | 'datacenter')[];\n  blockHighRiskIps: boolean;\n  riskScoreThreshold: number;\n}\n\nconst vpnPolicy: VpnPolicy = {\n  allowVpn: true,           // Generally allow VPNs (common for privacy/work)\n  allowTor: false,          // Block Tor (high anonymity risk)\n  allowDatacenter: false,   // Block datacenter IPs (common for bots)\n  requireVerificationFor: ['vpn'], // Challenge VPN users on location change\n  blockHighRiskIps: true,\n  riskScoreThreshold: 85    // Block IPs with risk score > 85\n};\n\nasync function evaluateIpRisk(\n  ipAddress: string,\n  userTrustLevel: 'new' | 'established' | 'trusted'\n): Promise<{ allowed: boolean; action: ResponseAction; reason?: string }> {\n  \n  let ipInfo: IpIntelligence;\n  \n  try {\n    ipInfo = await getIpIntelligence(ipAddress);\n  } catch (error) {\n    // If IP intelligence service is unavailable, fail open for established users\n    logger.warn('IP intelligence check failed', { ipAddress, error });\n    \n    if (userTrustLevel === 'new') {\n      // Be more cautious with new users\n      return { \n        allowed: true, \n        action: ResponseAction.CHALLENGE,\n        reason: 'ip_intelligence_unavailable' \n      };\n    }\n    \n    return { allowed: true, action: ResponseAction.LOG };\n  }\n  \n  // Block Tor\n  if (ipInfo.isTor && !vpnPolicy.allowTor) {\n    return { \n      allowed: false, \n      action: ResponseAction.BLOCK,\n      reason: 'tor_not_allowed' \n    };\n  }\n  \n  // Block datacenter IPs\n  if (ipInfo.isDatacenter && !vpnPolicy.allowDatacenter) {\n    return { \n      allowed: false, \n      action: ResponseAction.BLOCK,\n      reason: 'datacenter_ip_not_allowed' \n    };\n  }\n  \n  // Block high-risk IPs\n  if (vpnPolicy.blockHighRiskIps && \n      ipInfo.riskScore > vpnPolicy.riskScoreThreshold) {\n    return { \n      allowed: false, \n      action: ResponseAction.BLOCK,\n      reason: `high_risk_score_${ipInfo.riskScore}` \n    };\n  }\n  \n  // Challenge VPN users (especially on location change)\n  if (ipInfo.isVpn && \n      vpnPolicy.requireVerificationFor.includes('vpn') &&\n      userTrustLevel !== 'trusted') {\n    return { \n      allowed: true, \n      action: ResponseAction.CHALLENGE,\n      reason: 'vpn_detected' \n    };\n  }\n  \n  // Allow VPN for trusted users\n  if (ipInfo.isVpn && userTrustLevel === 'trusted') {\n    return { \n      allowed: true, \n      action: ResponseAction.LOG,\n      reason: 'vpn_detected_trusted_user' \n    };\n  }\n  \n  return { allowed: true, action: ResponseAction.ALLOW };\n}\n```\n\n### 3. User and Device Trust Levels\n\n```typescript\ninterface TrustFactors {\n  accountAge: number;           // Days since account creation\n  totalLogins: number;          // Lifetime login count\n  consistentLocations: number;  // Number of distinct locations\n  devicesUsed: number;          // Number of distinct devices\n  securityIncidents: number;    // Past security events\n  mfaEnabled: boolean;\n  recentLocationChanges: number; // Location changes in last 30 days\n  verifiedDevices: number;      // Explicitly verified devices\n}\n\nfunction calculateUserTrustLevel(factors: TrustFactors): 'new' | 'established' | 'trusted' {\n  // New user: < 7 days, < 10 logins\n  if (factors.accountAge < 7 || factors.totalLogins < 10) {\n    return 'new';\n  }\n  \n  // Trusted user: Multiple criteria\n  const trustedCriteria = [\n    factors.accountAge >= 90,              // 90+ days old\n    factors.totalLogins >= 50,             // 50+ logins\n    factors.consistentLocations <= 5,      // Consistent behavior\n    factors.securityIncidents === 0,       // No security issues\n    factors.mfaEnabled,                    // MFA enabled\n    factors.verifiedDevices >= 2           // Multiple verified devices\n  ];\n  \n  const trustedCount = trustedCriteria.filter(Boolean).length;\n  \n  if (trustedCount >= 4) {\n    return 'trusted';\n  }\n  \n  // Established: Between new and trusted\n  return 'established';\n}\n\nfunction calculateDeviceTrustLevel(\n  device: Device,\n  userTrustLevel: 'new' | 'established' | 'trusted'\n): 'new' | 'recognized' | 'trusted' {\n  \n  const deviceAge = Date.now() - device.first_seen_at.getTime();\n  const deviceAgeDays = deviceAge / (24 * 60 * 60 * 1000);\n  \n  // New device: < 1 day, < 5 uses\n  if (deviceAgeDays < 1 || device.times_used < 5) {\n    return 'new';\n  }\n  \n  // Trusted device: User is trusted + device is verified + consistent use\n  if (userTrustLevel === 'trusted' &&\n      device.verified &&\n      deviceAgeDays >= 30 &&\n      device.times_used >= 20 &&\n      device.security_incidents === 0) {\n    return 'trusted';\n  }\n  \n  // Recognized: Between new and trusted\n  return 'recognized';\n}\n\ninterface Device {\n  id: string;\n  device_id: string;\n  device_name: string;\n  device_type: string;\n  first_seen_at: Date;\n  last_seen_at: Date;\n  times_used: number;\n  verified: boolean; // User explicitly verified this device\n  security_incidents: number;\n}\n```\n\n### 4. Location Change Evaluation\n\n```typescript\nasync function evaluateLocationChange(\n  userId: string,\n  sessionId: string,\n  newIpAddress: string\n): Promise<LocationChangeDecision> {\n  \n  // 1. Get current and previous location\n  const session = await getSessionById(sessionId);\n  const prevIpInfo = await getCachedIpInfo(session.ip_address);\n  const newIpInfo = await getIpIntelligence(newIpAddress);\n  \n  // 2. Determine change type\n  const changeType = determineLocationChangeType(prevIpInfo, newIpInfo);\n  \n  // 3. Calculate time since last activity\n  const timeSinceLastActivity = Date.now() - session.last_used_at.getTime();\n  \n  // 4. Get user and device trust levels\n  const user = await getUserById(userId);\n  const trustFactors = await calculateUserTrustFactors(userId);\n  const userTrustLevel = calculateUserTrustLevel(trustFactors);\n  \n  const device = await getDeviceBySession(sessionId);\n  const deviceTrustLevel = calculateDeviceTrustLevel(device, userTrustLevel);\n  \n  // 5. Check IP risk (VPN, Tor, datacenter)\n  const ipRisk = await evaluateIpRisk(newIpAddress, userTrustLevel);\n  \n  if (!ipRisk.allowed) {\n    return {\n      allowed: false,\n      action: ipRisk.action,\n      reason: ipRisk.reason,\n      requiresStepUp: false\n    };\n  }\n  \n  // 6. Find matching policy\n  const policy = findMatchingLocationPolicy(\n    changeType,\n    timeSinceLastActivity,\n    userTrustLevel,\n    deviceTrustLevel\n  );\n  \n  // 7. Special handling for VPN users\n  if (newIpInfo.isVpn || prevIpInfo?.isVpn) {\n    return handleVpnLocationChange(\n      policy,\n      newIpInfo,\n      prevIpInfo,\n      userTrustLevel,\n      timeSinceLastActivity\n    );\n  }\n  \n  // 8. Apply policy\n  const decision = applyLocationPolicy(\n    policy,\n    userId,\n    sessionId,\n    prevIpInfo,\n    newIpInfo,\n    timeSinceLastActivity\n  );\n  \n  return decision;\n}\n\nfunction determineLocationChangeType(\n  prevLocation: IpIntelligence | null,\n  newLocation: IpIntelligence\n): LocationChangeType {\n  \n  if (!prevLocation) {\n    return LocationChangeType.NO_CHANGE;\n  }\n  \n  // Same city\n  if (prevLocation.city === newLocation.city) {\n    return LocationChangeType.NO_CHANGE;\n  }\n  \n  // Same region, different city\n  if (prevLocation.region === newLocation.region) {\n    return LocationChangeType.CITY_CHANGE;\n  }\n  \n  // Same country, different region\n  if (prevLocation.country === newLocation.country) {\n    return LocationChangeType.REGION_CHANGE;\n  }\n  \n  // Different country\n  const prevContinent = getContinent(prevLocation.country);\n  const newContinent = getContinent(newLocation.country);\n  \n  if (prevContinent === newContinent) {\n    return LocationChangeType.COUNTRY_CHANGE;\n  }\n  \n  // Different continent\n  return LocationChangeType.CONTINENT_CHANGE;\n}\n\nfunction handleVpnLocationChange(\n  policy: LocationPolicy,\n  newIpInfo: IpIntelligence,\n  prevIpInfo: IpIntelligence | null,\n  userTrustLevel: 'new' | 'established' | 'trusted',\n  timeSinceLastActivity: number\n): LocationChangeDecision {\n  \n  // Both IPs are VPN - can't reliably detect location change\n  if (prevIpInfo?.isVpn && newIpInfo.isVpn) {\n    \n    // Check if same VPN provider (via ASN)\n    if (prevIpInfo.asn === newIpInfo.asn) {\n      // Same VPN provider - likely legitimate\n      return {\n        allowed: true,\n        action: ResponseAction.LOG,\n        reason: 'vpn_location_change_same_provider',\n        requiresStepUp: false\n      };\n    }\n    \n    // Different VPN providers\n    if (userTrustLevel === 'trusted') {\n      // Trusted user switching VPNs - allow with notification\n      return {\n        allowed: true,\n        action: ResponseAction.NOTIFY,\n        reason: 'vpn_provider_change',\n        requiresStepUp: false,\n        notificationDelay: 5 * 60 * 1000 // 5 minutes\n      };\n    }\n    \n    // Non-trusted user switching VPNs - challenge\n    return {\n      allowed: true,\n      action: ResponseAction.CHALLENGE,\n      reason: 'vpn_provider_change_untrusted',\n      requiresStepUp: true\n    };\n  }\n  \n  // Switching from non-VPN to VPN or vice versa\n  if (prevIpInfo && !prevIpInfo.isVpn && newIpInfo.isVpn) {\n    // User enabled VPN\n    if (userTrustLevel === 'trusted') {\n      return {\n        allowed: true,\n        action: ResponseAction.LOG,\n        reason: 'vpn_enabled',\n        requiresStepUp: false\n      };\n    }\n    \n    // New/established user enabling VPN - challenge\n    return {\n      allowed: true,\n      action: ResponseAction.CHALLENGE,\n      reason: 'vpn_enabled_untrusted',\n      requiresStepUp: true\n    };\n  }\n  \n  if (prevIpInfo?.isVpn && !newIpInfo.isVpn) {\n    // User disabled VPN - now we see real location\n    // Check if the revealed location makes sense\n    const changeType = determineLocationChangeType(\n      { ...prevIpInfo, isVpn: false }, // Treat as if previous was non-VPN\n      newIpInfo\n    );\n    \n    // Apply normal policy to revealed location\n    const revealedPolicy = findMatchingLocationPolicy(\n      changeType,\n      timeSinceLastActivity,\n      userTrustLevel,\n      'recognized' // Treat device as recognized\n    );\n    \n    return {\n      allowed: true,\n      action: revealedPolicy.action,\n      reason: 'vpn_disabled_location_revealed',\n      requiresStepUp: revealedPolicy.action === ResponseAction.CHALLENGE\n    };\n  }\n  \n  // Fallback to original policy\n  return {\n    allowed: true,\n    action: policy.action,\n    reason: 'location_change_with_vpn',\n    requiresStepUp: policy.action === ResponseAction.CHALLENGE\n  };\n}\n\nfunction findMatchingLocationPolicy(\n  changeType: LocationChangeType,\n  timeSinceLastActivity: number,\n  userTrustLevel: 'new' | 'established' | 'trusted',\n  deviceTrustLevel: 'new' | 'recognized' | 'trusted'\n): LocationPolicy {\n  \n  // Find most specific matching policy\n  const matches = locationPolicies.filter(policy =>\n    policy.changeType === changeType &&\n    timeSinceLastActivity <= policy.timeWindow &&\n    (policy.userTrustLevel === userTrustLevel || !policy.userTrustLevel) &&\n    (policy.deviceTrustLevel === deviceTrustLevel || !policy.deviceTrustLevel)\n  );\n  \n  if (matches.length === 0) {\n    // Default fallback: CHALLENGE on significant change\n    return {\n      changeType,\n      timeWindow: Infinity,\n      userTrustLevel,\n      deviceTrustLevel,\n      action: changeType >= LocationChangeType.COUNTRY_CHANGE \n        ? ResponseAction.CHALLENGE \n        : ResponseAction.LOG\n    };\n  }\n  \n  // Return most restrictive policy if multiple matches\n  return matches.sort((a, b) => {\n    const actionPriority = {\n      [ResponseAction.BLOCK]: 5,\n      [ResponseAction.CHALLENGE]: 4,\n      [ResponseAction.NOTIFY]: 3,\n      [ResponseAction.LOG]: 2,\n      [ResponseAction.ALLOW]: 1\n    };\n    \n    return actionPriority[b.action] - actionPriority[a.action];\n  })[0];\n}\n\nasync function applyLocationPolicy(\n  policy: LocationPolicy,\n  userId: string,\n  sessionId: string,\n  prevLocation: IpIntelligence | null,\n  newLocation: IpIntelligence,\n  timeSinceLastActivity: number\n): Promise<LocationChangeDecision> {\n  \n  const changeDetails = {\n    userId,\n    sessionId,\n    changeType: policy.changeType,\n    fromCountry: prevLocation?.country,\n    toCountry: newLocation.country,\n    fromCity: prevLocation?.city,\n    toCity: newLocation.city,\n    timeSinceLastActivity,\n    isVpn: newLocation.isVpn\n  };\n  \n  switch (policy.action) {\n    case ResponseAction.ALLOW:\n      // No action needed\n      return {\n        allowed: true,\n        action: ResponseAction.ALLOW,\n        requiresStepUp: false\n      };\n    \n    case ResponseAction.LOG:\n      // Log event for audit trail\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE',\n        severity: 'LOW',\n        details: changeDetails\n      });\n      \n      return {\n        allowed: true,\n        action: ResponseAction.LOG,\n        requiresStepUp: false\n      };\n    \n    case ResponseAction.NOTIFY:\n      // Log event\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE',\n        severity: 'MEDIUM',\n        details: changeDetails\n      });\n      \n      // Schedule notification (delayed to avoid false positives during travel)\n      if (policy.notificationDelay) {\n        setTimeout(async () => {\n          // Check if user is still in new location before notifying\n          const currentSession = await getSessionById(sessionId);\n          if (currentSession.ip_address === newLocation) {\n            await notifyUserLocationChange(userId, changeDetails);\n          }\n        }, policy.notificationDelay);\n      } else {\n        await notifyUserLocationChange(userId, changeDetails);\n      }\n      \n      return {\n        allowed: true,\n        action: ResponseAction.NOTIFY,\n        requiresStepUp: false\n      };\n    \n    case ResponseAction.CHALLENGE:\n      // Log high-severity event\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE_CHALLENGE',\n        severity: 'HIGH',\n        details: changeDetails\n      });\n      \n      // Require step-up authentication\n      return {\n        allowed: true,\n        action: ResponseAction.CHALLENGE,\n        requiresStepUp: true,\n        challengeType: 'location_verification',\n        challengeData: {\n          newLocation: `${newLocation.city}, ${newLocation.country}`,\n          prevLocation: prevLocation \n            ? `${prevLocation.city}, ${prevLocation.country}` \n            : 'Unknown'\n        }\n      };\n    \n    case ResponseAction.BLOCK:\n      // Log critical event\n      await createSecurityEvent({\n        userId,\n        eventType: 'LOCATION_CHANGE_BLOCKED',\n        severity: 'CRITICAL',\n        details: changeDetails\n      });\n      \n      // Revoke session\n      await revokeToken(sessionId, 'IMPOSSIBLE_TRAVEL_DETECTED');\n      \n      // Notify user immediately\n      await notifyUserSuspiciousActivity(userId, {\n        type: 'IMPOSSIBLE_TRAVEL',\n        details: changeDetails\n      });\n      \n      return {\n        allowed: false,\n        action: ResponseAction.BLOCK,\n        requiresStepUp: false,\n        reason: 'impossible_travel_detected'\n      };\n  }\n}\n\ninterface LocationChangeDecision {\n  allowed: boolean;\n  action: ResponseAction;\n  requiresStepUp: boolean;\n  challengeType?: 'location_verification' | 'mfa' | 'device_verification';\n  challengeData?: any;\n  reason?: string;\n  notificationDelay?: number;\n}\n```\n\n### 5. Mobile User Considerations\n\n```typescript\nasync function handleMobileLocationChange(\n  userId: string,\n  sessionId: string,\n  newIpInfo: IpIntelligence,\n  prevIpInfo: IpIntelligence | null\n): Promise<LocationChangeDecision> {\n  \n  // Mobile IPs change frequently due to cell tower switching\n  // Be more lenient with location changes for mobile users\n  \n  // Check if both IPs are from same mobile carrier\n  if (newIpInfo.carrier && prevIpInfo?.carrier && \n      newIpInfo.carrier === prevIpInfo.carrier) {\n    \n    // Same carrier - likely just cell tower switching\n    if (newIpInfo.region === prevIpInfo.region) {\n      // Within same region - definitely normal\n      return {\n        allowed: true,\n        action: ResponseAction.ALLOW,\n        requiresStepUp: false,\n        reason: 'mobile_carrier_ip_rotation'\n      };\n    }\n    \n    // Different region but same carrier - still likely legitimate\n    return {\n      allowed: true,\n      action: ResponseAction.LOG,\n      requiresStepUp: false,\n      reason: 'mobile_carrier_region_change'\n    };\n  }\n  \n  // Carrier changed or not mobile - apply normal location policy\n  // but with more lenient thresholds\n  return evaluateLocationChange(userId, sessionId, newIpInfo.toString());\n}\n```\n\n## Rationale\n\n### Redis Failover Strategy\n\n1. **Multi-layer caching**: Provides defense in depth. If Redis fails, we have in-memory cache and Postgres as fallbacks.\n\n2. **Circuit breaker**: Prevents cascading failures. If Redis is struggling, we stop sending requests and fail fast, allowing the system to recover.\n\n3. **Risk-based token validation**: Not all tokens need the same level of scrutiny. Low-risk read operations can tolerate slightly elevated risk when Redis is down. High-risk operations (admin actions, writes) require authoritative checks.\n\n4. **Postgres as source of truth**: All critical data (blacklist, refresh tokens, rate limits) is persisted to Postgres. Redis is a performance optimization, not a single point of failure.\n\n5. **Graceful degradation**: System remains available (albeit slower) when Redis fails, rather than complete outage.\n\n### Geographic Location Policy\n\n1. **Trust-based approach**: Trusted users with established patterns get more flexibility. New users face stricter scrutiny.\n\n2. **Context-aware decisions**: Time since last activity matters. A country change after 8 hours (flight time) is different from 30 minutes (impossible).\n\n3. **VPN-aware**: Recognizes that VPN users will have unpredictable location changes. Same VPN provider (ASN) suggests legitimate use.\n\n4. **Mobile-friendly**: Mobile carrier IPs change frequently. Policy accounts for this without compromising security.\n\n5. **Balanced notifications**: Delayed notifications for travel scenarios prevent alarm fatigue. Immediate alerts for suspicious activity.\n\n6. **Progressive responses**: Start with logging, escalate to notification, then challenge, finally block. Matches severity to risk.\n\n## What I Changed\n\n### From Previous Design\n\n1. **Added comprehensive Redis failover**: Previous design relied heavily on Redis without fallback strategy. Now we have circuit breaker, multi-layer cache, and Postgres fallback.\n\n2. **Implemented risk-based token validation**: Instead of binary \"check blacklist or fail\", we now assess risk level and apply appropriate fallback (Postgres check for medium risk, allow for low risk, block for high risk).\n\n3. **Added Postgres advisory locks**: Fallback for distributed locking when Redis unavailable. Ensures token refresh remains safe even during Redis outage.\n\n4. **Created in-memory rate limiting**: With Postgres sync every 10 seconds. Prevents complete rate limiting failure when Redis down.\n\n5. **Detailed location change policy matrix**: Moved from vague \"check location\" to explicit matrix of change types, time windows, trust levels, and actions.\n\n6. **VPN-specific handling**: Recognizes VPN usage patterns, distinguishes between same/different VPN providers, handles VPN enable/disable scenarios.\n\n7. **Mobile carrier awareness**: Treats mobile carrier IP changes more leniently, as they're frequent and expected.\n\n8. **IP intelligence integration**: Added VPN/Tor/datacenter detection with risk scoring. Blocks high-risk IPs proactively.\n\n9. **Trust level calculations**: Quantified user and device trust based on objective factors (account age, login count, MFA, etc.).\n\n10. **Progressive notification delays**: For travel scenarios, delay notifications to avoid false alarms. Immediate alerts for suspicious patterns.\n\n## What I Kept\n\nFrom the previous design's strong foundation:\n\n1. **15-minute access token lifetime** with **30-day refresh tokens**\n2. **Opaque refresh tokens** with cryptographic randomness\n3. **httpOnly cookies** with SameSite=Strict\n4. **Token family tracking** for compromise detection\n5. **Device fingerprinting** for additional binding\n6. **Comprehensive data models** (users, refresh_tokens, security_events)\n7. **Token rotation on refresh** with one-time use\n8. **Multi-device session management** with limits\n9. **Rate limiting** across multiple dimensions\n10. **Security event logging** with severity levels\n\n## Open Questions\n\n### 1. Redis HA Configuration Complexity\n\n**Question**: Should we mandate Redis Sentinel (3+ nodes) or allow simpler Redis configurations for smaller deployments?\n\n**Trade-off**: \n- Sentinel provides automatic failover but adds operational complexity\n- Smaller teams may struggle with Sentinel configuration\n- Fallback strategy makes Redis HA less critical than before\n\n**Recommendation**: Make Sentinel optional but strongly recommended. Document both paths clearly.\n\n### 2. In-Memory Cache Size Limits\n\n**Question**: What's the appropriate size limit for in-memory cache (LRU)?\n\n**Considerations**:\n- Larger cache = more memory usage per instance\n- Smaller cache = more Postgres fallback queries\n- Cache contains sensitive data (token hashes, user IDs)\n- Need to balance coverage vs. memory constraints\n\n**Current proposal**: 10,000 entries. Needs load testing to validate.\n\n### 3. Postgres Write Amplification at Scale\n\n**Concern**: Every token refresh writes to:\n- `refresh_tokens` table (INSERT + UPDATE parent)\n- `token_blacklist` table (when old token has active access token)\n- `rate_limit_counters` table (sync every 10s)\n\nAt 1M active users with 1 refresh/hour = 278 writes/second to Postgres.\n\n**Potential optimizations**:\n- Batch token updates (trade-off: slight delay in revocation)\n- Use write-behind caching for non-critical fields (last_used_at)\n- Partition refresh_tokens table by created_at\n- Consider separate write-optimized database for audit logs\n\n**Question**: At what scale does this become a problem? Need load testing.\n\n### 4. IP Intelligence Service Dependency\n\n**Concern**: IP intelligence (VPN detection, geolocation, risk scoring) adds external dependency.\n\n**Failure scenarios**:\n- Service outage\n- Rate limit exceeded\n- Slow response times\n\n**Current handling**: Fail open for established users, fail closed (challenge) for new users.\n\n**Question**: Should we cache IP intelligence results longer? Build internal IP intelligence database?\n\n### 5. Cross-Region Redis Replication\n\n**Question**: For global deployments, should we use:\n- Single Redis cluster (latency for distant regions)\n- Regional Redis clusters (consistency challenges)\n- Redis Enterprise with active-active geo-replication (expensive)\n\n**Current design**: Assumes single-region deployment. Needs adaptation for global scale.\n\n### 6. Token Blacklist Growth\n\n**Concern**: Token blacklist grows with every logout, password change, and revocation.\n\n**Math**: \n- 1M users\n- Average 2 logouts/day\n- 15-minute token lifetime\n- Blacklist entry TTL = 15 minutes\n\n= 1M * 2 / (24 * 60 / 15) = ~21K concurrent blacklist entries\n\nManageable, but growth is linear with user activity.\n\n**Question**: Should we implement bloom filter for blacklist checks (space-efficient, some false positives)?\n\n## Convergence Status\n\n**ITERATING** - This is round 2. The design now addresses:\n- \u2705 Redis failure scenarios with comprehensive fallback strategy\n- \u2705 Geographic location policy with detailed matrix\n- \u2705 VPN and mobile user considerations\n- \u2705 Risk-based decision making\n\nHowever, we need another iteration to validate:\n- Scaling characteristics (write amplification, cache sizing)\n- Operational complexity (Redis Sentinel setup, monitoring requirements)\n- Cross-region deployment strategy\n- Performance under load (especially Postgres fallback paths)\n\nThe architecture is now resilient to Redis failures, but we should stress-test the fallback paths to ensure performance is acceptable.\n\n## PROMPT_FOR_CRITIC:\n\nPlease evaluate the **Postgres fallback performance** under Redis failure scenarios:\n\n1. **Token validation fallback**: When checking blacklist in Postgres instead of Redis, what's the expected latency impact? Will the `token_blacklist` table index (`idx_token_blacklist_expires`) be sufficient for fast lookups at scale?\n\n2. **Refresh token locking**: Postgres advisory locks are used as fallback for distributed locking. What's the contention risk when many users refresh simultaneously? Should we implement lock sharding?\n\n3. **Rate limiting sync**: Syncing in-memory counters to Postgres every 10 seconds creates burst write load. How should we batch these updates efficiently? Should we use UPSERT with ON CONFLICT or separate INSERT/UPDATE?\n\n4. **Circuit breaker thresholds**: I proposed 50% failure rate over 100 requests. Is this too aggressive (trips too easily) or too lenient (allows too much failure)?\n\nAdditionally, spot any **security vulnerabilities** introduced by the fallback mechanisms. Does graceful degradation create new attack surfaces?"
      },
      "agent_b": {
        "content": "## Design\n\n# JWT Authentication System with Redis Failover & Geo-Location Policy\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 (Redis HA)  \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502             \u2502\n                                           \u25bc             \u25bc\n                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                    \u2502  Database   \u2502 \u2502  Fallback   \u2502\n                                    \u2502  (Postgres) \u2502 \u2502  Cache      \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  (In-Mem)   \u2502\n                                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Redis High Availability & Failover Strategy\n\n### 1. Redis Deployment Architecture\n\n**Primary Configuration: Redis Sentinel**\n```yaml\n# Redis Sentinel for automatic failover\nredis-sentinel:\n  master:\n    name: auth-master\n    host: redis-master.internal\n    port: 6379\n  replicas:\n    - redis-replica-1.internal:6379\n    - redis-replica-2.internal:6379\n  sentinels:\n    - sentinel-1.internal:26379\n    - sentinel-2.internal:26379\n    - sentinel-3.internal:26379\n  quorum: 2  # Minimum sentinels needed for failover decision\n  \n  # Connection from application\n  connection_strategy: sentinel_aware\n  sentinel_failover_timeout: 10000  # 10 seconds\n  max_retry_attempts: 3\n```\n\n**Circuit Breaker Configuration**\n```typescript\ninterface CircuitBreakerConfig {\n  // Trip circuit if 30% of requests fail in window (REFINED)\n  failureThreshold: 0.3;\n  \n  // Consider last 50 requests (REFINED - faster detection)\n  requestVolumeThreshold: 50;\n  \n  // Check health every 5 seconds when open (REFINED - faster recovery)\n  resetTimeout: 5000;\n  \n  // Operations timeout\n  timeout: 1000; // 1 second (REFINED - more aggressive)\n  \n  // Consecutive successes needed to close from half-open (NEW)\n  successThreshold: 5;\n}\n\nenum CircuitState {\n  CLOSED = 'closed',    // Normal operation\n  OPEN = 'open',        // Redis unavailable, use fallback\n  HALF_OPEN = 'half_open' // Testing if Redis recovered\n}\n```\n\n### 2. Multi-Layer Caching Strategy with Read-Through/Write-Through\n\n**Layer 1: Redis (Primary)**\n- Fastest access path\n- Distributed across application instances\n- Handles 99.9% of requests under normal conditions\n\n**Layer 2: In-Process Cache (Fallback)**\n- LRU cache in application memory\n- Size-limited based on memory budget\n- Synchronizes with Redis when available\n- Used when Redis is unreachable\n\n**Layer 3: Postgres (Source of Truth)**\n- Authoritative data store\n- Slower but always available\n- Used for cache misses and rebuilding\n\n```typescript\ninterface CacheLayer {\n  get(key: string): Promise<any | null>;\n  set(key: string, value: any, ttl: number): Promise<void>;\n  delete(key: string): Promise<void>;\n  isAvailable(): Promise<boolean>;\n}\n\nclass MultiLayerCache {\n  private redis: RedisClient;\n  private localCache: LRUCache;\n  private postgres: PostgresClient;\n  private circuitBreaker: CircuitBreaker;\n  private writeBuffer: WriteBuffer; // NEW: Batch writes\n  \n  constructor() {\n    this.redis = new RedisClient({\n      sentinels: [...],\n      name: 'auth-master',\n      retryStrategy: (times) => Math.min(times * 50, 2000),\n      enableReadyCheck: true,\n      maxRetriesPerRequest: 2 // REFINED: Fail faster\n    });\n    \n    // REFINED: Dynamic sizing based on available memory\n    const availableMemory = process.memoryUsage().heapTotal;\n    const cacheMemoryBudget = availableMemory * 0.1; // 10% of heap\n    const avgEntrySize = 512; // bytes (estimate)\n    const maxEntries = Math.floor(cacheMemoryBudget / avgEntrySize);\n    \n    this.localCache = new LRUCache({\n      max: Math.min(maxEntries, 50000), // Cap at 50k entries\n      maxSize: cacheMemoryBudget,\n      sizeCalculation: (value) => JSON.stringify(value).length,\n      ttl: 300000, // 5 minutes default TTL\n      updateAgeOnGet: true,\n      updateAgeOnHas: true,\n      allowStale: true // NEW: Serve stale during Redis failure\n    });\n    \n    this.circuitBreaker = new CircuitBreaker(\n      async (operation) => this.redis[operation.method](...operation.args),\n      circuitBreakerConfig\n    );\n    \n    // NEW: Batch write buffer\n    this.writeBuffer = new WriteBuffer({\n      flushInterval: 100, // Flush every 100ms\n      maxBatchSize: 100,\n      onFlush: (batch) => this.flushWriteBatch(batch)\n    });\n  }\n  \n  async get(key: string): Promise<any | null> {\n    const startTime = performance.now();\n    \n    // Try Redis first (through circuit breaker)\n    try {\n      const redisResult = await this.circuitBreaker.execute({\n        method: 'get',\n        args: [key]\n      });\n      \n      if (redisResult !== null) {\n        const parsed = JSON.parse(redisResult);\n        // Update local cache\n        this.localCache.set(key, parsed);\n        \n        metrics.histogram('cache.get.latency', performance.now() - startTime, {\n          layer: 'redis',\n          hit: 'true'\n        });\n        \n        return parsed;\n      }\n    } catch (error) {\n      logger.warn('Redis unavailable, using fallback', { key, error: error.message });\n      metrics.increment('cache.redis.failure');\n    }\n    \n    // Try local cache (including stale entries during Redis failure)\n    const localResult = this.localCache.get(key, { allowStale: true });\n    if (localResult !== undefined) {\n      metrics.increment('cache.local.hit');\n      metrics.histogram('cache.get.latency', performance.now() - startTime, {\n        layer: 'local',\n        hit: 'true'\n      });\n      \n      return localResult;\n    }\n    \n    // Fall back to Postgres\n    metrics.increment('cache.postgres.fallback');\n    const dbResult = await this.getFromPostgres(key);\n    \n    metrics.histogram('cache.get.latency', performance.now() - startTime, {\n      layer: 'postgres',\n      hit: dbResult !== null ? 'true' : 'false'\n    });\n    \n    // Populate caches for future requests\n    if (dbResult !== null) {\n      this.localCache.set(key, dbResult);\n      // Try to update Redis in background (best effort)\n      this.updateRedisAsync(key, dbResult).catch(() => {});\n    }\n    \n    return dbResult;\n  }\n  \n  async set(key: string, value: any, ttl: number): Promise<void> {\n    // NEW: Write-through strategy with buffering\n    \n    // Update local cache immediately (for read-your-writes consistency)\n    this.localCache.set(key, value, { ttl });\n    \n    // Buffer write to Postgres and Redis (will be batched)\n    this.writeBuffer.add({\n      key,\n      value,\n      ttl,\n      timestamp: Date.now()\n    });\n  }\n  \n  private async flushWriteBatch(batch: WriteOperation[]): Promise<void> {\n    // Write to Postgres first (source of truth) - BATCHED\n    try {\n      await this.batchWriteToPostgres(batch);\n    } catch (error) {\n      logger.error('Failed to write batch to Postgres', { \n        batchSize: batch.length, \n        error \n      });\n      // CRITICAL: Re-queue failed writes for retry\n      batch.forEach(op => this.writeBuffer.add(op));\n      return;\n    }\n    \n    // Try to update Redis (best effort) - PIPELINED\n    try {\n      await this.circuitBreaker.execute({\n        method: 'pipeline',\n        args: [batch.map(op => ['setex', op.key, op.ttl, JSON.stringify(op.value)])]\n      });\n      \n      metrics.increment('cache.write_batch.success', { size: batch.length });\n    } catch (error) {\n      logger.warn('Redis batch write failed, data safe in Postgres', { \n        batchSize: batch.length \n      });\n      metrics.increment('cache.redis.write_failure');\n      // Not a critical failure - data is in Postgres and local cache\n    }\n  }\n  \n  private async batchWriteToPostgres(batch: WriteOperation[]): Promise<void> {\n    // Group by operation type for efficient batch processing\n    const blacklistOps = batch.filter(op => op.key.startsWith('blacklist:'));\n    const refreshOps = batch.filter(op => op.key.startsWith('refresh:'));\n    const rateLimitOps = batch.filter(op => op.key.startsWith('ratelimit:'));\n    \n    await Promise.all([\n      blacklistOps.length > 0 && this.batchInsertBlacklist(blacklistOps),\n      refreshOps.length > 0 && this.batchInsertRefreshTokens(refreshOps),\n      rateLimitOps.length > 0 && this.batchUpsertRateLimits(rateLimitOps)\n    ].filter(Boolean));\n  }\n  \n  private async batchInsertBlacklist(ops: WriteOperation[]): Promise<void> {\n    // NEW: Use COPY for bulk insert (much faster than individual INSERTs)\n    const copyStream = db.query(\n      copyFrom('COPY token_blacklist (jti, user_id, expires_at, reason) FROM STDIN')\n    );\n    \n    for (const op of ops) {\n      const keyParts = this.parseKey(op.key);\n      const expiresAt = new Date(Date.now() + op.ttl * 1000);\n      copyStream.write(`${keyParts.jti}\\t${op.value.userId}\\t${expiresAt.toISOString()}\\t${op.value.reason}\\n`);\n    }\n    \n    copyStream.end();\n    await copyStream;\n  }\n  \n  private async batchUpsertRateLimits(ops: WriteOperation[]): Promise<void> {\n    // NEW: Use ON CONFLICT for efficient upserts\n    const values = ops.map(op => {\n      const expiresAt = new Date(Date.now() + op.ttl * 1000);\n      return {\n        key: op.key,\n        count: op.value.count || 1,\n        window_start: op.value.windowStart || new Date(),\n        window_end: expiresAt,\n        synced_at: new Date()\n      };\n    });\n    \n    if (values.length === 0) return;\n    \n    // Build parameterized query\n    const placeholders = values.map((_, i) => \n      `($${i*5+1}, $${i*5+2}, $${i*5+3}, $${i*5+4}, $${i*5+5})`\n    ).join(', ');\n    \n    await db.query(\n      `INSERT INTO rate_limit_counters (key, count, window_start, window_end, synced_at) \n       VALUES ${placeholders}\n       ON CONFLICT (key) DO UPDATE SET \n         count = GREATEST(rate_limit_counters.count, EXCLUDED.count),\n         window_end = GREATEST(rate_limit_counters.window_end, EXCLUDED.window_end),\n         synced_at = EXCLUDED.synced_at`,\n      values.flatMap(v => [v.key, v.count, v.window_start, v.window_end, v.synced_at])\n    );\n  }\n  \n  async delete(key: string): Promise<void> {\n    // Delete from all layers immediately (no buffering for deletes)\n    await Promise.allSettled([\n      this.deleteFromPostgres(key),\n      this.circuitBreaker.execute({ method: 'del', args: [key] }),\n      Promise.resolve(this.localCache.delete(key))\n    ]);\n  }\n  \n  private async getFromPostgres(key: string): Promise<any | null> {\n    const keyParts = this.parseKey(key);\n    \n    switch (keyParts.type) {\n      case 'blacklist':\n        return this.getBlacklistedToken(keyParts.jti);\n      case 'refresh':\n        return this.getRefreshToken(keyParts.tokenHash);\n      case 'ratelimit':\n        return this.getRateLimitCounter(keyParts.identifier);\n      default:\n        return null;\n    }\n  }\n  \n  private parseKey(key: string): { type: string; [key: string]: string } {\n    // Parse keys like \"blacklist:jti:{jti}\", \"refresh:{hash}\", \"ratelimit:login:{ip}:{email}\"\n    const parts = key.split(':');\n    return {\n      type: parts[0],\n      ...this.extractKeyComponents(parts)\n    };\n  }\n}\n\n// NEW: Write buffering for batched Postgres writes\nclass WriteBuffer {\n  private buffer: WriteOperation[] = [];\n  private flushTimer: NodeJS.Timeout | null = null;\n  \n  constructor(private config: {\n    flushInterval: number;\n    maxBatchSize: number;\n    onFlush: (batch: WriteOperation[]) => Promise<void>;\n  }) {}\n  \n  add(operation: WriteOperation): void {\n    this.buffer.push(operation);\n    \n    // Flush immediately if batch is full\n    if (this.buffer.length >= this.config.maxBatchSize) {\n      this.flush();\n      return;\n    }\n    \n    // Schedule flush if not already scheduled\n    if (!this.flushTimer) {\n      this.flushTimer = setTimeout(() => this.flush(), this.config.flushInterval);\n    }\n  }\n  \n  private async flush(): Promise<void> {\n    if (this.flushTimer) {\n      clearTimeout(this.flushTimer);\n      this.flushTimer = null;\n    }\n    \n    if (this.buffer.length === 0) return;\n    \n    const batch = this.buffer.splice(0, this.buffer.length);\n    await this.config.onFlush(batch);\n  }\n}\n\ninterface WriteOperation {\n  key: string;\n  value: any;\n  ttl: number;\n  timestamp: number;\n}\n```\n\n### 3. Token Blacklist Failover Strategy (REFINED)\n\n**Problem**: Access token validation needs to check blacklist. If Redis is down, we risk:\n- Option A: Reject all requests (poor availability)\n- Option B: Accept all tokens (security risk)\n\n**Solution**: Bloom Filter + Postgres Fallback (ENHANCED)\n\n```typescript\ninterface TokenValidationStrategy {\n  // How critical is blacklist checking for this token?\n  riskLevel: 'low' | 'medium' | 'high';\n  \n  // What to do if Redis unavailable?\n  fallbackBehavior: 'accept' | 'reject' | 'accept_with_logging' | 'check_postgres';\n  \n  // Additional checks to perform when Redis down\n  compensatingControls: string[];\n}\n\n// NEW: Bloom filter for space-efficient blacklist checking\nclass BlacklistBloomFilter {\n  private filter: BloomFilter;\n  private lastSync: number = 0;\n  private syncInterval = 60000; // Sync every 60 seconds\n  \n  constructor() {\n    // Size for 1M entries with 0.01% false positive rate\n    this.filter = new BloomFilter(\n      1000000, // expected entries\n      0.0001   // false positive rate\n    );\n  }\n  \n  async maybeBlacklisted(jti: string): Promise<boolean> {\n    // Check if we need to resync\n    if (Date.now() - this.lastSync > this.syncInterval) {\n      await this.syncFromPostgres();\n    }\n    \n    return this.filter.has(jti);\n  }\n  \n  add(jti: string): void {\n    this.filter.add(jti);\n  }\n  \n  private async syncFromPostgres(): Promise<void> {\n    // Load all active blacklist entries\n    const result = await db.query(\n      'SELECT jti FROM token_blacklist WHERE expires_at > NOW()'\n    );\n    \n    // Rebuild filter\n    this.filter.clear();\n    for (const row of result.rows) {\n      this.filter.add(row.jti);\n    }\n    \n    this.lastSync = Date.now();\n    \n    logger.info('Synced blacklist bloom filter', { \n      entries: result.rowCount \n    });\n  }\n}\n\nconst blacklistBloomFilter = new BlacklistBloomFilter();\n\nasync function validateAccessTokenWithFailover(\n  token: string,\n  req: Request\n): Promise<TokenValidation> {\n  // 1. Verify JWT signature and basic claims\n  const payload = jwt.verify(token, getPublicKey(token), {\n    algorithms: ['RS256'],\n    issuer: 'auth.example.com',\n    audience: 'api.example.com',\n    clockTolerance: 10\n  });\n  \n  // 2. Determine risk level for this request\n  const riskLevel = calculateRiskLevel(payload, req);\n  \n  // 3. Try to check blacklist (with circuit breaker)\n  let isBlacklisted: boolean | null = null;\n  \n  try {\n    isBlacklisted = await cache.get(`blacklist:jti:${payload.jti}`) !== null;\n    \n    if (isBlacklisted) {\n      return { valid: false, reason: 'TOKEN_REVOKED' };\n    }\n  } catch (error) {\n    // Redis unavailable - use bloom filter + Postgres fallback\n    logger.warn('Blacklist check failed, using bloom filter + Postgres fallback', {\n      jti: payload.jti,\n      riskLevel,\n      error: error.message\n    });\n    \n    metrics.increment('auth.blacklist_check.failure', {\n      risk_level: riskLevel\n    });\n    \n    // NEW: Check bloom filter first (fast, local)\n    const maybeBlacklisted = await blacklistBloomFilter.maybeBlacklisted(payload.jti);\n    \n    if (!maybeBlacklisted) {\n      // Definitely not blacklisted (bloom filter guarantees no false negatives)\n      return await proceedWithValidation(payload, req, riskLevel, 'bloom_filter_negative');\n    }\n    \n    // Possibly blacklisted (bloom filter may have false positives)\n    // Check Postgres for authoritative answer based on risk level\n    return await handlePossibleBlacklist(payload, req, riskLevel);\n  }\n  \n  // 4. Additional validation\n  return await proceedWithValidation(payload, req, riskLevel, 'redis_check_passed');\n}\n\nasync function handlePossibleBlacklist(\n  payload: JWTPayload,\n  req: Request,\n  riskLevel: 'low' | 'medium' | 'high'\n): Promise<TokenValidation> {\n  \n  // For high/medium risk, check Postgres immediately\n  if (riskLevel === 'high' || riskLevel === 'medium') {\n    const isBlacklistedInDb = await checkBlacklistInPostgres(payload.jti);\n    \n    if (isBlacklistedInDb) {\n      return { valid: false, reason: 'TOKEN_REVOKED' };\n    }\n    \n    // False positive - proceed\n    metrics.increment('auth.bloom_filter.false_positive', { risk_level: riskLevel });\n    return await proceedWithValidation(payload, req, riskLevel, 'postgres_check_negative');\n  }\n  \n  // Low risk: Accept with logging (bloom filter false positive acceptable)\n  logger.info('Accepting low-risk request despite bloom filter positive', {\n    jti: payload.jti,\n    userId: payload.sub\n  });\n  \n  metrics.increment('auth.blacklist_bypass.low_risk_bloom_positive');\n  \n  // Check Postgres asynchronously for audit\n  checkBlacklistInPostgres(payload.jti).then(isBlacklisted => {\n    if (isBlacklisted) {\n      // Should have been blocked! Log security event\n      createSecurityEvent({\n        userId: payload.sub,\n        eventType: 'BLACKLISTED_TOKEN_ACCEPTED',\n        severity: 'HIGH',\n        details: {\n          jti: payload.jti,\n          reason: 'low_risk_bypass',\n          endpoint: req.path\n        }\n      });\n    }\n  });\n  \n  return await proceedWithValidation(payload, req, riskLevel, 'bloom_positive_low_risk_bypass');\n}\n\nasync function proceedWithValidation(\n  payload: JWTPayload,\n  req: Request,\n  riskLevel: 'low' | 'medium' | 'high',\n  checkMethod: string\n): Promise<TokenValidation> {\n  \n  // Perform compensating controls\n  try {\n    await performCompensatingControls(payload, req);\n  } catch (error) {\n    return { valid: false, reason: error.message };\n  }\n  \n  return { \n    valid: true, \n    fallbackMode: checkMethod !== 'redis_check_passed',\n    riskLevel,\n    checkMethod,\n    user: {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions\n    }\n  };\n}\n\nfunction calculateRiskLevel(\n  payload: JWTPayload,\n  req: Request\n): 'low' | 'medium' | 'high' {\n  const factors = {\n    // Token age (older = lower risk of revocation)\n    tokenAge: Date.now() - payload.iat * 1000,\n    \n    // Request type\n    isReadOnly: ['GET', 'HEAD', 'OPTIONS'].includes(req.method),\n    \n    // Endpoint sensitivity (NEW: Pattern matching)\n    isSensitiveEndpoint: /\\/(admin|settings|payment|delete|revoke)/.test(req.path),\n    \n    // User role\n    hasAdminRole: payload.roles?.includes('admin'),\n    \n    // Recent activity\n    sessionAge: Date.now() - (payload.session_started_at || payload.iat) * 1000,\n    \n    // NEW: Token version (old version = higher risk)\n    isOldTokenVersion: payload.token_version < payload.user_token_version\n  };\n  \n  // High risk: Admin actions, sensitive endpoints, write operations, old token version\n  if (factors.hasAdminRole || \n      factors.isSensitiveEndpoint ||\n      !factors.isReadOnly || \n      factors.isOldTokenVersion ||\n      factors.sessionAge < 60000) { // < 1 minute old\n    return 'high';\n  }\n  \n  // Low risk: Read-only, non-sensitive, old token, mature session\n  if (factors.isReadOnly && \n      !factors.isSensitiveEndpoint &&\n      factors.tokenAge > 300000 && // > 5 minutes old\n      factors.sessionAge > 3600000) { // > 1 hour old\n    return 'low';\n  }\n  \n  return 'medium';\n}\n\nasync function performCompensatingControls(\n  payload: JWTPayload,\n  req: Request\n): Promise<void> {\n  // 1. Check if user still exists and is active\n  const user = await getUserById(payload.sub);\n  if (!user || user.locked_until > new Date()) {\n    throw new Error('USER_INACTIVE');\n  }\n  \n  // 2. Verify token version matches (password not changed)\n  if (payload.token_version !== user.token_version) {\n    throw new Error('TOKEN_VERSION_MISMATCH');\n  }\n  \n  // 3. NEW: Check session still exists and active (catches bulk revocations)\n  const session = await getRefreshTokenBySession(payload.session_id);\n  if (!session || session.revoked) {\n    throw new Error('SESSION_REVOKED');\n  }\n  \n  // 4. Log for audit trail\n  await createSecurityEvent({\n    userId: payload.sub,\n    eventType: 'TOKEN_VALIDATED_FALLBACK_MODE',\n    severity: 'MEDIUM',\n    details: {\n      jti: payload.jti,\n      endpoint: req.path,\n      method: req.method,\n      reason: 'redis_unavailable'\n    }\n  });\n}\n\nasync function checkBlacklistInPostgres(jti: string): Promise<boolean> {\n  // REFINED: Use prepared statement for better performance\n  const result = await db.query({\n    name: 'check_blacklist',\n    text: 'SELECT 1 FROM token_blacklist WHERE jti = $1 AND expires_at > NOW() LIMIT 1',\n    values: [jti]\n  });\n  \n  return result.rowCount > 0;\n}\n```\n\n**Token Blacklist Table (Postgres) - REFINED**\n```sql\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL, -- Same as original token expiry\n  reason VARCHAR(100),\n  session_id UUID,\n  ip_address INET,\n  user_agent TEXT\n);\n\n-- REFINED: Covering index for hot path query\nCREATE INDEX idx_token_blacklist_lookup ON token_blacklist(jti, expires_at) \n  WHERE expires_at > NOW();\n\n-- REFINED: Partitioning for large-scale deployments\nCREATE INDEX idx_token_blacklist_expires ON token_blacklist(expires_at);\nCREATE INDEX idx_token_blacklist_user ON token_blacklist(user_id, revoked_at DESC);\n\n-- NEW: Partition by expiry for efficient cleanup\n-- (For PostgreSQL 10+)\nCREATE TABLE token_blacklist_partitioned (\n  LIKE token_blacklist INCLUDING ALL\n) PARTITION BY RANGE (expires_at);\n\n-- Create partitions for next 7 days\nCREATE TABLE token_blacklist_p0 PARTITION OF token_blacklist_partitioned\n  FOR VALUES FROM (NOW()) TO (NOW() + INTERVAL '1 day');\n-- ... repeat for each day\n\n-- Automatic partition management via pg_partman or cron\n-- Drop old partitions instead of DELETE (much faster)\n\n-- Automatic cleanup of expired entries\nCREATE OR REPLACE FUNCTION cleanup_expired_blacklist()\nRETURNS void AS $$\nBEGIN\n  -- Use DELETE with small batch size to avoid lock contention\n  DELETE FROM token_blacklist \n  WHERE jti IN (\n    SELECT jti FROM token_blacklist \n    WHERE expires_at < NOW() - INTERVAL '1 day'\n    LIMIT 1000\n  );\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Run cleanup every hour (more frequent than daily)\nSELECT cron.schedule('cleanup-blacklist', '0 * * * *', 'SELECT cleanup_expired_blacklist()');\n```\n\n### 4. Refresh Token Locking Fallback (REFINED)\n\n**Problem**: Distributed locks prevent concurrent refresh. If Redis is down, we risk:\n- Race conditions causing token reuse false positives\n- Complete inability to refresh tokens\n\n**Solution**: Postgres Row-Level Locking (BETTER than advisory locks)\n\n```typescript\nasync function performTokenRefreshWithFailover(\n  refreshToken: string,\n  fingerprint: string,\n  requestId: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // Try Redis distributed lock first\n  let lockAcquired = false;\n  let useFallbackLock = false;\n  \n  try {\n    lockAcquired = await acquireRedisLock(tokenHash, requestId, 5000);\n  } catch (error) {\n    logger.warn('Redis lock unavailable, using Postgres row-level lock', {\n      tokenHash: tokenHash.substring(0, 16),\n      error: error.message\n    });\n    \n    metrics.increment('auth.refresh_lock.fallback_to_postgres');\n    useFallbackLock = true;\n  }\n  \n  if (useFallbackLock) {\n    // NEW: Use Postgres row-level lock (SELECT FOR UPDATE)\n    // This is BETTER than advisory locks because:\n    // 1. Automatically released on transaction commit/rollback\n    // 2. No need to track lock IDs\n    // 3. Works naturally with refresh token row\n    return await performRefreshWithPostgresLock(refreshToken, fingerprint);\n  }\n  \n  if (!lockAcquired) {\n    // Another process is refreshing - wait and check for result\n    return await waitForConcurrentRefresh(tokenHash, requestId, false);\n  }\n  \n  try {\n    // Perform refresh operation\n    const result = await executeTokenRefresh(refreshToken, fingerprint);\n    \n    // Store result for concurrent requests (best effort)\n    try {\n      await cache.set(\n        `refresh_result:${tokenHash}`,\n        result,\n        5 // 5 seconds\n      );\n    } catch {\n      // Not critical if this fails\n    }\n    \n    return result;\n    \n  } finally {\n    // Release lock\n    await releaseRedisLock(tokenHash, requestId);\n  }\n}\n\n// NEW: Use row-level locking instead of advisory locks\nasync function performRefreshWithPostgresLock(\n  refreshToken: string,\n  fingerprint: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // Start transaction\n  const client = await db.getClient();\n  \n  try {\n    await client.query('BEGIN');\n    \n    // 1. Lock the refresh token row (blocks concurrent refreshes)\n    const lockResult = await client.query(\n      'SELECT * FROM refresh_tokens ' +\n      'WHERE token_hash = $1 ' +\n      'FOR UPDATE NOWAIT', // Fail immediately if already locked\n      [tokenHash]\n    );\n    \n    if (lockResult.rowCount === 0) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n    \n    const tokenData = lockResult.rows[0];\n    \n    // 2. Validate token\n    if (tokenData.expires_at < new Date()) {\n      throw new AuthError('REFRESH_TOKEN_EXPIRED');\n    }\n    \n    if (tokenData.revoked) {\n      throw new AuthError('REFRESH_TOKEN_REVOKED');\n    }\n    \n    // 3. Check if already used (CRITICAL for security)\n    if (tokenData.used) {\n      // Token reuse detected - trigger compromise response\n      await handleTokenFamilyCompromise(\n        tokenData.token_family_id,\n        'TOKEN_REUSE',\n        client // Use same transaction\n      );\n      throw new AuthError('TOKEN_REUSE_DETECTED');\n    }\n    \n    // 4. Validate fingerprint\n    if (tokenData.device_fingerprint_hash !== hashFingerprint(fingerprint)) {\n      throw new AuthError('FINGERPRINT_MISMATCH');\n    }\n    \n    // 5. Mark token as used (within same transaction)\n    await client.query(\n      'UPDATE refresh_tokens SET used = true, used_at = NOW() ' +\n      'WHERE token_hash = $1',\n      [tokenHash]\n    );\n    \n    // 6. Generate new token pair\n    const user = await getUserById(tokenData.user_id);\n    \n    const newAccessToken = generateAccessToken(user, {\n      sessionId: tokenData.session_id,\n      deviceId: tokenData.device_id,\n      tokenVersion: user.token_version\n    });\n    \n    const newRefreshTokenData = await generateAndStoreRefreshToken(\n      user, \n      {\n        sessionId: tokenData.session_id,\n        deviceId: tokenData.device_id,\n        deviceFingerprint: fingerprint,\n        parentTokenId: tokenData.id,\n        tokenFamilyId: tokenData.token_family_id,\n        timesRefreshed: tokenData.times_refreshed + 1\n      },\n      client // Use same transaction\n    );\n    \n    // Commit transaction (releases lock)\n    await client.query('COMMIT');\n    \n    // Update cache (best effort)\n    try {\n      await cache.set(\n        `refresh:${tokenHash}:used`,\n        true,\n        900 // 15 minutes\n      );\n    } catch {\n      // Not critical - Postgres is source of truth\n    }\n    \n    return {\n      accessToken: newAccessToken.token,\n      refreshToken: newRefreshTokenData.token,\n      expiresIn: 900,\n      tokenType: 'Bearer',\n      refreshExpiresIn: 2592000\n    };\n    \n  } catch (error) {\n    await client.query('ROLLBACK');\n    \n    if (error.code === '55P03') {\n      // Lock not available (NOWAIT failed)\n      // Another process is refreshing this token\n      throw new AuthError('CONCURRENT_REFRESH_IN_PROGRESS');\n    }\n    \n    throw error;\n    \n  } finally {\n    client.release();\n  }\n}\n\nasync function executeTokenRefresh(\n  refreshToken: string,\n  fingerprint: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // 1. Retrieve token from cache (multi-layer)\n  let tokenData = await cache.get(`refresh:${tokenHash}`);\n  \n  if (!tokenData) {\n    // Load from Postgres if cache miss\n    tokenData = await getRefreshTokenFromPostgres(tokenHash);\n    \n    if (!tokenData) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n  }\n  \n  // 2. Validate token\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Check if already used (CRITICAL for security)\n  // NEW: Check cache first for fast rejection\n  const alreadyUsed = await cache.get(`refresh:${tokenHash}:used`);\n  if (alreadyUsed || tokenData.used) {\n    // Token reuse detected - trigger compromise response\n    await handleTokenFamilyCompromise(\n      tokenData.token_family_id,\n      'TOKEN_REUSE'\n    );\n    throw new AuthError('TOKEN_REUSE_DETECTED');\n  }\n  \n  // 4. Validate fingerprint\n  if (tokenData.device_fingerprint_hash !== hashFingerprint(fingerprint)) {\n    throw new AuthError('FINGERPRINT_MISMATCH');\n  }\n  \n  // 5. Mark token as used (atomic operation in Postgres)\n  const updateResult = await db.query(\n    'UPDATE refresh_tokens SET used = true, used_at = NOW() ' +\n    'WHERE token_hash = $1 AND used = false ' +\n    'RETURNING id', // Ensure exactly one row updated\n    [tokenHash]\n  );\n  \n  if (updateResult.rowCount === 0) {\n    // Race condition: Token was used by another request\n    await handleTokenFamilyCompromise(\n      tokenData.token_family_id,\n      'TOKEN_REUSE'\n    );\n    throw new AuthError('TOKEN_REUSE_DETECTED');\n  }\n  \n  // Mark in cache immediately (before generating new tokens)\n  try {\n    await cache.set(\n      `refresh:${tokenHash}:used`,\n      true,\n      tokenData.ttl_remaining || 900\n    );\n  } catch {\n    // Not critical - Postgres is source of truth\n  }\n  \n  // 6. Generate new token pair\n  const user = await getUserById(tokenData.user_id);\n  \n  const newAccessToken = generateAccessToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    tokenVersion: user.token_version\n  });\n  \n  const newRefreshToken = await generateRefreshToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    deviceFingerprint: fingerprint,\n    parentTokenId: tokenData.id,\n    tokenFamilyId: tokenData.token_family_id,\n    timesRefreshed: tokenData.times_refreshed + 1\n  });\n  \n  return {\n    accessToken: newAccessToken.token,\n    refreshToken: newRefreshToken.token,\n    expiresIn: 900,\n    tokenType: 'Bearer',\n    refreshExpiresIn: 2592000\n  };\n}\n```\n\n### 5. Rate Limiting Fallback (REFINED)\n\n**Problem**: Rate limiting requires counters in Redis. If Redis is down:\n- Option A: Disable rate limiting (security risk)\n- Option B: Reject all requests (poor availability)\n\n**Solution**: In-Memory Rate Limiting with Periodic Postgres Sync (IMPROVED)\n\n```typescript\nclass RateLimiterWithFailover {\n  private redis: RedisClient;\n  private localRateLimiter: Map<string, RateLimitCounter>;\n  private circuitBreaker: CircuitBreaker;\n  private syncInterval: NodeJS.Timeout;\n  private pendingSyncs: Set<string> = new Set(); // NEW: Track dirty keys\n  \n  constructor() {\n    this.localRateLimiter = new Map();\n    \n    // NEW: Sync only dirty counters every 10 seconds (reduce write load)\n    this.syncInterval = setInterval(\n      () => this.syncDirtyCountersToPostgres(),\n      10000\n    );\n    \n    // Load initial state from Postgres on startup\n    this.loadCountersFromPostgres();\n  }\n  \n  async checkRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    // Try Redis first\n    try {\n      const result = await this.checkRedisRateLimit(key, max, windowSec);\n      return result;\n    } catch (error) {\n      logger.warn('Redis rate limit check failed, using local fallback', {\n        key: this.sanitizeKey(key),\n        error: error.message\n      });\n      \n      metrics.increment('ratelimit.fallback_to_local');\n    }\n    \n    // Use in-memory fallback\n    return this.checkLocalRateLimit(key, max, windowSec);\n  }\n  \n  private async checkRedisRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    // Use Redis pipeline for atomic increment + TTL check\n    const pipeline = this.redis.pipeline();\n    pipeline.incr(key);\n    pipeline.ttl(key);\n    \n    const results = await this.circuitBreaker.execute({\n      method: 'exec',\n      args: [pipeline]\n    });\n    \n    const count = results[0][1]; // First command result\n    const ttl = results[1][1];   // Second command result\n    \n    if (ttl === -1) {\n      // Key exists but has no TTL - set it\n      await this.redis.expire(key, windowSec);\n    }\n    \n    if (count > max) {\n      const retryAfter = ttl > 0 ? ttl : windowSec;\n      return { allowed: false, retryAfter };\n    }\n    \n    return { allowed: true };\n  }\n  \n  private checkLocalRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): { allowed: boolean; retryAfter?: number } {\n    \n    const now = Date.now();\n    const windowMs = windowSec * 1000;\n    \n    // Get or create counter\n    let counter = this.localRateLimiter.get(key);\n    \n    if (!counter || now > counter.windowEnd) {\n      // Start new window\n      counter = {\n        count: 1,\n        windowStart: now,\n        windowEnd: now + windowMs,\n        key: key,\n        dirty: true // NEW: Mark as needing sync\n      };\n      this.localRateLimiter.set(key, counter);\n      this.pendingSyncs.add(key); // NEW: Track for next sync\n      return { allowed: true };\n    }\n    \n    // Increment counter\n    counter.count++;\n    counter.dirty = true;\n    this.pendingSyncs.add(key);\n    \n    if (counter.count > max) {\n      const retryAfter = Math.ceil((counter.windowEnd - now) / 1000);\n      return { allowed: false, retryAfter };\n    }\n    \n    return { allowed: true };\n  }\n  \n  // NEW: Only sync dirty counters (reduces write load)\n  private async syncDirtyCountersToPostgres(): Promise<void> {\n    if (this.pendingSyncs.size === 0) return;\n    \n    const keysToSync = Array.from(this.pendingSyncs);\n    const counters = keysToSync\n      .map(key => this.localRateLimiter.get(key))\n      .filter((c): c is RateLimitCounter => c !== undefined && c.dirty);\n    \n    if (counters.length === 0) return;\n    \n    try {\n      // NEW: Use single upsert with multiple values (more efficient)\n      const values = counters.map(counter => ({\n        key: counter.key,\n        count: counter.count,\n        window_start: new Date(counter.windowStart),\n        window_end: new Date(counter.windowEnd),\n        synced_at: new Date()\n      }));\n      \n      // Build parameterized query\n      const placeholders = values.map((_, i) => \n        `($${i*5+1}, $${i*5+2}, $${i*5+3}, $${i*5+4}, $${i*5+5})`\n      ).join(', ');\n      \n      await db.query(\n        `INSERT INTO rate_limit_counters (key, count, window_start, window_end, synced_at) \n         VALUES ${placeholders}\n         ON CONFLICT (key) DO UPDATE SET \n           count = GREATEST(rate_limit_counters.count, EXCLUDED.count),\n           window_end = GREATEST(rate_limit_counters.window_end, EXCLUDED.window_end),\n           synced_at = EXCLUDED.synced_at`,\n        values.flatMap(v => [v.key, v.count, v.window_start, v.window_end, v.synced_at])\n      );\n      \n      logger.debug('Synced rate limit counters to Postgres', {\n        count: counters.length\n      });\n      \n      // Mark as clean\n      counters.forEach(c => c.dirty = false);\n      this.pendingSyncs.clear();\n      \n    } catch (error) {\n      logger.error('Failed to sync rate limit counters', { error });\n      // Keep pending syncs for retry\n    }\n    \n    // Clean up expired counters\n    this.cleanupExpiredCounters();\n  }\n  \n  private cleanupExpiredCounters(): void {\n    const now = Date.now();\n    for (const [key, counter] of this.localRateLimiter.entries()) {\n      if (now > counter.windowEnd + 60000) { // 1 minute grace\n        this.localRateLimiter.delete(key);\n        this.pendingSyncs.delete(key);\n      }\n    }\n  }\n  \n  async loadCountersFromPostgres(): Promise<void> {\n    // On startup, load recent counters from Postgres\n    // This provides awareness of rate limits from other instances\n    \n    const result = await db.query(\n      'SELECT key, count, window_start, window_end FROM rate_limit_counters ' +\n      'WHERE window_end > NOW() - INTERVAL \\'5 minutes\\'' // Only recent windows\n    );\n    \n    for (const row of result.rows) {\n      this.localRateLimiter.set(row.key, {\n        count: row.count,\n        windowStart: row.window_start.getTime(),\n        windowEnd: row.window_end.getTime(),\n        key: row.key,\n        dirty: false\n      });\n    }\n    \n    logger.info('Loaded rate limit counters from Postgres', {\n      count: result.rowCount\n    });\n  }\n  \n  private sanitizeKey(key: string): string {\n    // Remove sensitive data (emails, IPs) from logs\n    return key.replace(/:[^:]+@/, ':***@').replace(/:\\d+\\.\\d+\\.\\d+\\.\\d+/, ':***');\n  }\n}\n\ninterface RateLimitCounter {\n  count: number;\n  windowStart: number;\n  windowEnd: number;\n  key: string;\n  dirty: boolean; // NEW: Track if needs sync\n}\n```\n\n**Rate Limit Counters Table (Postgres) - REFINED**\n```sql\nCREATE TABLE rate_limit_counters (\n  key VARCHAR(500) PRIMARY KEY,\n  count INTEGER NOT NULL,\n  window_start TIMESTAMP NOT NULL,\n  window_end TIMESTAMP NOT NULL,\n  synced_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\n-- REFINED: Partial index for active windows only\nCREATE INDEX idx_rate_limit_active ON rate_limit_counters(window_end) \n  WHERE window_end > NOW();\n\n-- NEW: Index for startup loading\nCREATE INDEX idx_rate_limit_recent ON rate_limit_counters(synced_at DESC)\n  WHERE window_end > NOW() - INTERVAL '5 minutes';\n\n-- Cleanup expired counters more frequently\nCREATE OR REPLACE FUNCTION cleanup_rate_limit_counters()\nRETURNS void AS $$\nBEGIN\n  -- Use batch delete to avoid lock contention\n  DELETE FROM rate_limit_counters \n  WHERE key IN (\n    SELECT key FROM rate_limit_counters\n    WHERE window_end < NOW() - INTERVAL '1 hour'\n    LIMIT 1000\n  );\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Run cleanup every 15 minutes\nSELECT cron.schedule('cleanup-rate-limits', '*/15 * * * *', \n  'SELECT cleanup_rate_limit_counters()');\n```\n\n### 6. Circuit Breaker Implementation (REFINED)\n\n```typescript\nclass CircuitBreaker {\n  private state: CircuitState = CircuitState.CLOSED;\n  private failureCount: number = 0;\n  private successCount: number = 0;\n  private requestCount: number = 0; // NEW: Track total requests\n  private lastFailureTime: number = 0;\n  private nextAttemptTime: number = 0;\n  private consecutiveSuccesses: number = 0; // NEW: For half-open state\n  \n  constructor(\n    private operation: (args: any) => Promise<any>,\n    private config: CircuitBreakerConfig\n  ) {}\n  \n  async execute(args: any): Promise<any> {\n    // Check if circuit should transition to half-open\n    if (this.state === CircuitState.OPEN && \n        Date.now() >= this.nextAttemptTime) {\n      this.state = CircuitState.HALF_OPEN;\n      this.consecutiveSuccesses = 0;\n      logger.info('Circuit breaker entering HALF_OPEN state');\n      metrics.gauge('circuit_breaker.state', 0.5);\n    }\n    \n    // Reject immediately if circuit is open\n    if (this.state === CircuitState.OPEN) {\n      const error = new Error('Circuit breaker is OPEN');\n      error.name = 'CircuitBreakerOpen';\n      metrics.increment('circuit_breaker.rejected');\n      throw error;\n    }\n    \n    // In half-open, only allow limited requests\n    if (this.state === CircuitState.HALF_OPEN && \n        this.consecutiveSuccesses >= this.config.successThreshold) {\n      // Enough successes - close circuit\n      this.closeCircuit();\n    }\n    \n    try {\n      // Execute with timeout\n      const result = await Promise.race([\n        this.operation(args),\n        this.timeoutPromise(this.config.timeout)\n      ]);\n      \n      // Success\n      this.onSuccess();\n      return result;\n      \n    } catch (error) {\n      // Failure\n      this.onFailure();\n      throw error;\n    }\n  }\n  \n  private onSuccess(): void {\n    this.requestCount++;\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      this.consecutiveSuccesses++;\n      \n      // After enough successful requests, close circuit\n      if (this.consecutiveSuccesses >= this.config.successThreshold) {\n        this.closeCircuit();\n      }\n    }\n    \n    // In closed state, decay failure count on success\n    if (this.state === CircuitState.CLOSED && this.failureCount > 0) {\n      this.failureCount = Math.max(0, this.failureCount - 1);\n    }\n  }\n  \n  private onFailure(): void {\n    this.failureCount++;\n    this.requestCount++;\n    this.lastFailureTime = Date.now();\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      // Failure in half-open state - immediately re-open circuit\n      this.openCircuit();\n      return;\n    }\n    \n    // Check if we should open the circuit\n    // NEW: Only evaluate after minimum request volume\n    if (this.requestCount >= this.config.requestVolumeThreshold) {\n      const failureRate = this.failureCount / this.requestCount;\n      \n      if (failureRate >= this.config.failureThreshold) {\n        this.openCircuit();\n      }\n    }\n  }\n  \n  private openCircuit(): void {\n    this.state = CircuitState.OPEN;\n    this.nextAttemptTime = Date.now() + this.config.resetTimeout;\n    this.consecutiveSuccesses = 0;\n    \n    logger.error('Circuit breaker opened', {\n      failureCount: this.failureCount,\n      requestCount: this.requestCount,\n      failureRate: (this.failureCount / this.requestCount * 100).toFixed(2) + '%',\n      lastFailureTime: new Date(this.lastFailureTime),\n      nextAttemptTime: new Date(this.nextAttemptTime)\n    });\n    \n    metrics.gauge('circuit_breaker.state', 1);\n    metrics.increment('circuit_breaker.opened');\n  }\n  \n  private closeCircuit(): void {\n    this.state = CircuitState.CLOSED;\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.requestCount = 0;\n    this.consecutiveSuccesses = 0;\n    \n    logger.info('Circuit breaker closed after successful recovery');\n    metrics.gauge('circuit_breaker.state', 0);\n    metrics.increment('circuit_breaker.closed');\n  }\n  \n  private timeoutPromise(ms: number): Promise<never> {\n    return new Promise((_, reject) => {\n      setTimeout(() => {\n        const error = new Error(`Operation timed out after ${ms}ms`);\n        error.name = 'TimeoutError';\n        reject(error);\n      }, ms);\n    });\n  }\n  \n  getState(): CircuitState {\n    return this.state;\n  }\n  \n  getMetrics(): CircuitBreakerMetrics {\n    return {\n      state: this.state,\n      failureCount: this.failureCount,\n      requestCount: this.requestCount,\n      failureRate: this.requestCount > 0 ? this.failureCount / this.requestCount : 0,\n      consecutiveSuccesses: this.consecutiveSuccesses\n    };\n  }\n}\n\ninterface CircuitBreakerMetrics {\n  state: CircuitState;\n  failureCount: number;\n  requestCount: number;\n  failureRate: number;\n  consecutiveSuccesses: number;\n}\n```\n\n## Geographic Location Policy\n\n[KEEPING ENTIRE SECTION - Already well-designed, no changes needed]\n\n### 1. Location Change Risk Matrix\n[Same as original - already comprehensive]\n\n### 2. VPN and Proxy Detection\n[Same as original - already comprehensive]\n\n### 3. User and Device Trust Levels\n[Same as original - already comprehensive]\n\n### 4. Location Change Evaluation\n[Same as original - already comprehensive]\n\n### 5. Mobile User Considerations\n[Same as original - already comprehensive]\n\n## Performance Characteristics & Scaling Analysis\n\n### Postgres Fallback Performance Analysis\n\n#### 1. Token Blacklist Lookup Performance\n\n**Normal Operation (Redis)**:\n- Latency: ~1-2ms\n- Throughput: 100,000+ req/s per Redis instance\n\n**Fallback Operation (Postgres)**:\n- With proposed index (`idx_token_blacklist_lookup`):\n  - Latency: ~5-15ms (depending on connection pool, disk I/O)\n  - Throughput: ~2,000-5,000 req/s per Postgres instance\n  \n**Optimization: Add Bloom Filter Layer**\n- Bloom filter check: ~0.1ms (in-memory)\n- False positive rate: 0.01% (1 in 10,000)\n- Only 0.01% of requests hit Postgres\n- Effective Postgres load: 1 req/s (from 10,000 req/s traffic)\n\n**Verdict**: \u2705 **Bloom filter makes Postgres fallback viable** even at high scale.\n\n#### 2. Refresh Token Locking Performance\n\n**Redis Distributed Lock**:\n- Lock acquisition: ~2-5ms\n- Lock release: ~1-2ms\n- Contention handling: Excellent (sub-ms spinning)\n\n**Postgres Row-Level Lock (SELECT FOR UPDATE NOWAIT)**:\n- Lock acquisition: ~5-10ms (includes query time)\n- Lock release: Automatic on COMMIT (~2-5ms)\n- Contention handling: **IMMEDIATE FAILURE** with NOWAIT (good for our use case)\n\n**Contention Analysis**:\n- Scenario: 1M active users, 1 refresh per hour = 278 refreshes/second\n- Probability of collision: 278 / (60 * 60) / 1M = **negligible**\n- Even at 10x traffic (2,780 req/s), collision rate < 0.001%\n\n**Lock Sharding**: NOT NEEDED because:\n- Each refresh token has unique row (natural sharding)\n- NOWAIT ensures no blocking (fast failure)\n- Collisions only occur for SAME token (already error case)\n\n**Verdict**: \u2705 **Row-level locking is BETTER than advisory locks** and performs well.\n\n#### 3. Rate Limiting Sync Performance\n\n**Write Load**:\n- Scenario: 1M requests/minute = 16,667 req/s\n- Sync interval: 10 seconds\n- Unique keys per interval: ~1,000-5,000 (users x endpoints)\n\n**Batch Upsert Performance**:\n- Single upsert: ~2-5ms\n- Batched upsert (100 rows): ~10-20ms\n- **Amortized cost**: 0.2ms per row\n\n**Proposed Approach**:\n```sql\nINSERT INTO rate_limit_counters (...) VALUES (...)\nON CONFLICT (key) DO UPDATE SET \n  count = GREATEST(rate_limit_counters.count, EXCLUDED.count),\n  ...\n```\n\n**Why GREATEST()**?\n- Handles race between multiple app instances\n- Always keeps highest count (conservative for rate limiting)\n- Avoids lost updates\n\n**Write Amplification**:\n- Peak: 500 upserts every 10 seconds = 50 writes/sec\n- **Negligible** compared to typical Postgres capacity (10,000+ writes/sec)\n\n**Verdict**: \u2705 **Batched upserts with ON CONFLICT are efficient and correct**.\n\n#### 4. Circuit Breaker Threshold Tuning\n\n**Original Proposal**: 50% failure over 100 requests\n\n**Analysis**:\n- **Too lenient**: Allows 50 failed requests before tripping\n- At 1,000 req/s, takes 100ms to trip (50-500 requests may fail)\n\n**Refined Proposal**: 30% failure over 50 requests\n\n**Why Better**:\n- Faster detection: 15 failures trigger circuit (vs 50)\n- At 1,000 req/s, trips in 50ms (faster failover)\n- Lower failure volume before fallback activates\n\n**Recovery Settings**:\n- Reset timeout: 5 seconds (vs 10) - faster recovery attempts\n- Success threshold: 5 consecutive (ensure stability before closing)\n\n**Verdict**: \u2705 **Refined thresholds provide faster failover with stable recovery**.\n\n## Security Vulnerability Analysis\n\n### Potential Attack Surfaces Introduced by Fallbacks\n\n#### 1. \u274c **VULNERABILITY: Bloom Filter False Negatives**\n\n**Issue**: Bloom filters CAN have false negatives if implementation is wrong.\n\n**Mitigation**: \n- Use well-tested library (e.g., `bloom-filters` npm package)\n- Add integration tests to verify no false negatives\n- Monitor discrepancies between bloom filter and Postgres\n\n**Verdict**: Low risk with proper implementation.\n\n---\n\n#### 2. \u26a0\ufe0f **RISK: Time-of-Check-Time-of-Use (TOCTOU) in Token Validation**\n\n**Scenario**:\n1. Token validated against stale local cache (not blacklisted)\n2. Between validation and request processing, token is blacklisted\n3. Request proceeds with revoked token\n\n**Impact**: \n- Window: Up to 5 minutes (local cache TTL)\n- Scope: Only affects requests during Redis outage\n- Risk level: **MEDIUM** (compensating controls mitigate)\n\n**Mitigations**:\n- Compensating controls (user status, token version, session check)\n- Short access token lifetime (15 min)\n- Security event logging for audit\n- Risk-based policy (high-risk operations check Postgres)\n\n**Verdict**: \u2705 **Acceptable risk with compensating controls**.\n\n---\n\n#### 3. \u26a0\ufe0f **RISK: Rate Limit Bypass Across Instances**\n\n**Scenario**:\n1. Redis down, instances use local rate limiters\n2. Attacker distributes requests across multiple instances\n3. Each instance counts independently\n\n**Impact**:\n- Effective rate limit multiplied by instance count\n- Example: 100 req/min limit \u00d7 10 instances = 1,000 req/min possible\n\n**Mitigations**:\n- Sync to Postgres every 10 seconds (limits window)\n- Load balancer sticky sessions (same user \u2192 same instance)\n- Use GREATEST() in upsert (never decrease count)\n- Exponential backoff on repeated violations\n\n**Additional Mitigation** (NEW):\n```typescript\n// Load cross-instance counts on critical operations\nasync function checkGlobalRateLimit(key: string): Promise<number> {\n  const localCount = this.localRateLimiter.get(key)?.count || 0;\n  const pgCount = await this.getPostgresCount(key);\n  \n  // Use higher of local vs Postgres (conservative)\n  return Math.max(localCount, pgCount);\n}\n```\n\n**Verdict**: \u26a0\ufe0f **Medium risk - requires load balancer configuration + Postgres sync**.\n\n---\n\n#### 4. \u2705 **NON-ISSUE: Token Reuse During Postgres Lock**\n\n**Question**: Can token reuse attack succeed during Postgres locking?\n\n**Answer**: **NO** - Transaction isolation prevents it:\n1. `SELECT FOR UPDATE NOWAIT` locks row\n2. Second request fails immediately (NOWAIT)\n3. No race condition possible\n\n**Verdict**: \u2705 **Secure by design**.\n\n---\n\n#### 5. \u274c **VULNERABILITY: Write Buffer Data Loss**\n\n**Scenario**:\n1. Application writes to buffer\n2. Crash before flush to Postgres\n3. Blacklist entries lost \u2192 revoked tokens accepted\n\n**Impact**: **CRITICAL** for security-critical writes (blacklist, revocations)\n\n**Mitigation**: \n```typescript\nasync function blacklistToken(jti: string, reason: string): Promise<void> {\n  // CRITICAL: Write to Postgres IMMEDIATELY (no buffering)\n  await db.query(\n    'INSERT INTO token_blacklist (jti, user_id, expires_at, reason) VALUES ($1, $2, $3, $4)',\n    [jti, userId, expiresAt, reason]\n  );\n  \n  // Update caches AFTER Postgres write succeeds\n  try {\n    await cache.delete(`token:${jti}`);\n    blacklistBloomFilter.add(jti);\n  } catch {\n    // Non-critical if cache update fails\n  }\n}\n```\n\n**Rule**: \u2705 **NEVER buffer security-critical writes. Only buffer performance-optimized data (rate limits, last_used_at timestamps).**\n\n---\n\n## Monitoring & Observability\n\n### Key Metrics to Track\n\n```typescript\n// Circuit breaker metrics\nmetrics.gauge('redis.circuit_breaker.state', state); // 0=closed, 0.5=half-open, 1=open\nmetrics.increment('redis.circuit_breaker.opened');\nmetrics.increment('redis.circuit_breaker.closed');\n\n// Fallback usage\nmetrics.increment('cache.fallback.local', { reason: 'redis_unavailable' });\nmetrics.increment('cache.fallback.postgres', { reason: 'cache_miss' });\nmetrics.increment('auth.blacklist_check.fallback', { method: 'bloom_filter' });\n\n// Performance\nmetrics.histogram('cache.get.latency', latency, { layer: 'redis|local|postgres' });\nmetrics.histogram('auth.token_validation.latency', latency, { fallback: 'true|false' });\n\n// Security events\nmetrics.increment('security.token_reuse.detected');\nmetrics.increment('security.blacklisted_token_accepted', { risk: 'low|medium|high' });\n\n// Rate limit effectiveness\nmetrics.increment('ratelimit.violation', { fallback_mode: 'true|false' });\nmetrics.increment('ratelimit.cross_instance_correction');\n```\n\n### Alerts\n\n```yaml\n# Critical: Redis down for >1 minute\n- alert: RedisUnavailable\n  expr: redis_circuit_breaker_state == 1 for 1m\n  severity: critical\n  \n# Warning: High Postgres fallback usage\n- alert: HighPostgresFallback\n  expr: rate(cache_fallback_postgres_total[5m]) > 100\n  severity: warning\n  \n# Security: Blacklisted tokens accepted\n- alert: BlacklistedTokenAccepted\n  expr: increase(security_blacklisted_token_accepted_total[5m]) > 0\n  severity: high\n```\n\n## What I Improved\n\n### Performance & Scaling\n1. **Bloom filter for blacklist**: Reduces Postgres load by 99.99% during Redis failure\n2. **Write buffering with batching**: Reduces Postgres write load by 10-100x\n3. **Row-level locking**: More reliable and performant than advisory locks\n4. **COPY for bulk inserts**: 10x faster than individual INSERTs\n5. **Dynamic cache sizing**: Adapts to available memory instead of fixed limit\n6. **Dirty tracking for rate limits**: Only sync changed counters (reduces write amplification)\n7. **Faster circuit breaker**: 30% threshold over 50 requests (vs 50% over 100)\n\n### Security\n8. **No buffering for security writes**: Blacklist writes go directly to Postgres (prevents data loss)\n9. **Bloom filter false positive handling**: Risk-based Postgres check for positives\n10. **Cross-instance rate limit loading**: Check Postgres on critical operations\n11. **Token version checking**: Added to risk level calculation\n12. **Stale cache tolerance**: Serve stale data during Redis failure (availability vs staleness tradeoff)\n\n### Reliability\n13. **Faster recovery**: 5-second reset timeout (vs 10) for quicker Redis reconnection\n14. **Success threshold**: Require 5 consecutive successes before closing circuit (prevents flapping)\n15. **Request count tracking**: Circuit breaker only evaluates after minimum volume\n16. **Graceful failure decay**: Success decreases failure count in closed state\n\n### Operational Excellence\n17. **Comprehensive metrics**: Added histograms for latency tracking by layer\n18. **Security event tracking**: Monitor blacklist bypasses and token reuse\n19. **Prepared statements**: Faster blacklist checks with query caching\n20. **Partition support**: Schema ready for time-based partitioning at scale\n\n## What I Kept\n\nThe location policy section was **already excellent** - comprehensive risk matrix, VPN handling, mobile considerations, trust levels, and progressive response strategy. No changes needed.\n\nAlso kept:\n- 15-minute access tokens, 30-day refresh tokens\n- Opaque refresh tokens with device fingerprinting\n- Token family tracking for compromise detection\n- httpOnly cookies with SameSite=Strict\n- Multi-device session management\n\n## Remaining Concerns\n\n### 1. Cross-Region Deployment\n\n**Challenge**: Design assumes single-region deployment.\n\n**For global scale**:\n- Option A: Regional Redis clusters with eventual consistency\n  - Pro: Low latency\n  - Con: Blacklist propagation delay (security risk)\n  \n- Option B: Global Redis with active-active replication\n  - Pro: Strong consistency\n  - Con: Expensive, complex setup\n  \n- Option C: Regional Postgres read replicas + write to primary\n  - Pro: Leverages existing Postgres HA\n  - Con: Write latency for distant regions\n\n**Recommendation**: Start single-region, add read replicas when needed.\n\n---\n\n### 2. Bloom Filter Memory Growth\n\n**Current**: 1M entries at 0.01% FPR \u2248 2.4 MB\n\n**At scale**: \n- 10M users \u00d7 2 tokens/day \u00d7 15-min lifetime = ~208K concurrent blacklist entries\n- Bloom filter size: ~500 KB (manageable)\n- 100M users: ~5 MB (still fine)\n\n**Mitigation**: \n- Partition bloom filter by time bucket (only load recent entries)\n- Rebuild every hour from Postgres\n\n**Verdict**: Not a concern until 1B+ users.\n\n---\n\n### 3. Postgres Connection Pool Saturation\n\n**Risk**: During Redis failure, all instances hit Postgres simultaneously.\n\n**Calculation**:\n- 10 app instances \u00d7 50 connections each = 500 connections\n- Postgres max_connections: Default 100 (\u274c not enough!)\n\n**Mitigation**:\n```\n# Increase Postgres connections\nmax_connections = 1000\n\n# Use PgBouncer for connection pooling\n# Transaction mode for short queries (blacklist checks)\n```\n\n**Verdict**: \u26a0\ufe0f **Must configure connection pooling for Postgres fallback**.\n\n---\n\n### 4. Write Amplification on Token Refresh\n\n**Current Design**:\n- Every refresh writes to: `refresh_tokens` (INSERT + UPDATE), `token_blacklist` (if access token active)\n- 1M users, 1 refresh/hour = 278 writes/sec\n\n**At 10M users**: 2,780 writes/sec (approaching limits for single Postgres instance)\n\n**Optimization Options**:\n1. **Partition refresh_tokens by created_at** (distribute I/O)\n2. **Batch blacklist inserts** (already implemented via write buffer)\n3. **Use separate write-optimized DB** for audit logs\n4. **Lazy cleanup** (mark as used without immediate blacklist write)\n\n**Verdict**: \u26a0\ufe0f **Need partitioning beyond 10M users**.\n\n## Convergence Status\n\n**ITERATING** - This is round 3. Major improvements made:\n\n\u2705 **Resolved**:\n- Postgres fallback performance (bloom filter, batching, row locks)\n- Circuit breaker thresholds (faster detection, stable recovery)\n- Security vulnerabilities (write buffering, TOCTOU, rate limit bypass)\n\n\u26a0\ufe0f **Still needs validation**:\n- Cross-region deployment strategy\n- Connection pool sizing for Postgres fallback\n- Partitioning strategy for 10M+ users\n- Load testing of failover paths under realistic traffic\n\nThe design is now **production-ready for single-region deployments up to 10M users**. Beyond that scale, we need partitioning and potentially multi-region strategy.\n\n---\n\n##",
        "prompt_for_other": "The design is now solid for single-region deployments. Please focus on:\n\n1. **Cross-Region Strategy**: How should we handle multi-region deployments? Specifically:\n   - Should blacklist propagation be eventually consistent across regions?\n   - Can we tolerate 100-500ms inter-region blacklist sync delay?\n   - How do we handle token refresh when user roams between regions?\n\n2. **Load Testing Plan**: What realistic failure scenarios should we test?\n   - Redis failure during peak traffic (1M req/s)\n   - Postgres connection pool exhaustion\n   - Circuit breaker flapping (Redis intermittent failures)\n   - Cross-instance rate limit effectiveness\n\n3. **Operational Runbook**: What should the incident response be when:\n   - Circuit breaker opens (Redis down)?\n   - Bloom filter false positive rate spikes?\n   - Postgres fallback latency exceeds SLA?\n\nIf you believe the design is ready for implementation without further iteration, signal **ACCEPTING_FINAL**.",
        "convergence_signal": "ACCEPTING_FINAL",
        "raw_response": "## Design\n\n# JWT Authentication System with Redis Failover & Geo-Location Policy\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client    \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   API Gateway \u2502\u2500\u2500\u2500\u2500\u2500\u25b6\u2502   Auth      \u2502\n\u2502   (SPA/App) \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   + Auth      \u2502\u25c0\u2500\u2500\u2500\u2500\u2500\u2502   Service   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502   Middleware  \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n                            \u2502                     \u2502\n                            \u25bc                     \u25bc\n                     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                     \u2502  Application \u2502      \u2502  Token      \u2502\n                     \u2502  Services    \u2502      \u2502  Store      \u2502\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502 (Redis HA)  \u2502\n                                           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n                                                  \u2502\n                                           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                           \u2502             \u2502\n                                           \u25bc             \u25bc\n                                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n                                    \u2502  Database   \u2502 \u2502  Fallback   \u2502\n                                    \u2502  (Postgres) \u2502 \u2502  Cache      \u2502\n                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502  (In-Mem)   \u2502\n                                                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Redis High Availability & Failover Strategy\n\n### 1. Redis Deployment Architecture\n\n**Primary Configuration: Redis Sentinel**\n```yaml\n# Redis Sentinel for automatic failover\nredis-sentinel:\n  master:\n    name: auth-master\n    host: redis-master.internal\n    port: 6379\n  replicas:\n    - redis-replica-1.internal:6379\n    - redis-replica-2.internal:6379\n  sentinels:\n    - sentinel-1.internal:26379\n    - sentinel-2.internal:26379\n    - sentinel-3.internal:26379\n  quorum: 2  # Minimum sentinels needed for failover decision\n  \n  # Connection from application\n  connection_strategy: sentinel_aware\n  sentinel_failover_timeout: 10000  # 10 seconds\n  max_retry_attempts: 3\n```\n\n**Circuit Breaker Configuration**\n```typescript\ninterface CircuitBreakerConfig {\n  // Trip circuit if 30% of requests fail in window (REFINED)\n  failureThreshold: 0.3;\n  \n  // Consider last 50 requests (REFINED - faster detection)\n  requestVolumeThreshold: 50;\n  \n  // Check health every 5 seconds when open (REFINED - faster recovery)\n  resetTimeout: 5000;\n  \n  // Operations timeout\n  timeout: 1000; // 1 second (REFINED - more aggressive)\n  \n  // Consecutive successes needed to close from half-open (NEW)\n  successThreshold: 5;\n}\n\nenum CircuitState {\n  CLOSED = 'closed',    // Normal operation\n  OPEN = 'open',        // Redis unavailable, use fallback\n  HALF_OPEN = 'half_open' // Testing if Redis recovered\n}\n```\n\n### 2. Multi-Layer Caching Strategy with Read-Through/Write-Through\n\n**Layer 1: Redis (Primary)**\n- Fastest access path\n- Distributed across application instances\n- Handles 99.9% of requests under normal conditions\n\n**Layer 2: In-Process Cache (Fallback)**\n- LRU cache in application memory\n- Size-limited based on memory budget\n- Synchronizes with Redis when available\n- Used when Redis is unreachable\n\n**Layer 3: Postgres (Source of Truth)**\n- Authoritative data store\n- Slower but always available\n- Used for cache misses and rebuilding\n\n```typescript\ninterface CacheLayer {\n  get(key: string): Promise<any | null>;\n  set(key: string, value: any, ttl: number): Promise<void>;\n  delete(key: string): Promise<void>;\n  isAvailable(): Promise<boolean>;\n}\n\nclass MultiLayerCache {\n  private redis: RedisClient;\n  private localCache: LRUCache;\n  private postgres: PostgresClient;\n  private circuitBreaker: CircuitBreaker;\n  private writeBuffer: WriteBuffer; // NEW: Batch writes\n  \n  constructor() {\n    this.redis = new RedisClient({\n      sentinels: [...],\n      name: 'auth-master',\n      retryStrategy: (times) => Math.min(times * 50, 2000),\n      enableReadyCheck: true,\n      maxRetriesPerRequest: 2 // REFINED: Fail faster\n    });\n    \n    // REFINED: Dynamic sizing based on available memory\n    const availableMemory = process.memoryUsage().heapTotal;\n    const cacheMemoryBudget = availableMemory * 0.1; // 10% of heap\n    const avgEntrySize = 512; // bytes (estimate)\n    const maxEntries = Math.floor(cacheMemoryBudget / avgEntrySize);\n    \n    this.localCache = new LRUCache({\n      max: Math.min(maxEntries, 50000), // Cap at 50k entries\n      maxSize: cacheMemoryBudget,\n      sizeCalculation: (value) => JSON.stringify(value).length,\n      ttl: 300000, // 5 minutes default TTL\n      updateAgeOnGet: true,\n      updateAgeOnHas: true,\n      allowStale: true // NEW: Serve stale during Redis failure\n    });\n    \n    this.circuitBreaker = new CircuitBreaker(\n      async (operation) => this.redis[operation.method](...operation.args),\n      circuitBreakerConfig\n    );\n    \n    // NEW: Batch write buffer\n    this.writeBuffer = new WriteBuffer({\n      flushInterval: 100, // Flush every 100ms\n      maxBatchSize: 100,\n      onFlush: (batch) => this.flushWriteBatch(batch)\n    });\n  }\n  \n  async get(key: string): Promise<any | null> {\n    const startTime = performance.now();\n    \n    // Try Redis first (through circuit breaker)\n    try {\n      const redisResult = await this.circuitBreaker.execute({\n        method: 'get',\n        args: [key]\n      });\n      \n      if (redisResult !== null) {\n        const parsed = JSON.parse(redisResult);\n        // Update local cache\n        this.localCache.set(key, parsed);\n        \n        metrics.histogram('cache.get.latency', performance.now() - startTime, {\n          layer: 'redis',\n          hit: 'true'\n        });\n        \n        return parsed;\n      }\n    } catch (error) {\n      logger.warn('Redis unavailable, using fallback', { key, error: error.message });\n      metrics.increment('cache.redis.failure');\n    }\n    \n    // Try local cache (including stale entries during Redis failure)\n    const localResult = this.localCache.get(key, { allowStale: true });\n    if (localResult !== undefined) {\n      metrics.increment('cache.local.hit');\n      metrics.histogram('cache.get.latency', performance.now() - startTime, {\n        layer: 'local',\n        hit: 'true'\n      });\n      \n      return localResult;\n    }\n    \n    // Fall back to Postgres\n    metrics.increment('cache.postgres.fallback');\n    const dbResult = await this.getFromPostgres(key);\n    \n    metrics.histogram('cache.get.latency', performance.now() - startTime, {\n      layer: 'postgres',\n      hit: dbResult !== null ? 'true' : 'false'\n    });\n    \n    // Populate caches for future requests\n    if (dbResult !== null) {\n      this.localCache.set(key, dbResult);\n      // Try to update Redis in background (best effort)\n      this.updateRedisAsync(key, dbResult).catch(() => {});\n    }\n    \n    return dbResult;\n  }\n  \n  async set(key: string, value: any, ttl: number): Promise<void> {\n    // NEW: Write-through strategy with buffering\n    \n    // Update local cache immediately (for read-your-writes consistency)\n    this.localCache.set(key, value, { ttl });\n    \n    // Buffer write to Postgres and Redis (will be batched)\n    this.writeBuffer.add({\n      key,\n      value,\n      ttl,\n      timestamp: Date.now()\n    });\n  }\n  \n  private async flushWriteBatch(batch: WriteOperation[]): Promise<void> {\n    // Write to Postgres first (source of truth) - BATCHED\n    try {\n      await this.batchWriteToPostgres(batch);\n    } catch (error) {\n      logger.error('Failed to write batch to Postgres', { \n        batchSize: batch.length, \n        error \n      });\n      // CRITICAL: Re-queue failed writes for retry\n      batch.forEach(op => this.writeBuffer.add(op));\n      return;\n    }\n    \n    // Try to update Redis (best effort) - PIPELINED\n    try {\n      await this.circuitBreaker.execute({\n        method: 'pipeline',\n        args: [batch.map(op => ['setex', op.key, op.ttl, JSON.stringify(op.value)])]\n      });\n      \n      metrics.increment('cache.write_batch.success', { size: batch.length });\n    } catch (error) {\n      logger.warn('Redis batch write failed, data safe in Postgres', { \n        batchSize: batch.length \n      });\n      metrics.increment('cache.redis.write_failure');\n      // Not a critical failure - data is in Postgres and local cache\n    }\n  }\n  \n  private async batchWriteToPostgres(batch: WriteOperation[]): Promise<void> {\n    // Group by operation type for efficient batch processing\n    const blacklistOps = batch.filter(op => op.key.startsWith('blacklist:'));\n    const refreshOps = batch.filter(op => op.key.startsWith('refresh:'));\n    const rateLimitOps = batch.filter(op => op.key.startsWith('ratelimit:'));\n    \n    await Promise.all([\n      blacklistOps.length > 0 && this.batchInsertBlacklist(blacklistOps),\n      refreshOps.length > 0 && this.batchInsertRefreshTokens(refreshOps),\n      rateLimitOps.length > 0 && this.batchUpsertRateLimits(rateLimitOps)\n    ].filter(Boolean));\n  }\n  \n  private async batchInsertBlacklist(ops: WriteOperation[]): Promise<void> {\n    // NEW: Use COPY for bulk insert (much faster than individual INSERTs)\n    const copyStream = db.query(\n      copyFrom('COPY token_blacklist (jti, user_id, expires_at, reason) FROM STDIN')\n    );\n    \n    for (const op of ops) {\n      const keyParts = this.parseKey(op.key);\n      const expiresAt = new Date(Date.now() + op.ttl * 1000);\n      copyStream.write(`${keyParts.jti}\\t${op.value.userId}\\t${expiresAt.toISOString()}\\t${op.value.reason}\\n`);\n    }\n    \n    copyStream.end();\n    await copyStream;\n  }\n  \n  private async batchUpsertRateLimits(ops: WriteOperation[]): Promise<void> {\n    // NEW: Use ON CONFLICT for efficient upserts\n    const values = ops.map(op => {\n      const expiresAt = new Date(Date.now() + op.ttl * 1000);\n      return {\n        key: op.key,\n        count: op.value.count || 1,\n        window_start: op.value.windowStart || new Date(),\n        window_end: expiresAt,\n        synced_at: new Date()\n      };\n    });\n    \n    if (values.length === 0) return;\n    \n    // Build parameterized query\n    const placeholders = values.map((_, i) => \n      `($${i*5+1}, $${i*5+2}, $${i*5+3}, $${i*5+4}, $${i*5+5})`\n    ).join(', ');\n    \n    await db.query(\n      `INSERT INTO rate_limit_counters (key, count, window_start, window_end, synced_at) \n       VALUES ${placeholders}\n       ON CONFLICT (key) DO UPDATE SET \n         count = GREATEST(rate_limit_counters.count, EXCLUDED.count),\n         window_end = GREATEST(rate_limit_counters.window_end, EXCLUDED.window_end),\n         synced_at = EXCLUDED.synced_at`,\n      values.flatMap(v => [v.key, v.count, v.window_start, v.window_end, v.synced_at])\n    );\n  }\n  \n  async delete(key: string): Promise<void> {\n    // Delete from all layers immediately (no buffering for deletes)\n    await Promise.allSettled([\n      this.deleteFromPostgres(key),\n      this.circuitBreaker.execute({ method: 'del', args: [key] }),\n      Promise.resolve(this.localCache.delete(key))\n    ]);\n  }\n  \n  private async getFromPostgres(key: string): Promise<any | null> {\n    const keyParts = this.parseKey(key);\n    \n    switch (keyParts.type) {\n      case 'blacklist':\n        return this.getBlacklistedToken(keyParts.jti);\n      case 'refresh':\n        return this.getRefreshToken(keyParts.tokenHash);\n      case 'ratelimit':\n        return this.getRateLimitCounter(keyParts.identifier);\n      default:\n        return null;\n    }\n  }\n  \n  private parseKey(key: string): { type: string; [key: string]: string } {\n    // Parse keys like \"blacklist:jti:{jti}\", \"refresh:{hash}\", \"ratelimit:login:{ip}:{email}\"\n    const parts = key.split(':');\n    return {\n      type: parts[0],\n      ...this.extractKeyComponents(parts)\n    };\n  }\n}\n\n// NEW: Write buffering for batched Postgres writes\nclass WriteBuffer {\n  private buffer: WriteOperation[] = [];\n  private flushTimer: NodeJS.Timeout | null = null;\n  \n  constructor(private config: {\n    flushInterval: number;\n    maxBatchSize: number;\n    onFlush: (batch: WriteOperation[]) => Promise<void>;\n  }) {}\n  \n  add(operation: WriteOperation): void {\n    this.buffer.push(operation);\n    \n    // Flush immediately if batch is full\n    if (this.buffer.length >= this.config.maxBatchSize) {\n      this.flush();\n      return;\n    }\n    \n    // Schedule flush if not already scheduled\n    if (!this.flushTimer) {\n      this.flushTimer = setTimeout(() => this.flush(), this.config.flushInterval);\n    }\n  }\n  \n  private async flush(): Promise<void> {\n    if (this.flushTimer) {\n      clearTimeout(this.flushTimer);\n      this.flushTimer = null;\n    }\n    \n    if (this.buffer.length === 0) return;\n    \n    const batch = this.buffer.splice(0, this.buffer.length);\n    await this.config.onFlush(batch);\n  }\n}\n\ninterface WriteOperation {\n  key: string;\n  value: any;\n  ttl: number;\n  timestamp: number;\n}\n```\n\n### 3. Token Blacklist Failover Strategy (REFINED)\n\n**Problem**: Access token validation needs to check blacklist. If Redis is down, we risk:\n- Option A: Reject all requests (poor availability)\n- Option B: Accept all tokens (security risk)\n\n**Solution**: Bloom Filter + Postgres Fallback (ENHANCED)\n\n```typescript\ninterface TokenValidationStrategy {\n  // How critical is blacklist checking for this token?\n  riskLevel: 'low' | 'medium' | 'high';\n  \n  // What to do if Redis unavailable?\n  fallbackBehavior: 'accept' | 'reject' | 'accept_with_logging' | 'check_postgres';\n  \n  // Additional checks to perform when Redis down\n  compensatingControls: string[];\n}\n\n// NEW: Bloom filter for space-efficient blacklist checking\nclass BlacklistBloomFilter {\n  private filter: BloomFilter;\n  private lastSync: number = 0;\n  private syncInterval = 60000; // Sync every 60 seconds\n  \n  constructor() {\n    // Size for 1M entries with 0.01% false positive rate\n    this.filter = new BloomFilter(\n      1000000, // expected entries\n      0.0001   // false positive rate\n    );\n  }\n  \n  async maybeBlacklisted(jti: string): Promise<boolean> {\n    // Check if we need to resync\n    if (Date.now() - this.lastSync > this.syncInterval) {\n      await this.syncFromPostgres();\n    }\n    \n    return this.filter.has(jti);\n  }\n  \n  add(jti: string): void {\n    this.filter.add(jti);\n  }\n  \n  private async syncFromPostgres(): Promise<void> {\n    // Load all active blacklist entries\n    const result = await db.query(\n      'SELECT jti FROM token_blacklist WHERE expires_at > NOW()'\n    );\n    \n    // Rebuild filter\n    this.filter.clear();\n    for (const row of result.rows) {\n      this.filter.add(row.jti);\n    }\n    \n    this.lastSync = Date.now();\n    \n    logger.info('Synced blacklist bloom filter', { \n      entries: result.rowCount \n    });\n  }\n}\n\nconst blacklistBloomFilter = new BlacklistBloomFilter();\n\nasync function validateAccessTokenWithFailover(\n  token: string,\n  req: Request\n): Promise<TokenValidation> {\n  // 1. Verify JWT signature and basic claims\n  const payload = jwt.verify(token, getPublicKey(token), {\n    algorithms: ['RS256'],\n    issuer: 'auth.example.com',\n    audience: 'api.example.com',\n    clockTolerance: 10\n  });\n  \n  // 2. Determine risk level for this request\n  const riskLevel = calculateRiskLevel(payload, req);\n  \n  // 3. Try to check blacklist (with circuit breaker)\n  let isBlacklisted: boolean | null = null;\n  \n  try {\n    isBlacklisted = await cache.get(`blacklist:jti:${payload.jti}`) !== null;\n    \n    if (isBlacklisted) {\n      return { valid: false, reason: 'TOKEN_REVOKED' };\n    }\n  } catch (error) {\n    // Redis unavailable - use bloom filter + Postgres fallback\n    logger.warn('Blacklist check failed, using bloom filter + Postgres fallback', {\n      jti: payload.jti,\n      riskLevel,\n      error: error.message\n    });\n    \n    metrics.increment('auth.blacklist_check.failure', {\n      risk_level: riskLevel\n    });\n    \n    // NEW: Check bloom filter first (fast, local)\n    const maybeBlacklisted = await blacklistBloomFilter.maybeBlacklisted(payload.jti);\n    \n    if (!maybeBlacklisted) {\n      // Definitely not blacklisted (bloom filter guarantees no false negatives)\n      return await proceedWithValidation(payload, req, riskLevel, 'bloom_filter_negative');\n    }\n    \n    // Possibly blacklisted (bloom filter may have false positives)\n    // Check Postgres for authoritative answer based on risk level\n    return await handlePossibleBlacklist(payload, req, riskLevel);\n  }\n  \n  // 4. Additional validation\n  return await proceedWithValidation(payload, req, riskLevel, 'redis_check_passed');\n}\n\nasync function handlePossibleBlacklist(\n  payload: JWTPayload,\n  req: Request,\n  riskLevel: 'low' | 'medium' | 'high'\n): Promise<TokenValidation> {\n  \n  // For high/medium risk, check Postgres immediately\n  if (riskLevel === 'high' || riskLevel === 'medium') {\n    const isBlacklistedInDb = await checkBlacklistInPostgres(payload.jti);\n    \n    if (isBlacklistedInDb) {\n      return { valid: false, reason: 'TOKEN_REVOKED' };\n    }\n    \n    // False positive - proceed\n    metrics.increment('auth.bloom_filter.false_positive', { risk_level: riskLevel });\n    return await proceedWithValidation(payload, req, riskLevel, 'postgres_check_negative');\n  }\n  \n  // Low risk: Accept with logging (bloom filter false positive acceptable)\n  logger.info('Accepting low-risk request despite bloom filter positive', {\n    jti: payload.jti,\n    userId: payload.sub\n  });\n  \n  metrics.increment('auth.blacklist_bypass.low_risk_bloom_positive');\n  \n  // Check Postgres asynchronously for audit\n  checkBlacklistInPostgres(payload.jti).then(isBlacklisted => {\n    if (isBlacklisted) {\n      // Should have been blocked! Log security event\n      createSecurityEvent({\n        userId: payload.sub,\n        eventType: 'BLACKLISTED_TOKEN_ACCEPTED',\n        severity: 'HIGH',\n        details: {\n          jti: payload.jti,\n          reason: 'low_risk_bypass',\n          endpoint: req.path\n        }\n      });\n    }\n  });\n  \n  return await proceedWithValidation(payload, req, riskLevel, 'bloom_positive_low_risk_bypass');\n}\n\nasync function proceedWithValidation(\n  payload: JWTPayload,\n  req: Request,\n  riskLevel: 'low' | 'medium' | 'high',\n  checkMethod: string\n): Promise<TokenValidation> {\n  \n  // Perform compensating controls\n  try {\n    await performCompensatingControls(payload, req);\n  } catch (error) {\n    return { valid: false, reason: error.message };\n  }\n  \n  return { \n    valid: true, \n    fallbackMode: checkMethod !== 'redis_check_passed',\n    riskLevel,\n    checkMethod,\n    user: {\n      id: payload.sub,\n      email: payload.email,\n      roles: payload.roles,\n      permissions: payload.permissions\n    }\n  };\n}\n\nfunction calculateRiskLevel(\n  payload: JWTPayload,\n  req: Request\n): 'low' | 'medium' | 'high' {\n  const factors = {\n    // Token age (older = lower risk of revocation)\n    tokenAge: Date.now() - payload.iat * 1000,\n    \n    // Request type\n    isReadOnly: ['GET', 'HEAD', 'OPTIONS'].includes(req.method),\n    \n    // Endpoint sensitivity (NEW: Pattern matching)\n    isSensitiveEndpoint: /\\/(admin|settings|payment|delete|revoke)/.test(req.path),\n    \n    // User role\n    hasAdminRole: payload.roles?.includes('admin'),\n    \n    // Recent activity\n    sessionAge: Date.now() - (payload.session_started_at || payload.iat) * 1000,\n    \n    // NEW: Token version (old version = higher risk)\n    isOldTokenVersion: payload.token_version < payload.user_token_version\n  };\n  \n  // High risk: Admin actions, sensitive endpoints, write operations, old token version\n  if (factors.hasAdminRole || \n      factors.isSensitiveEndpoint ||\n      !factors.isReadOnly || \n      factors.isOldTokenVersion ||\n      factors.sessionAge < 60000) { // < 1 minute old\n    return 'high';\n  }\n  \n  // Low risk: Read-only, non-sensitive, old token, mature session\n  if (factors.isReadOnly && \n      !factors.isSensitiveEndpoint &&\n      factors.tokenAge > 300000 && // > 5 minutes old\n      factors.sessionAge > 3600000) { // > 1 hour old\n    return 'low';\n  }\n  \n  return 'medium';\n}\n\nasync function performCompensatingControls(\n  payload: JWTPayload,\n  req: Request\n): Promise<void> {\n  // 1. Check if user still exists and is active\n  const user = await getUserById(payload.sub);\n  if (!user || user.locked_until > new Date()) {\n    throw new Error('USER_INACTIVE');\n  }\n  \n  // 2. Verify token version matches (password not changed)\n  if (payload.token_version !== user.token_version) {\n    throw new Error('TOKEN_VERSION_MISMATCH');\n  }\n  \n  // 3. NEW: Check session still exists and active (catches bulk revocations)\n  const session = await getRefreshTokenBySession(payload.session_id);\n  if (!session || session.revoked) {\n    throw new Error('SESSION_REVOKED');\n  }\n  \n  // 4. Log for audit trail\n  await createSecurityEvent({\n    userId: payload.sub,\n    eventType: 'TOKEN_VALIDATED_FALLBACK_MODE',\n    severity: 'MEDIUM',\n    details: {\n      jti: payload.jti,\n      endpoint: req.path,\n      method: req.method,\n      reason: 'redis_unavailable'\n    }\n  });\n}\n\nasync function checkBlacklistInPostgres(jti: string): Promise<boolean> {\n  // REFINED: Use prepared statement for better performance\n  const result = await db.query({\n    name: 'check_blacklist',\n    text: 'SELECT 1 FROM token_blacklist WHERE jti = $1 AND expires_at > NOW() LIMIT 1',\n    values: [jti]\n  });\n  \n  return result.rowCount > 0;\n}\n```\n\n**Token Blacklist Table (Postgres) - REFINED**\n```sql\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL, -- Same as original token expiry\n  reason VARCHAR(100),\n  session_id UUID,\n  ip_address INET,\n  user_agent TEXT\n);\n\n-- REFINED: Covering index for hot path query\nCREATE INDEX idx_token_blacklist_lookup ON token_blacklist(jti, expires_at) \n  WHERE expires_at > NOW();\n\n-- REFINED: Partitioning for large-scale deployments\nCREATE INDEX idx_token_blacklist_expires ON token_blacklist(expires_at);\nCREATE INDEX idx_token_blacklist_user ON token_blacklist(user_id, revoked_at DESC);\n\n-- NEW: Partition by expiry for efficient cleanup\n-- (For PostgreSQL 10+)\nCREATE TABLE token_blacklist_partitioned (\n  LIKE token_blacklist INCLUDING ALL\n) PARTITION BY RANGE (expires_at);\n\n-- Create partitions for next 7 days\nCREATE TABLE token_blacklist_p0 PARTITION OF token_blacklist_partitioned\n  FOR VALUES FROM (NOW()) TO (NOW() + INTERVAL '1 day');\n-- ... repeat for each day\n\n-- Automatic partition management via pg_partman or cron\n-- Drop old partitions instead of DELETE (much faster)\n\n-- Automatic cleanup of expired entries\nCREATE OR REPLACE FUNCTION cleanup_expired_blacklist()\nRETURNS void AS $$\nBEGIN\n  -- Use DELETE with small batch size to avoid lock contention\n  DELETE FROM token_blacklist \n  WHERE jti IN (\n    SELECT jti FROM token_blacklist \n    WHERE expires_at < NOW() - INTERVAL '1 day'\n    LIMIT 1000\n  );\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Run cleanup every hour (more frequent than daily)\nSELECT cron.schedule('cleanup-blacklist', '0 * * * *', 'SELECT cleanup_expired_blacklist()');\n```\n\n### 4. Refresh Token Locking Fallback (REFINED)\n\n**Problem**: Distributed locks prevent concurrent refresh. If Redis is down, we risk:\n- Race conditions causing token reuse false positives\n- Complete inability to refresh tokens\n\n**Solution**: Postgres Row-Level Locking (BETTER than advisory locks)\n\n```typescript\nasync function performTokenRefreshWithFailover(\n  refreshToken: string,\n  fingerprint: string,\n  requestId: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // Try Redis distributed lock first\n  let lockAcquired = false;\n  let useFallbackLock = false;\n  \n  try {\n    lockAcquired = await acquireRedisLock(tokenHash, requestId, 5000);\n  } catch (error) {\n    logger.warn('Redis lock unavailable, using Postgres row-level lock', {\n      tokenHash: tokenHash.substring(0, 16),\n      error: error.message\n    });\n    \n    metrics.increment('auth.refresh_lock.fallback_to_postgres');\n    useFallbackLock = true;\n  }\n  \n  if (useFallbackLock) {\n    // NEW: Use Postgres row-level lock (SELECT FOR UPDATE)\n    // This is BETTER than advisory locks because:\n    // 1. Automatically released on transaction commit/rollback\n    // 2. No need to track lock IDs\n    // 3. Works naturally with refresh token row\n    return await performRefreshWithPostgresLock(refreshToken, fingerprint);\n  }\n  \n  if (!lockAcquired) {\n    // Another process is refreshing - wait and check for result\n    return await waitForConcurrentRefresh(tokenHash, requestId, false);\n  }\n  \n  try {\n    // Perform refresh operation\n    const result = await executeTokenRefresh(refreshToken, fingerprint);\n    \n    // Store result for concurrent requests (best effort)\n    try {\n      await cache.set(\n        `refresh_result:${tokenHash}`,\n        result,\n        5 // 5 seconds\n      );\n    } catch {\n      // Not critical if this fails\n    }\n    \n    return result;\n    \n  } finally {\n    // Release lock\n    await releaseRedisLock(tokenHash, requestId);\n  }\n}\n\n// NEW: Use row-level locking instead of advisory locks\nasync function performRefreshWithPostgresLock(\n  refreshToken: string,\n  fingerprint: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // Start transaction\n  const client = await db.getClient();\n  \n  try {\n    await client.query('BEGIN');\n    \n    // 1. Lock the refresh token row (blocks concurrent refreshes)\n    const lockResult = await client.query(\n      'SELECT * FROM refresh_tokens ' +\n      'WHERE token_hash = $1 ' +\n      'FOR UPDATE NOWAIT', // Fail immediately if already locked\n      [tokenHash]\n    );\n    \n    if (lockResult.rowCount === 0) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n    \n    const tokenData = lockResult.rows[0];\n    \n    // 2. Validate token\n    if (tokenData.expires_at < new Date()) {\n      throw new AuthError('REFRESH_TOKEN_EXPIRED');\n    }\n    \n    if (tokenData.revoked) {\n      throw new AuthError('REFRESH_TOKEN_REVOKED');\n    }\n    \n    // 3. Check if already used (CRITICAL for security)\n    if (tokenData.used) {\n      // Token reuse detected - trigger compromise response\n      await handleTokenFamilyCompromise(\n        tokenData.token_family_id,\n        'TOKEN_REUSE',\n        client // Use same transaction\n      );\n      throw new AuthError('TOKEN_REUSE_DETECTED');\n    }\n    \n    // 4. Validate fingerprint\n    if (tokenData.device_fingerprint_hash !== hashFingerprint(fingerprint)) {\n      throw new AuthError('FINGERPRINT_MISMATCH');\n    }\n    \n    // 5. Mark token as used (within same transaction)\n    await client.query(\n      'UPDATE refresh_tokens SET used = true, used_at = NOW() ' +\n      'WHERE token_hash = $1',\n      [tokenHash]\n    );\n    \n    // 6. Generate new token pair\n    const user = await getUserById(tokenData.user_id);\n    \n    const newAccessToken = generateAccessToken(user, {\n      sessionId: tokenData.session_id,\n      deviceId: tokenData.device_id,\n      tokenVersion: user.token_version\n    });\n    \n    const newRefreshTokenData = await generateAndStoreRefreshToken(\n      user, \n      {\n        sessionId: tokenData.session_id,\n        deviceId: tokenData.device_id,\n        deviceFingerprint: fingerprint,\n        parentTokenId: tokenData.id,\n        tokenFamilyId: tokenData.token_family_id,\n        timesRefreshed: tokenData.times_refreshed + 1\n      },\n      client // Use same transaction\n    );\n    \n    // Commit transaction (releases lock)\n    await client.query('COMMIT');\n    \n    // Update cache (best effort)\n    try {\n      await cache.set(\n        `refresh:${tokenHash}:used`,\n        true,\n        900 // 15 minutes\n      );\n    } catch {\n      // Not critical - Postgres is source of truth\n    }\n    \n    return {\n      accessToken: newAccessToken.token,\n      refreshToken: newRefreshTokenData.token,\n      expiresIn: 900,\n      tokenType: 'Bearer',\n      refreshExpiresIn: 2592000\n    };\n    \n  } catch (error) {\n    await client.query('ROLLBACK');\n    \n    if (error.code === '55P03') {\n      // Lock not available (NOWAIT failed)\n      // Another process is refreshing this token\n      throw new AuthError('CONCURRENT_REFRESH_IN_PROGRESS');\n    }\n    \n    throw error;\n    \n  } finally {\n    client.release();\n  }\n}\n\nasync function executeTokenRefresh(\n  refreshToken: string,\n  fingerprint: string\n): Promise<AuthResponse> {\n  const tokenHash = hashToken(refreshToken);\n  \n  // 1. Retrieve token from cache (multi-layer)\n  let tokenData = await cache.get(`refresh:${tokenHash}`);\n  \n  if (!tokenData) {\n    // Load from Postgres if cache miss\n    tokenData = await getRefreshTokenFromPostgres(tokenHash);\n    \n    if (!tokenData) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n  }\n  \n  // 2. Validate token\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Check if already used (CRITICAL for security)\n  // NEW: Check cache first for fast rejection\n  const alreadyUsed = await cache.get(`refresh:${tokenHash}:used`);\n  if (alreadyUsed || tokenData.used) {\n    // Token reuse detected - trigger compromise response\n    await handleTokenFamilyCompromise(\n      tokenData.token_family_id,\n      'TOKEN_REUSE'\n    );\n    throw new AuthError('TOKEN_REUSE_DETECTED');\n  }\n  \n  // 4. Validate fingerprint\n  if (tokenData.device_fingerprint_hash !== hashFingerprint(fingerprint)) {\n    throw new AuthError('FINGERPRINT_MISMATCH');\n  }\n  \n  // 5. Mark token as used (atomic operation in Postgres)\n  const updateResult = await db.query(\n    'UPDATE refresh_tokens SET used = true, used_at = NOW() ' +\n    'WHERE token_hash = $1 AND used = false ' +\n    'RETURNING id', // Ensure exactly one row updated\n    [tokenHash]\n  );\n  \n  if (updateResult.rowCount === 0) {\n    // Race condition: Token was used by another request\n    await handleTokenFamilyCompromise(\n      tokenData.token_family_id,\n      'TOKEN_REUSE'\n    );\n    throw new AuthError('TOKEN_REUSE_DETECTED');\n  }\n  \n  // Mark in cache immediately (before generating new tokens)\n  try {\n    await cache.set(\n      `refresh:${tokenHash}:used`,\n      true,\n      tokenData.ttl_remaining || 900\n    );\n  } catch {\n    // Not critical - Postgres is source of truth\n  }\n  \n  // 6. Generate new token pair\n  const user = await getUserById(tokenData.user_id);\n  \n  const newAccessToken = generateAccessToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    tokenVersion: user.token_version\n  });\n  \n  const newRefreshToken = await generateRefreshToken(user, {\n    sessionId: tokenData.session_id,\n    deviceId: tokenData.device_id,\n    deviceFingerprint: fingerprint,\n    parentTokenId: tokenData.id,\n    tokenFamilyId: tokenData.token_family_id,\n    timesRefreshed: tokenData.times_refreshed + 1\n  });\n  \n  return {\n    accessToken: newAccessToken.token,\n    refreshToken: newRefreshToken.token,\n    expiresIn: 900,\n    tokenType: 'Bearer',\n    refreshExpiresIn: 2592000\n  };\n}\n```\n\n### 5. Rate Limiting Fallback (REFINED)\n\n**Problem**: Rate limiting requires counters in Redis. If Redis is down:\n- Option A: Disable rate limiting (security risk)\n- Option B: Reject all requests (poor availability)\n\n**Solution**: In-Memory Rate Limiting with Periodic Postgres Sync (IMPROVED)\n\n```typescript\nclass RateLimiterWithFailover {\n  private redis: RedisClient;\n  private localRateLimiter: Map<string, RateLimitCounter>;\n  private circuitBreaker: CircuitBreaker;\n  private syncInterval: NodeJS.Timeout;\n  private pendingSyncs: Set<string> = new Set(); // NEW: Track dirty keys\n  \n  constructor() {\n    this.localRateLimiter = new Map();\n    \n    // NEW: Sync only dirty counters every 10 seconds (reduce write load)\n    this.syncInterval = setInterval(\n      () => this.syncDirtyCountersToPostgres(),\n      10000\n    );\n    \n    // Load initial state from Postgres on startup\n    this.loadCountersFromPostgres();\n  }\n  \n  async checkRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    // Try Redis first\n    try {\n      const result = await this.checkRedisRateLimit(key, max, windowSec);\n      return result;\n    } catch (error) {\n      logger.warn('Redis rate limit check failed, using local fallback', {\n        key: this.sanitizeKey(key),\n        error: error.message\n      });\n      \n      metrics.increment('ratelimit.fallback_to_local');\n    }\n    \n    // Use in-memory fallback\n    return this.checkLocalRateLimit(key, max, windowSec);\n  }\n  \n  private async checkRedisRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): Promise<{ allowed: boolean; retryAfter?: number }> {\n    \n    // Use Redis pipeline for atomic increment + TTL check\n    const pipeline = this.redis.pipeline();\n    pipeline.incr(key);\n    pipeline.ttl(key);\n    \n    const results = await this.circuitBreaker.execute({\n      method: 'exec',\n      args: [pipeline]\n    });\n    \n    const count = results[0][1]; // First command result\n    const ttl = results[1][1];   // Second command result\n    \n    if (ttl === -1) {\n      // Key exists but has no TTL - set it\n      await this.redis.expire(key, windowSec);\n    }\n    \n    if (count > max) {\n      const retryAfter = ttl > 0 ? ttl : windowSec;\n      return { allowed: false, retryAfter };\n    }\n    \n    return { allowed: true };\n  }\n  \n  private checkLocalRateLimit(\n    key: string,\n    max: number,\n    windowSec: number\n  ): { allowed: boolean; retryAfter?: number } {\n    \n    const now = Date.now();\n    const windowMs = windowSec * 1000;\n    \n    // Get or create counter\n    let counter = this.localRateLimiter.get(key);\n    \n    if (!counter || now > counter.windowEnd) {\n      // Start new window\n      counter = {\n        count: 1,\n        windowStart: now,\n        windowEnd: now + windowMs,\n        key: key,\n        dirty: true // NEW: Mark as needing sync\n      };\n      this.localRateLimiter.set(key, counter);\n      this.pendingSyncs.add(key); // NEW: Track for next sync\n      return { allowed: true };\n    }\n    \n    // Increment counter\n    counter.count++;\n    counter.dirty = true;\n    this.pendingSyncs.add(key);\n    \n    if (counter.count > max) {\n      const retryAfter = Math.ceil((counter.windowEnd - now) / 1000);\n      return { allowed: false, retryAfter };\n    }\n    \n    return { allowed: true };\n  }\n  \n  // NEW: Only sync dirty counters (reduces write load)\n  private async syncDirtyCountersToPostgres(): Promise<void> {\n    if (this.pendingSyncs.size === 0) return;\n    \n    const keysToSync = Array.from(this.pendingSyncs);\n    const counters = keysToSync\n      .map(key => this.localRateLimiter.get(key))\n      .filter((c): c is RateLimitCounter => c !== undefined && c.dirty);\n    \n    if (counters.length === 0) return;\n    \n    try {\n      // NEW: Use single upsert with multiple values (more efficient)\n      const values = counters.map(counter => ({\n        key: counter.key,\n        count: counter.count,\n        window_start: new Date(counter.windowStart),\n        window_end: new Date(counter.windowEnd),\n        synced_at: new Date()\n      }));\n      \n      // Build parameterized query\n      const placeholders = values.map((_, i) => \n        `($${i*5+1}, $${i*5+2}, $${i*5+3}, $${i*5+4}, $${i*5+5})`\n      ).join(', ');\n      \n      await db.query(\n        `INSERT INTO rate_limit_counters (key, count, window_start, window_end, synced_at) \n         VALUES ${placeholders}\n         ON CONFLICT (key) DO UPDATE SET \n           count = GREATEST(rate_limit_counters.count, EXCLUDED.count),\n           window_end = GREATEST(rate_limit_counters.window_end, EXCLUDED.window_end),\n           synced_at = EXCLUDED.synced_at`,\n        values.flatMap(v => [v.key, v.count, v.window_start, v.window_end, v.synced_at])\n      );\n      \n      logger.debug('Synced rate limit counters to Postgres', {\n        count: counters.length\n      });\n      \n      // Mark as clean\n      counters.forEach(c => c.dirty = false);\n      this.pendingSyncs.clear();\n      \n    } catch (error) {\n      logger.error('Failed to sync rate limit counters', { error });\n      // Keep pending syncs for retry\n    }\n    \n    // Clean up expired counters\n    this.cleanupExpiredCounters();\n  }\n  \n  private cleanupExpiredCounters(): void {\n    const now = Date.now();\n    for (const [key, counter] of this.localRateLimiter.entries()) {\n      if (now > counter.windowEnd + 60000) { // 1 minute grace\n        this.localRateLimiter.delete(key);\n        this.pendingSyncs.delete(key);\n      }\n    }\n  }\n  \n  async loadCountersFromPostgres(): Promise<void> {\n    // On startup, load recent counters from Postgres\n    // This provides awareness of rate limits from other instances\n    \n    const result = await db.query(\n      'SELECT key, count, window_start, window_end FROM rate_limit_counters ' +\n      'WHERE window_end > NOW() - INTERVAL \\'5 minutes\\'' // Only recent windows\n    );\n    \n    for (const row of result.rows) {\n      this.localRateLimiter.set(row.key, {\n        count: row.count,\n        windowStart: row.window_start.getTime(),\n        windowEnd: row.window_end.getTime(),\n        key: row.key,\n        dirty: false\n      });\n    }\n    \n    logger.info('Loaded rate limit counters from Postgres', {\n      count: result.rowCount\n    });\n  }\n  \n  private sanitizeKey(key: string): string {\n    // Remove sensitive data (emails, IPs) from logs\n    return key.replace(/:[^:]+@/, ':***@').replace(/:\\d+\\.\\d+\\.\\d+\\.\\d+/, ':***');\n  }\n}\n\ninterface RateLimitCounter {\n  count: number;\n  windowStart: number;\n  windowEnd: number;\n  key: string;\n  dirty: boolean; // NEW: Track if needs sync\n}\n```\n\n**Rate Limit Counters Table (Postgres) - REFINED**\n```sql\nCREATE TABLE rate_limit_counters (\n  key VARCHAR(500) PRIMARY KEY,\n  count INTEGER NOT NULL,\n  window_start TIMESTAMP NOT NULL,\n  window_end TIMESTAMP NOT NULL,\n  synced_at TIMESTAMP NOT NULL DEFAULT NOW()\n);\n\n-- REFINED: Partial index for active windows only\nCREATE INDEX idx_rate_limit_active ON rate_limit_counters(window_end) \n  WHERE window_end > NOW();\n\n-- NEW: Index for startup loading\nCREATE INDEX idx_rate_limit_recent ON rate_limit_counters(synced_at DESC)\n  WHERE window_end > NOW() - INTERVAL '5 minutes';\n\n-- Cleanup expired counters more frequently\nCREATE OR REPLACE FUNCTION cleanup_rate_limit_counters()\nRETURNS void AS $$\nBEGIN\n  -- Use batch delete to avoid lock contention\n  DELETE FROM rate_limit_counters \n  WHERE key IN (\n    SELECT key FROM rate_limit_counters\n    WHERE window_end < NOW() - INTERVAL '1 hour'\n    LIMIT 1000\n  );\nEND;\n$$ LANGUAGE plpgsql;\n\n-- Run cleanup every 15 minutes\nSELECT cron.schedule('cleanup-rate-limits', '*/15 * * * *', \n  'SELECT cleanup_rate_limit_counters()');\n```\n\n### 6. Circuit Breaker Implementation (REFINED)\n\n```typescript\nclass CircuitBreaker {\n  private state: CircuitState = CircuitState.CLOSED;\n  private failureCount: number = 0;\n  private successCount: number = 0;\n  private requestCount: number = 0; // NEW: Track total requests\n  private lastFailureTime: number = 0;\n  private nextAttemptTime: number = 0;\n  private consecutiveSuccesses: number = 0; // NEW: For half-open state\n  \n  constructor(\n    private operation: (args: any) => Promise<any>,\n    private config: CircuitBreakerConfig\n  ) {}\n  \n  async execute(args: any): Promise<any> {\n    // Check if circuit should transition to half-open\n    if (this.state === CircuitState.OPEN && \n        Date.now() >= this.nextAttemptTime) {\n      this.state = CircuitState.HALF_OPEN;\n      this.consecutiveSuccesses = 0;\n      logger.info('Circuit breaker entering HALF_OPEN state');\n      metrics.gauge('circuit_breaker.state', 0.5);\n    }\n    \n    // Reject immediately if circuit is open\n    if (this.state === CircuitState.OPEN) {\n      const error = new Error('Circuit breaker is OPEN');\n      error.name = 'CircuitBreakerOpen';\n      metrics.increment('circuit_breaker.rejected');\n      throw error;\n    }\n    \n    // In half-open, only allow limited requests\n    if (this.state === CircuitState.HALF_OPEN && \n        this.consecutiveSuccesses >= this.config.successThreshold) {\n      // Enough successes - close circuit\n      this.closeCircuit();\n    }\n    \n    try {\n      // Execute with timeout\n      const result = await Promise.race([\n        this.operation(args),\n        this.timeoutPromise(this.config.timeout)\n      ]);\n      \n      // Success\n      this.onSuccess();\n      return result;\n      \n    } catch (error) {\n      // Failure\n      this.onFailure();\n      throw error;\n    }\n  }\n  \n  private onSuccess(): void {\n    this.requestCount++;\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      this.consecutiveSuccesses++;\n      \n      // After enough successful requests, close circuit\n      if (this.consecutiveSuccesses >= this.config.successThreshold) {\n        this.closeCircuit();\n      }\n    }\n    \n    // In closed state, decay failure count on success\n    if (this.state === CircuitState.CLOSED && this.failureCount > 0) {\n      this.failureCount = Math.max(0, this.failureCount - 1);\n    }\n  }\n  \n  private onFailure(): void {\n    this.failureCount++;\n    this.requestCount++;\n    this.lastFailureTime = Date.now();\n    \n    if (this.state === CircuitState.HALF_OPEN) {\n      // Failure in half-open state - immediately re-open circuit\n      this.openCircuit();\n      return;\n    }\n    \n    // Check if we should open the circuit\n    // NEW: Only evaluate after minimum request volume\n    if (this.requestCount >= this.config.requestVolumeThreshold) {\n      const failureRate = this.failureCount / this.requestCount;\n      \n      if (failureRate >= this.config.failureThreshold) {\n        this.openCircuit();\n      }\n    }\n  }\n  \n  private openCircuit(): void {\n    this.state = CircuitState.OPEN;\n    this.nextAttemptTime = Date.now() + this.config.resetTimeout;\n    this.consecutiveSuccesses = 0;\n    \n    logger.error('Circuit breaker opened', {\n      failureCount: this.failureCount,\n      requestCount: this.requestCount,\n      failureRate: (this.failureCount / this.requestCount * 100).toFixed(2) + '%',\n      lastFailureTime: new Date(this.lastFailureTime),\n      nextAttemptTime: new Date(this.nextAttemptTime)\n    });\n    \n    metrics.gauge('circuit_breaker.state', 1);\n    metrics.increment('circuit_breaker.opened');\n  }\n  \n  private closeCircuit(): void {\n    this.state = CircuitState.CLOSED;\n    this.failureCount = 0;\n    this.successCount = 0;\n    this.requestCount = 0;\n    this.consecutiveSuccesses = 0;\n    \n    logger.info('Circuit breaker closed after successful recovery');\n    metrics.gauge('circuit_breaker.state', 0);\n    metrics.increment('circuit_breaker.closed');\n  }\n  \n  private timeoutPromise(ms: number): Promise<never> {\n    return new Promise((_, reject) => {\n      setTimeout(() => {\n        const error = new Error(`Operation timed out after ${ms}ms`);\n        error.name = 'TimeoutError';\n        reject(error);\n      }, ms);\n    });\n  }\n  \n  getState(): CircuitState {\n    return this.state;\n  }\n  \n  getMetrics(): CircuitBreakerMetrics {\n    return {\n      state: this.state,\n      failureCount: this.failureCount,\n      requestCount: this.requestCount,\n      failureRate: this.requestCount > 0 ? this.failureCount / this.requestCount : 0,\n      consecutiveSuccesses: this.consecutiveSuccesses\n    };\n  }\n}\n\ninterface CircuitBreakerMetrics {\n  state: CircuitState;\n  failureCount: number;\n  requestCount: number;\n  failureRate: number;\n  consecutiveSuccesses: number;\n}\n```\n\n## Geographic Location Policy\n\n[KEEPING ENTIRE SECTION - Already well-designed, no changes needed]\n\n### 1. Location Change Risk Matrix\n[Same as original - already comprehensive]\n\n### 2. VPN and Proxy Detection\n[Same as original - already comprehensive]\n\n### 3. User and Device Trust Levels\n[Same as original - already comprehensive]\n\n### 4. Location Change Evaluation\n[Same as original - already comprehensive]\n\n### 5. Mobile User Considerations\n[Same as original - already comprehensive]\n\n## Performance Characteristics & Scaling Analysis\n\n### Postgres Fallback Performance Analysis\n\n#### 1. Token Blacklist Lookup Performance\n\n**Normal Operation (Redis)**:\n- Latency: ~1-2ms\n- Throughput: 100,000+ req/s per Redis instance\n\n**Fallback Operation (Postgres)**:\n- With proposed index (`idx_token_blacklist_lookup`):\n  - Latency: ~5-15ms (depending on connection pool, disk I/O)\n  - Throughput: ~2,000-5,000 req/s per Postgres instance\n  \n**Optimization: Add Bloom Filter Layer**\n- Bloom filter check: ~0.1ms (in-memory)\n- False positive rate: 0.01% (1 in 10,000)\n- Only 0.01% of requests hit Postgres\n- Effective Postgres load: 1 req/s (from 10,000 req/s traffic)\n\n**Verdict**: \u2705 **Bloom filter makes Postgres fallback viable** even at high scale.\n\n#### 2. Refresh Token Locking Performance\n\n**Redis Distributed Lock**:\n- Lock acquisition: ~2-5ms\n- Lock release: ~1-2ms\n- Contention handling: Excellent (sub-ms spinning)\n\n**Postgres Row-Level Lock (SELECT FOR UPDATE NOWAIT)**:\n- Lock acquisition: ~5-10ms (includes query time)\n- Lock release: Automatic on COMMIT (~2-5ms)\n- Contention handling: **IMMEDIATE FAILURE** with NOWAIT (good for our use case)\n\n**Contention Analysis**:\n- Scenario: 1M active users, 1 refresh per hour = 278 refreshes/second\n- Probability of collision: 278 / (60 * 60) / 1M = **negligible**\n- Even at 10x traffic (2,780 req/s), collision rate < 0.001%\n\n**Lock Sharding**: NOT NEEDED because:\n- Each refresh token has unique row (natural sharding)\n- NOWAIT ensures no blocking (fast failure)\n- Collisions only occur for SAME token (already error case)\n\n**Verdict**: \u2705 **Row-level locking is BETTER than advisory locks** and performs well.\n\n#### 3. Rate Limiting Sync Performance\n\n**Write Load**:\n- Scenario: 1M requests/minute = 16,667 req/s\n- Sync interval: 10 seconds\n- Unique keys per interval: ~1,000-5,000 (users x endpoints)\n\n**Batch Upsert Performance**:\n- Single upsert: ~2-5ms\n- Batched upsert (100 rows): ~10-20ms\n- **Amortized cost**: 0.2ms per row\n\n**Proposed Approach**:\n```sql\nINSERT INTO rate_limit_counters (...) VALUES (...)\nON CONFLICT (key) DO UPDATE SET \n  count = GREATEST(rate_limit_counters.count, EXCLUDED.count),\n  ...\n```\n\n**Why GREATEST()**?\n- Handles race between multiple app instances\n- Always keeps highest count (conservative for rate limiting)\n- Avoids lost updates\n\n**Write Amplification**:\n- Peak: 500 upserts every 10 seconds = 50 writes/sec\n- **Negligible** compared to typical Postgres capacity (10,000+ writes/sec)\n\n**Verdict**: \u2705 **Batched upserts with ON CONFLICT are efficient and correct**.\n\n#### 4. Circuit Breaker Threshold Tuning\n\n**Original Proposal**: 50% failure over 100 requests\n\n**Analysis**:\n- **Too lenient**: Allows 50 failed requests before tripping\n- At 1,000 req/s, takes 100ms to trip (50-500 requests may fail)\n\n**Refined Proposal**: 30% failure over 50 requests\n\n**Why Better**:\n- Faster detection: 15 failures trigger circuit (vs 50)\n- At 1,000 req/s, trips in 50ms (faster failover)\n- Lower failure volume before fallback activates\n\n**Recovery Settings**:\n- Reset timeout: 5 seconds (vs 10) - faster recovery attempts\n- Success threshold: 5 consecutive (ensure stability before closing)\n\n**Verdict**: \u2705 **Refined thresholds provide faster failover with stable recovery**.\n\n## Security Vulnerability Analysis\n\n### Potential Attack Surfaces Introduced by Fallbacks\n\n#### 1. \u274c **VULNERABILITY: Bloom Filter False Negatives**\n\n**Issue**: Bloom filters CAN have false negatives if implementation is wrong.\n\n**Mitigation**: \n- Use well-tested library (e.g., `bloom-filters` npm package)\n- Add integration tests to verify no false negatives\n- Monitor discrepancies between bloom filter and Postgres\n\n**Verdict**: Low risk with proper implementation.\n\n---\n\n#### 2. \u26a0\ufe0f **RISK: Time-of-Check-Time-of-Use (TOCTOU) in Token Validation**\n\n**Scenario**:\n1. Token validated against stale local cache (not blacklisted)\n2. Between validation and request processing, token is blacklisted\n3. Request proceeds with revoked token\n\n**Impact**: \n- Window: Up to 5 minutes (local cache TTL)\n- Scope: Only affects requests during Redis outage\n- Risk level: **MEDIUM** (compensating controls mitigate)\n\n**Mitigations**:\n- Compensating controls (user status, token version, session check)\n- Short access token lifetime (15 min)\n- Security event logging for audit\n- Risk-based policy (high-risk operations check Postgres)\n\n**Verdict**: \u2705 **Acceptable risk with compensating controls**.\n\n---\n\n#### 3. \u26a0\ufe0f **RISK: Rate Limit Bypass Across Instances**\n\n**Scenario**:\n1. Redis down, instances use local rate limiters\n2. Attacker distributes requests across multiple instances\n3. Each instance counts independently\n\n**Impact**:\n- Effective rate limit multiplied by instance count\n- Example: 100 req/min limit \u00d7 10 instances = 1,000 req/min possible\n\n**Mitigations**:\n- Sync to Postgres every 10 seconds (limits window)\n- Load balancer sticky sessions (same user \u2192 same instance)\n- Use GREATEST() in upsert (never decrease count)\n- Exponential backoff on repeated violations\n\n**Additional Mitigation** (NEW):\n```typescript\n// Load cross-instance counts on critical operations\nasync function checkGlobalRateLimit(key: string): Promise<number> {\n  const localCount = this.localRateLimiter.get(key)?.count || 0;\n  const pgCount = await this.getPostgresCount(key);\n  \n  // Use higher of local vs Postgres (conservative)\n  return Math.max(localCount, pgCount);\n}\n```\n\n**Verdict**: \u26a0\ufe0f **Medium risk - requires load balancer configuration + Postgres sync**.\n\n---\n\n#### 4. \u2705 **NON-ISSUE: Token Reuse During Postgres Lock**\n\n**Question**: Can token reuse attack succeed during Postgres locking?\n\n**Answer**: **NO** - Transaction isolation prevents it:\n1. `SELECT FOR UPDATE NOWAIT` locks row\n2. Second request fails immediately (NOWAIT)\n3. No race condition possible\n\n**Verdict**: \u2705 **Secure by design**.\n\n---\n\n#### 5. \u274c **VULNERABILITY: Write Buffer Data Loss**\n\n**Scenario**:\n1. Application writes to buffer\n2. Crash before flush to Postgres\n3. Blacklist entries lost \u2192 revoked tokens accepted\n\n**Impact**: **CRITICAL** for security-critical writes (blacklist, revocations)\n\n**Mitigation**: \n```typescript\nasync function blacklistToken(jti: string, reason: string): Promise<void> {\n  // CRITICAL: Write to Postgres IMMEDIATELY (no buffering)\n  await db.query(\n    'INSERT INTO token_blacklist (jti, user_id, expires_at, reason) VALUES ($1, $2, $3, $4)',\n    [jti, userId, expiresAt, reason]\n  );\n  \n  // Update caches AFTER Postgres write succeeds\n  try {\n    await cache.delete(`token:${jti}`);\n    blacklistBloomFilter.add(jti);\n  } catch {\n    // Non-critical if cache update fails\n  }\n}\n```\n\n**Rule**: \u2705 **NEVER buffer security-critical writes. Only buffer performance-optimized data (rate limits, last_used_at timestamps).**\n\n---\n\n## Monitoring & Observability\n\n### Key Metrics to Track\n\n```typescript\n// Circuit breaker metrics\nmetrics.gauge('redis.circuit_breaker.state', state); // 0=closed, 0.5=half-open, 1=open\nmetrics.increment('redis.circuit_breaker.opened');\nmetrics.increment('redis.circuit_breaker.closed');\n\n// Fallback usage\nmetrics.increment('cache.fallback.local', { reason: 'redis_unavailable' });\nmetrics.increment('cache.fallback.postgres', { reason: 'cache_miss' });\nmetrics.increment('auth.blacklist_check.fallback', { method: 'bloom_filter' });\n\n// Performance\nmetrics.histogram('cache.get.latency', latency, { layer: 'redis|local|postgres' });\nmetrics.histogram('auth.token_validation.latency', latency, { fallback: 'true|false' });\n\n// Security events\nmetrics.increment('security.token_reuse.detected');\nmetrics.increment('security.blacklisted_token_accepted', { risk: 'low|medium|high' });\n\n// Rate limit effectiveness\nmetrics.increment('ratelimit.violation', { fallback_mode: 'true|false' });\nmetrics.increment('ratelimit.cross_instance_correction');\n```\n\n### Alerts\n\n```yaml\n# Critical: Redis down for >1 minute\n- alert: RedisUnavailable\n  expr: redis_circuit_breaker_state == 1 for 1m\n  severity: critical\n  \n# Warning: High Postgres fallback usage\n- alert: HighPostgresFallback\n  expr: rate(cache_fallback_postgres_total[5m]) > 100\n  severity: warning\n  \n# Security: Blacklisted tokens accepted\n- alert: BlacklistedTokenAccepted\n  expr: increase(security_blacklisted_token_accepted_total[5m]) > 0\n  severity: high\n```\n\n## What I Improved\n\n### Performance & Scaling\n1. **Bloom filter for blacklist**: Reduces Postgres load by 99.99% during Redis failure\n2. **Write buffering with batching**: Reduces Postgres write load by 10-100x\n3. **Row-level locking**: More reliable and performant than advisory locks\n4. **COPY for bulk inserts**: 10x faster than individual INSERTs\n5. **Dynamic cache sizing**: Adapts to available memory instead of fixed limit\n6. **Dirty tracking for rate limits**: Only sync changed counters (reduces write amplification)\n7. **Faster circuit breaker**: 30% threshold over 50 requests (vs 50% over 100)\n\n### Security\n8. **No buffering for security writes**: Blacklist writes go directly to Postgres (prevents data loss)\n9. **Bloom filter false positive handling**: Risk-based Postgres check for positives\n10. **Cross-instance rate limit loading**: Check Postgres on critical operations\n11. **Token version checking**: Added to risk level calculation\n12. **Stale cache tolerance**: Serve stale data during Redis failure (availability vs staleness tradeoff)\n\n### Reliability\n13. **Faster recovery**: 5-second reset timeout (vs 10) for quicker Redis reconnection\n14. **Success threshold**: Require 5 consecutive successes before closing circuit (prevents flapping)\n15. **Request count tracking**: Circuit breaker only evaluates after minimum volume\n16. **Graceful failure decay**: Success decreases failure count in closed state\n\n### Operational Excellence\n17. **Comprehensive metrics**: Added histograms for latency tracking by layer\n18. **Security event tracking**: Monitor blacklist bypasses and token reuse\n19. **Prepared statements**: Faster blacklist checks with query caching\n20. **Partition support**: Schema ready for time-based partitioning at scale\n\n## What I Kept\n\nThe location policy section was **already excellent** - comprehensive risk matrix, VPN handling, mobile considerations, trust levels, and progressive response strategy. No changes needed.\n\nAlso kept:\n- 15-minute access tokens, 30-day refresh tokens\n- Opaque refresh tokens with device fingerprinting\n- Token family tracking for compromise detection\n- httpOnly cookies with SameSite=Strict\n- Multi-device session management\n\n## Remaining Concerns\n\n### 1. Cross-Region Deployment\n\n**Challenge**: Design assumes single-region deployment.\n\n**For global scale**:\n- Option A: Regional Redis clusters with eventual consistency\n  - Pro: Low latency\n  - Con: Blacklist propagation delay (security risk)\n  \n- Option B: Global Redis with active-active replication\n  - Pro: Strong consistency\n  - Con: Expensive, complex setup\n  \n- Option C: Regional Postgres read replicas + write to primary\n  - Pro: Leverages existing Postgres HA\n  - Con: Write latency for distant regions\n\n**Recommendation**: Start single-region, add read replicas when needed.\n\n---\n\n### 2. Bloom Filter Memory Growth\n\n**Current**: 1M entries at 0.01% FPR \u2248 2.4 MB\n\n**At scale**: \n- 10M users \u00d7 2 tokens/day \u00d7 15-min lifetime = ~208K concurrent blacklist entries\n- Bloom filter size: ~500 KB (manageable)\n- 100M users: ~5 MB (still fine)\n\n**Mitigation**: \n- Partition bloom filter by time bucket (only load recent entries)\n- Rebuild every hour from Postgres\n\n**Verdict**: Not a concern until 1B+ users.\n\n---\n\n### 3. Postgres Connection Pool Saturation\n\n**Risk**: During Redis failure, all instances hit Postgres simultaneously.\n\n**Calculation**:\n- 10 app instances \u00d7 50 connections each = 500 connections\n- Postgres max_connections: Default 100 (\u274c not enough!)\n\n**Mitigation**:\n```\n# Increase Postgres connections\nmax_connections = 1000\n\n# Use PgBouncer for connection pooling\n# Transaction mode for short queries (blacklist checks)\n```\n\n**Verdict**: \u26a0\ufe0f **Must configure connection pooling for Postgres fallback**.\n\n---\n\n### 4. Write Amplification on Token Refresh\n\n**Current Design**:\n- Every refresh writes to: `refresh_tokens` (INSERT + UPDATE), `token_blacklist` (if access token active)\n- 1M users, 1 refresh/hour = 278 writes/sec\n\n**At 10M users**: 2,780 writes/sec (approaching limits for single Postgres instance)\n\n**Optimization Options**:\n1. **Partition refresh_tokens by created_at** (distribute I/O)\n2. **Batch blacklist inserts** (already implemented via write buffer)\n3. **Use separate write-optimized DB** for audit logs\n4. **Lazy cleanup** (mark as used without immediate blacklist write)\n\n**Verdict**: \u26a0\ufe0f **Need partitioning beyond 10M users**.\n\n## Convergence Status\n\n**ITERATING** - This is round 3. Major improvements made:\n\n\u2705 **Resolved**:\n- Postgres fallback performance (bloom filter, batching, row locks)\n- Circuit breaker thresholds (faster detection, stable recovery)\n- Security vulnerabilities (write buffering, TOCTOU, rate limit bypass)\n\n\u26a0\ufe0f **Still needs validation**:\n- Cross-region deployment strategy\n- Connection pool sizing for Postgres fallback\n- Partitioning strategy for 10M+ users\n- Load testing of failover paths under realistic traffic\n\nThe design is now **production-ready for single-region deployments up to 10M users**. Beyond that scale, we need partitioning and potentially multi-region strategy.\n\n---\n\n## PROMPT_FOR_ARCHITECT:\n\nThe design is now solid for single-region deployments. Please focus on:\n\n1. **Cross-Region Strategy**: How should we handle multi-region deployments? Specifically:\n   - Should blacklist propagation be eventually consistent across regions?\n   - Can we tolerate 100-500ms inter-region blacklist sync delay?\n   - How do we handle token refresh when user roams between regions?\n\n2. **Load Testing Plan**: What realistic failure scenarios should we test?\n   - Redis failure during peak traffic (1M req/s)\n   - Postgres connection pool exhaustion\n   - Circuit breaker flapping (Redis intermittent failures)\n   - Cross-instance rate limit effectiveness\n\n3. **Operational Runbook**: What should the incident response be when:\n   - Circuit breaker opens (Redis down)?\n   - Bloom filter false positive rate spikes?\n   - Postgres fallback latency exceeds SLA?\n\nIf you believe the design is ready for implementation without further iteration, signal **ACCEPTING_FINAL**."
      }
    },
    {
      "round_number": 3,
      "agent_a": {
        "content": "## Design\n\n# JWT Authentication System with Global Distribution & Production Hardening\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        GLOBAL REGIONS                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502   US-EAST        \u2502         \u2502   EU-WEST        \u2502            \u2502\n\u2502  \u2502                  \u2502         \u2502                  \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 API Gateway\u2502  \u2502         \u2502  \u2502 API Gateway\u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Redis      \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Redis      \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 Regional   \u2502  \u2502 Sync    \u2502  \u2502 Regional   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 App Nodes  \u2502  \u2502         \u2502  \u2502 App Nodes  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (3+ pods)  \u2502  \u2502         \u2502  \u2502 (3+ pods)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Postgres   \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Postgres   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (Primary)  \u2502  \u2502 Replica \u2502  \u2502 (Replica)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502           \u2193                             \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502         Global Blacklist Propagation Bus             \u2502     \u2502\n\u2502  \u2502         (Pub/Sub: Redis Streams + Postgres WAL)     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Multi-Region Strategy with Consistency Guarantees\n\n### 1. Token Types and Regional Behavior\n\n**Access Tokens** (15-minute lifetime):\n- \u2705 **Stateless validation** - verify signature + check local blacklist\n- \u2705 **Regional Redis cache** - 99.9% of validations hit local cache\n- \u26a0\ufe0f **Blacklist propagation delay** - acceptable (see security analysis)\n\n**Refresh Tokens** (30-day lifetime):\n- \u2705 **Always write to PRIMARY Postgres** (strong consistency)\n- \u2705 **Refresh allowed in any region** (global mobility)\n- \u2705 **Token family tracking** - detects cross-region reuse attacks\n\n### 2. Blacklist Propagation Strategy\n\n**Design Principle**: **\"Eventually consistent blacklist with risk-based synchronous verification\"**\n\n```typescript\ninterface BlacklistPropagationConfig {\n  // How fast does blacklist propagate between regions?\n  propagationTarget: {\n    p50: 50,      // 50ms - Redis Streams\n    p99: 500,     // 500ms - includes Postgres replication\n    p99.9: 2000   // 2 seconds - degraded network\n  };\n  \n  // Which operations require synchronous cross-region check?\n  requireSyncCheck: {\n    highRiskOperations: true,    // Admin actions, delete account, payment\n    tokenAge: '<5min',           // Recently issued tokens (likely from logout)\n    userRiskScore: '>0.7',       // Suspicious activity score\n    crossRegionRefresh: true     // User moved to different region\n  };\n  \n  // Fallback behavior during propagation delay\n  acceptanceCriteria: {\n    maxAge: 300000,              // 5 minutes since token issued\n    requireCompensatingControls: true,\n    logSecurityEvent: true\n  };\n}\n\n// Global blacklist propagation bus\nclass GlobalBlacklistBus {\n  private regionalRedis: Map<Region, RedisClient>;\n  private postgresWalSubscription: PostgresReplicationClient;\n  private publishQueue: RedisStream;\n  \n  constructor() {\n    // Subscribe to Postgres WAL for blacklist INSERTs\n    this.postgresWalSubscription = new PostgresReplicationClient({\n      tables: ['token_blacklist'],\n      operations: ['INSERT'],\n      onMessage: (change) => this.propagateBlacklistEntry(change)\n    });\n    \n    // Redis Streams for cross-region pub/sub\n    this.publishQueue = new RedisStream('blacklist:global');\n  }\n  \n  async blacklistToken(\n    jti: string, \n    userId: string, \n    reason: string,\n    metadata: BlacklistMetadata\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    // 1. Write to PRIMARY Postgres immediately (source of truth)\n    const expiresAt = new Date(Date.now() + 15 * 60 * 1000);\n    await db.query(\n      `INSERT INTO token_blacklist \n       (jti, user_id, expires_at, reason, blacklisted_at, source_region, session_id)\n       VALUES ($1, $2, $3, $4, NOW(), $5, $6)`,\n      [jti, userId, expiresAt, reason, getCurrentRegion(), metadata.sessionId]\n    );\n    \n    logger.info('Token blacklisted in primary DB', {\n      jti,\n      userId,\n      reason,\n      latency: performance.now() - startTime\n    });\n    \n    // 2. Publish to Redis Stream for immediate regional propagation\n    await this.publishQueue.add('blacklist', {\n      jti,\n      userId,\n      expiresAt: expiresAt.toISOString(),\n      reason,\n      sourceRegion: getCurrentRegion(),\n      timestamp: Date.now()\n    });\n    \n    // 3. Update local region Redis immediately\n    await this.regionalRedis.get(getCurrentRegion())?.setex(\n      `blacklist:jti:${jti}`,\n      15 * 60,\n      JSON.stringify({ userId, reason, blacklistedAt: new Date() })\n    );\n    \n    // 4. Add to local bloom filter immediately\n    blacklistBloomFilter.add(jti);\n    \n    metrics.histogram('blacklist.propagation.latency', performance.now() - startTime);\n  }\n  \n  async propagateBlacklistEntry(entry: BlacklistEntry): Promise<void> {\n    // Propagate to all regional Redis clusters\n    const propagationPromises = Array.from(this.regionalRedis.entries())\n      .filter(([region]) => region !== entry.sourceRegion) // Skip source\n      .map(([region, redis]) => \n        this.propagateToRegion(region, redis, entry)\n      );\n    \n    await Promise.allSettled(propagationPromises);\n  }\n  \n  private async propagateToRegion(\n    region: Region,\n    redis: RedisClient,\n    entry: BlacklistEntry\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    try {\n      await redis.setex(\n        `blacklist:jti:${entry.jti}`,\n        entry.ttlSeconds,\n        JSON.stringify(entry)\n      );\n      \n      metrics.histogram('blacklist.cross_region_propagation', \n        performance.now() - startTime,\n        { source: entry.sourceRegion, target: region }\n      );\n      \n    } catch (error) {\n      logger.error('Failed to propagate blacklist to region', {\n        region,\n        jti: entry.jti,\n        error: error.message\n      });\n      \n      metrics.increment('blacklist.propagation.failure', {\n        source: entry.sourceRegion,\n        target: region\n      });\n      \n      // Not critical - Postgres replication will eventually sync\n    }\n  }\n  \n  // Check blacklist with cross-region awareness\n  async checkBlacklist(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): Promise<BlacklistCheckResult> {\n    \n    // 1. Check local Redis first (99.9% of requests)\n    const localResult = await this.regionalRedis.get(currentRegion)?.get(\n      `blacklist:jti:${jti}`\n    );\n    \n    if (localResult !== null) {\n      return { \n        blacklisted: true, \n        source: 'local_redis',\n        latency: 1 \n      };\n    }\n    \n    // 2. Determine if we need synchronous cross-region check\n    const needsSyncCheck = this.shouldCheckAllRegions(\n      jti,\n      riskLevel,\n      tokenIssuedAt,\n      currentRegion,\n      tokenIssuedInRegion\n    );\n    \n    if (!needsSyncCheck) {\n      // Trust local cache (low risk)\n      return { \n        blacklisted: false, \n        source: 'local_redis',\n        latency: 1\n      };\n    }\n    \n    // 3. Check PRIMARY Postgres (authoritative source)\n    // This handles cross-region edge cases\n    const startTime = performance.now();\n    const pgResult = await db.query({\n      name: 'check_blacklist_global',\n      text: `SELECT 1 FROM token_blacklist \n             WHERE jti = $1 AND expires_at > NOW() \n             LIMIT 1`,\n      values: [jti]\n    });\n    \n    const latency = performance.now() - startTime;\n    \n    metrics.histogram('blacklist.cross_region_check.latency', latency, {\n      risk_level: riskLevel,\n      cross_region: tokenIssuedInRegion !== currentRegion\n    });\n    \n    return {\n      blacklisted: pgResult.rowCount > 0,\n      source: 'postgres_primary',\n      latency\n    };\n  }\n  \n  private shouldCheckAllRegions(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): boolean {\n    \n    // Always check Postgres for high-risk operations\n    if (riskLevel === 'high') {\n      return true;\n    }\n    \n    // Check if token was issued in different region (possible logout in other region)\n    if (tokenIssuedInRegion && tokenIssuedInRegion !== currentRegion) {\n      return true;\n    }\n    \n    // Check if token is very recently issued (< 5 minutes)\n    // Likely from recent logout that may not have propagated yet\n    const tokenAge = Date.now() - tokenIssuedAt;\n    if (tokenAge < 5 * 60 * 1000) {\n      return true;\n    }\n    \n    return false;\n  }\n}\n\ninterface BlacklistEntry {\n  jti: string;\n  userId: string;\n  expiresAt: string;\n  reason: string;\n  sourceRegion: Region;\n  ttlSeconds: number;\n  timestamp: number;\n  sessionId?: string;\n}\n\ninterface BlacklistMetadata {\n  sessionId?: string;\n  deviceId?: string;\n  ipAddress?: string;\n  userAgent?: string;\n}\n\ninterface BlacklistCheckResult {\n  blacklisted: boolean;\n  source: 'local_redis' | 'remote_redis' | 'postgres_primary' | 'bloom_filter';\n  latency: number;\n}\n\ntype Region = 'us-east' | 'us-west' | 'eu-west' | 'ap-southeast' | 'ap-northeast';\n\nfunction getCurrentRegion(): Region {\n  return process.env.AWS_REGION as Region || 'us-east';\n}\n```\n\n### 3. Cross-Region Token Refresh Strategy\n\n**Challenge**: User logs in US-EAST, travels to EU-WEST, refreshes token.\n\n**Requirements**:\n- \u2705 Refresh must succeed (user mobility)\n- \u2705 Prevent token reuse attacks across regions\n- \u2705 Detect compromised token families\n\n**Solution**: **Regional read replicas + primary write with conflict detection**\n\n```typescript\nasync function performCrossRegionRefresh(\n  refreshToken: string,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<AuthResponse> {\n  \n  const tokenHash = hashToken(refreshToken);\n  const startTime = performance.now();\n  \n  // 1. Acquire distributed lock (prefer local Redis, fallback to Postgres)\n  const lockAcquired = await acquireCrossRegionLock(tokenHash, currentRegion);\n  \n  if (!lockAcquired) {\n    throw new AuthError('CONCURRENT_REFRESH_IN_PROGRESS');\n  }\n  \n  try {\n    // 2. Read token from PRIMARY Postgres (not replica)\n    // This ensures we see the absolute latest state across all regions\n    const client = await getPrimaryDbClient();\n    \n    await client.query('BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE');\n    \n    const tokenResult = await client.query(\n      `SELECT * FROM refresh_tokens \n       WHERE token_hash = $1 \n       FOR UPDATE`,\n      [tokenHash]\n    );\n    \n    if (tokenResult.rowCount === 0) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n    \n    const tokenData = tokenResult.rows[0];\n    \n    // 3. Enhanced validation for cross-region refresh\n    await validateRefreshToken(tokenData, fingerprint, currentRegion);\n    \n    // 4. Check for token reuse (CRITICAL for cross-region security)\n    if (tokenData.used) {\n      // Token reuse detected - revoke entire family\n      await handleTokenFamilyCompromise(\n        tokenData.token_family_id,\n        'CROSS_REGION_TOKEN_REUSE',\n        client\n      );\n      \n      throw new AuthError('TOKEN_REUSE_DETECTED');\n    }\n    \n    // 5. Mark token as used (in PRIMARY database)\n    await client.query(\n      `UPDATE refresh_tokens \n       SET used = true, \n           used_at = NOW(),\n           used_in_region = $2\n       WHERE token_hash = $1`,\n      [tokenHash, currentRegion]\n    );\n    \n    // 6. Generate new token pair\n    const user = await getUserById(tokenData.user_id);\n    \n    const newAccessToken = generateAccessToken(user, {\n      sessionId: tokenData.session_id,\n      deviceId: tokenData.device_id,\n      tokenVersion: user.token_version,\n      issuedInRegion: currentRegion,  // NEW: Track issuing region\n      previousRegion: tokenData.issued_in_region\n    });\n    \n    const newRefreshToken = await generateAndStoreRefreshToken(\n      user,\n      {\n        sessionId: tokenData.session_id,\n        deviceId: tokenData.device_id,\n        deviceFingerprint: fingerprint,\n        parentTokenId: tokenData.id,\n        tokenFamilyId: tokenData.token_family_id,\n        timesRefreshed: tokenData.times_refreshed + 1,\n        issuedInRegion: currentRegion,  // NEW: Track issuing region\n        previousRegion: tokenData.issued_in_region\n      },\n      client\n    );\n    \n    await client.query('COMMIT');\n    \n    // 7. Update regional cache (eventual consistency is OK here)\n    try {\n      await cache.set(\n        `refresh:${tokenHash}:used`,\n        true,\n        900\n      );\n    } catch {\n      // Not critical\n    }\n    \n    metrics.histogram('auth.cross_region_refresh.latency', \n      performance.now() - startTime,\n      { \n        from_region: tokenData.issued_in_region,\n        to_region: currentRegion\n      }\n    );\n    \n    return {\n      accessToken: newAccessToken.token,\n      refreshToken: newRefreshToken.token,\n      expiresIn: 900,\n      tokenType: 'Bearer',\n      refreshExpiresIn: 2592000\n    };\n    \n  } finally {\n    await releaseCrossRegionLock(tokenHash, currentRegion);\n  }\n}\n\nasync function validateRefreshToken(\n  tokenData: RefreshTokenRow,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<void> {\n  \n  // 1. Check expiry\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  // 2. Check revocation\n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Validate fingerprint\n  const fingerprintHash = hashFingerprint(fingerprint);\n  if (tokenData.device_fingerprint_hash !== fingerprintHash) {\n    \n    // Cross-region fingerprint mismatch could be legitimate (VPN, proxy)\n    // Check if user has history of cross-region usage\n    const userTravelPattern = await getUserTravelPattern(tokenData.user_id);\n    \n    if (!userTravelPattern.crossRegionAllowed) {\n      // User has never used service from multiple regions - suspicious\n      await createSecurityEvent({\n        userId: tokenData.user_id,\n        eventType: 'CROSS_REGION_FINGERPRINT_MISMATCH',\n        severity: 'HIGH',\n        details: {\n          originalRegion: tokenData.issued_in_region,\n          currentRegion,\n          originalFingerprint: tokenData.device_fingerprint_hash.substring(0, 16),\n          currentFingerprint: fingerprintHash.substring(0, 16)\n        }\n      });\n      \n      throw new AuthError('FINGERPRINT_MISMATCH');\n    }\n    \n    // Log but allow (user has established cross-region pattern)\n    logger.warn('Cross-region fingerprint mismatch allowed', {\n      userId: tokenData.user_id,\n      fromRegion: tokenData.issued_in_region,\n      toRegion: currentRegion\n    });\n  }\n  \n  // 4. Check refresh count (detect infinite refresh loops)\n  if (tokenData.times_refreshed > 1000) {\n    // Suspicious - single refresh chain shouldn't exceed 1000 refreshes\n    // (30 days / 15 min = 2880 max normal refreshes, but user should re-login)\n    await createSecurityEvent({\n      userId: tokenData.user_id,\n      eventType: 'EXCESSIVE_REFRESH_COUNT',\n      severity: 'MEDIUM',\n      details: {\n        timesRefreshed: tokenData.times_refreshed,\n        tokenFamilyId: tokenData.token_family_id\n      }\n    });\n    \n    throw new AuthError('REFRESH_LIMIT_EXCEEDED');\n  }\n}\n\n// Cross-region distributed lock using Redis + Postgres fallback\nasync function acquireCrossRegionLock(\n  tokenHash: string,\n  region: Region,\n  timeoutMs: number = 5000\n): Promise<boolean> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  const lockValue = `${region}:${uuidv4()}:${Date.now()}`;\n  \n  // Try regional Redis first\n  const regionalRedis = getRegionalRedis(region);\n  \n  try {\n    const acquired = await regionalRedis.set(\n      lockKey,\n      lockValue,\n      'NX',\n      'PX',\n      timeoutMs\n    );\n    \n    if (acquired) {\n      return true;\n    }\n  } catch (error) {\n    logger.warn('Regional Redis lock failed, trying Postgres', {\n      tokenHash: tokenHash.substring(0, 16),\n      region,\n      error: error.message\n    });\n  }\n  \n  // Fallback to Postgres advisory lock\n  // Use hash of token as lock ID (deterministic across regions)\n  const lockId = hashToInt64(tokenHash);\n  \n  const result = await db.query(\n    'SELECT pg_try_advisory_lock($1) as acquired',\n    [lockId]\n  );\n  \n  return result.rows[0].acquired;\n}\n\nasync function releaseCrossRegionLock(\n  tokenHash: string,\n  region: Region\n): Promise<void> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  \n  // Try Redis first\n  try {\n    const regionalRedis = getRegionalRedis(region);\n    await regionalRedis.del(lockKey);\n  } catch (error) {\n    // Fallback to Postgres advisory lock release\n    const lockId = hashToInt64(tokenHash);\n    await db.query('SELECT pg_advisory_unlock($1)', [lockId]);\n  }\n}\n```\n\n### 4. Enhanced Schema for Multi-Region Support\n\n```sql\n-- Refresh tokens table with region tracking\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  token_hash VARCHAR(64) NOT NULL UNIQUE,\n  user_id UUID NOT NULL REFERENCES users(id),\n  session_id UUID NOT NULL,\n  device_id VARCHAR(255) NOT NULL,\n  device_fingerprint_hash VARCHAR(64) NOT NULL,\n  \n  -- NEW: Region tracking\n  issued_in_region VARCHAR(50) NOT NULL,\n  used_in_region VARCHAR(50),\n  \n  expires_at TIMESTAMP NOT NULL,\n  issued_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  used BOOLEAN DEFAULT FALSE,\n  used_at TIMESTAMP,\n  revoked BOOLEAN DEFAULT FALSE,\n  revoked_at TIMESTAMP,\n  \n  -- Token family tracking\n  token_family_id UUID NOT NULL,\n  parent_token_id UUID REFERENCES refresh_tokens(id),\n  times_refreshed INTEGER DEFAULT 0,\n  \n  -- Audit\n  ip_address INET,\n  user_agent TEXT,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Composite index for cross-region lookups\nCREATE INDEX idx_refresh_tokens_hash_region ON refresh_tokens(token_hash, issued_in_region);\nCREATE INDEX idx_refresh_tokens_family ON refresh_tokens(token_family_id, issued_at DESC);\nCREATE INDEX idx_refresh_tokens_user_active ON refresh_tokens(user_id, expires_at) \n  WHERE NOT used AND NOT revoked;\n\n-- Token blacklist with region tracking\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  reason VARCHAR(100),\n  session_id UUID,\n  \n  -- NEW: Region tracking for propagation monitoring\n  source_region VARCHAR(50) NOT NULL,\n  blacklisted_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  \n  ip_address INET,\n  user_agent TEXT\n);\n\n-- Index for cross-region blacklist checks\nCREATE INDEX idx_blacklist_active ON token_blacklist(jti, expires_at)\n  WHERE expires_at > NOW();\nCREATE INDEX idx_blacklist_user_recent ON token_blacklist(user_id, revoked_at DESC);\nCREATE INDEX idx_blacklist_region ON token_blacklist(source_region, blacklisted_at DESC);\n\n-- User travel patterns (for risk assessment)\nCREATE TABLE user_region_history (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id),\n  region VARCHAR(50) NOT NULL,\n  first_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  last_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  request_count INTEGER DEFAULT 1,\n  \n  UNIQUE(user_id, region)\n);\n\nCREATE INDEX idx_user_region_history_user ON user_region_history(user_id, last_seen DESC);\n```\n\n## Load Testing & Failure Scenarios\n\n### 1. Load Testing Plan\n\n```typescript\n// Load test scenarios\nconst loadTestScenarios = [\n  {\n    name: 'Peak Traffic - Normal Operation',\n    duration: '10min',\n    rps: 100000, // 100K requests/sec\n    operations: {\n      tokenValidation: 0.70,  // 70% reads (API calls)\n      tokenRefresh: 0.25,     // 25% refresh operations\n      logout: 0.05            // 5% logout (blacklist writes)\n    },\n    assertions: {\n      p99Latency: '<50ms',\n      errorRate: '<0.01%',\n      redisHitRate: '>99.5%'\n    }\n  },\n  \n  {\n    name: 'Redis Failure - Circuit Breaker Activation',\n    duration: '5min',\n    setup: 'Stop Redis at t=60s, restart at t=180s',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerOpens: '<10s after Redis down',\n      fallbackLatency: '<100ms p99',\n      errorRate: '<1%',\n      recoveryTime: '<30s after Redis up',\n      noDataLoss: 'All blacklist entries in Postgres'\n    }\n  },\n  \n  {\n    name: 'Postgres Connection Pool Exhaustion',\n    duration: '3min',\n    setup: 'max_connections=100, 20 app instances',\n    rps: 10000,\n    operations: {\n      // Force all requests to Postgres (Redis disabled)\n      tokenValidation: 0.90,\n      tokenRefresh: 0.10\n    },\n    assertions: {\n      connectionPoolUtilization: '<80%',\n      p99Latency: '<200ms',\n      errorRate: '<5%',\n      noConnectionLeaks: 'Pool size stable'\n    }\n  },\n  \n  {\n    name: 'Circuit Breaker Flapping',\n    duration: '10min',\n    setup: 'Intermittent Redis failures (30s down, 30s up)',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerFlaps: '<5 times',\n      halfOpenDuration: '<30s',\n      errorRate: '<2%',\n      cacheCoherence: 'Local cache syncs after recovery'\n    }\n  },\n  \n  {\n    name: 'Cross-Instance Rate Limit Bypass',\n    duration: '5min',\n    setup: '10 app instances, rate limit 100 req/min per user',\n    rps: 5000,\n    operations: {\n      // Single user attacking across all instances\n      singleUserAttack: true,\n      targetRate: 1000 // 10x rate limit\n    },\n    assertions: {\n      effectiveRateLimit: '<150 req/min', // Allow 50% overage\n      postgresSync: 'Within 10 seconds',\n      blockAfterSync: 'Within 1 request'\n    }\n  },\n  \n  {\n    name: 'Cross-Region Token Refresh',\n    duration: '10min',\n    setup: 'Users refresh from different region than login',\n    rps: 10000,\n    operations: {\n      loginRegion: 'us-east',\n      refreshRegion: 'eu-west',\n      percentCrossRegion: 50\n    },\n    assertions: {\n      crossRegionLatency: '<200ms p99',\n      tokenReuseDetection: '100%',\n      blacklistPropagation: '<500ms p99',\n      noFalsePositives: 'Zero legitimate refreshes blocked'\n    }\n  },\n  \n  {\n    name: 'Bloom Filter False Positive Rate',\n    duration: '30min',\n    setup: '1M active tokens, 100K blacklisted',\n    rps: 100000,\n    operations: {\n      tokenValidation: 1.0\n    },\n    assertions: {\n      bloomFalsePositiveRate: '<0.01%',\n      postgresCheckRate: '<10 req/s',\n      noFalseNegatives: 'Zero blacklisted tokens accepted'\n    }\n  }\n];\n\n// Chaos engineering scenarios\nconst chaosScenarios = [\n  {\n    name: 'Network Partition - Region Isolation',\n    setup: 'Isolate EU-WEST region for 60 seconds',\n    expectedBehavior: [\n      'EU-WEST continues serving from local Redis + Postgres replica',\n      'New blacklists in US-EAST do not propagate to EU-WEST',\n      'High-risk operations in EU-WEST check PRIMARY Postgres',\n      'After partition heals, blacklists propagate within 5 seconds'\n    ]\n  },\n  \n  {\n    name: 'Postgres Primary Failover',\n    setup: 'Promote replica to primary (30s downtime)',\n    expectedBehavior: [\n      'Token refreshes fail during failover window',\n      'Token validation continues (cached data)',\n      'After promotion, refreshes resume within 10 seconds',\n      'No token reuse vulnerabilities introduced'\n    ]\n  },\n  \n  {\n    name: 'Memory Pressure - Cache Eviction',\n    setup: 'Fill LRU cache to capacity, continue adding entries',\n    expectedBehavior: [\n      'LRU evicts oldest entries',\n      'Cache hit rate remains >95%',\n      'No memory leak (heap size stable)',\n      'Evicted entries refetched from Redis/Postgres'\n    ]\n  }\n];\n```\n\n### 2. Load Testing Implementation\n\n```typescript\n// Load testing harness using k6 or artillery\nimport { check, group, sleep } from 'k6';\nimport http from 'k6/http';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 50000 }, // Ramp up\n    { duration: '5m', target: 100000 }, // Peak\n    { duration: '2m', target: 0 }     // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(99)<50'],\n    http_req_failed: ['rate<0.0001'],\n    'circuit_breaker_open': ['count<1']\n  }\n};\n\nexport default function() {\n  const scenario = Math.random();\n  \n  if (scenario < 0.70) {\n    // Token validation (70% of traffic)\n    testTokenValidation();\n  } else if (scenario < 0.95) {\n    // Token refresh (25% of traffic)\n    testTokenRefresh();\n  } else {\n    // Logout (5% of traffic)\n    testLogout();\n  }\n  \n  sleep(Math.random() * 2); // Random delay 0-2s\n}\n\nfunction testTokenValidation() {\n  const token = __ENV.TEST_ACCESS_TOKEN;\n  \n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${token}` }\n  });\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'latency < 50ms': (r) => r.timings.duration < 50,\n    'cache hit': (r) => r.headers['X-Cache-Layer'] === 'redis'\n  });\n}\n\nfunction testTokenRefresh() {\n  const refreshToken = __ENV.TEST_REFRESH_TOKEN;\n  \n  const res = http.post('https://api.example.com/auth/refresh', \n    JSON.stringify({ refreshToken }),\n    { headers: { 'Content-Type': 'application/json' } }\n  );\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'returns new tokens': (r) => r.json('accessToken') !== undefined,\n    'latency < 100ms': (r) => r.timings.duration < 100\n  });\n}\n\n// Failure injection for chaos testing\nexport function injectRedisFailure() {\n  // Use Kubernetes pod deletion or network policy\n  exec(`kubectl delete pod -l app=redis-cache -n production`);\n  \n  sleep(10); // Wait for circuit breaker to trip\n  \n  // Verify fallback behavior\n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${__ENV.TEST_ACCESS_TOKEN}` }\n  });\n  \n  check(res, {\n    'fallback successful': (r) => r.status === 200,\n    'postgres fallback used': (r) => r.headers['X-Cache-Layer'] === 'postgres'\n  });\n}\n```\n\n## Operational Runbook\n\n### 1. Incident Response Playbooks\n\n```yaml\n# Runbook: Redis Unavailable (Circuit Breaker Open)\n\nincident_type: \"Redis Unavailable\"\nseverity: P2 (High)\ndetection: Alert \"RedisCircuitBreakerOpen\" fires\n\nimmediate_actions:\n  - verify_fallback:\n      cmd: \"curl https://api.example.com/health/fallback\"\n      expected: '{\"status\":\"degraded\",\"fallback\":\"postgres\",\"latency_p99\":150}'\n      \n  - check_postgres_load:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'active'\"\n      threshold: \"< 80% of max_connections\"\n      \n  - verify_no_data_loss:\n      cmd: \"SELECT COUNT(*) FROM token_blacklist WHERE blacklisted_at > NOW() - INTERVAL '5 minutes'\"\n      compare_with: \"Redis blacklist count (should match)\"\n\ninvestigation:\n  - check_redis_health:\n      - \"kubectl get pods -n production -l app=redis\"\n      - \"kubectl logs -n production -l app=redis --tail=100\"\n      - \"redis-cli -h redis.internal PING\"\n      \n  - check_sentinel_status:\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL masters\"\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL slaves auth-master\"\n      \n  - check_network:\n      - \"kubectl get networkpolicies -n production\"\n      - \"traceroute redis.internal\"\n\nmitigation:\n  - if_redis_pod_crash:\n      action: \"Kubernetes should auto-restart\"\n      verify: \"Wait 30s, check if circuit breaker closes\"\n      \n  - if_sentinel_failover:\n      action: \"Wait for automatic failover (< 30s)\"\n      verify: \"SENTINEL masters shows new master\"\n      \n  - if_persistent_failure:\n      action: \"Scale Postgres read replicas\"\n      cmd: \"kubectl scale deployment postgres-replica --replicas=5\"\n\nrecovery_verification:\n  - circuit_breaker_closed:\n      query: \"circuit_breaker_state{service='redis'} == 0\"\n      \n  - latency_normal:\n      query: \"histogram_quantile(0.99, cache_get_latency) < 10\"\n      \n  - error_rate_normal:\n      query: \"rate(http_requests_total{status=~'5..'}[5m]) < 0.001\"\n\npost_mortem:\n  - analyze_root_cause: true\n  - update_circuit_breaker_thresholds: \"if flapping observed\"\n  - review_postgres_capacity: \"if connection pool saturated\"\n```\n\n```yaml\n# Runbook: Bloom Filter False Positive Rate Spike\n\nincident_type: \"Bloom Filter False Positive Rate High\"\nseverity: P3 (Medium)\ndetection: Alert \"BloomFilterFalsePositiveRateHigh\" fires\n\nimmediate_actions:\n  - check_metrics:\n      query: \"rate(auth_bloom_filter_false_positive_total[5m])\"\n      threshold: \"> 0.01%\"\n      \n  - verify_no_false_negatives:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE event_type = 'BLACKLISTED_TOKEN_ACCEPTED' AND created_at > NOW() - INTERVAL '5 minutes'\"\n      expected: \"0\"\n\ninvestigation:\n  - check_bloom_filter_size:\n      cmd: \"curl http://api.example.com/internal/bloom-filter/stats\"\n      expected: '{\"entries\":100000,\"capacity\":1000000,\"load_factor\":0.1}'\n      \n  - check_sync_lag:\n      cmd: \"SELECT MAX(NOW() - blacklisted_at) FROM token_blacklist\"\n      threshold: \"< 60 seconds\"\n      \n  - analyze_blacklist_growth:\n      query: \"rate(token_blacklist_inserts_total[1h])\"\n      compare_with: \"Historical average\"\n\nmitigation:\n  - if_overloaded:\n      condition: \"load_factor > 0.8\"\n      action: \"Rebuild bloom filter with larger capacity\"\n      cmd: \"curl -X POST http://api.example.com/internal/bloom-filter/rebuild?capacity=10000000\"\n      \n  - if_sync_lag:\n      condition: \"sync lag > 60s\"\n      action: \"Trigger immediate sync from Postgres\"\n      cmd: \"curl -X POST http://api.example.com/internal/bloom-filter/sync\"\n\nrecovery_verification:\n  - false_positive_rate_normal:\n      query: \"rate(auth_bloom_filter_false_positive_total[5m]) < 0.0001\"\n      \n  - no_security_incidents:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE severity = 'HIGH' AND created_at > NOW() - INTERVAL '10 minutes'\"\n      expected: \"0\"\n\nprevention:\n  - implement_auto_scaling:\n      description: \"Automatically resize bloom filter when load_factor > 0.7\"\n      \n  - add_capacity_alerts:\n      alert: \"BloomFilterCapacityWarning\"\n      threshold: \"load_factor > 0.6\"\n```\n\n```yaml\n# Runbook: Postgres Fallback Latency High\n\nincident_type: \"Postgres Fallback Latency Exceeds SLA\"\nseverity: P2 (High)\ndetection: Alert \"PostgresFallbackLatencyHigh\" fires\n\nimmediate_actions:\n  - check_sla_breach:\n      query: \"histogram_quantile(0.99, blacklist_check_latency{source='postgres'}) > 200\"\n      \n  - verify_connection_pool:\n      cmd: \"SELECT count(*) FROM pg_stat_activity\"\n      threshold: \"< max_connections * 0.8\"\n      \n  - check_query_performance:\n      cmd: \"SELECT query, mean_exec_time, calls FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10\"\n\ninvestigation:\n  - analyze_slow_queries:\n      cmd: \"SELECT pid, query_start, state, query FROM pg_stat_activity WHERE state = 'active' AND query_start < NOW() - INTERVAL '1 second'\"\n      \n  - check_index_usage:\n      cmd: \"SELECT schemaname, tablename, indexname, idx_scan FROM pg_stat_user_indexes WHERE schemaname = 'public' AND idx_scan = 0\"\n      \n  - check_table_bloat:\n      cmd: \"SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) FROM pg_tables WHERE schemaname = 'public' ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\"\n\nmitigation:\n  - if_connection_pool_exhausted:\n      action: \"Increase connection pool size\"\n      cmd: \"kubectl set env deployment/api-server DB_POOL_SIZE=200\"\n      \n  - if_missing_index:\n      action: \"Create missing indexes\"\n      cmd: \"psql -c 'CREATE INDEX CONCURRENTLY idx_missing ON table(column)'\"\n      \n  - if_table_bloat:\n      action: \"Run VACUUM ANALYZE\"\n      cmd: \"psql -c 'VACUUM ANALYZE token_blacklist'\"\n      \n  - if_query_slow:\n      action: \"Optimize query or add caching\"\n      review: \"src/auth/blacklist-check.ts\"\n\nemergency_mitigation:\n  - reduce_postgres_checks:\n      description: \"Temporarily reduce risk threshold for Postgres checks\"\n      cmd: \"kubectl set env deployment/api-server BLACKLIST_CHECK_RISK_THRESHOLD=high\"\n      impact: \"Medium/low risk operations skip Postgres check (accept stale cache)\"\n      rollback: \"After Postgres performance recovers\"\n\nrecovery_verification:\n  - latency_within_sla:\n      query: \"histogram_quantile(0.99, blacklist_check_latency{source='postgres'}) < 100\"\n      \n  - connection_pool_healthy:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'idle'\"\n      expected: \"> 20% of pool size\"\n\npost_mortem:\n  - review_query_plans: true\n  - optimize_indexes: true\n  - consider_read_replicas: \"if load is consistently high\"\n  - review_cache_ttl: \"increase TTL to reduce Postgres load\"\n```\n\n### 2. Monitoring Dashboard\n\n```typescript\n// Grafana dashboard JSON\nconst authSystemDashboard = {\n  title: \"JWT Auth System - Production Monitoring\",\n  panels: [\n    {\n      title: \"Request Rate by Operation\",\n      targets: [\n        'sum(rate(http_requests_total{service=\"auth\"}[5m])) by (operation)',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Cache Hit Rate by Layer\",\n      targets: [\n        'sum(rate(cache_hit_total[5m])) by (layer) / sum(rate(cache_requests_total[5m])) by (layer)',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.95, color: \"green\" },\n        { value: 0.90, color: \"yellow\" },\n        { value: 0.85, color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Circuit Breaker State\",\n      targets: [\n        'circuit_breaker_state{service=\"redis\"}',\n      ],\n      visualization: \"stat\",\n      mappings: [\n        { value: 0, text: \"CLOSED\", color: \"green\" },\n        { value: 0.5, text: \"HALF_OPEN\", color: \"yellow\" },\n        { value: 1, text: \"OPEN\", color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Token Validation Latency (p50, p99, p99.9)\",\n      targets: [\n        'histogram_quantile(0.50, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.99, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.999, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Postgres Connection Pool Utilization\",\n      targets: [\n        'pg_stat_activity_count / pg_settings_max_connections',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.80, color: \"red\" },\n        { value: 0.60, color: \"yellow\" },\n        { value: 0.40, color: \"green\" }\n      ]\n    },\n    \n    {\n      title: \"Blacklist Propagation Latency (Cross-Region)\",\n      targets: [\n        'histogram_quantile(0.99, sum(rate(blacklist_cross_region_propagation_bucket[5m])) by (source, target, le))',\n      ],\n      visualization: \"heatmap\"\n    },\n    \n    {\n      title: \"Security Events (Last 1 Hour)\",\n      targets: [\n        'sum(increase(security_events_total[1h])) by (event_type)',\n      ],\n      visualization: \"table\"\n    },\n    \n    {\n      title: \"Bloom Filter Statistics\",\n      targets: [\n        'bloom_filter_entries',\n        'bloom_filter_capacity',\n        'bloom_filter_load_factor',\n        'rate(bloom_filter_false_positive_total[5m])',\n      ],\n      visualization: \"stat\"\n    }\n  ],\n  \n  alerts: [\n    {\n      name: \"RedisCircuitBreakerOpen\",\n      condition: \"circuit_breaker_state{service='redis'} == 1\",\n      for: \"1m\",\n      severity: \"critical\"\n    },\n    {\n      name: \"HighPostgresFallback\",\n      condition: \"rate(cache_fallback_postgres_total[5m]) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"TokenValidationLatencyHigh\",\n      condition: \"histogram_quantile(0.99, auth_token_validation_latency_bucket) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"BloomFilterFalsePositiveRateHigh\",\n      condition: \"rate(bloom_filter_false_positive_total[5m]) > 0.0001\",\n      for: \"10m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"PostgresConnectionPoolSaturated\",\n      condition: \"pg_stat_activity_count / pg_settings_max_connections > 0.8\",\n      for: \"5m\",\n      severity: \"critical\"\n    },\n    {\n      name: \"CrossRegionBlacklistPropagationSlow\",\n      condition: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) > 1000\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"SecurityEventSpike\",\n      condition: \"rate(security_events_total{severity='HIGH'}[5m]) > 10\",\n      for: \"2m\",\n      severity: \"critical\"\n    }\n  ]\n};\n```\n\n### 3. Health Check Endpoints\n\n```typescript\n// Health check implementation\napp.get('/health', async (req, res) => {\n  const health = {\n    status: 'healthy',\n    timestamp: new Date().toISOString(),\n    version: process.env.APP_VERSION,\n    uptime: process.uptime(),\n    \n    components: {\n      redis: await checkRedisHealth(),\n      postgres: await checkPostgresHealth(),\n      bloomFilter: await checkBloomFilterHealth()\n    },\n    \n    metrics: {\n      circuitBreakerState: circuitBreaker.getState(),\n      cacheHitRate: await getCacheHitRate(),\n      requestRate: await getRequestRate(),\n      errorRate: await getErrorRate()\n    }\n  };\n  \n  // Overall health based on components\n  if (health.components.redis.status === 'unhealthy' && \n      health.components.postgres.status === 'unhealthy') {\n    health.status = 'unhealthy';\n    return res.status(503).json(health);\n  }\n  \n  if (health.components.redis.status === 'unhealthy' ||\n      health.components.postgres.status === 'degraded') {\n    health.status = 'degraded';\n    return res.status(200).json(health);\n  }\n  \n  res.status(200).json(health);\n});\n\nasync function checkRedisHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await redis.ping();\n    const latency = Date.now() - start;\n    \n    return {\n      status: latency < 10 ? 'healthy' : 'degraded',\n      latency,\n      message: 'Redis operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Redis unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkPostgresHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await db.query('SELECT 1');\n    const latency = Date.now() - start;\n    \n    const poolSize = await db.query('SELECT count(*) FROM pg_stat_activity');\n    const maxConnections = await db.query('SHOW max_connections');\n    const utilization = poolSize.rows[0].count / maxConnections.rows[0].max_connections;\n    \n    return {\n      status: latency < 50 && utilization < 0.8 ? 'healthy' : 'degraded',\n      latency,\n      metadata: {\n        poolUtilization: `${(utilization * 100).toFixed(1)}%`,\n        activeConnections: poolSize.rows[0].count\n      },\n      message: 'Postgres operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Postgres unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkBloomFilterHealth(): Promise<ComponentHealth> {\n  const stats = blacklistBloomFilter.getStats();\n  const loadFactor = stats.entries / stats.capacity;\n  \n  return {\n    status: loadFactor < 0.8 ? 'healthy' : 'degraded',\n    metadata: {\n      entries: stats.entries,\n      capacity: stats.capacity,\n      loadFactor: `${(loadFactor * 100).toFixed(1)}%`,\n      lastSync: stats.lastSync\n    },\n    message: loadFactor < 0.8 ? 'Bloom filter healthy' : 'Bloom filter approaching capacity'\n  };\n}\n\ninterface ComponentHealth {\n  status: 'healthy' | 'degraded' | 'unhealthy';\n  latency?: number | null;\n  metadata?: Record<string, any>;\n  message: string;\n}\n```\n\n## Rationale\n\n### Why This Design is Production-Ready\n\n1. **Cross-Region Consistency Model**: Eventually consistent blacklist with synchronous checks for high-risk operations balances global scalability with security. The 50-500ms propagation window is acceptable because:\n   - Access tokens are short-lived (15 min)\n   - High-risk operations check PRIMARY Postgres (strong consistency)\n   - Compensating controls (user status, token version) provide defense-in-depth\n\n2. **Postgres as Source of Truth**: All security-critical state lives in Postgres first. Caches are performance optimizations, not correctness requirements. This means:\n   - Redis failure = degraded performance, not security breach\n   - Cross-region eventually consistent, but single source of truth\n   - Simpler reasoning about correctness under failures\n\n3. **Risk-Based Blacklist Checking**: Not all operations need synchronous global checks. By categorizing requests by risk level, we can:\n   - Serve 99.9% of requests from local cache (low latency)\n   - Only pay cross-region latency for truly high-risk operations\n   - Maintain security without sacrificing performance\n\n4. **Comprehensive Failure Testing**: The load test plan covers realistic failure modes:\n   - Redis failures (most common infrastructure failure)\n   - Postgres connection exhaustion (scaling bottleneck)\n   - Circuit breaker flapping (network instability)\n   - Cross-region edge cases (user mobility)\n\n5. **Operational Observability**: The monitoring and runbook approach ensures:\n   - Incidents detected within 1 minute\n   - Clear mitigation steps for on-call engineers\n   - No \"tribal knowledge\" required (runbooks are exhaustive)\n   - Post-mortem culture built into incident response\n\n## What I Changed\n\n### From Previous Design\n\n1. **Added Multi-Region Support**: Complete cross-region architecture with Redis Streams + Postgres WAL for blacklist propagation (addresses open question #1)\n\n2. **Enhanced Load Testing Plan**: 7 comprehensive load test scenarios covering all critical failure modes (addresses open question #2)\n\n3. **Operational Runbooks**: 3 detailed incident response playbooks with specific commands and thresholds (addresses open question #3)\n\n4. **Risk-Based Blacklist Checking**: Synchronous vs asynchronous blacklist checks based on operation risk level (solves consistency/latency tradeoff)\n\n5. **User Travel Pattern Tracking**: `user_region_history` table tracks legitimate cross-region usage to reduce false positives on fingerprint mismatches\n\n6. **Region-Aware Token Schema**: Added `issued_in_region` and `used_in_region` columns to detect cross-region token reuse attacks\n\n7. **Health Check Endpoints**: Comprehensive `/health` endpoint with component-level health and degradation detection\n\n8. **Monitoring Dashboard**: Complete Grafana dashboard with 8 panels and 7 alerts covering all critical metrics\n\n9. **Chaos Engineering Scenarios**: 3 chaos tests for network partitions, database failovers, and memory pressure\n\n10. **Cross-Region Lock Strategy**: Redis + Postgres advisory lock fallback for distributed token refresh locking\n\n## What I Kept\n\nThe previous design's core resilience patterns were excellent:\n\n- \u2705 Multi-layer caching (Redis \u2192 Local \u2192 Postgres)\n- \u2705 Circuit breaker with refined thresholds (30% over 50 requests)\n- \u2705 Bloom filter for blacklist (space-efficient, no false negatives)\n- \u2705 Write buffering with batching (10-100x write reduction)\n- \u2705 Row-level locking for refresh tokens (better than advisory locks)\n- \u2705 Compensating controls for token validation (user status, token version, session checks)\n- \u2705 Security event logging for audit trails\n- \u2705 Dynamic cache sizing based on available memory\n\n## Open Questions\n\n### 1. Blacklist Propagation Delay Tolerance\n\n**Question**: Is 50-500ms blacklist propagation delay acceptable for your threat model?\n\n**Context**: \n- p50: 50ms (Redis Streams)\n- p99: 500ms (includes Postgres replication)\n- p99.9: 2 seconds (degraded network)\n\n**Tradeoffs**:\n- \u2705 **Accept delay**: 99.9% of requests served from local cache (low latency)\n- \u274c **Reject delay**: Every logout requires synchronous cross-region write (high latency)\n\n**My recommendation**: Accept delay because:\n- Access tokens expire in 15 minutes anyway (limited blast radius)\n- High-risk operations check PRIMARY Postgres (strong consistency)\n- Compensating controls prevent most attack scenarios\n\n**But you should consider rejecting if**:\n- You handle extremely sensitive data (healthcare, finance)\n- Regulatory requirements mandate immediate revocation\n- You can tolerate 200ms+ p99 latency on every logout\n\n---\n\n### 2. Postgres Connection Pool Sizing\n\n**Question**: What is your target Postgres capacity?\n\n**Current design assumes**:\n- 10 app instances \u00d7 50 connections = 500 connections\n- Postgres `max_connections = 1000` (requires tuning from default 100)\n- PgBouncer in transaction mode for connection pooling\n\n**At 100M users**:\n- 278 refreshes/sec (normal) \u2192 2,780 refreshes/sec (10x scale)\n- Postgres write capacity: ~10,000 writes/sec (need partitioning)\n\n**Decision point**: Do you want to:\n- Option A: Vertical scaling (larger Postgres instance up to 10M users)\n- Option B: Horizontal scaling (partition refresh_tokens by user_id range)\n- Option C: Separate write-optimized database (e.g., Cassandra for audit logs)\n\n**My recommendation**: Start with Option A (vertical scaling) until 10M users, then add partitioning (Option B).\n\n---\n\n### 3. Bloom Filter Capacity Planning\n\n**Question**: What is your expected blacklist size?\n\n**Current design**:\n- 1M entry capacity\n- 0.01% false positive rate\n- ~2.4 MB memory per instance\n\n**Scaling calculation**:\n- 10M users \u00d7 2 tokens/day \u00d7 15-min lifetime = ~208K concurrent entries \u2705\n- 100M users = ~2M concurrent entries \u26a0\ufe0f (need larger bloom filter or partitioning)\n\n**Decision point**: Should bloom filter be:\n- Option A: Single large filter (simple, 10 MB memory for 100M users)\n- Option B: Partitioned by time bucket (only load recent entries, more complex)\n- Option C: Distributed bloom filter (sharded across instances, even more complex)\n\n**My recommendation**: Option A (single large filter) is fine until 100M users. 10 MB per instance is negligible.\n\n## Convergence Status\n\n**PROPOSING_FINAL** - This design is complete and production-ready for:\n\n\u2705 **Single-region deployments** (original scope)\n\u2705 **Multi-region deployments** (new scope addressed)\n\u2705 **Load testing plan** (comprehensive scenarios)\n\u2705 **Operational runbooks** (detailed incident response)\n\u2705 **Monitoring & observability** (dashboards + alerts)\n\u2705 **Chaos engineering** (failure scenarios)\n\nThe design has been iterated 3+ times and addresses all critical concerns:\n- \u2705 Cross-region token refresh security\n- \u2705 Blacklist propagation strategy\n- \u2705 Postgres fallback performance\n- \u2705 Circuit breaker tuning\n- \u2705 Operational procedures\n\n**Remaining decisions** are deployment-specific (connection pool sizing, bloom filter capacity) and can be tuned based on your scale.\n\n---\n\n##",
        "prompt_for_other": "This design is **READY FOR IMPLEMENTATION**. Please validate:\n\n1. **Cross-Region Propagation**: Is the 50-500ms blacklist propagation delay acceptable for your security requirements? Do you need stronger consistency guarantees?\n\n2. **Load Test Coverage**: Are there any additional failure scenarios you want tested that aren't covered in the 7 load test scenarios + 3 chaos tests?\n\n3. **Operational Procedures**: Are the runbooks detailed enough for your on-call engineers? Do you have any existing runbook templates I should follow?\n\n4. **Scale Assumptions**: I designed for 10M users (vertical scaling) \u2192 100M users (partitioning required). Is this the right scale target?\n\nIf you agree this design is complete, signal **ACCEPTING_FINAL** and we can move to implementation planning.",
        "convergence_signal": "PROPOSING_FINAL",
        "raw_response": "## Design\n\n# JWT Authentication System with Global Distribution & Production Hardening\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        GLOBAL REGIONS                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502   US-EAST        \u2502         \u2502   EU-WEST        \u2502            \u2502\n\u2502  \u2502                  \u2502         \u2502                  \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 API Gateway\u2502  \u2502         \u2502  \u2502 API Gateway\u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Redis      \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Redis      \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 Regional   \u2502  \u2502 Sync    \u2502  \u2502 Regional   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 App Nodes  \u2502  \u2502         \u2502  \u2502 App Nodes  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (3+ pods)  \u2502  \u2502         \u2502  \u2502 (3+ pods)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Postgres   \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Postgres   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (Primary)  \u2502  \u2502 Replica \u2502  \u2502 (Replica)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502           \u2193                             \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502         Global Blacklist Propagation Bus             \u2502     \u2502\n\u2502  \u2502         (Pub/Sub: Redis Streams + Postgres WAL)     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Multi-Region Strategy with Consistency Guarantees\n\n### 1. Token Types and Regional Behavior\n\n**Access Tokens** (15-minute lifetime):\n- \u2705 **Stateless validation** - verify signature + check local blacklist\n- \u2705 **Regional Redis cache** - 99.9% of validations hit local cache\n- \u26a0\ufe0f **Blacklist propagation delay** - acceptable (see security analysis)\n\n**Refresh Tokens** (30-day lifetime):\n- \u2705 **Always write to PRIMARY Postgres** (strong consistency)\n- \u2705 **Refresh allowed in any region** (global mobility)\n- \u2705 **Token family tracking** - detects cross-region reuse attacks\n\n### 2. Blacklist Propagation Strategy\n\n**Design Principle**: **\"Eventually consistent blacklist with risk-based synchronous verification\"**\n\n```typescript\ninterface BlacklistPropagationConfig {\n  // How fast does blacklist propagate between regions?\n  propagationTarget: {\n    p50: 50,      // 50ms - Redis Streams\n    p99: 500,     // 500ms - includes Postgres replication\n    p99.9: 2000   // 2 seconds - degraded network\n  };\n  \n  // Which operations require synchronous cross-region check?\n  requireSyncCheck: {\n    highRiskOperations: true,    // Admin actions, delete account, payment\n    tokenAge: '<5min',           // Recently issued tokens (likely from logout)\n    userRiskScore: '>0.7',       // Suspicious activity score\n    crossRegionRefresh: true     // User moved to different region\n  };\n  \n  // Fallback behavior during propagation delay\n  acceptanceCriteria: {\n    maxAge: 300000,              // 5 minutes since token issued\n    requireCompensatingControls: true,\n    logSecurityEvent: true\n  };\n}\n\n// Global blacklist propagation bus\nclass GlobalBlacklistBus {\n  private regionalRedis: Map<Region, RedisClient>;\n  private postgresWalSubscription: PostgresReplicationClient;\n  private publishQueue: RedisStream;\n  \n  constructor() {\n    // Subscribe to Postgres WAL for blacklist INSERTs\n    this.postgresWalSubscription = new PostgresReplicationClient({\n      tables: ['token_blacklist'],\n      operations: ['INSERT'],\n      onMessage: (change) => this.propagateBlacklistEntry(change)\n    });\n    \n    // Redis Streams for cross-region pub/sub\n    this.publishQueue = new RedisStream('blacklist:global');\n  }\n  \n  async blacklistToken(\n    jti: string, \n    userId: string, \n    reason: string,\n    metadata: BlacklistMetadata\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    // 1. Write to PRIMARY Postgres immediately (source of truth)\n    const expiresAt = new Date(Date.now() + 15 * 60 * 1000);\n    await db.query(\n      `INSERT INTO token_blacklist \n       (jti, user_id, expires_at, reason, blacklisted_at, source_region, session_id)\n       VALUES ($1, $2, $3, $4, NOW(), $5, $6)`,\n      [jti, userId, expiresAt, reason, getCurrentRegion(), metadata.sessionId]\n    );\n    \n    logger.info('Token blacklisted in primary DB', {\n      jti,\n      userId,\n      reason,\n      latency: performance.now() - startTime\n    });\n    \n    // 2. Publish to Redis Stream for immediate regional propagation\n    await this.publishQueue.add('blacklist', {\n      jti,\n      userId,\n      expiresAt: expiresAt.toISOString(),\n      reason,\n      sourceRegion: getCurrentRegion(),\n      timestamp: Date.now()\n    });\n    \n    // 3. Update local region Redis immediately\n    await this.regionalRedis.get(getCurrentRegion())?.setex(\n      `blacklist:jti:${jti}`,\n      15 * 60,\n      JSON.stringify({ userId, reason, blacklistedAt: new Date() })\n    );\n    \n    // 4. Add to local bloom filter immediately\n    blacklistBloomFilter.add(jti);\n    \n    metrics.histogram('blacklist.propagation.latency', performance.now() - startTime);\n  }\n  \n  async propagateBlacklistEntry(entry: BlacklistEntry): Promise<void> {\n    // Propagate to all regional Redis clusters\n    const propagationPromises = Array.from(this.regionalRedis.entries())\n      .filter(([region]) => region !== entry.sourceRegion) // Skip source\n      .map(([region, redis]) => \n        this.propagateToRegion(region, redis, entry)\n      );\n    \n    await Promise.allSettled(propagationPromises);\n  }\n  \n  private async propagateToRegion(\n    region: Region,\n    redis: RedisClient,\n    entry: BlacklistEntry\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    try {\n      await redis.setex(\n        `blacklist:jti:${entry.jti}`,\n        entry.ttlSeconds,\n        JSON.stringify(entry)\n      );\n      \n      metrics.histogram('blacklist.cross_region_propagation', \n        performance.now() - startTime,\n        { source: entry.sourceRegion, target: region }\n      );\n      \n    } catch (error) {\n      logger.error('Failed to propagate blacklist to region', {\n        region,\n        jti: entry.jti,\n        error: error.message\n      });\n      \n      metrics.increment('blacklist.propagation.failure', {\n        source: entry.sourceRegion,\n        target: region\n      });\n      \n      // Not critical - Postgres replication will eventually sync\n    }\n  }\n  \n  // Check blacklist with cross-region awareness\n  async checkBlacklist(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): Promise<BlacklistCheckResult> {\n    \n    // 1. Check local Redis first (99.9% of requests)\n    const localResult = await this.regionalRedis.get(currentRegion)?.get(\n      `blacklist:jti:${jti}`\n    );\n    \n    if (localResult !== null) {\n      return { \n        blacklisted: true, \n        source: 'local_redis',\n        latency: 1 \n      };\n    }\n    \n    // 2. Determine if we need synchronous cross-region check\n    const needsSyncCheck = this.shouldCheckAllRegions(\n      jti,\n      riskLevel,\n      tokenIssuedAt,\n      currentRegion,\n      tokenIssuedInRegion\n    );\n    \n    if (!needsSyncCheck) {\n      // Trust local cache (low risk)\n      return { \n        blacklisted: false, \n        source: 'local_redis',\n        latency: 1\n      };\n    }\n    \n    // 3. Check PRIMARY Postgres (authoritative source)\n    // This handles cross-region edge cases\n    const startTime = performance.now();\n    const pgResult = await db.query({\n      name: 'check_blacklist_global',\n      text: `SELECT 1 FROM token_blacklist \n             WHERE jti = $1 AND expires_at > NOW() \n             LIMIT 1`,\n      values: [jti]\n    });\n    \n    const latency = performance.now() - startTime;\n    \n    metrics.histogram('blacklist.cross_region_check.latency', latency, {\n      risk_level: riskLevel,\n      cross_region: tokenIssuedInRegion !== currentRegion\n    });\n    \n    return {\n      blacklisted: pgResult.rowCount > 0,\n      source: 'postgres_primary',\n      latency\n    };\n  }\n  \n  private shouldCheckAllRegions(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): boolean {\n    \n    // Always check Postgres for high-risk operations\n    if (riskLevel === 'high') {\n      return true;\n    }\n    \n    // Check if token was issued in different region (possible logout in other region)\n    if (tokenIssuedInRegion && tokenIssuedInRegion !== currentRegion) {\n      return true;\n    }\n    \n    // Check if token is very recently issued (< 5 minutes)\n    // Likely from recent logout that may not have propagated yet\n    const tokenAge = Date.now() - tokenIssuedAt;\n    if (tokenAge < 5 * 60 * 1000) {\n      return true;\n    }\n    \n    return false;\n  }\n}\n\ninterface BlacklistEntry {\n  jti: string;\n  userId: string;\n  expiresAt: string;\n  reason: string;\n  sourceRegion: Region;\n  ttlSeconds: number;\n  timestamp: number;\n  sessionId?: string;\n}\n\ninterface BlacklistMetadata {\n  sessionId?: string;\n  deviceId?: string;\n  ipAddress?: string;\n  userAgent?: string;\n}\n\ninterface BlacklistCheckResult {\n  blacklisted: boolean;\n  source: 'local_redis' | 'remote_redis' | 'postgres_primary' | 'bloom_filter';\n  latency: number;\n}\n\ntype Region = 'us-east' | 'us-west' | 'eu-west' | 'ap-southeast' | 'ap-northeast';\n\nfunction getCurrentRegion(): Region {\n  return process.env.AWS_REGION as Region || 'us-east';\n}\n```\n\n### 3. Cross-Region Token Refresh Strategy\n\n**Challenge**: User logs in US-EAST, travels to EU-WEST, refreshes token.\n\n**Requirements**:\n- \u2705 Refresh must succeed (user mobility)\n- \u2705 Prevent token reuse attacks across regions\n- \u2705 Detect compromised token families\n\n**Solution**: **Regional read replicas + primary write with conflict detection**\n\n```typescript\nasync function performCrossRegionRefresh(\n  refreshToken: string,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<AuthResponse> {\n  \n  const tokenHash = hashToken(refreshToken);\n  const startTime = performance.now();\n  \n  // 1. Acquire distributed lock (prefer local Redis, fallback to Postgres)\n  const lockAcquired = await acquireCrossRegionLock(tokenHash, currentRegion);\n  \n  if (!lockAcquired) {\n    throw new AuthError('CONCURRENT_REFRESH_IN_PROGRESS');\n  }\n  \n  try {\n    // 2. Read token from PRIMARY Postgres (not replica)\n    // This ensures we see the absolute latest state across all regions\n    const client = await getPrimaryDbClient();\n    \n    await client.query('BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE');\n    \n    const tokenResult = await client.query(\n      `SELECT * FROM refresh_tokens \n       WHERE token_hash = $1 \n       FOR UPDATE`,\n      [tokenHash]\n    );\n    \n    if (tokenResult.rowCount === 0) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n    \n    const tokenData = tokenResult.rows[0];\n    \n    // 3. Enhanced validation for cross-region refresh\n    await validateRefreshToken(tokenData, fingerprint, currentRegion);\n    \n    // 4. Check for token reuse (CRITICAL for cross-region security)\n    if (tokenData.used) {\n      // Token reuse detected - revoke entire family\n      await handleTokenFamilyCompromise(\n        tokenData.token_family_id,\n        'CROSS_REGION_TOKEN_REUSE',\n        client\n      );\n      \n      throw new AuthError('TOKEN_REUSE_DETECTED');\n    }\n    \n    // 5. Mark token as used (in PRIMARY database)\n    await client.query(\n      `UPDATE refresh_tokens \n       SET used = true, \n           used_at = NOW(),\n           used_in_region = $2\n       WHERE token_hash = $1`,\n      [tokenHash, currentRegion]\n    );\n    \n    // 6. Generate new token pair\n    const user = await getUserById(tokenData.user_id);\n    \n    const newAccessToken = generateAccessToken(user, {\n      sessionId: tokenData.session_id,\n      deviceId: tokenData.device_id,\n      tokenVersion: user.token_version,\n      issuedInRegion: currentRegion,  // NEW: Track issuing region\n      previousRegion: tokenData.issued_in_region\n    });\n    \n    const newRefreshToken = await generateAndStoreRefreshToken(\n      user,\n      {\n        sessionId: tokenData.session_id,\n        deviceId: tokenData.device_id,\n        deviceFingerprint: fingerprint,\n        parentTokenId: tokenData.id,\n        tokenFamilyId: tokenData.token_family_id,\n        timesRefreshed: tokenData.times_refreshed + 1,\n        issuedInRegion: currentRegion,  // NEW: Track issuing region\n        previousRegion: tokenData.issued_in_region\n      },\n      client\n    );\n    \n    await client.query('COMMIT');\n    \n    // 7. Update regional cache (eventual consistency is OK here)\n    try {\n      await cache.set(\n        `refresh:${tokenHash}:used`,\n        true,\n        900\n      );\n    } catch {\n      // Not critical\n    }\n    \n    metrics.histogram('auth.cross_region_refresh.latency', \n      performance.now() - startTime,\n      { \n        from_region: tokenData.issued_in_region,\n        to_region: currentRegion\n      }\n    );\n    \n    return {\n      accessToken: newAccessToken.token,\n      refreshToken: newRefreshToken.token,\n      expiresIn: 900,\n      tokenType: 'Bearer',\n      refreshExpiresIn: 2592000\n    };\n    \n  } finally {\n    await releaseCrossRegionLock(tokenHash, currentRegion);\n  }\n}\n\nasync function validateRefreshToken(\n  tokenData: RefreshTokenRow,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<void> {\n  \n  // 1. Check expiry\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  // 2. Check revocation\n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Validate fingerprint\n  const fingerprintHash = hashFingerprint(fingerprint);\n  if (tokenData.device_fingerprint_hash !== fingerprintHash) {\n    \n    // Cross-region fingerprint mismatch could be legitimate (VPN, proxy)\n    // Check if user has history of cross-region usage\n    const userTravelPattern = await getUserTravelPattern(tokenData.user_id);\n    \n    if (!userTravelPattern.crossRegionAllowed) {\n      // User has never used service from multiple regions - suspicious\n      await createSecurityEvent({\n        userId: tokenData.user_id,\n        eventType: 'CROSS_REGION_FINGERPRINT_MISMATCH',\n        severity: 'HIGH',\n        details: {\n          originalRegion: tokenData.issued_in_region,\n          currentRegion,\n          originalFingerprint: tokenData.device_fingerprint_hash.substring(0, 16),\n          currentFingerprint: fingerprintHash.substring(0, 16)\n        }\n      });\n      \n      throw new AuthError('FINGERPRINT_MISMATCH');\n    }\n    \n    // Log but allow (user has established cross-region pattern)\n    logger.warn('Cross-region fingerprint mismatch allowed', {\n      userId: tokenData.user_id,\n      fromRegion: tokenData.issued_in_region,\n      toRegion: currentRegion\n    });\n  }\n  \n  // 4. Check refresh count (detect infinite refresh loops)\n  if (tokenData.times_refreshed > 1000) {\n    // Suspicious - single refresh chain shouldn't exceed 1000 refreshes\n    // (30 days / 15 min = 2880 max normal refreshes, but user should re-login)\n    await createSecurityEvent({\n      userId: tokenData.user_id,\n      eventType: 'EXCESSIVE_REFRESH_COUNT',\n      severity: 'MEDIUM',\n      details: {\n        timesRefreshed: tokenData.times_refreshed,\n        tokenFamilyId: tokenData.token_family_id\n      }\n    });\n    \n    throw new AuthError('REFRESH_LIMIT_EXCEEDED');\n  }\n}\n\n// Cross-region distributed lock using Redis + Postgres fallback\nasync function acquireCrossRegionLock(\n  tokenHash: string,\n  region: Region,\n  timeoutMs: number = 5000\n): Promise<boolean> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  const lockValue = `${region}:${uuidv4()}:${Date.now()}`;\n  \n  // Try regional Redis first\n  const regionalRedis = getRegionalRedis(region);\n  \n  try {\n    const acquired = await regionalRedis.set(\n      lockKey,\n      lockValue,\n      'NX',\n      'PX',\n      timeoutMs\n    );\n    \n    if (acquired) {\n      return true;\n    }\n  } catch (error) {\n    logger.warn('Regional Redis lock failed, trying Postgres', {\n      tokenHash: tokenHash.substring(0, 16),\n      region,\n      error: error.message\n    });\n  }\n  \n  // Fallback to Postgres advisory lock\n  // Use hash of token as lock ID (deterministic across regions)\n  const lockId = hashToInt64(tokenHash);\n  \n  const result = await db.query(\n    'SELECT pg_try_advisory_lock($1) as acquired',\n    [lockId]\n  );\n  \n  return result.rows[0].acquired;\n}\n\nasync function releaseCrossRegionLock(\n  tokenHash: string,\n  region: Region\n): Promise<void> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  \n  // Try Redis first\n  try {\n    const regionalRedis = getRegionalRedis(region);\n    await regionalRedis.del(lockKey);\n  } catch (error) {\n    // Fallback to Postgres advisory lock release\n    const lockId = hashToInt64(tokenHash);\n    await db.query('SELECT pg_advisory_unlock($1)', [lockId]);\n  }\n}\n```\n\n### 4. Enhanced Schema for Multi-Region Support\n\n```sql\n-- Refresh tokens table with region tracking\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  token_hash VARCHAR(64) NOT NULL UNIQUE,\n  user_id UUID NOT NULL REFERENCES users(id),\n  session_id UUID NOT NULL,\n  device_id VARCHAR(255) NOT NULL,\n  device_fingerprint_hash VARCHAR(64) NOT NULL,\n  \n  -- NEW: Region tracking\n  issued_in_region VARCHAR(50) NOT NULL,\n  used_in_region VARCHAR(50),\n  \n  expires_at TIMESTAMP NOT NULL,\n  issued_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  used BOOLEAN DEFAULT FALSE,\n  used_at TIMESTAMP,\n  revoked BOOLEAN DEFAULT FALSE,\n  revoked_at TIMESTAMP,\n  \n  -- Token family tracking\n  token_family_id UUID NOT NULL,\n  parent_token_id UUID REFERENCES refresh_tokens(id),\n  times_refreshed INTEGER DEFAULT 0,\n  \n  -- Audit\n  ip_address INET,\n  user_agent TEXT,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Composite index for cross-region lookups\nCREATE INDEX idx_refresh_tokens_hash_region ON refresh_tokens(token_hash, issued_in_region);\nCREATE INDEX idx_refresh_tokens_family ON refresh_tokens(token_family_id, issued_at DESC);\nCREATE INDEX idx_refresh_tokens_user_active ON refresh_tokens(user_id, expires_at) \n  WHERE NOT used AND NOT revoked;\n\n-- Token blacklist with region tracking\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  reason VARCHAR(100),\n  session_id UUID,\n  \n  -- NEW: Region tracking for propagation monitoring\n  source_region VARCHAR(50) NOT NULL,\n  blacklisted_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  \n  ip_address INET,\n  user_agent TEXT\n);\n\n-- Index for cross-region blacklist checks\nCREATE INDEX idx_blacklist_active ON token_blacklist(jti, expires_at)\n  WHERE expires_at > NOW();\nCREATE INDEX idx_blacklist_user_recent ON token_blacklist(user_id, revoked_at DESC);\nCREATE INDEX idx_blacklist_region ON token_blacklist(source_region, blacklisted_at DESC);\n\n-- User travel patterns (for risk assessment)\nCREATE TABLE user_region_history (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id),\n  region VARCHAR(50) NOT NULL,\n  first_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  last_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  request_count INTEGER DEFAULT 1,\n  \n  UNIQUE(user_id, region)\n);\n\nCREATE INDEX idx_user_region_history_user ON user_region_history(user_id, last_seen DESC);\n```\n\n## Load Testing & Failure Scenarios\n\n### 1. Load Testing Plan\n\n```typescript\n// Load test scenarios\nconst loadTestScenarios = [\n  {\n    name: 'Peak Traffic - Normal Operation',\n    duration: '10min',\n    rps: 100000, // 100K requests/sec\n    operations: {\n      tokenValidation: 0.70,  // 70% reads (API calls)\n      tokenRefresh: 0.25,     // 25% refresh operations\n      logout: 0.05            // 5% logout (blacklist writes)\n    },\n    assertions: {\n      p99Latency: '<50ms',\n      errorRate: '<0.01%',\n      redisHitRate: '>99.5%'\n    }\n  },\n  \n  {\n    name: 'Redis Failure - Circuit Breaker Activation',\n    duration: '5min',\n    setup: 'Stop Redis at t=60s, restart at t=180s',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerOpens: '<10s after Redis down',\n      fallbackLatency: '<100ms p99',\n      errorRate: '<1%',\n      recoveryTime: '<30s after Redis up',\n      noDataLoss: 'All blacklist entries in Postgres'\n    }\n  },\n  \n  {\n    name: 'Postgres Connection Pool Exhaustion',\n    duration: '3min',\n    setup: 'max_connections=100, 20 app instances',\n    rps: 10000,\n    operations: {\n      // Force all requests to Postgres (Redis disabled)\n      tokenValidation: 0.90,\n      tokenRefresh: 0.10\n    },\n    assertions: {\n      connectionPoolUtilization: '<80%',\n      p99Latency: '<200ms',\n      errorRate: '<5%',\n      noConnectionLeaks: 'Pool size stable'\n    }\n  },\n  \n  {\n    name: 'Circuit Breaker Flapping',\n    duration: '10min',\n    setup: 'Intermittent Redis failures (30s down, 30s up)',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerFlaps: '<5 times',\n      halfOpenDuration: '<30s',\n      errorRate: '<2%',\n      cacheCoherence: 'Local cache syncs after recovery'\n    }\n  },\n  \n  {\n    name: 'Cross-Instance Rate Limit Bypass',\n    duration: '5min',\n    setup: '10 app instances, rate limit 100 req/min per user',\n    rps: 5000,\n    operations: {\n      // Single user attacking across all instances\n      singleUserAttack: true,\n      targetRate: 1000 // 10x rate limit\n    },\n    assertions: {\n      effectiveRateLimit: '<150 req/min', // Allow 50% overage\n      postgresSync: 'Within 10 seconds',\n      blockAfterSync: 'Within 1 request'\n    }\n  },\n  \n  {\n    name: 'Cross-Region Token Refresh',\n    duration: '10min',\n    setup: 'Users refresh from different region than login',\n    rps: 10000,\n    operations: {\n      loginRegion: 'us-east',\n      refreshRegion: 'eu-west',\n      percentCrossRegion: 50\n    },\n    assertions: {\n      crossRegionLatency: '<200ms p99',\n      tokenReuseDetection: '100%',\n      blacklistPropagation: '<500ms p99',\n      noFalsePositives: 'Zero legitimate refreshes blocked'\n    }\n  },\n  \n  {\n    name: 'Bloom Filter False Positive Rate',\n    duration: '30min',\n    setup: '1M active tokens, 100K blacklisted',\n    rps: 100000,\n    operations: {\n      tokenValidation: 1.0\n    },\n    assertions: {\n      bloomFalsePositiveRate: '<0.01%',\n      postgresCheckRate: '<10 req/s',\n      noFalseNegatives: 'Zero blacklisted tokens accepted'\n    }\n  }\n];\n\n// Chaos engineering scenarios\nconst chaosScenarios = [\n  {\n    name: 'Network Partition - Region Isolation',\n    setup: 'Isolate EU-WEST region for 60 seconds',\n    expectedBehavior: [\n      'EU-WEST continues serving from local Redis + Postgres replica',\n      'New blacklists in US-EAST do not propagate to EU-WEST',\n      'High-risk operations in EU-WEST check PRIMARY Postgres',\n      'After partition heals, blacklists propagate within 5 seconds'\n    ]\n  },\n  \n  {\n    name: 'Postgres Primary Failover',\n    setup: 'Promote replica to primary (30s downtime)',\n    expectedBehavior: [\n      'Token refreshes fail during failover window',\n      'Token validation continues (cached data)',\n      'After promotion, refreshes resume within 10 seconds',\n      'No token reuse vulnerabilities introduced'\n    ]\n  },\n  \n  {\n    name: 'Memory Pressure - Cache Eviction',\n    setup: 'Fill LRU cache to capacity, continue adding entries',\n    expectedBehavior: [\n      'LRU evicts oldest entries',\n      'Cache hit rate remains >95%',\n      'No memory leak (heap size stable)',\n      'Evicted entries refetched from Redis/Postgres'\n    ]\n  }\n];\n```\n\n### 2. Load Testing Implementation\n\n```typescript\n// Load testing harness using k6 or artillery\nimport { check, group, sleep } from 'k6';\nimport http from 'k6/http';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 50000 }, // Ramp up\n    { duration: '5m', target: 100000 }, // Peak\n    { duration: '2m', target: 0 }     // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(99)<50'],\n    http_req_failed: ['rate<0.0001'],\n    'circuit_breaker_open': ['count<1']\n  }\n};\n\nexport default function() {\n  const scenario = Math.random();\n  \n  if (scenario < 0.70) {\n    // Token validation (70% of traffic)\n    testTokenValidation();\n  } else if (scenario < 0.95) {\n    // Token refresh (25% of traffic)\n    testTokenRefresh();\n  } else {\n    // Logout (5% of traffic)\n    testLogout();\n  }\n  \n  sleep(Math.random() * 2); // Random delay 0-2s\n}\n\nfunction testTokenValidation() {\n  const token = __ENV.TEST_ACCESS_TOKEN;\n  \n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${token}` }\n  });\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'latency < 50ms': (r) => r.timings.duration < 50,\n    'cache hit': (r) => r.headers['X-Cache-Layer'] === 'redis'\n  });\n}\n\nfunction testTokenRefresh() {\n  const refreshToken = __ENV.TEST_REFRESH_TOKEN;\n  \n  const res = http.post('https://api.example.com/auth/refresh', \n    JSON.stringify({ refreshToken }),\n    { headers: { 'Content-Type': 'application/json' } }\n  );\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'returns new tokens': (r) => r.json('accessToken') !== undefined,\n    'latency < 100ms': (r) => r.timings.duration < 100\n  });\n}\n\n// Failure injection for chaos testing\nexport function injectRedisFailure() {\n  // Use Kubernetes pod deletion or network policy\n  exec(`kubectl delete pod -l app=redis-cache -n production`);\n  \n  sleep(10); // Wait for circuit breaker to trip\n  \n  // Verify fallback behavior\n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${__ENV.TEST_ACCESS_TOKEN}` }\n  });\n  \n  check(res, {\n    'fallback successful': (r) => r.status === 200,\n    'postgres fallback used': (r) => r.headers['X-Cache-Layer'] === 'postgres'\n  });\n}\n```\n\n## Operational Runbook\n\n### 1. Incident Response Playbooks\n\n```yaml\n# Runbook: Redis Unavailable (Circuit Breaker Open)\n\nincident_type: \"Redis Unavailable\"\nseverity: P2 (High)\ndetection: Alert \"RedisCircuitBreakerOpen\" fires\n\nimmediate_actions:\n  - verify_fallback:\n      cmd: \"curl https://api.example.com/health/fallback\"\n      expected: '{\"status\":\"degraded\",\"fallback\":\"postgres\",\"latency_p99\":150}'\n      \n  - check_postgres_load:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'active'\"\n      threshold: \"< 80% of max_connections\"\n      \n  - verify_no_data_loss:\n      cmd: \"SELECT COUNT(*) FROM token_blacklist WHERE blacklisted_at > NOW() - INTERVAL '5 minutes'\"\n      compare_with: \"Redis blacklist count (should match)\"\n\ninvestigation:\n  - check_redis_health:\n      - \"kubectl get pods -n production -l app=redis\"\n      - \"kubectl logs -n production -l app=redis --tail=100\"\n      - \"redis-cli -h redis.internal PING\"\n      \n  - check_sentinel_status:\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL masters\"\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL slaves auth-master\"\n      \n  - check_network:\n      - \"kubectl get networkpolicies -n production\"\n      - \"traceroute redis.internal\"\n\nmitigation:\n  - if_redis_pod_crash:\n      action: \"Kubernetes should auto-restart\"\n      verify: \"Wait 30s, check if circuit breaker closes\"\n      \n  - if_sentinel_failover:\n      action: \"Wait for automatic failover (< 30s)\"\n      verify: \"SENTINEL masters shows new master\"\n      \n  - if_persistent_failure:\n      action: \"Scale Postgres read replicas\"\n      cmd: \"kubectl scale deployment postgres-replica --replicas=5\"\n\nrecovery_verification:\n  - circuit_breaker_closed:\n      query: \"circuit_breaker_state{service='redis'} == 0\"\n      \n  - latency_normal:\n      query: \"histogram_quantile(0.99, cache_get_latency) < 10\"\n      \n  - error_rate_normal:\n      query: \"rate(http_requests_total{status=~'5..'}[5m]) < 0.001\"\n\npost_mortem:\n  - analyze_root_cause: true\n  - update_circuit_breaker_thresholds: \"if flapping observed\"\n  - review_postgres_capacity: \"if connection pool saturated\"\n```\n\n```yaml\n# Runbook: Bloom Filter False Positive Rate Spike\n\nincident_type: \"Bloom Filter False Positive Rate High\"\nseverity: P3 (Medium)\ndetection: Alert \"BloomFilterFalsePositiveRateHigh\" fires\n\nimmediate_actions:\n  - check_metrics:\n      query: \"rate(auth_bloom_filter_false_positive_total[5m])\"\n      threshold: \"> 0.01%\"\n      \n  - verify_no_false_negatives:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE event_type = 'BLACKLISTED_TOKEN_ACCEPTED' AND created_at > NOW() - INTERVAL '5 minutes'\"\n      expected: \"0\"\n\ninvestigation:\n  - check_bloom_filter_size:\n      cmd: \"curl http://api.example.com/internal/bloom-filter/stats\"\n      expected: '{\"entries\":100000,\"capacity\":1000000,\"load_factor\":0.1}'\n      \n  - check_sync_lag:\n      cmd: \"SELECT MAX(NOW() - blacklisted_at) FROM token_blacklist\"\n      threshold: \"< 60 seconds\"\n      \n  - analyze_blacklist_growth:\n      query: \"rate(token_blacklist_inserts_total[1h])\"\n      compare_with: \"Historical average\"\n\nmitigation:\n  - if_overloaded:\n      condition: \"load_factor > 0.8\"\n      action: \"Rebuild bloom filter with larger capacity\"\n      cmd: \"curl -X POST http://api.example.com/internal/bloom-filter/rebuild?capacity=10000000\"\n      \n  - if_sync_lag:\n      condition: \"sync lag > 60s\"\n      action: \"Trigger immediate sync from Postgres\"\n      cmd: \"curl -X POST http://api.example.com/internal/bloom-filter/sync\"\n\nrecovery_verification:\n  - false_positive_rate_normal:\n      query: \"rate(auth_bloom_filter_false_positive_total[5m]) < 0.0001\"\n      \n  - no_security_incidents:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE severity = 'HIGH' AND created_at > NOW() - INTERVAL '10 minutes'\"\n      expected: \"0\"\n\nprevention:\n  - implement_auto_scaling:\n      description: \"Automatically resize bloom filter when load_factor > 0.7\"\n      \n  - add_capacity_alerts:\n      alert: \"BloomFilterCapacityWarning\"\n      threshold: \"load_factor > 0.6\"\n```\n\n```yaml\n# Runbook: Postgres Fallback Latency High\n\nincident_type: \"Postgres Fallback Latency Exceeds SLA\"\nseverity: P2 (High)\ndetection: Alert \"PostgresFallbackLatencyHigh\" fires\n\nimmediate_actions:\n  - check_sla_breach:\n      query: \"histogram_quantile(0.99, blacklist_check_latency{source='postgres'}) > 200\"\n      \n  - verify_connection_pool:\n      cmd: \"SELECT count(*) FROM pg_stat_activity\"\n      threshold: \"< max_connections * 0.8\"\n      \n  - check_query_performance:\n      cmd: \"SELECT query, mean_exec_time, calls FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10\"\n\ninvestigation:\n  - analyze_slow_queries:\n      cmd: \"SELECT pid, query_start, state, query FROM pg_stat_activity WHERE state = 'active' AND query_start < NOW() - INTERVAL '1 second'\"\n      \n  - check_index_usage:\n      cmd: \"SELECT schemaname, tablename, indexname, idx_scan FROM pg_stat_user_indexes WHERE schemaname = 'public' AND idx_scan = 0\"\n      \n  - check_table_bloat:\n      cmd: \"SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) FROM pg_tables WHERE schemaname = 'public' ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\"\n\nmitigation:\n  - if_connection_pool_exhausted:\n      action: \"Increase connection pool size\"\n      cmd: \"kubectl set env deployment/api-server DB_POOL_SIZE=200\"\n      \n  - if_missing_index:\n      action: \"Create missing indexes\"\n      cmd: \"psql -c 'CREATE INDEX CONCURRENTLY idx_missing ON table(column)'\"\n      \n  - if_table_bloat:\n      action: \"Run VACUUM ANALYZE\"\n      cmd: \"psql -c 'VACUUM ANALYZE token_blacklist'\"\n      \n  - if_query_slow:\n      action: \"Optimize query or add caching\"\n      review: \"src/auth/blacklist-check.ts\"\n\nemergency_mitigation:\n  - reduce_postgres_checks:\n      description: \"Temporarily reduce risk threshold for Postgres checks\"\n      cmd: \"kubectl set env deployment/api-server BLACKLIST_CHECK_RISK_THRESHOLD=high\"\n      impact: \"Medium/low risk operations skip Postgres check (accept stale cache)\"\n      rollback: \"After Postgres performance recovers\"\n\nrecovery_verification:\n  - latency_within_sla:\n      query: \"histogram_quantile(0.99, blacklist_check_latency{source='postgres'}) < 100\"\n      \n  - connection_pool_healthy:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'idle'\"\n      expected: \"> 20% of pool size\"\n\npost_mortem:\n  - review_query_plans: true\n  - optimize_indexes: true\n  - consider_read_replicas: \"if load is consistently high\"\n  - review_cache_ttl: \"increase TTL to reduce Postgres load\"\n```\n\n### 2. Monitoring Dashboard\n\n```typescript\n// Grafana dashboard JSON\nconst authSystemDashboard = {\n  title: \"JWT Auth System - Production Monitoring\",\n  panels: [\n    {\n      title: \"Request Rate by Operation\",\n      targets: [\n        'sum(rate(http_requests_total{service=\"auth\"}[5m])) by (operation)',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Cache Hit Rate by Layer\",\n      targets: [\n        'sum(rate(cache_hit_total[5m])) by (layer) / sum(rate(cache_requests_total[5m])) by (layer)',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.95, color: \"green\" },\n        { value: 0.90, color: \"yellow\" },\n        { value: 0.85, color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Circuit Breaker State\",\n      targets: [\n        'circuit_breaker_state{service=\"redis\"}',\n      ],\n      visualization: \"stat\",\n      mappings: [\n        { value: 0, text: \"CLOSED\", color: \"green\" },\n        { value: 0.5, text: \"HALF_OPEN\", color: \"yellow\" },\n        { value: 1, text: \"OPEN\", color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Token Validation Latency (p50, p99, p99.9)\",\n      targets: [\n        'histogram_quantile(0.50, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.99, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.999, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Postgres Connection Pool Utilization\",\n      targets: [\n        'pg_stat_activity_count / pg_settings_max_connections',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.80, color: \"red\" },\n        { value: 0.60, color: \"yellow\" },\n        { value: 0.40, color: \"green\" }\n      ]\n    },\n    \n    {\n      title: \"Blacklist Propagation Latency (Cross-Region)\",\n      targets: [\n        'histogram_quantile(0.99, sum(rate(blacklist_cross_region_propagation_bucket[5m])) by (source, target, le))',\n      ],\n      visualization: \"heatmap\"\n    },\n    \n    {\n      title: \"Security Events (Last 1 Hour)\",\n      targets: [\n        'sum(increase(security_events_total[1h])) by (event_type)',\n      ],\n      visualization: \"table\"\n    },\n    \n    {\n      title: \"Bloom Filter Statistics\",\n      targets: [\n        'bloom_filter_entries',\n        'bloom_filter_capacity',\n        'bloom_filter_load_factor',\n        'rate(bloom_filter_false_positive_total[5m])',\n      ],\n      visualization: \"stat\"\n    }\n  ],\n  \n  alerts: [\n    {\n      name: \"RedisCircuitBreakerOpen\",\n      condition: \"circuit_breaker_state{service='redis'} == 1\",\n      for: \"1m\",\n      severity: \"critical\"\n    },\n    {\n      name: \"HighPostgresFallback\",\n      condition: \"rate(cache_fallback_postgres_total[5m]) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"TokenValidationLatencyHigh\",\n      condition: \"histogram_quantile(0.99, auth_token_validation_latency_bucket) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"BloomFilterFalsePositiveRateHigh\",\n      condition: \"rate(bloom_filter_false_positive_total[5m]) > 0.0001\",\n      for: \"10m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"PostgresConnectionPoolSaturated\",\n      condition: \"pg_stat_activity_count / pg_settings_max_connections > 0.8\",\n      for: \"5m\",\n      severity: \"critical\"\n    },\n    {\n      name: \"CrossRegionBlacklistPropagationSlow\",\n      condition: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) > 1000\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"SecurityEventSpike\",\n      condition: \"rate(security_events_total{severity='HIGH'}[5m]) > 10\",\n      for: \"2m\",\n      severity: \"critical\"\n    }\n  ]\n};\n```\n\n### 3. Health Check Endpoints\n\n```typescript\n// Health check implementation\napp.get('/health', async (req, res) => {\n  const health = {\n    status: 'healthy',\n    timestamp: new Date().toISOString(),\n    version: process.env.APP_VERSION,\n    uptime: process.uptime(),\n    \n    components: {\n      redis: await checkRedisHealth(),\n      postgres: await checkPostgresHealth(),\n      bloomFilter: await checkBloomFilterHealth()\n    },\n    \n    metrics: {\n      circuitBreakerState: circuitBreaker.getState(),\n      cacheHitRate: await getCacheHitRate(),\n      requestRate: await getRequestRate(),\n      errorRate: await getErrorRate()\n    }\n  };\n  \n  // Overall health based on components\n  if (health.components.redis.status === 'unhealthy' && \n      health.components.postgres.status === 'unhealthy') {\n    health.status = 'unhealthy';\n    return res.status(503).json(health);\n  }\n  \n  if (health.components.redis.status === 'unhealthy' ||\n      health.components.postgres.status === 'degraded') {\n    health.status = 'degraded';\n    return res.status(200).json(health);\n  }\n  \n  res.status(200).json(health);\n});\n\nasync function checkRedisHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await redis.ping();\n    const latency = Date.now() - start;\n    \n    return {\n      status: latency < 10 ? 'healthy' : 'degraded',\n      latency,\n      message: 'Redis operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Redis unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkPostgresHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await db.query('SELECT 1');\n    const latency = Date.now() - start;\n    \n    const poolSize = await db.query('SELECT count(*) FROM pg_stat_activity');\n    const maxConnections = await db.query('SHOW max_connections');\n    const utilization = poolSize.rows[0].count / maxConnections.rows[0].max_connections;\n    \n    return {\n      status: latency < 50 && utilization < 0.8 ? 'healthy' : 'degraded',\n      latency,\n      metadata: {\n        poolUtilization: `${(utilization * 100).toFixed(1)}%`,\n        activeConnections: poolSize.rows[0].count\n      },\n      message: 'Postgres operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Postgres unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkBloomFilterHealth(): Promise<ComponentHealth> {\n  const stats = blacklistBloomFilter.getStats();\n  const loadFactor = stats.entries / stats.capacity;\n  \n  return {\n    status: loadFactor < 0.8 ? 'healthy' : 'degraded',\n    metadata: {\n      entries: stats.entries,\n      capacity: stats.capacity,\n      loadFactor: `${(loadFactor * 100).toFixed(1)}%`,\n      lastSync: stats.lastSync\n    },\n    message: loadFactor < 0.8 ? 'Bloom filter healthy' : 'Bloom filter approaching capacity'\n  };\n}\n\ninterface ComponentHealth {\n  status: 'healthy' | 'degraded' | 'unhealthy';\n  latency?: number | null;\n  metadata?: Record<string, any>;\n  message: string;\n}\n```\n\n## Rationale\n\n### Why This Design is Production-Ready\n\n1. **Cross-Region Consistency Model**: Eventually consistent blacklist with synchronous checks for high-risk operations balances global scalability with security. The 50-500ms propagation window is acceptable because:\n   - Access tokens are short-lived (15 min)\n   - High-risk operations check PRIMARY Postgres (strong consistency)\n   - Compensating controls (user status, token version) provide defense-in-depth\n\n2. **Postgres as Source of Truth**: All security-critical state lives in Postgres first. Caches are performance optimizations, not correctness requirements. This means:\n   - Redis failure = degraded performance, not security breach\n   - Cross-region eventually consistent, but single source of truth\n   - Simpler reasoning about correctness under failures\n\n3. **Risk-Based Blacklist Checking**: Not all operations need synchronous global checks. By categorizing requests by risk level, we can:\n   - Serve 99.9% of requests from local cache (low latency)\n   - Only pay cross-region latency for truly high-risk operations\n   - Maintain security without sacrificing performance\n\n4. **Comprehensive Failure Testing**: The load test plan covers realistic failure modes:\n   - Redis failures (most common infrastructure failure)\n   - Postgres connection exhaustion (scaling bottleneck)\n   - Circuit breaker flapping (network instability)\n   - Cross-region edge cases (user mobility)\n\n5. **Operational Observability**: The monitoring and runbook approach ensures:\n   - Incidents detected within 1 minute\n   - Clear mitigation steps for on-call engineers\n   - No \"tribal knowledge\" required (runbooks are exhaustive)\n   - Post-mortem culture built into incident response\n\n## What I Changed\n\n### From Previous Design\n\n1. **Added Multi-Region Support**: Complete cross-region architecture with Redis Streams + Postgres WAL for blacklist propagation (addresses open question #1)\n\n2. **Enhanced Load Testing Plan**: 7 comprehensive load test scenarios covering all critical failure modes (addresses open question #2)\n\n3. **Operational Runbooks**: 3 detailed incident response playbooks with specific commands and thresholds (addresses open question #3)\n\n4. **Risk-Based Blacklist Checking**: Synchronous vs asynchronous blacklist checks based on operation risk level (solves consistency/latency tradeoff)\n\n5. **User Travel Pattern Tracking**: `user_region_history` table tracks legitimate cross-region usage to reduce false positives on fingerprint mismatches\n\n6. **Region-Aware Token Schema**: Added `issued_in_region` and `used_in_region` columns to detect cross-region token reuse attacks\n\n7. **Health Check Endpoints**: Comprehensive `/health` endpoint with component-level health and degradation detection\n\n8. **Monitoring Dashboard**: Complete Grafana dashboard with 8 panels and 7 alerts covering all critical metrics\n\n9. **Chaos Engineering Scenarios**: 3 chaos tests for network partitions, database failovers, and memory pressure\n\n10. **Cross-Region Lock Strategy**: Redis + Postgres advisory lock fallback for distributed token refresh locking\n\n## What I Kept\n\nThe previous design's core resilience patterns were excellent:\n\n- \u2705 Multi-layer caching (Redis \u2192 Local \u2192 Postgres)\n- \u2705 Circuit breaker with refined thresholds (30% over 50 requests)\n- \u2705 Bloom filter for blacklist (space-efficient, no false negatives)\n- \u2705 Write buffering with batching (10-100x write reduction)\n- \u2705 Row-level locking for refresh tokens (better than advisory locks)\n- \u2705 Compensating controls for token validation (user status, token version, session checks)\n- \u2705 Security event logging for audit trails\n- \u2705 Dynamic cache sizing based on available memory\n\n## Open Questions\n\n### 1. Blacklist Propagation Delay Tolerance\n\n**Question**: Is 50-500ms blacklist propagation delay acceptable for your threat model?\n\n**Context**: \n- p50: 50ms (Redis Streams)\n- p99: 500ms (includes Postgres replication)\n- p99.9: 2 seconds (degraded network)\n\n**Tradeoffs**:\n- \u2705 **Accept delay**: 99.9% of requests served from local cache (low latency)\n- \u274c **Reject delay**: Every logout requires synchronous cross-region write (high latency)\n\n**My recommendation**: Accept delay because:\n- Access tokens expire in 15 minutes anyway (limited blast radius)\n- High-risk operations check PRIMARY Postgres (strong consistency)\n- Compensating controls prevent most attack scenarios\n\n**But you should consider rejecting if**:\n- You handle extremely sensitive data (healthcare, finance)\n- Regulatory requirements mandate immediate revocation\n- You can tolerate 200ms+ p99 latency on every logout\n\n---\n\n### 2. Postgres Connection Pool Sizing\n\n**Question**: What is your target Postgres capacity?\n\n**Current design assumes**:\n- 10 app instances \u00d7 50 connections = 500 connections\n- Postgres `max_connections = 1000` (requires tuning from default 100)\n- PgBouncer in transaction mode for connection pooling\n\n**At 100M users**:\n- 278 refreshes/sec (normal) \u2192 2,780 refreshes/sec (10x scale)\n- Postgres write capacity: ~10,000 writes/sec (need partitioning)\n\n**Decision point**: Do you want to:\n- Option A: Vertical scaling (larger Postgres instance up to 10M users)\n- Option B: Horizontal scaling (partition refresh_tokens by user_id range)\n- Option C: Separate write-optimized database (e.g., Cassandra for audit logs)\n\n**My recommendation**: Start with Option A (vertical scaling) until 10M users, then add partitioning (Option B).\n\n---\n\n### 3. Bloom Filter Capacity Planning\n\n**Question**: What is your expected blacklist size?\n\n**Current design**:\n- 1M entry capacity\n- 0.01% false positive rate\n- ~2.4 MB memory per instance\n\n**Scaling calculation**:\n- 10M users \u00d7 2 tokens/day \u00d7 15-min lifetime = ~208K concurrent entries \u2705\n- 100M users = ~2M concurrent entries \u26a0\ufe0f (need larger bloom filter or partitioning)\n\n**Decision point**: Should bloom filter be:\n- Option A: Single large filter (simple, 10 MB memory for 100M users)\n- Option B: Partitioned by time bucket (only load recent entries, more complex)\n- Option C: Distributed bloom filter (sharded across instances, even more complex)\n\n**My recommendation**: Option A (single large filter) is fine until 100M users. 10 MB per instance is negligible.\n\n## Convergence Status\n\n**PROPOSING_FINAL** - This design is complete and production-ready for:\n\n\u2705 **Single-region deployments** (original scope)\n\u2705 **Multi-region deployments** (new scope addressed)\n\u2705 **Load testing plan** (comprehensive scenarios)\n\u2705 **Operational runbooks** (detailed incident response)\n\u2705 **Monitoring & observability** (dashboards + alerts)\n\u2705 **Chaos engineering** (failure scenarios)\n\nThe design has been iterated 3+ times and addresses all critical concerns:\n- \u2705 Cross-region token refresh security\n- \u2705 Blacklist propagation strategy\n- \u2705 Postgres fallback performance\n- \u2705 Circuit breaker tuning\n- \u2705 Operational procedures\n\n**Remaining decisions** are deployment-specific (connection pool sizing, bloom filter capacity) and can be tuned based on your scale.\n\n---\n\n## PROMPT_FOR_CRITIC:\n\nThis design is **READY FOR IMPLEMENTATION**. Please validate:\n\n1. **Cross-Region Propagation**: Is the 50-500ms blacklist propagation delay acceptable for your security requirements? Do you need stronger consistency guarantees?\n\n2. **Load Test Coverage**: Are there any additional failure scenarios you want tested that aren't covered in the 7 load test scenarios + 3 chaos tests?\n\n3. **Operational Procedures**: Are the runbooks detailed enough for your on-call engineers? Do you have any existing runbook templates I should follow?\n\n4. **Scale Assumptions**: I designed for 10M users (vertical scaling) \u2192 100M users (partitioning required). Is this the right scale target?\n\nIf you agree this design is complete, signal **ACCEPTING_FINAL** and we can move to implementation planning."
      },
      "agent_b": {
        "content": "## Design\n\n# JWT Authentication System with Global Distribution & Production Hardening\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        GLOBAL REGIONS                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502   US-EAST        \u2502         \u2502   EU-WEST        \u2502            \u2502\n\u2502  \u2502                  \u2502         \u2502                  \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 API Gateway\u2502  \u2502         \u2502  \u2502 API Gateway\u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Redis      \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Redis      \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 Regional   \u2502  \u2502 Sync    \u2502  \u2502 Regional   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 App Nodes  \u2502  \u2502         \u2502  \u2502 App Nodes  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (3+ pods)  \u2502  \u2502         \u2502  \u2502 (3+ pods)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Postgres   \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Postgres   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (Primary)  \u2502  \u2502 Replica \u2502  \u2502 (Replica)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502           \u2193                             \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502         Global Blacklist Propagation Bus             \u2502     \u2502\n\u2502  \u2502         (Pub/Sub: Redis Streams + Postgres WAL)     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Multi-Region Strategy with Consistency Guarantees\n\n### 1. Token Types and Regional Behavior\n\n**Access Tokens** (15-minute lifetime):\n- \u2705 **Stateless validation** - verify signature + check local blacklist\n- \u2705 **Regional Redis cache** - 99.9% of validations hit local cache\n- \u26a0\ufe0f **Blacklist propagation delay** - acceptable (see security analysis)\n\n**Refresh Tokens** (30-day lifetime):\n- \u2705 **Always write to PRIMARY Postgres** (strong consistency)\n- \u2705 **Refresh allowed in any region** (global mobility)\n- \u2705 **Token family tracking** - detects cross-region reuse attacks\n\n### 2. Blacklist Propagation Strategy\n\n**Design Principle**: **\"Eventually consistent blacklist with risk-based synchronous verification\"**\n\n```typescript\ninterface BlacklistPropagationConfig {\n  // How fast does blacklist propagate between regions?\n  propagationTarget: {\n    p50: 50,      // 50ms - Redis Streams\n    p99: 500,     // 500ms - includes Postgres replication\n    p99.9: 2000   // 2 seconds - degraded network\n  };\n  \n  // Which operations require synchronous cross-region check?\n  requireSyncCheck: {\n    highRiskOperations: true,    // Admin actions, delete account, payment\n    tokenAge: '<5min',           // Recently issued tokens (likely from logout)\n    userRiskScore: '>0.7',       // Suspicious activity score\n    crossRegionRefresh: true     // User moved to different region\n  };\n  \n  // Fallback behavior during propagation delay\n  acceptanceCriteria: {\n    maxAge: 300000,              // 5 minutes since token issued\n    requireCompensatingControls: true,\n    logSecurityEvent: true\n  };\n}\n\n// Global blacklist propagation bus\nclass GlobalBlacklistBus {\n  private regionalRedis: Map<Region, RedisClient>;\n  private postgresWalSubscription: PostgresReplicationClient;\n  private publishQueue: RedisStream;\n  \n  constructor() {\n    // Subscribe to Postgres WAL for blacklist INSERTs\n    this.postgresWalSubscription = new PostgresReplicationClient({\n      tables: ['token_blacklist'],\n      operations: ['INSERT'],\n      onMessage: (change) => this.propagateBlacklistEntry(change)\n    });\n    \n    // Redis Streams for cross-region pub/sub\n    this.publishQueue = new RedisStream('blacklist:global');\n  }\n  \n  async blacklistToken(\n    jti: string, \n    userId: string, \n    reason: string,\n    metadata: BlacklistMetadata\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    // 1. Write to PRIMARY Postgres immediately (source of truth)\n    const expiresAt = new Date(Date.now() + 15 * 60 * 1000);\n    await db.query(\n      `INSERT INTO token_blacklist \n       (jti, user_id, expires_at, reason, blacklisted_at, source_region, session_id)\n       VALUES ($1, $2, $3, $4, NOW(), $5, $6)`,\n      [jti, userId, expiresAt, reason, getCurrentRegion(), metadata.sessionId]\n    );\n    \n    logger.info('Token blacklisted in primary DB', {\n      jti,\n      userId,\n      reason,\n      latency: performance.now() - startTime\n    });\n    \n    // 2. Publish to Redis Stream for immediate regional propagation\n    await this.publishQueue.add('blacklist', {\n      jti,\n      userId,\n      expiresAt: expiresAt.toISOString(),\n      reason,\n      sourceRegion: getCurrentRegion(),\n      timestamp: Date.now()\n    });\n    \n    // 3. Update local region Redis immediately\n    await this.regionalRedis.get(getCurrentRegion())?.setex(\n      `blacklist:jti:${jti}`,\n      15 * 60,\n      JSON.stringify({ userId, reason, blacklistedAt: new Date() })\n    );\n    \n    // 4. Add to local bloom filter immediately\n    blacklistBloomFilter.add(jti);\n    \n    metrics.histogram('blacklist.propagation.latency', performance.now() - startTime);\n  }\n  \n  async propagateBlacklistEntry(entry: BlacklistEntry): Promise<void> {\n    // Propagate to all regional Redis clusters\n    const propagationPromises = Array.from(this.regionalRedis.entries())\n      .filter(([region]) => region !== entry.sourceRegion) // Skip source\n      .map(([region, redis]) => \n        this.propagateToRegion(region, redis, entry)\n      );\n    \n    await Promise.allSettled(propagationPromises);\n  }\n  \n  private async propagateToRegion(\n    region: Region,\n    redis: RedisClient,\n    entry: BlacklistEntry\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    try {\n      await redis.setex(\n        `blacklist:jti:${entry.jti}`,\n        entry.ttlSeconds,\n        JSON.stringify(entry)\n      );\n      \n      metrics.histogram('blacklist.cross_region_propagation', \n        performance.now() - startTime,\n        { source: entry.sourceRegion, target: region }\n      );\n      \n    } catch (error) {\n      logger.error('Failed to propagate blacklist to region', {\n        region,\n        jti: entry.jti,\n        error: error.message\n      });\n      \n      metrics.increment('blacklist.propagation.failure', {\n        source: entry.sourceRegion,\n        target: region\n      });\n      \n      // Not critical - Postgres replication will eventually sync\n    }\n  }\n  \n  // Check blacklist with cross-region awareness\n  async checkBlacklist(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): Promise<BlacklistCheckResult> {\n    \n    // 1. Check local Redis first (99.9% of requests)\n    const localResult = await this.regionalRedis.get(currentRegion)?.get(\n      `blacklist:jti:${jti}`\n    );\n    \n    if (localResult !== null) {\n      return { \n        blacklisted: true, \n        source: 'local_redis',\n        latency: 1 \n      };\n    }\n    \n    // 2. Determine if we need synchronous cross-region check\n    const needsSyncCheck = this.shouldCheckAllRegions(\n      jti,\n      riskLevel,\n      tokenIssuedAt,\n      currentRegion,\n      tokenIssuedInRegion\n    );\n    \n    if (!needsSyncCheck) {\n      // Trust local cache (low risk)\n      return { \n        blacklisted: false, \n        source: 'local_redis',\n        latency: 1\n      };\n    }\n    \n    // 3. Check PRIMARY Postgres (authoritative source)\n    // This handles cross-region edge cases\n    const startTime = performance.now();\n    const pgResult = await db.query({\n      name: 'check_blacklist_global',\n      text: `SELECT 1 FROM token_blacklist \n             WHERE jti = $1 AND expires_at > NOW() \n             LIMIT 1`,\n      values: [jti]\n    });\n    \n    const latency = performance.now() - startTime;\n    \n    metrics.histogram('blacklist.cross_region_check.latency', latency, {\n      risk_level: riskLevel,\n      cross_region: tokenIssuedInRegion !== currentRegion\n    });\n    \n    return {\n      blacklisted: pgResult.rowCount > 0,\n      source: 'postgres_primary',\n      latency\n    };\n  }\n  \n  private shouldCheckAllRegions(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): boolean {\n    \n    // Always check Postgres for high-risk operations\n    if (riskLevel === 'high') {\n      return true;\n    }\n    \n    // Check if token was issued in different region (possible logout in other region)\n    if (tokenIssuedInRegion && tokenIssuedInRegion !== currentRegion) {\n      return true;\n    }\n    \n    // Check if token is very recently issued (< 5 minutes)\n    // Likely from recent logout that may not have propagated yet\n    const tokenAge = Date.now() - tokenIssuedAt;\n    if (tokenAge < 5 * 60 * 1000) {\n      return true;\n    }\n    \n    return false;\n  }\n}\n\ninterface BlacklistEntry {\n  jti: string;\n  userId: string;\n  expiresAt: string;\n  reason: string;\n  sourceRegion: Region;\n  ttlSeconds: number;\n  timestamp: number;\n  sessionId?: string;\n}\n\ninterface BlacklistMetadata {\n  sessionId?: string;\n  deviceId?: string;\n  ipAddress?: string;\n  userAgent?: string;\n}\n\ninterface BlacklistCheckResult {\n  blacklisted: boolean;\n  source: 'local_redis' | 'remote_redis' | 'postgres_primary' | 'bloom_filter';\n  latency: number;\n}\n\ntype Region = 'us-east' | 'us-west' | 'eu-west' | 'ap-southeast' | 'ap-northeast';\n\nfunction getCurrentRegion(): Region {\n  return process.env.AWS_REGION as Region || 'us-east';\n}\n```\n\n### 3. Cross-Region Token Refresh Strategy\n\n**Challenge**: User logs in US-EAST, travels to EU-WEST, refreshes token.\n\n**Requirements**:\n- \u2705 Refresh must succeed (user mobility)\n- \u2705 Prevent token reuse attacks across regions\n- \u2705 Detect compromised token families\n\n**Solution**: **Regional read replicas + primary write with conflict detection**\n\n```typescript\nasync function performCrossRegionRefresh(\n  refreshToken: string,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<AuthResponse> {\n  \n  const tokenHash = hashToken(refreshToken);\n  const startTime = performance.now();\n  \n  // 1. Acquire distributed lock (prefer local Redis, fallback to Postgres)\n  const lockAcquired = await acquireCrossRegionLock(tokenHash, currentRegion);\n  \n  if (!lockAcquired) {\n    throw new AuthError('CONCURRENT_REFRESH_IN_PROGRESS');\n  }\n  \n  try {\n    // 2. Read token from PRIMARY Postgres (not replica)\n    // This ensures we see the absolute latest state across all regions\n    const client = await getPrimaryDbClient();\n    \n    await client.query('BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE');\n    \n    const tokenResult = await client.query(\n      `SELECT * FROM refresh_tokens \n       WHERE token_hash = $1 \n       FOR UPDATE`,\n      [tokenHash]\n    );\n    \n    if (tokenResult.rowCount === 0) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n    \n    const tokenData = tokenResult.rows[0];\n    \n    // 3. Enhanced validation for cross-region refresh\n    await validateRefreshToken(tokenData, fingerprint, currentRegion);\n    \n    // 4. Check for token reuse (CRITICAL for cross-region security)\n    if (tokenData.used) {\n      // Token reuse detected - revoke entire family\n      await handleTokenFamilyCompromise(\n        tokenData.token_family_id,\n        'CROSS_REGION_TOKEN_REUSE',\n        client\n      );\n      \n      throw new AuthError('TOKEN_REUSE_DETECTED');\n    }\n    \n    // 5. Mark token as used (in PRIMARY database)\n    await client.query(\n      `UPDATE refresh_tokens \n       SET used = true, \n           used_at = NOW(),\n           used_in_region = $2\n       WHERE token_hash = $1`,\n      [tokenHash, currentRegion]\n    );\n    \n    // 6. Generate new token pair\n    const user = await getUserById(tokenData.user_id);\n    \n    const newAccessToken = generateAccessToken(user, {\n      sessionId: tokenData.session_id,\n      deviceId: tokenData.device_id,\n      tokenVersion: user.token_version,\n      issuedInRegion: currentRegion,  // NEW: Track issuing region\n      previousRegion: tokenData.issued_in_region\n    });\n    \n    const newRefreshToken = await generateAndStoreRefreshToken(\n      user,\n      {\n        sessionId: tokenData.session_id,\n        deviceId: tokenData.device_id,\n        deviceFingerprint: fingerprint,\n        parentTokenId: tokenData.id,\n        tokenFamilyId: tokenData.token_family_id,\n        timesRefreshed: tokenData.times_refreshed + 1,\n        issuedInRegion: currentRegion,  // NEW: Track issuing region\n        previousRegion: tokenData.issued_in_region\n      },\n      client\n    );\n    \n    await client.query('COMMIT');\n    \n    // 7. Update regional cache (eventual consistency is OK here)\n    try {\n      await cache.set(\n        `refresh:${tokenHash}:used`,\n        true,\n        900\n      );\n    } catch {\n      // Not critical\n    }\n    \n    metrics.histogram('auth.cross_region_refresh.latency', \n      performance.now() - startTime,\n      { \n        from_region: tokenData.issued_in_region,\n        to_region: currentRegion\n      }\n    );\n    \n    return {\n      accessToken: newAccessToken.token,\n      refreshToken: newRefreshToken.token,\n      expiresIn: 900,\n      tokenType: 'Bearer',\n      refreshExpiresIn: 2592000\n    };\n    \n  } finally {\n    await releaseCrossRegionLock(tokenHash, currentRegion);\n  }\n}\n\nasync function validateRefreshToken(\n  tokenData: RefreshTokenRow,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<void> {\n  \n  // 1. Check expiry\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  // 2. Check revocation\n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Validate fingerprint\n  const fingerprintHash = hashFingerprint(fingerprint);\n  if (tokenData.device_fingerprint_hash !== fingerprintHash) {\n    \n    // Cross-region fingerprint mismatch could be legitimate (VPN, proxy)\n    // Check if user has history of cross-region usage\n    const userTravelPattern = await getUserTravelPattern(tokenData.user_id);\n    \n    if (!userTravelPattern.crossRegionAllowed) {\n      // User has never used service from multiple regions - suspicious\n      await createSecurityEvent({\n        userId: tokenData.user_id,\n        eventType: 'CROSS_REGION_FINGERPRINT_MISMATCH',\n        severity: 'HIGH',\n        details: {\n          originalRegion: tokenData.issued_in_region,\n          currentRegion,\n          originalFingerprint: tokenData.device_fingerprint_hash.substring(0, 16),\n          currentFingerprint: fingerprintHash.substring(0, 16)\n        }\n      });\n      \n      throw new AuthError('FINGERPRINT_MISMATCH');\n    }\n    \n    // Log but allow (user has established cross-region pattern)\n    logger.warn('Cross-region fingerprint mismatch allowed', {\n      userId: tokenData.user_id,\n      fromRegion: tokenData.issued_in_region,\n      toRegion: currentRegion\n    });\n  }\n  \n  // 4. Check refresh count (detect infinite refresh loops)\n  if (tokenData.times_refreshed > 1000) {\n    // Suspicious - single refresh chain shouldn't exceed 1000 refreshes\n    // (30 days / 15 min = 2880 max normal refreshes, but user should re-login)\n    await createSecurityEvent({\n      userId: tokenData.user_id,\n      eventType: 'EXCESSIVE_REFRESH_COUNT',\n      severity: 'MEDIUM',\n      details: {\n        timesRefreshed: tokenData.times_refreshed,\n        tokenFamilyId: tokenData.token_family_id\n      }\n    });\n    \n    throw new AuthError('REFRESH_LIMIT_EXCEEDED');\n  }\n}\n\n// Cross-region distributed lock using Redis + Postgres fallback\nasync function acquireCrossRegionLock(\n  tokenHash: string,\n  region: Region,\n  timeoutMs: number = 5000\n): Promise<boolean> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  const lockValue = `${region}:${uuidv4()}:${Date.now()}`;\n  \n  // Try regional Redis first\n  const regionalRedis = getRegionalRedis(region);\n  \n  try {\n    const acquired = await regionalRedis.set(\n      lockKey,\n      lockValue,\n      'NX',\n      'PX',\n      timeoutMs\n    );\n    \n    if (acquired) {\n      return true;\n    }\n  } catch (error) {\n    logger.warn('Regional Redis lock failed, trying Postgres', {\n      tokenHash: tokenHash.substring(0, 16),\n      region,\n      error: error.message\n    });\n  }\n  \n  // Fallback to Postgres advisory lock\n  // Use hash of token as lock ID (deterministic across regions)\n  const lockId = hashToInt64(tokenHash);\n  \n  const result = await db.query(\n    'SELECT pg_try_advisory_lock($1) as acquired',\n    [lockId]\n  );\n  \n  return result.rows[0].acquired;\n}\n\nasync function releaseCrossRegionLock(\n  tokenHash: string,\n  region: Region\n): Promise<void> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  \n  // Try Redis first\n  try {\n    const regionalRedis = getRegionalRedis(region);\n    await regionalRedis.del(lockKey);\n  } catch (error) {\n    // Fallback to Postgres advisory lock release\n    const lockId = hashToInt64(tokenHash);\n    await db.query('SELECT pg_advisory_unlock($1)', [lockId]);\n  }\n}\n```\n\n### 4. Enhanced Schema for Multi-Region Support\n\n```sql\n-- Refresh tokens table with region tracking\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  token_hash VARCHAR(64) NOT NULL UNIQUE,\n  user_id UUID NOT NULL REFERENCES users(id),\n  session_id UUID NOT NULL,\n  device_id VARCHAR(255) NOT NULL,\n  device_fingerprint_hash VARCHAR(64) NOT NULL,\n  \n  -- NEW: Region tracking\n  issued_in_region VARCHAR(50) NOT NULL,\n  used_in_region VARCHAR(50),\n  \n  expires_at TIMESTAMP NOT NULL,\n  issued_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  used BOOLEAN DEFAULT FALSE,\n  used_at TIMESTAMP,\n  revoked BOOLEAN DEFAULT FALSE,\n  revoked_at TIMESTAMP,\n  \n  -- Token family tracking\n  token_family_id UUID NOT NULL,\n  parent_token_id UUID REFERENCES refresh_tokens(id),\n  times_refreshed INTEGER DEFAULT 0,\n  \n  -- Audit\n  ip_address INET,\n  user_agent TEXT,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Composite index for cross-region lookups\nCREATE INDEX idx_refresh_tokens_hash_region ON refresh_tokens(token_hash, issued_in_region);\nCREATE INDEX idx_refresh_tokens_family ON refresh_tokens(token_family_id, issued_at DESC);\nCREATE INDEX idx_refresh_tokens_user_active ON refresh_tokens(user_id, expires_at) \n  WHERE NOT used AND NOT revoked;\n\n-- Token blacklist with region tracking\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  reason VARCHAR(100),\n  session_id UUID,\n  \n  -- NEW: Region tracking for propagation monitoring\n  source_region VARCHAR(50) NOT NULL,\n  blacklisted_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  \n  ip_address INET,\n  user_agent TEXT\n);\n\n-- Index for cross-region blacklist checks\nCREATE INDEX idx_blacklist_active ON token_blacklist(jti, expires_at)\n  WHERE expires_at > NOW();\nCREATE INDEX idx_blacklist_user_recent ON token_blacklist(user_id, revoked_at DESC);\nCREATE INDEX idx_blacklist_region ON token_blacklist(source_region, blacklisted_at DESC);\n\n-- User travel patterns (for risk assessment)\nCREATE TABLE user_region_history (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id),\n  region VARCHAR(50) NOT NULL,\n  first_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  last_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  request_count INTEGER DEFAULT 1,\n  \n  UNIQUE(user_id, region)\n);\n\nCREATE INDEX idx_user_region_history_user ON user_region_history(user_id, last_seen DESC);\n```\n\n## Load Testing & Failure Scenarios\n\n### 1. Load Testing Plan\n\n```typescript\n// Load test scenarios\nconst loadTestScenarios = [\n  {\n    name: 'Peak Traffic - Normal Operation',\n    duration: '10min',\n    rps: 100000, // 100K requests/sec\n    operations: {\n      tokenValidation: 0.70,  // 70% reads (API calls)\n      tokenRefresh: 0.25,     // 25% refresh operations\n      logout: 0.05            // 5% logout (blacklist writes)\n    },\n    assertions: {\n      p99Latency: '<50ms',\n      errorRate: '<0.01%',\n      redisHitRate: '>99.5%'\n    }\n  },\n  \n  {\n    name: 'Redis Failure - Circuit Breaker Activation',\n    duration: '5min',\n    setup: 'Stop Redis at t=60s, restart at t=180s',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerOpens: '<10s after Redis down',\n      fallbackLatency: '<100ms p99',\n      errorRate: '<1%',\n      recoveryTime: '<30s after Redis up',\n      noDataLoss: 'All blacklist entries in Postgres'\n    }\n  },\n  \n  {\n    name: 'Postgres Connection Pool Exhaustion',\n    duration: '3min',\n    setup: 'max_connections=100, 20 app instances',\n    rps: 10000,\n    operations: {\n      // Force all requests to Postgres (Redis disabled)\n      tokenValidation: 0.90,\n      tokenRefresh: 0.10\n    },\n    assertions: {\n      connectionPoolUtilization: '<80%',\n      p99Latency: '<200ms',\n      errorRate: '<5%',\n      noConnectionLeaks: 'Pool size stable'\n    }\n  },\n  \n  {\n    name: 'Circuit Breaker Flapping',\n    duration: '10min',\n    setup: 'Intermittent Redis failures (30s down, 30s up)',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerFlaps: '<5 times',\n      halfOpenDuration: '<30s',\n      errorRate: '<2%',\n      cacheCoherence: 'Local cache syncs after recovery'\n    }\n  },\n  \n  {\n    name: 'Cross-Instance Rate Limit Bypass',\n    duration: '5min',\n    setup: '10 app instances, rate limit 100 req/min per user',\n    rps: 5000,\n    operations: {\n      // Single user attacking across all instances\n      singleUserAttack: true,\n      targetRate: 1000 // 10x rate limit\n    },\n    assertions: {\n      effectiveRateLimit: '<150 req/min', // Allow 50% overage\n      postgresSync: 'Within 10 seconds',\n      blockAfterSync: 'Within 1 request'\n    }\n  },\n  \n  {\n    name: 'Cross-Region Token Refresh',\n    duration: '10min',\n    setup: 'Users refresh from different region than login',\n    rps: 10000,\n    operations: {\n      loginRegion: 'us-east',\n      refreshRegion: 'eu-west',\n      percentCrossRegion: 50\n    },\n    assertions: {\n      crossRegionLatency: '<200ms p99',\n      tokenReuseDetection: '100%',\n      blacklistPropagation: '<500ms p99',\n      noFalsePositives: 'Zero legitimate refreshes blocked'\n    }\n  },\n  \n  {\n    name: 'Bloom Filter False Positive Rate',\n    duration: '30min',\n    setup: '1M active tokens, 100K blacklisted',\n    rps: 100000,\n    operations: {\n      tokenValidation: 1.0\n    },\n    assertions: {\n      bloomFalsePositiveRate: '<0.01%',\n      postgresCheckRate: '<10 req/s',\n      noFalseNegatives: 'Zero blacklisted tokens accepted'\n    }\n  },\n  \n  {\n    name: 'Postgres Primary Failover During Refresh',\n    duration: '5min',\n    setup: 'Trigger Postgres failover at t=120s',\n    rps: 5000,\n    operations: {\n      tokenRefresh: 1.0\n    },\n    assertions: {\n      failoverWindow: '<30s',\n      errorsDuringFailover: 'Expected (graceful degradation)',\n      noTokenReuseBugs: 'Zero duplicate refresh tokens',\n      recoveryTime: '<10s after promotion'\n    }\n  }\n];\n\n// Chaos engineering scenarios\nconst chaosScenarios = [\n  {\n    name: 'Network Partition - Region Isolation',\n    setup: 'Isolate EU-WEST region for 60 seconds',\n    expectedBehavior: [\n      'EU-WEST continues serving from local Redis + Postgres replica',\n      'New blacklists in US-EAST do not propagate to EU-WEST',\n      'High-risk operations in EU-WEST check PRIMARY Postgres (may fail)',\n      'After partition heals, blacklists propagate within 5 seconds',\n      'No token reuse exploits during or after partition'\n    ],\n    securityValidation: [\n      'Verify high-risk ops fail safely during partition',\n      'Verify no blacklisted tokens accepted in EU-WEST',\n      'Verify token family compromise detected cross-region'\n    ]\n  },\n  \n  {\n    name: 'Postgres Primary Failover',\n    setup: 'Promote replica to primary (30s downtime)',\n    expectedBehavior: [\n      'Token refreshes fail during failover window',\n      'Token validation continues (cached data)',\n      'After promotion, refreshes resume within 10 seconds',\n      'No token reuse vulnerabilities introduced',\n      'No orphaned locks or leaked connections'\n    ]\n  },\n  \n  {\n    name: 'Memory Pressure - Cache Eviction',\n    setup: 'Fill LRU cache to capacity, continue adding entries',\n    expectedBehavior: [\n      'LRU evicts oldest entries',\n      'Cache hit rate remains >95%',\n      'No memory leak (heap size stable)',\n      'Evicted entries refetched from Redis/Postgres',\n      'No blacklisted tokens incorrectly evicted'\n    ]\n  },\n  \n  {\n    name: 'Redis Sentinel Failover',\n    setup: 'Kill Redis master, wait for sentinel promotion',\n    expectedBehavior: [\n      'Circuit breaker opens within 10s',\n      'Fallback to Postgres during failover (<30s)',\n      'After promotion, circuit breaker closes',\n      'No blacklist entries lost during failover',\n      'Cache rebuilt from Postgres on reconnect'\n    ]\n  }\n];\n```\n\n### 2. Load Testing Implementation\n\n```typescript\n// Load testing harness using k6\nimport { check, group, sleep } from 'k6';\nimport http from 'k6/http';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 50000 }, // Ramp up\n    { duration: '5m', target: 100000 }, // Peak\n    { duration: '2m', target: 0 }     // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(99)<50'],\n    http_req_failed: ['rate<0.0001'],\n    'circuit_breaker_open': ['count<1']\n  }\n};\n\nexport default function() {\n  const scenario = Math.random();\n  \n  if (scenario < 0.70) {\n    // Token validation (70% of traffic)\n    testTokenValidation();\n  } else if (scenario < 0.95) {\n    // Token refresh (25% of traffic)\n    testTokenRefresh();\n  } else {\n    // Logout (5% of traffic)\n    testLogout();\n  }\n  \n  sleep(Math.random() * 2); // Random delay 0-2s\n}\n\nfunction testTokenValidation() {\n  const token = __ENV.TEST_ACCESS_TOKEN;\n  \n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${token}` }\n  });\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'latency < 50ms': (r) => r.timings.duration < 50,\n    'cache hit': (r) => r.headers['X-Cache-Layer'] === 'redis'\n  });\n}\n\nfunction testTokenRefresh() {\n  const refreshToken = __ENV.TEST_REFRESH_TOKEN;\n  \n  const res = http.post('https://api.example.com/auth/refresh', \n    JSON.stringify({ refreshToken }),\n    { headers: { 'Content-Type': 'application/json' } }\n  );\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'returns new tokens': (r) => r.json('accessToken') !== undefined,\n    'latency < 100ms': (r) => r.timings.duration < 100\n  });\n}\n\n// Failure injection for chaos testing\nexport function injectRedisFailure() {\n  // Use Kubernetes pod deletion or network policy\n  exec(`kubectl delete pod -l app=redis-cache -n production`);\n  \n  sleep(10); // Wait for circuit breaker to trip\n  \n  // Verify fallback behavior\n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${__ENV.TEST_ACCESS_TOKEN}` }\n  });\n  \n  check(res, {\n    'fallback successful': (r) => r.status === 200,\n    'postgres fallback used': (r) => r.headers['X-Cache-Layer'] === 'postgres'\n  });\n}\n```\n\n## Operational Runbook\n\n### 1. Incident Response Playbooks\n\n```yaml\n# Runbook: Redis Unavailable (Circuit Breaker Open)\n\nincident_type: \"Redis Unavailable\"\nseverity: P2 (High)\ndetection: Alert \"RedisCircuitBreakerOpen\" fires\n\nimmediate_actions:\n  - verify_fallback:\n      cmd: \"curl https://api.example.com/health/fallback\"\n      expected: '{\"status\":\"degraded\",\"fallback\":\"postgres\",\"latency_p99\":150}'\n      \n  - check_postgres_load:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'active'\"\n      threshold: \"< 80% of max_connections\"\n      \n  - verify_no_data_loss:\n      cmd: \"SELECT COUNT(*) FROM token_blacklist WHERE blacklisted_at > NOW() - INTERVAL '5 minutes'\"\n      compare_with: \"Should match expected logout rate\"\n\ninvestigation:\n  - check_redis_health:\n      - \"kubectl get pods -n production -l app=redis\"\n      - \"kubectl logs -n production -l app=redis --tail=100\"\n      - \"redis-cli -h redis.internal PING\"\n      \n  - check_sentinel_status:\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL masters\"\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL slaves auth-master\"\n      \n  - check_network:\n      - \"kubectl get networkpolicies -n production\"\n      - \"traceroute redis.internal\"\n\nmitigation:\n  - if_redis_pod_crash:\n      action: \"Kubernetes should auto-restart\"\n      verify: \"Wait 30s, check if circuit breaker closes\"\n      \n  - if_sentinel_failover:\n      action: \"Wait for automatic failover (< 30s)\"\n      verify: \"SENTINEL masters shows new master\"\n      \n  - if_persistent_failure:\n      action: \"Scale Postgres read replicas\"\n      cmd: \"kubectl scale deployment postgres-replica --replicas=5\"\n\nrecovery_verification:\n  - circuit_breaker_closed:\n      query: \"circuit_breaker_state{service='redis'} == 0\"\n      \n  - latency_normal:\n      query: \"histogram_quantile(0.99, cache_get_latency) < 10\"\n      \n  - error_rate_normal:\n      query: \"rate(http_requests_total{status=~'5..'}[5m]) < 0.001\"\n\npost_mortem:\n  - analyze_root_cause: true\n  - update_circuit_breaker_thresholds: \"if flapping observed\"\n  - review_postgres_capacity: \"if connection pool saturated\"\n```\n\n```yaml\n# Runbook: Cross-Region Blacklist Propagation Lag\n\nincident_type: \"Cross-Region Blacklist Propagation Delayed\"\nseverity: P3 (Medium)\ndetection: Alert \"BlacklistPropagationLatencyHigh\" fires\n\nimmediate_actions:\n  - check_propagation_metrics:\n      query: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) > 2000\"\n      \n  - verify_security_impact:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE event_type = 'BLACKLISTED_TOKEN_ACCEPTED' AND created_at > NOW() - INTERVAL '5 minutes'\"\n      expected: \"0 (no security breach)\"\n      \n  - check_redis_streams:\n      cmd: \"redis-cli XINFO STREAM blacklist:global\"\n      verify: \"Stream is processing entries\"\n\ninvestigation:\n  - check_network_latency:\n      cmd: \"ping -c 10 redis-eu-west.internal\"\n      threshold: \"< 100ms average\"\n      \n  - check_postgres_replication_lag:\n      cmd: \"SELECT EXTRACT(EPOCH FROM (NOW() - pg_last_xact_replay_timestamp())) AS lag_seconds FROM pg_stat_replication\"\n      threshold: \"< 5 seconds\"\n      \n  - analyze_propagation_failures:\n      query: \"sum(rate(blacklist_propagation_failure_total[5m])) by (source, target)\"\n\nmitigation:\n  - if_network_partition:\n      action: \"Wait for network to heal (automatic)\"\n      impact: \"High-risk ops will check PRIMARY Postgres\"\n      verify: \"Propagation resumes after partition heals\"\n      \n  - if_redis_streams_backlog:\n      action: \"Add more consumer groups\"\n      cmd: \"redis-cli XGROUP CREATE blacklist:global consumers-2 0 MKSTREAM\"\n      \n  - if_postgres_replication_lag:\n      action: \"Investigate replication bottleneck\"\n      escalate: \"Database team\"\n\nemergency_mitigation:\n  - force_postgres_checks:\n      description: \"Temporarily force all blacklist checks to PRIMARY Postgres\"\n      cmd: \"kubectl set env deployment/api-server BLACKLIST_FORCE_POSTGRES_CHECK=true\"\n      impact: \"Increased latency (p99: 50ms \u2192 150ms), but guaranteed consistency\"\n      rollback: \"After propagation lag resolves\"\n\nrecovery_verification:\n  - propagation_latency_normal:\n      query: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) < 500\"\n      \n  - no_security_incidents:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE severity IN ('HIGH', 'CRITICAL') AND created_at > NOW() - INTERVAL '10 minutes'\"\n      expected: \"0\"\n\npost_mortem:\n  - analyze_network_topology: true\n  - review_replication_configuration: true\n  - consider_dedicated_propagation_channel: \"if recurring issue\"\n```\n\n```yaml\n# Runbook: Token Reuse Attack Detected\n\nincident_type: \"Token Reuse Attack Detected\"\nseverity: P1 (Critical)\ndetection: Alert \"TokenReuseAttackDetected\" fires\n\nimmediate_actions:\n  - identify_affected_users:\n      cmd: \"SELECT DISTINCT user_id FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' AND created_at > NOW() - INTERVAL '5 minutes'\"\n      \n  - verify_family_revocation:\n      cmd: \"SELECT COUNT(*) FROM refresh_tokens WHERE token_family_id IN (SELECT DISTINCT token_family_id FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' AND created_at > NOW() - INTERVAL '5 minutes') AND revoked = false\"\n      expected: \"0 (all families revoked)\"\n      \n  - check_attack_scope:\n      query: \"sum(rate(security_events_total{event_type='TOKEN_REUSE_DETECTED'}[5m]))\"\n      threshold: \"> 10/min indicates widespread attack\"\n\ninvestigation:\n  - analyze_attack_pattern:\n      cmd: \"SELECT user_id, session_id, ip_address, user_agent, issued_in_region, used_in_region FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' ORDER BY created_at DESC LIMIT 100\"\n      \n  - check_for_compromised_devices:\n      cmd: \"SELECT device_id, COUNT(DISTINCT user_id) FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' GROUP BY device_id HAVING COUNT(DISTINCT user_id) > 1\"\n      \n  - correlate_with_other_attacks:\n      cmd: \"SELECT event_type, COUNT(*) FROM security_events WHERE created_at > NOW() - INTERVAL '1 hour' GROUP BY event_type ORDER BY COUNT(*) DESC\"\n\nmitigation:\n  - if_single_user_compromised:\n      action: \"User's tokens already revoked (automatic)\"\n      followup: \"Notify user of security incident\"\n      cmd: \"Send security alert email\"\n      \n  - if_widespread_attack:\n      action: \"Enable enhanced monitoring\"\n      cmd: \"kubectl set env deployment/api-server TOKEN_REUSE_DETECTION_SENSITIVITY=high\"\n      \n  - if_credential_stuffing:\n      action: \"Enable rate limiting on /auth/refresh\"\n      cmd: \"kubectl set env deployment/api-server REFRESH_RATE_LIMIT=10\"\n\nemergency_mitigation:\n  - if_mass_compromise:\n      description: \"Rotate signing keys to invalidate all tokens\"\n      cmd: \"kubectl exec -it api-server-0 -- node scripts/rotate-jwt-keys.js\"\n      impact: \"All users forced to re-login\"\n      approval_required: \"VP Engineering\"\n\nrecovery_verification:\n  - attack_stopped:\n      query: \"rate(security_events_total{event_type='TOKEN_REUSE_DETECTED'}[5m]) == 0\"\n      \n  - no_new_compromises:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE event_type IN ('TOKEN_REUSE_DETECTED', 'CROSS_REGION_TOKEN_REUSE') AND created_at > NOW() - INTERVAL '10 minutes'\"\n      expected: \"0\"\n      \n  - affected_users_notified:\n      verify: \"Check notification logs\"\n\npost_mortem:\n  - forensic_analysis: \"How were tokens stolen?\"\n  - improve_detection: \"Add additional signals to security_events\"\n  - user_communication: \"Transparency about incident\"\n  - regulatory_notification: \"GDPR/CCPA breach notification if required\"\n```\n\n### 2. Monitoring Dashboard\n\n```typescript\n// Grafana dashboard JSON\nconst authSystemDashboard = {\n  title: \"JWT Auth System - Production Monitoring\",\n  panels: [\n    {\n      title: \"Request Rate by Operation\",\n      targets: [\n        'sum(rate(http_requests_total{service=\"auth\"}[5m])) by (operation)',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Cache Hit Rate by Layer\",\n      targets: [\n        'sum(rate(cache_hit_total[5m])) by (layer) / sum(rate(cache_requests_total[5m])) by (layer)',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.95, color: \"green\" },\n        { value: 0.90, color: \"yellow\" },\n        { value: 0.85, color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Circuit Breaker State\",\n      targets: [\n        'circuit_breaker_state{service=\"redis\"}',\n      ],\n      visualization: \"stat\",\n      mappings: [\n        { value: 0, text: \"CLOSED\", color: \"green\" },\n        { value: 0.5, text: \"HALF_OPEN\", color: \"yellow\" },\n        { value: 1, text: \"OPEN\", color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Token Validation Latency (p50, p99, p99.9)\",\n      targets: [\n        'histogram_quantile(0.50, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.99, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.999, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Postgres Connection Pool Utilization\",\n      targets: [\n        'pg_stat_activity_count / pg_settings_max_connections',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.80, color: \"red\" },\n        { value: 0.60, color: \"yellow\" },\n        { value: 0.40, color: \"green\" }\n      ]\n    },\n    \n    {\n      title: \"Blacklist Propagation Latency (Cross-Region)\",\n      targets: [\n        'histogram_quantile(0.99, sum(rate(blacklist_cross_region_propagation_bucket[5m])) by (source, target, le))',\n      ],\n      visualization: \"heatmap\"\n    },\n    \n    {\n      title: \"Security Events (Last 1 Hour)\",\n      targets: [\n        'sum(increase(security_events_total[1h])) by (event_type)',\n      ],\n      visualization: \"table\",\n      transformations: [\n        {\n          id: 'sortBy',\n          options: { fields: {}, sort: [{ field: 'Value', desc: true }] }\n        }\n      ]\n    },\n    \n    {\n      title: \"Bloom Filter Statistics\",\n      targets: [\n        'bloom_filter_entries',\n        'bloom_filter_capacity',\n        'bloom_filter_load_factor',\n        'rate(bloom_filter_false_positive_total[5m])',\n      ],\n      visualization: \"stat\"\n    },\n    \n    {\n      title: \"Cross-Region Refresh Success Rate\",\n      targets: [\n        'sum(rate(auth_cross_region_refresh_success_total[5m])) / sum(rate(auth_cross_region_refresh_total[5m]))',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.99, color: \"green\" },\n        { value: 0.95, color: \"yellow\" },\n        { value: 0.90, color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Token Reuse Detection Rate\",\n      targets: [\n        'rate(security_events_total{event_type=\"TOKEN_REUSE_DETECTED\"}[5m])',\n      ],\n      visualization: \"timeseries\",\n      alert: {\n        condition: '> 10',\n        severity: 'critical',\n        message: 'Token reuse attack detected - possible credential compromise'\n      }\n    }\n  ],\n  \n  alerts: [\n    {\n      name: \"RedisCircuitBreakerOpen\",\n      condition: \"circuit_breaker_state{service='redis'} == 1\",\n      for: \"1m\",\n      severity: \"critical\",\n      runbook: \"https://wiki.internal/runbooks/redis-unavailable\"\n    },\n    {\n      name: \"HighPostgresFallback\",\n      condition: \"rate(cache_fallback_postgres_total[5m]) > 100\",\n      for: \"5m\",\n      severity: \"warning\",\n      runbook: \"https://wiki.internal/runbooks/high-postgres-fallback\"\n    },\n    {\n      name: \"TokenValidationLatencyHigh\",\n      condition: \"histogram_quantile(0.99, auth_token_validation_latency_bucket) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"BloomFilterFalsePositiveRateHigh\",\n      condition: \"rate(bloom_filter_false_positive_total[5m]) > 0.0001\",\n      for: \"10m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"PostgresConnectionPoolSaturated\",\n      condition: \"pg_stat_activity_count / pg_settings_max_connections > 0.8\",\n      for: \"5m\",\n      severity: \"critical\",\n      runbook: \"https://wiki.internal/runbooks/postgres-connection-pool-saturated\"\n    },\n    {\n      name: \"CrossRegionBlacklistPropagationSlow\",\n      condition: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) > 1000\",\n      for: \"5m\",\n      severity: \"warning\",\n      runbook: \"https://wiki.internal/runbooks/blacklist-propagation-lag\"\n    },\n    {\n      name: \"SecurityEventSpike\",\n      condition: \"rate(security_events_total{severity='HIGH'}[5m]) > 10\",\n      for: \"2m\",\n      severity: \"critical\",\n      pagerduty: true\n    },\n    {\n      name: \"TokenReuseAttackDetected\",\n      condition: \"rate(security_events_total{event_type='TOKEN_REUSE_DETECTED'}[5m]) > 5\",\n      for: \"1m\",\n      severity: \"critical\",\n      pagerduty: true,\n      runbook: \"https://wiki.internal/runbooks/token-reuse-attack\"\n    },\n    {\n      name: \"CrossRegionRefreshFailureRateHigh\",\n      condition: \"1 - (sum(rate(auth_cross_region_refresh_success_total[5m])) / sum(rate(auth_cross_region_refresh_total[5m]))) > 0.05\",\n      for: \"5m\",\n      severity: \"warning\"\n    }\n  ]\n};\n```\n\n### 3. Health Check Endpoints\n\n```typescript\n// Health check implementation\napp.get('/health', async (req, res) => {\n  const health = {\n    status: 'healthy',\n    timestamp: new Date().toISOString(),\n    version: process.env.APP_VERSION,\n    uptime: process.uptime(),\n    region: getCurrentRegion(),\n    \n    components: {\n      redis: await checkRedisHealth(),\n      postgres: await checkPostgresHealth(),\n      bloomFilter: await checkBloomFilterHealth(),\n      propagationBus: await checkPropagationBusHealth()\n    },\n    \n    metrics: {\n      circuitBreakerState: circuitBreaker.getState(),\n      cacheHitRate: await getCacheHitRate(),\n      requestRate: await getRequestRate(),\n      errorRate: await getErrorRate(),\n      blacklistPropagationLag: await getBlacklistPropagationLag()\n    }\n  };\n  \n  // Overall health based on components\n  if (health.components.redis.status === 'unhealthy' && \n      health.components.postgres.status === 'unhealthy') {\n    health.status = 'unhealthy';\n    return res.status(503).json(health);\n  }\n  \n  if (health.components.redis.status === 'unhealthy' ||\n      health.components.postgres.status === 'degraded') {\n    health.status = 'degraded';\n    return res.status(200).json(health);\n  }\n  \n  res.status(200).json(health);\n});\n\nasync function checkRedisHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await redis.ping();\n    const latency = Date.now() - start;\n    \n    return {\n      status: latency < 10 ? 'healthy' : 'degraded',\n      latency,\n      message: 'Redis operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Redis unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkPostgresHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await db.query('SELECT 1');\n    const latency = Date.now() - start;\n    \n    const poolSize = await db.query('SELECT count(*) FROM pg_stat_activity');\n    const maxConnections = await db.query('SHOW max_connections');\n    const utilization = poolSize.rows[0].count / maxConnections.rows[0].max_connections;\n    \n    return {\n      status: latency < 50 && utilization < 0.8 ? 'healthy' : 'degraded',\n      latency,\n      metadata: {\n        poolUtilization: `${(utilization * 100).toFixed(1)}%`,\n        activeConnections: poolSize.rows[0].count\n      },\n      message: 'Postgres operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Postgres unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkBloomFilterHealth(): Promise<ComponentHealth> {\n  const stats = blacklistBloomFilter.getStats();\n  const loadFactor = stats.entries / stats.capacity;\n  \n  return {\n    status: loadFactor < 0.8 ? 'healthy' : 'degraded',\n    metadata: {\n      entries: stats.entries,\n      capacity: stats.capacity,\n      loadFactor: `${(loadFactor * 100).toFixed(1)}%`,\n      lastSync: stats.lastSync\n    },\n    message: loadFactor < 0.8 ? 'Bloom filter healthy' : 'Bloom filter approaching capacity'\n  };\n}\n\nasync function checkPropagationBusHealth(): Promise<ComponentHealth> {\n  try {\n    const streamInfo = await redis.xinfo('STREAM', 'blacklist:global');\n    const lag = await getBlacklistPropagationLag();\n    \n    return {\n      status: lag < 1000 ? 'healthy' : 'degraded',\n      metadata: {\n        streamLength: streamInfo.length,\n        propagationLagMs: lag,\n        consumerGroups: streamInfo.groups\n      },\n      message: lag < 1000 ? 'Propagation bus healthy' : 'Propagation lag detected'\n    };\n  } catch (error) {\n    return {\n      status: 'degraded',\n      message: `Propagation bus check failed: ${error.message}`\n    };\n  }\n}\n\nasync function getBlacklistPropagationLag(): Promise<number> {\n  const result = await db.query(\n    `SELECT EXTRACT(EPOCH FROM (NOW() - MAX(blacklisted_at))) * 1000 AS lag_ms\n     FROM token_blacklist\n     WHERE source_region != $1\n     LIMIT 1`,\n    [getCurrentRegion()]\n  );\n  \n  return result.rows[0]?.lag_ms || 0;\n}\n\ninterface ComponentHealth {\n  status: 'healthy' | 'degraded' | 'unhealthy';\n  latency?: number | null;\n  metadata?: Record<string, any>;\n  message: string;\n}\n```\n\n## What I Improved\n\n### 1. **Added Postgres Primary Failover Load Test**\nThe original design had 7 load tests but didn't explicitly test Postgres failover during refresh operations\u2014the most critical write path. I added an 8th scenario specifically testing refresh token operations during database failover to validate there are no token reuse bugs introduced during the failover window.\n\n### 2. **Enhanced Network Partition Chaos Test with Security Validation**\nThe original network partition scenario was good, but I added explicit security validation steps:\n- Verify high-risk operations fail safely (not just continue)\n- Confirm no blacklisted tokens are accepted in isolated region\n- Validate token family compromise detection works cross-region during partition\n\nThis ensures the system degrades **securely** during partitions, not just functionally.\n\n### 3. **Added Cross-Region Blacklist Propagation Lag Runbook**\nThe original design had a runbook for Redis failures but not for blacklist propagation lag\u2014a critical multi-region failure mode. I added a complete runbook including:\n- Detection via Prometheus alerts\n- Investigation steps for network vs. Postgres replication lag\n- Emergency mitigation (force Postgres checks globally)\n- Clear rollback procedures\n\n### 4. **Added Token Reuse Attack Incident Response Runbook**\nThis is the highest severity security incident possible in this system. I added a detailed P1 runbook covering:\n- Immediate identification of affected users\n- Verification of automatic family revocation\n- Attack scope analysis (single user vs. mass compromise)\n- Emergency mitigation including JWT key rotation\n- Post-mortem and regulatory notification requirements\n\n### 5. **Enhanced Monitoring Dashboard with Security Metrics**\nAdded two critical panels the original design missed:\n- **Cross-Region Refresh Success Rate**: Gauge showing if cross-region mobility is working\n- **Token Reuse Detection Rate**: Time series showing active attacks in real-time\n\nAlso added sorting/transformation to the Security Events table to prioritize high-severity events.\n\n### 6. **Added Runbook URLs to Prometheus Alerts**\nAll alerts now include direct links to runbooks (`runbook: \"https://wiki.internal/...\"`). This reduces MTTM (mean time to mitigation) by eliminating the \"where's the runbook?\" step during incidents.\n\n### 7. **Added PagerDuty Integration to Critical Security Alerts**\nTwo alerts now have `pagerduty: true`:\n- SecurityEventSpike (multiple high-severity events)\n- TokenReuseAttackDetected (active attack)\n\nThese require immediate human response and shouldn't rely on passive monitoring.\n\n### 8. **Added Health Check for Propagation Bus**\nThe original `/health` endpoint checked Redis, Postgres, and Bloom Filter, but not the propagation bus itself. I added `checkPropagationBusHealth()` which:\n- Verifies Redis Streams is operational\n- Measures current propagation lag\n- Reports consumer group health\n\nThis gives a complete picture of multi-region health.\n\n### 9. **Added Region Metadata to Health Check Response**\nThe health check now includes `region: getCurrentRegion()` and `blacklistPropagationLag` in the metrics. This helps diagnose cross-region issues by making it obvious which region is reporting and how stale its blacklist is.\n\n### 10. **Added Redis Sentinel Failover Chaos Test**\nThe original chaos tests covered network partitions, Postgres failover, and memory pressure, but not Redis Sentinel failover\u2014a common failure mode in production. I added a scenario testing:\n- Circuit breaker behavior during sentinel promotion\n- Fallback to Postgres during ~30s failover window\n- Cache rebuild from Postgres after reconnection\n- No blacklist data loss\n\n## What I Kept\n\nThe architect's design was excellent. I kept 95% of it:\n\n- \u2705 **Complete multi-region architecture** with Redis Streams + Postgres WAL\n- \u2705 **Risk-based blacklist checking** (low/medium/high risk operations)\n- \u2705 **Cross-region token refresh with token family tracking**\n- \u2705 **User travel pattern tracking** to reduce false positives\n- \u2705 **Region-aware schema** with `issued_in_region` and `used_in_region`\n- \u2705 **Distributed locking** with Redis + Postgres advisory lock fallback\n- \u2705 **Comprehensive fingerprint validation** with cross-region awareness\n- \u2705 **Circuit breaker patterns** with multi-layer caching\n- \u2705 **7 core load test scenarios** (I only added 1 more)\n- \u2705 **3 chaos engineering scenarios** (I added 1 more)\n- \u2705 **Detailed database schema** with proper indexes\n- \u2705 **k6 load testing implementation**\n- \u2705 **Grafana dashboard structure** with 8 panels\n- \u2705 **Component health checks**\n\nThe architecture is production-ready. My improvements focused on **operational gaps** (runbooks, security incident response, monitoring blind spots) rather than architectural changes.\n\n## Remaining Concerns\n\n### 1. **Postgres WAL Subscription Reliability**\n\n**Concern**: The `PostgresReplicationClient` subscribing to WAL for blacklist propagation is a single point of failure. If the WAL subscription drops:\n- Blacklists only propagate via Postgres replication (slower)\n- Redis Streams may get out of sync with Postgres\n\n**Questions**:\n- How do we detect WAL subscription failures?\n- Should we add a health check for WAL subscription status?\n- Do we need a reconciliation loop to compare Redis vs. Postgres blacklists?\n\n**My recommendation**: Add a background job that periodically (every 30s) compares a sample of recent Postgres blacklist entries against Redis. If divergence > 1%, alert and force a full resync.\n\n---\n\n### 2. **Emergency JWT Key Rotation Blast Radius**\n\n**Concern**: The Token Reuse Attack runbook includes emergency key rotation as a mitigation, which forces all users to re-login. For a system with 10M+ users:\n- Mass re-login creates huge load spike on auth system\n- Customer support gets flooded with \"why was I logged out?\" tickets\n- Potential revenue impact if users don't complete re-login\n\n**Questions**:\n- Do we have capacity modeling for \"all users re-login in 1 hour\" scenario?\n- Should we implement gradual key rotation (issue tokens with both old and new keys, accept both for 24h)?\n- What's the user communication plan for this scenario?\n\n**My recommendation**: Implement dual-key verification (accept tokens signed with either key for 24h) to allow gradual key rotation without mass logout.\n\n---\n\n### 3. **Cross-Region Fingerprint Validation Bypass**\n\n**Concern**: The design allows fingerprint mismatches for users with established cross-region patterns (`userTravelPattern.crossRegionAllowed`). This could be exploited:\n- Attacker steals token from user with cross-region history\n- Attacker uses token from different device/fingerprint\n- System allows it because user has `crossRegionAllowed = true`\n\n**Questions**:\n- Should we still require fingerprint match within the same region?\n- Should we have a \"trust level\" for cross-region usage (e.g., allow mismatch only if user crossed regions in last 7 days)?\n- Do we need additional compensating controls (device reputation, IP geolocation)?\n\n**My recommendation**: Add time-based decay to `crossRegionAllowed`. If user hasn't traveled in 90 days, reset to `false`. Also, always enforce fingerprint match within same region.\n\n---\n\n### 4. **Bloom Filter Rebuild Strategy**\n\n**Concern**: The design mentions rebuilding the bloom filter when `load_factor > 0.8`, but doesn't specify the strategy:\n- Do all instances rebuild simultaneously (thundering herd on Postgres)?\n- Do we rebuild in-place (briefly higher false positive rate)?\n- How long does rebuild take for 1M entries?\n\n**Questions**:\n- Should we use leader election (only one instance rebuilds, then distributes)?\n- Should we build new filter alongside old filter, then atomic swap?\n- What's the impact on request latency during rebuild?\n\n**My recommendation**: Use blue/green bloom filter deployment: build new filter in background, validate against Postgres, then atomic swap. Stagger rebuilds across instances (1 instance every 30s).\n\n---\n\n### 5. **Token Family Compromise Detection in Postgres Replica Lag**\n\n**Concern**: Token reuse detection relies on checking `used = true` in the database. If cross-region refresh reads from a lagging replica:\n- Token marked `used` in US-EAST\n- Attacker immediately uses same token in EU-WEST\n- EU-WEST reads from replica still showing `used = false`\n- Attack succeeds until replication catches up\n\n**Questions**:\n- Should cross-region refresh ALWAYS read from PRIMARY (not replica)?\n- Do we need to add replica lag detection to the refresh flow?\n- What's acceptable replica lag before we reject cross-region refreshes?\n\n**My recommendation**: The design already specifies reading from PRIMARY for refresh (`await getPrimaryDbClient()`), so this concern is addressed. However, we should add a safeguard: if cross-region refresh is attempted and replica lag > 5s, force read from PRIMARY with higher timeout.\n\n## Convergence Status\n\n**ITERATING** - Design is 95% complete but needs refinement on operational failure modes.\n\nThis is a **very strong design** that's nearly production-ready. The core architecture, security model, and resilience patterns are excellent. My improvements focused on operational gaps:\n\n\u2705 **Completed**:\n- Multi-region architecture\n- Load testing plan\n- Chaos engineering\n- Core monitoring\n\n\u26a0\ufe0f **Needs Refinement**:\n- WAL subscription reliability monitoring\n- Emergency key rotation procedure\n- Bloom filter rebuild strategy\n- Cross-region fingerprint validation edge cases\n\nThe remaining concerns are **operational edge cases**, not architectural flaws. With 1 more iteration focused on these operational details, this design will be ready for implementation.\n\n---\n\n##",
        "prompt_for_other": "Your design is excellent! I found only **5 operational edge cases** that need clarification:\n\n1. **WAL Subscription Monitoring**: How do we detect if the Postgres WAL subscription fails? Should we add a reconciliation loop comparing Redis vs. Postgres blacklists?\n\n2. **Emergency Key Rotation**: The runbook includes JWT key rotation as emergency mitigation, which forces all users to re-login. Should we implement dual-key verification (accept both old and new keys for 24h) to avoid mass logout?\n\n3. **Cross-Region Fingerprint Bypass**: Users with `crossRegionAllowed = true` can bypass fingerprint checks. Should we add time-based decay (reset to false after 90 days of no travel) or additional compensating controls?\n\n4. **Bloom Filter Rebuild Strategy**: When load_factor > 0.8, how do we rebuild without thundering herd? Should we use blue/green deployment (build new filter in background, atomic swap)?\n\n5. **Token Reuse + Replica Lag**: If cross-region refresh happens during Postgres replica lag, could we miss a token reuse attack? (I think you already handle this by reading from PRIMARY, but want to confirm).\n\nPlease address these 5 operational concerns and I'll signal **ACCEPTING_FINAL**.",
        "convergence_signal": "ACCEPTING_FINAL",
        "raw_response": "## Design\n\n# JWT Authentication System with Global Distribution & Production Hardening\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        GLOBAL REGIONS                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502   US-EAST        \u2502         \u2502   EU-WEST        \u2502            \u2502\n\u2502  \u2502                  \u2502         \u2502                  \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 API Gateway\u2502  \u2502         \u2502  \u2502 API Gateway\u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Redis      \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Redis      \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 Regional   \u2502  \u2502 Sync    \u2502  \u2502 Regional   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 App Nodes  \u2502  \u2502         \u2502  \u2502 App Nodes  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (3+ pods)  \u2502  \u2502         \u2502  \u2502 (3+ pods)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Postgres   \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Postgres   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (Primary)  \u2502  \u2502 Replica \u2502  \u2502 (Replica)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502           \u2193                             \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502         Global Blacklist Propagation Bus             \u2502     \u2502\n\u2502  \u2502         (Pub/Sub: Redis Streams + Postgres WAL)     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Multi-Region Strategy with Consistency Guarantees\n\n### 1. Token Types and Regional Behavior\n\n**Access Tokens** (15-minute lifetime):\n- \u2705 **Stateless validation** - verify signature + check local blacklist\n- \u2705 **Regional Redis cache** - 99.9% of validations hit local cache\n- \u26a0\ufe0f **Blacklist propagation delay** - acceptable (see security analysis)\n\n**Refresh Tokens** (30-day lifetime):\n- \u2705 **Always write to PRIMARY Postgres** (strong consistency)\n- \u2705 **Refresh allowed in any region** (global mobility)\n- \u2705 **Token family tracking** - detects cross-region reuse attacks\n\n### 2. Blacklist Propagation Strategy\n\n**Design Principle**: **\"Eventually consistent blacklist with risk-based synchronous verification\"**\n\n```typescript\ninterface BlacklistPropagationConfig {\n  // How fast does blacklist propagate between regions?\n  propagationTarget: {\n    p50: 50,      // 50ms - Redis Streams\n    p99: 500,     // 500ms - includes Postgres replication\n    p99.9: 2000   // 2 seconds - degraded network\n  };\n  \n  // Which operations require synchronous cross-region check?\n  requireSyncCheck: {\n    highRiskOperations: true,    // Admin actions, delete account, payment\n    tokenAge: '<5min',           // Recently issued tokens (likely from logout)\n    userRiskScore: '>0.7',       // Suspicious activity score\n    crossRegionRefresh: true     // User moved to different region\n  };\n  \n  // Fallback behavior during propagation delay\n  acceptanceCriteria: {\n    maxAge: 300000,              // 5 minutes since token issued\n    requireCompensatingControls: true,\n    logSecurityEvent: true\n  };\n}\n\n// Global blacklist propagation bus\nclass GlobalBlacklistBus {\n  private regionalRedis: Map<Region, RedisClient>;\n  private postgresWalSubscription: PostgresReplicationClient;\n  private publishQueue: RedisStream;\n  \n  constructor() {\n    // Subscribe to Postgres WAL for blacklist INSERTs\n    this.postgresWalSubscription = new PostgresReplicationClient({\n      tables: ['token_blacklist'],\n      operations: ['INSERT'],\n      onMessage: (change) => this.propagateBlacklistEntry(change)\n    });\n    \n    // Redis Streams for cross-region pub/sub\n    this.publishQueue = new RedisStream('blacklist:global');\n  }\n  \n  async blacklistToken(\n    jti: string, \n    userId: string, \n    reason: string,\n    metadata: BlacklistMetadata\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    // 1. Write to PRIMARY Postgres immediately (source of truth)\n    const expiresAt = new Date(Date.now() + 15 * 60 * 1000);\n    await db.query(\n      `INSERT INTO token_blacklist \n       (jti, user_id, expires_at, reason, blacklisted_at, source_region, session_id)\n       VALUES ($1, $2, $3, $4, NOW(), $5, $6)`,\n      [jti, userId, expiresAt, reason, getCurrentRegion(), metadata.sessionId]\n    );\n    \n    logger.info('Token blacklisted in primary DB', {\n      jti,\n      userId,\n      reason,\n      latency: performance.now() - startTime\n    });\n    \n    // 2. Publish to Redis Stream for immediate regional propagation\n    await this.publishQueue.add('blacklist', {\n      jti,\n      userId,\n      expiresAt: expiresAt.toISOString(),\n      reason,\n      sourceRegion: getCurrentRegion(),\n      timestamp: Date.now()\n    });\n    \n    // 3. Update local region Redis immediately\n    await this.regionalRedis.get(getCurrentRegion())?.setex(\n      `blacklist:jti:${jti}`,\n      15 * 60,\n      JSON.stringify({ userId, reason, blacklistedAt: new Date() })\n    );\n    \n    // 4. Add to local bloom filter immediately\n    blacklistBloomFilter.add(jti);\n    \n    metrics.histogram('blacklist.propagation.latency', performance.now() - startTime);\n  }\n  \n  async propagateBlacklistEntry(entry: BlacklistEntry): Promise<void> {\n    // Propagate to all regional Redis clusters\n    const propagationPromises = Array.from(this.regionalRedis.entries())\n      .filter(([region]) => region !== entry.sourceRegion) // Skip source\n      .map(([region, redis]) => \n        this.propagateToRegion(region, redis, entry)\n      );\n    \n    await Promise.allSettled(propagationPromises);\n  }\n  \n  private async propagateToRegion(\n    region: Region,\n    redis: RedisClient,\n    entry: BlacklistEntry\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    try {\n      await redis.setex(\n        `blacklist:jti:${entry.jti}`,\n        entry.ttlSeconds,\n        JSON.stringify(entry)\n      );\n      \n      metrics.histogram('blacklist.cross_region_propagation', \n        performance.now() - startTime,\n        { source: entry.sourceRegion, target: region }\n      );\n      \n    } catch (error) {\n      logger.error('Failed to propagate blacklist to region', {\n        region,\n        jti: entry.jti,\n        error: error.message\n      });\n      \n      metrics.increment('blacklist.propagation.failure', {\n        source: entry.sourceRegion,\n        target: region\n      });\n      \n      // Not critical - Postgres replication will eventually sync\n    }\n  }\n  \n  // Check blacklist with cross-region awareness\n  async checkBlacklist(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): Promise<BlacklistCheckResult> {\n    \n    // 1. Check local Redis first (99.9% of requests)\n    const localResult = await this.regionalRedis.get(currentRegion)?.get(\n      `blacklist:jti:${jti}`\n    );\n    \n    if (localResult !== null) {\n      return { \n        blacklisted: true, \n        source: 'local_redis',\n        latency: 1 \n      };\n    }\n    \n    // 2. Determine if we need synchronous cross-region check\n    const needsSyncCheck = this.shouldCheckAllRegions(\n      jti,\n      riskLevel,\n      tokenIssuedAt,\n      currentRegion,\n      tokenIssuedInRegion\n    );\n    \n    if (!needsSyncCheck) {\n      // Trust local cache (low risk)\n      return { \n        blacklisted: false, \n        source: 'local_redis',\n        latency: 1\n      };\n    }\n    \n    // 3. Check PRIMARY Postgres (authoritative source)\n    // This handles cross-region edge cases\n    const startTime = performance.now();\n    const pgResult = await db.query({\n      name: 'check_blacklist_global',\n      text: `SELECT 1 FROM token_blacklist \n             WHERE jti = $1 AND expires_at > NOW() \n             LIMIT 1`,\n      values: [jti]\n    });\n    \n    const latency = performance.now() - startTime;\n    \n    metrics.histogram('blacklist.cross_region_check.latency', latency, {\n      risk_level: riskLevel,\n      cross_region: tokenIssuedInRegion !== currentRegion\n    });\n    \n    return {\n      blacklisted: pgResult.rowCount > 0,\n      source: 'postgres_primary',\n      latency\n    };\n  }\n  \n  private shouldCheckAllRegions(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): boolean {\n    \n    // Always check Postgres for high-risk operations\n    if (riskLevel === 'high') {\n      return true;\n    }\n    \n    // Check if token was issued in different region (possible logout in other region)\n    if (tokenIssuedInRegion && tokenIssuedInRegion !== currentRegion) {\n      return true;\n    }\n    \n    // Check if token is very recently issued (< 5 minutes)\n    // Likely from recent logout that may not have propagated yet\n    const tokenAge = Date.now() - tokenIssuedAt;\n    if (tokenAge < 5 * 60 * 1000) {\n      return true;\n    }\n    \n    return false;\n  }\n}\n\ninterface BlacklistEntry {\n  jti: string;\n  userId: string;\n  expiresAt: string;\n  reason: string;\n  sourceRegion: Region;\n  ttlSeconds: number;\n  timestamp: number;\n  sessionId?: string;\n}\n\ninterface BlacklistMetadata {\n  sessionId?: string;\n  deviceId?: string;\n  ipAddress?: string;\n  userAgent?: string;\n}\n\ninterface BlacklistCheckResult {\n  blacklisted: boolean;\n  source: 'local_redis' | 'remote_redis' | 'postgres_primary' | 'bloom_filter';\n  latency: number;\n}\n\ntype Region = 'us-east' | 'us-west' | 'eu-west' | 'ap-southeast' | 'ap-northeast';\n\nfunction getCurrentRegion(): Region {\n  return process.env.AWS_REGION as Region || 'us-east';\n}\n```\n\n### 3. Cross-Region Token Refresh Strategy\n\n**Challenge**: User logs in US-EAST, travels to EU-WEST, refreshes token.\n\n**Requirements**:\n- \u2705 Refresh must succeed (user mobility)\n- \u2705 Prevent token reuse attacks across regions\n- \u2705 Detect compromised token families\n\n**Solution**: **Regional read replicas + primary write with conflict detection**\n\n```typescript\nasync function performCrossRegionRefresh(\n  refreshToken: string,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<AuthResponse> {\n  \n  const tokenHash = hashToken(refreshToken);\n  const startTime = performance.now();\n  \n  // 1. Acquire distributed lock (prefer local Redis, fallback to Postgres)\n  const lockAcquired = await acquireCrossRegionLock(tokenHash, currentRegion);\n  \n  if (!lockAcquired) {\n    throw new AuthError('CONCURRENT_REFRESH_IN_PROGRESS');\n  }\n  \n  try {\n    // 2. Read token from PRIMARY Postgres (not replica)\n    // This ensures we see the absolute latest state across all regions\n    const client = await getPrimaryDbClient();\n    \n    await client.query('BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE');\n    \n    const tokenResult = await client.query(\n      `SELECT * FROM refresh_tokens \n       WHERE token_hash = $1 \n       FOR UPDATE`,\n      [tokenHash]\n    );\n    \n    if (tokenResult.rowCount === 0) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n    \n    const tokenData = tokenResult.rows[0];\n    \n    // 3. Enhanced validation for cross-region refresh\n    await validateRefreshToken(tokenData, fingerprint, currentRegion);\n    \n    // 4. Check for token reuse (CRITICAL for cross-region security)\n    if (tokenData.used) {\n      // Token reuse detected - revoke entire family\n      await handleTokenFamilyCompromise(\n        tokenData.token_family_id,\n        'CROSS_REGION_TOKEN_REUSE',\n        client\n      );\n      \n      throw new AuthError('TOKEN_REUSE_DETECTED');\n    }\n    \n    // 5. Mark token as used (in PRIMARY database)\n    await client.query(\n      `UPDATE refresh_tokens \n       SET used = true, \n           used_at = NOW(),\n           used_in_region = $2\n       WHERE token_hash = $1`,\n      [tokenHash, currentRegion]\n    );\n    \n    // 6. Generate new token pair\n    const user = await getUserById(tokenData.user_id);\n    \n    const newAccessToken = generateAccessToken(user, {\n      sessionId: tokenData.session_id,\n      deviceId: tokenData.device_id,\n      tokenVersion: user.token_version,\n      issuedInRegion: currentRegion,  // NEW: Track issuing region\n      previousRegion: tokenData.issued_in_region\n    });\n    \n    const newRefreshToken = await generateAndStoreRefreshToken(\n      user,\n      {\n        sessionId: tokenData.session_id,\n        deviceId: tokenData.device_id,\n        deviceFingerprint: fingerprint,\n        parentTokenId: tokenData.id,\n        tokenFamilyId: tokenData.token_family_id,\n        timesRefreshed: tokenData.times_refreshed + 1,\n        issuedInRegion: currentRegion,  // NEW: Track issuing region\n        previousRegion: tokenData.issued_in_region\n      },\n      client\n    );\n    \n    await client.query('COMMIT');\n    \n    // 7. Update regional cache (eventual consistency is OK here)\n    try {\n      await cache.set(\n        `refresh:${tokenHash}:used`,\n        true,\n        900\n      );\n    } catch {\n      // Not critical\n    }\n    \n    metrics.histogram('auth.cross_region_refresh.latency', \n      performance.now() - startTime,\n      { \n        from_region: tokenData.issued_in_region,\n        to_region: currentRegion\n      }\n    );\n    \n    return {\n      accessToken: newAccessToken.token,\n      refreshToken: newRefreshToken.token,\n      expiresIn: 900,\n      tokenType: 'Bearer',\n      refreshExpiresIn: 2592000\n    };\n    \n  } finally {\n    await releaseCrossRegionLock(tokenHash, currentRegion);\n  }\n}\n\nasync function validateRefreshToken(\n  tokenData: RefreshTokenRow,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<void> {\n  \n  // 1. Check expiry\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  // 2. Check revocation\n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Validate fingerprint\n  const fingerprintHash = hashFingerprint(fingerprint);\n  if (tokenData.device_fingerprint_hash !== fingerprintHash) {\n    \n    // Cross-region fingerprint mismatch could be legitimate (VPN, proxy)\n    // Check if user has history of cross-region usage\n    const userTravelPattern = await getUserTravelPattern(tokenData.user_id);\n    \n    if (!userTravelPattern.crossRegionAllowed) {\n      // User has never used service from multiple regions - suspicious\n      await createSecurityEvent({\n        userId: tokenData.user_id,\n        eventType: 'CROSS_REGION_FINGERPRINT_MISMATCH',\n        severity: 'HIGH',\n        details: {\n          originalRegion: tokenData.issued_in_region,\n          currentRegion,\n          originalFingerprint: tokenData.device_fingerprint_hash.substring(0, 16),\n          currentFingerprint: fingerprintHash.substring(0, 16)\n        }\n      });\n      \n      throw new AuthError('FINGERPRINT_MISMATCH');\n    }\n    \n    // Log but allow (user has established cross-region pattern)\n    logger.warn('Cross-region fingerprint mismatch allowed', {\n      userId: tokenData.user_id,\n      fromRegion: tokenData.issued_in_region,\n      toRegion: currentRegion\n    });\n  }\n  \n  // 4. Check refresh count (detect infinite refresh loops)\n  if (tokenData.times_refreshed > 1000) {\n    // Suspicious - single refresh chain shouldn't exceed 1000 refreshes\n    // (30 days / 15 min = 2880 max normal refreshes, but user should re-login)\n    await createSecurityEvent({\n      userId: tokenData.user_id,\n      eventType: 'EXCESSIVE_REFRESH_COUNT',\n      severity: 'MEDIUM',\n      details: {\n        timesRefreshed: tokenData.times_refreshed,\n        tokenFamilyId: tokenData.token_family_id\n      }\n    });\n    \n    throw new AuthError('REFRESH_LIMIT_EXCEEDED');\n  }\n}\n\n// Cross-region distributed lock using Redis + Postgres fallback\nasync function acquireCrossRegionLock(\n  tokenHash: string,\n  region: Region,\n  timeoutMs: number = 5000\n): Promise<boolean> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  const lockValue = `${region}:${uuidv4()}:${Date.now()}`;\n  \n  // Try regional Redis first\n  const regionalRedis = getRegionalRedis(region);\n  \n  try {\n    const acquired = await regionalRedis.set(\n      lockKey,\n      lockValue,\n      'NX',\n      'PX',\n      timeoutMs\n    );\n    \n    if (acquired) {\n      return true;\n    }\n  } catch (error) {\n    logger.warn('Regional Redis lock failed, trying Postgres', {\n      tokenHash: tokenHash.substring(0, 16),\n      region,\n      error: error.message\n    });\n  }\n  \n  // Fallback to Postgres advisory lock\n  // Use hash of token as lock ID (deterministic across regions)\n  const lockId = hashToInt64(tokenHash);\n  \n  const result = await db.query(\n    'SELECT pg_try_advisory_lock($1) as acquired',\n    [lockId]\n  );\n  \n  return result.rows[0].acquired;\n}\n\nasync function releaseCrossRegionLock(\n  tokenHash: string,\n  region: Region\n): Promise<void> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  \n  // Try Redis first\n  try {\n    const regionalRedis = getRegionalRedis(region);\n    await regionalRedis.del(lockKey);\n  } catch (error) {\n    // Fallback to Postgres advisory lock release\n    const lockId = hashToInt64(tokenHash);\n    await db.query('SELECT pg_advisory_unlock($1)', [lockId]);\n  }\n}\n```\n\n### 4. Enhanced Schema for Multi-Region Support\n\n```sql\n-- Refresh tokens table with region tracking\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  token_hash VARCHAR(64) NOT NULL UNIQUE,\n  user_id UUID NOT NULL REFERENCES users(id),\n  session_id UUID NOT NULL,\n  device_id VARCHAR(255) NOT NULL,\n  device_fingerprint_hash VARCHAR(64) NOT NULL,\n  \n  -- NEW: Region tracking\n  issued_in_region VARCHAR(50) NOT NULL,\n  used_in_region VARCHAR(50),\n  \n  expires_at TIMESTAMP NOT NULL,\n  issued_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  used BOOLEAN DEFAULT FALSE,\n  used_at TIMESTAMP,\n  revoked BOOLEAN DEFAULT FALSE,\n  revoked_at TIMESTAMP,\n  \n  -- Token family tracking\n  token_family_id UUID NOT NULL,\n  parent_token_id UUID REFERENCES refresh_tokens(id),\n  times_refreshed INTEGER DEFAULT 0,\n  \n  -- Audit\n  ip_address INET,\n  user_agent TEXT,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Composite index for cross-region lookups\nCREATE INDEX idx_refresh_tokens_hash_region ON refresh_tokens(token_hash, issued_in_region);\nCREATE INDEX idx_refresh_tokens_family ON refresh_tokens(token_family_id, issued_at DESC);\nCREATE INDEX idx_refresh_tokens_user_active ON refresh_tokens(user_id, expires_at) \n  WHERE NOT used AND NOT revoked;\n\n-- Token blacklist with region tracking\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  reason VARCHAR(100),\n  session_id UUID,\n  \n  -- NEW: Region tracking for propagation monitoring\n  source_region VARCHAR(50) NOT NULL,\n  blacklisted_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  \n  ip_address INET,\n  user_agent TEXT\n);\n\n-- Index for cross-region blacklist checks\nCREATE INDEX idx_blacklist_active ON token_blacklist(jti, expires_at)\n  WHERE expires_at > NOW();\nCREATE INDEX idx_blacklist_user_recent ON token_blacklist(user_id, revoked_at DESC);\nCREATE INDEX idx_blacklist_region ON token_blacklist(source_region, blacklisted_at DESC);\n\n-- User travel patterns (for risk assessment)\nCREATE TABLE user_region_history (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id),\n  region VARCHAR(50) NOT NULL,\n  first_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  last_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  request_count INTEGER DEFAULT 1,\n  \n  UNIQUE(user_id, region)\n);\n\nCREATE INDEX idx_user_region_history_user ON user_region_history(user_id, last_seen DESC);\n```\n\n## Load Testing & Failure Scenarios\n\n### 1. Load Testing Plan\n\n```typescript\n// Load test scenarios\nconst loadTestScenarios = [\n  {\n    name: 'Peak Traffic - Normal Operation',\n    duration: '10min',\n    rps: 100000, // 100K requests/sec\n    operations: {\n      tokenValidation: 0.70,  // 70% reads (API calls)\n      tokenRefresh: 0.25,     // 25% refresh operations\n      logout: 0.05            // 5% logout (blacklist writes)\n    },\n    assertions: {\n      p99Latency: '<50ms',\n      errorRate: '<0.01%',\n      redisHitRate: '>99.5%'\n    }\n  },\n  \n  {\n    name: 'Redis Failure - Circuit Breaker Activation',\n    duration: '5min',\n    setup: 'Stop Redis at t=60s, restart at t=180s',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerOpens: '<10s after Redis down',\n      fallbackLatency: '<100ms p99',\n      errorRate: '<1%',\n      recoveryTime: '<30s after Redis up',\n      noDataLoss: 'All blacklist entries in Postgres'\n    }\n  },\n  \n  {\n    name: 'Postgres Connection Pool Exhaustion',\n    duration: '3min',\n    setup: 'max_connections=100, 20 app instances',\n    rps: 10000,\n    operations: {\n      // Force all requests to Postgres (Redis disabled)\n      tokenValidation: 0.90,\n      tokenRefresh: 0.10\n    },\n    assertions: {\n      connectionPoolUtilization: '<80%',\n      p99Latency: '<200ms',\n      errorRate: '<5%',\n      noConnectionLeaks: 'Pool size stable'\n    }\n  },\n  \n  {\n    name: 'Circuit Breaker Flapping',\n    duration: '10min',\n    setup: 'Intermittent Redis failures (30s down, 30s up)',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerFlaps: '<5 times',\n      halfOpenDuration: '<30s',\n      errorRate: '<2%',\n      cacheCoherence: 'Local cache syncs after recovery'\n    }\n  },\n  \n  {\n    name: 'Cross-Instance Rate Limit Bypass',\n    duration: '5min',\n    setup: '10 app instances, rate limit 100 req/min per user',\n    rps: 5000,\n    operations: {\n      // Single user attacking across all instances\n      singleUserAttack: true,\n      targetRate: 1000 // 10x rate limit\n    },\n    assertions: {\n      effectiveRateLimit: '<150 req/min', // Allow 50% overage\n      postgresSync: 'Within 10 seconds',\n      blockAfterSync: 'Within 1 request'\n    }\n  },\n  \n  {\n    name: 'Cross-Region Token Refresh',\n    duration: '10min',\n    setup: 'Users refresh from different region than login',\n    rps: 10000,\n    operations: {\n      loginRegion: 'us-east',\n      refreshRegion: 'eu-west',\n      percentCrossRegion: 50\n    },\n    assertions: {\n      crossRegionLatency: '<200ms p99',\n      tokenReuseDetection: '100%',\n      blacklistPropagation: '<500ms p99',\n      noFalsePositives: 'Zero legitimate refreshes blocked'\n    }\n  },\n  \n  {\n    name: 'Bloom Filter False Positive Rate',\n    duration: '30min',\n    setup: '1M active tokens, 100K blacklisted',\n    rps: 100000,\n    operations: {\n      tokenValidation: 1.0\n    },\n    assertions: {\n      bloomFalsePositiveRate: '<0.01%',\n      postgresCheckRate: '<10 req/s',\n      noFalseNegatives: 'Zero blacklisted tokens accepted'\n    }\n  },\n  \n  {\n    name: 'Postgres Primary Failover During Refresh',\n    duration: '5min',\n    setup: 'Trigger Postgres failover at t=120s',\n    rps: 5000,\n    operations: {\n      tokenRefresh: 1.0\n    },\n    assertions: {\n      failoverWindow: '<30s',\n      errorsDuringFailover: 'Expected (graceful degradation)',\n      noTokenReuseBugs: 'Zero duplicate refresh tokens',\n      recoveryTime: '<10s after promotion'\n    }\n  }\n];\n\n// Chaos engineering scenarios\nconst chaosScenarios = [\n  {\n    name: 'Network Partition - Region Isolation',\n    setup: 'Isolate EU-WEST region for 60 seconds',\n    expectedBehavior: [\n      'EU-WEST continues serving from local Redis + Postgres replica',\n      'New blacklists in US-EAST do not propagate to EU-WEST',\n      'High-risk operations in EU-WEST check PRIMARY Postgres (may fail)',\n      'After partition heals, blacklists propagate within 5 seconds',\n      'No token reuse exploits during or after partition'\n    ],\n    securityValidation: [\n      'Verify high-risk ops fail safely during partition',\n      'Verify no blacklisted tokens accepted in EU-WEST',\n      'Verify token family compromise detected cross-region'\n    ]\n  },\n  \n  {\n    name: 'Postgres Primary Failover',\n    setup: 'Promote replica to primary (30s downtime)',\n    expectedBehavior: [\n      'Token refreshes fail during failover window',\n      'Token validation continues (cached data)',\n      'After promotion, refreshes resume within 10 seconds',\n      'No token reuse vulnerabilities introduced',\n      'No orphaned locks or leaked connections'\n    ]\n  },\n  \n  {\n    name: 'Memory Pressure - Cache Eviction',\n    setup: 'Fill LRU cache to capacity, continue adding entries',\n    expectedBehavior: [\n      'LRU evicts oldest entries',\n      'Cache hit rate remains >95%',\n      'No memory leak (heap size stable)',\n      'Evicted entries refetched from Redis/Postgres',\n      'No blacklisted tokens incorrectly evicted'\n    ]\n  },\n  \n  {\n    name: 'Redis Sentinel Failover',\n    setup: 'Kill Redis master, wait for sentinel promotion',\n    expectedBehavior: [\n      'Circuit breaker opens within 10s',\n      'Fallback to Postgres during failover (<30s)',\n      'After promotion, circuit breaker closes',\n      'No blacklist entries lost during failover',\n      'Cache rebuilt from Postgres on reconnect'\n    ]\n  }\n];\n```\n\n### 2. Load Testing Implementation\n\n```typescript\n// Load testing harness using k6\nimport { check, group, sleep } from 'k6';\nimport http from 'k6/http';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 50000 }, // Ramp up\n    { duration: '5m', target: 100000 }, // Peak\n    { duration: '2m', target: 0 }     // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(99)<50'],\n    http_req_failed: ['rate<0.0001'],\n    'circuit_breaker_open': ['count<1']\n  }\n};\n\nexport default function() {\n  const scenario = Math.random();\n  \n  if (scenario < 0.70) {\n    // Token validation (70% of traffic)\n    testTokenValidation();\n  } else if (scenario < 0.95) {\n    // Token refresh (25% of traffic)\n    testTokenRefresh();\n  } else {\n    // Logout (5% of traffic)\n    testLogout();\n  }\n  \n  sleep(Math.random() * 2); // Random delay 0-2s\n}\n\nfunction testTokenValidation() {\n  const token = __ENV.TEST_ACCESS_TOKEN;\n  \n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${token}` }\n  });\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'latency < 50ms': (r) => r.timings.duration < 50,\n    'cache hit': (r) => r.headers['X-Cache-Layer'] === 'redis'\n  });\n}\n\nfunction testTokenRefresh() {\n  const refreshToken = __ENV.TEST_REFRESH_TOKEN;\n  \n  const res = http.post('https://api.example.com/auth/refresh', \n    JSON.stringify({ refreshToken }),\n    { headers: { 'Content-Type': 'application/json' } }\n  );\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'returns new tokens': (r) => r.json('accessToken') !== undefined,\n    'latency < 100ms': (r) => r.timings.duration < 100\n  });\n}\n\n// Failure injection for chaos testing\nexport function injectRedisFailure() {\n  // Use Kubernetes pod deletion or network policy\n  exec(`kubectl delete pod -l app=redis-cache -n production`);\n  \n  sleep(10); // Wait for circuit breaker to trip\n  \n  // Verify fallback behavior\n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${__ENV.TEST_ACCESS_TOKEN}` }\n  });\n  \n  check(res, {\n    'fallback successful': (r) => r.status === 200,\n    'postgres fallback used': (r) => r.headers['X-Cache-Layer'] === 'postgres'\n  });\n}\n```\n\n## Operational Runbook\n\n### 1. Incident Response Playbooks\n\n```yaml\n# Runbook: Redis Unavailable (Circuit Breaker Open)\n\nincident_type: \"Redis Unavailable\"\nseverity: P2 (High)\ndetection: Alert \"RedisCircuitBreakerOpen\" fires\n\nimmediate_actions:\n  - verify_fallback:\n      cmd: \"curl https://api.example.com/health/fallback\"\n      expected: '{\"status\":\"degraded\",\"fallback\":\"postgres\",\"latency_p99\":150}'\n      \n  - check_postgres_load:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'active'\"\n      threshold: \"< 80% of max_connections\"\n      \n  - verify_no_data_loss:\n      cmd: \"SELECT COUNT(*) FROM token_blacklist WHERE blacklisted_at > NOW() - INTERVAL '5 minutes'\"\n      compare_with: \"Should match expected logout rate\"\n\ninvestigation:\n  - check_redis_health:\n      - \"kubectl get pods -n production -l app=redis\"\n      - \"kubectl logs -n production -l app=redis --tail=100\"\n      - \"redis-cli -h redis.internal PING\"\n      \n  - check_sentinel_status:\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL masters\"\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL slaves auth-master\"\n      \n  - check_network:\n      - \"kubectl get networkpolicies -n production\"\n      - \"traceroute redis.internal\"\n\nmitigation:\n  - if_redis_pod_crash:\n      action: \"Kubernetes should auto-restart\"\n      verify: \"Wait 30s, check if circuit breaker closes\"\n      \n  - if_sentinel_failover:\n      action: \"Wait for automatic failover (< 30s)\"\n      verify: \"SENTINEL masters shows new master\"\n      \n  - if_persistent_failure:\n      action: \"Scale Postgres read replicas\"\n      cmd: \"kubectl scale deployment postgres-replica --replicas=5\"\n\nrecovery_verification:\n  - circuit_breaker_closed:\n      query: \"circuit_breaker_state{service='redis'} == 0\"\n      \n  - latency_normal:\n      query: \"histogram_quantile(0.99, cache_get_latency) < 10\"\n      \n  - error_rate_normal:\n      query: \"rate(http_requests_total{status=~'5..'}[5m]) < 0.001\"\n\npost_mortem:\n  - analyze_root_cause: true\n  - update_circuit_breaker_thresholds: \"if flapping observed\"\n  - review_postgres_capacity: \"if connection pool saturated\"\n```\n\n```yaml\n# Runbook: Cross-Region Blacklist Propagation Lag\n\nincident_type: \"Cross-Region Blacklist Propagation Delayed\"\nseverity: P3 (Medium)\ndetection: Alert \"BlacklistPropagationLatencyHigh\" fires\n\nimmediate_actions:\n  - check_propagation_metrics:\n      query: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) > 2000\"\n      \n  - verify_security_impact:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE event_type = 'BLACKLISTED_TOKEN_ACCEPTED' AND created_at > NOW() - INTERVAL '5 minutes'\"\n      expected: \"0 (no security breach)\"\n      \n  - check_redis_streams:\n      cmd: \"redis-cli XINFO STREAM blacklist:global\"\n      verify: \"Stream is processing entries\"\n\ninvestigation:\n  - check_network_latency:\n      cmd: \"ping -c 10 redis-eu-west.internal\"\n      threshold: \"< 100ms average\"\n      \n  - check_postgres_replication_lag:\n      cmd: \"SELECT EXTRACT(EPOCH FROM (NOW() - pg_last_xact_replay_timestamp())) AS lag_seconds FROM pg_stat_replication\"\n      threshold: \"< 5 seconds\"\n      \n  - analyze_propagation_failures:\n      query: \"sum(rate(blacklist_propagation_failure_total[5m])) by (source, target)\"\n\nmitigation:\n  - if_network_partition:\n      action: \"Wait for network to heal (automatic)\"\n      impact: \"High-risk ops will check PRIMARY Postgres\"\n      verify: \"Propagation resumes after partition heals\"\n      \n  - if_redis_streams_backlog:\n      action: \"Add more consumer groups\"\n      cmd: \"redis-cli XGROUP CREATE blacklist:global consumers-2 0 MKSTREAM\"\n      \n  - if_postgres_replication_lag:\n      action: \"Investigate replication bottleneck\"\n      escalate: \"Database team\"\n\nemergency_mitigation:\n  - force_postgres_checks:\n      description: \"Temporarily force all blacklist checks to PRIMARY Postgres\"\n      cmd: \"kubectl set env deployment/api-server BLACKLIST_FORCE_POSTGRES_CHECK=true\"\n      impact: \"Increased latency (p99: 50ms \u2192 150ms), but guaranteed consistency\"\n      rollback: \"After propagation lag resolves\"\n\nrecovery_verification:\n  - propagation_latency_normal:\n      query: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) < 500\"\n      \n  - no_security_incidents:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE severity IN ('HIGH', 'CRITICAL') AND created_at > NOW() - INTERVAL '10 minutes'\"\n      expected: \"0\"\n\npost_mortem:\n  - analyze_network_topology: true\n  - review_replication_configuration: true\n  - consider_dedicated_propagation_channel: \"if recurring issue\"\n```\n\n```yaml\n# Runbook: Token Reuse Attack Detected\n\nincident_type: \"Token Reuse Attack Detected\"\nseverity: P1 (Critical)\ndetection: Alert \"TokenReuseAttackDetected\" fires\n\nimmediate_actions:\n  - identify_affected_users:\n      cmd: \"SELECT DISTINCT user_id FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' AND created_at > NOW() - INTERVAL '5 minutes'\"\n      \n  - verify_family_revocation:\n      cmd: \"SELECT COUNT(*) FROM refresh_tokens WHERE token_family_id IN (SELECT DISTINCT token_family_id FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' AND created_at > NOW() - INTERVAL '5 minutes') AND revoked = false\"\n      expected: \"0 (all families revoked)\"\n      \n  - check_attack_scope:\n      query: \"sum(rate(security_events_total{event_type='TOKEN_REUSE_DETECTED'}[5m]))\"\n      threshold: \"> 10/min indicates widespread attack\"\n\ninvestigation:\n  - analyze_attack_pattern:\n      cmd: \"SELECT user_id, session_id, ip_address, user_agent, issued_in_region, used_in_region FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' ORDER BY created_at DESC LIMIT 100\"\n      \n  - check_for_compromised_devices:\n      cmd: \"SELECT device_id, COUNT(DISTINCT user_id) FROM security_events WHERE event_type = 'TOKEN_REUSE_DETECTED' GROUP BY device_id HAVING COUNT(DISTINCT user_id) > 1\"\n      \n  - correlate_with_other_attacks:\n      cmd: \"SELECT event_type, COUNT(*) FROM security_events WHERE created_at > NOW() - INTERVAL '1 hour' GROUP BY event_type ORDER BY COUNT(*) DESC\"\n\nmitigation:\n  - if_single_user_compromised:\n      action: \"User's tokens already revoked (automatic)\"\n      followup: \"Notify user of security incident\"\n      cmd: \"Send security alert email\"\n      \n  - if_widespread_attack:\n      action: \"Enable enhanced monitoring\"\n      cmd: \"kubectl set env deployment/api-server TOKEN_REUSE_DETECTION_SENSITIVITY=high\"\n      \n  - if_credential_stuffing:\n      action: \"Enable rate limiting on /auth/refresh\"\n      cmd: \"kubectl set env deployment/api-server REFRESH_RATE_LIMIT=10\"\n\nemergency_mitigation:\n  - if_mass_compromise:\n      description: \"Rotate signing keys to invalidate all tokens\"\n      cmd: \"kubectl exec -it api-server-0 -- node scripts/rotate-jwt-keys.js\"\n      impact: \"All users forced to re-login\"\n      approval_required: \"VP Engineering\"\n\nrecovery_verification:\n  - attack_stopped:\n      query: \"rate(security_events_total{event_type='TOKEN_REUSE_DETECTED'}[5m]) == 0\"\n      \n  - no_new_compromises:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE event_type IN ('TOKEN_REUSE_DETECTED', 'CROSS_REGION_TOKEN_REUSE') AND created_at > NOW() - INTERVAL '10 minutes'\"\n      expected: \"0\"\n      \n  - affected_users_notified:\n      verify: \"Check notification logs\"\n\npost_mortem:\n  - forensic_analysis: \"How were tokens stolen?\"\n  - improve_detection: \"Add additional signals to security_events\"\n  - user_communication: \"Transparency about incident\"\n  - regulatory_notification: \"GDPR/CCPA breach notification if required\"\n```\n\n### 2. Monitoring Dashboard\n\n```typescript\n// Grafana dashboard JSON\nconst authSystemDashboard = {\n  title: \"JWT Auth System - Production Monitoring\",\n  panels: [\n    {\n      title: \"Request Rate by Operation\",\n      targets: [\n        'sum(rate(http_requests_total{service=\"auth\"}[5m])) by (operation)',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Cache Hit Rate by Layer\",\n      targets: [\n        'sum(rate(cache_hit_total[5m])) by (layer) / sum(rate(cache_requests_total[5m])) by (layer)',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.95, color: \"green\" },\n        { value: 0.90, color: \"yellow\" },\n        { value: 0.85, color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Circuit Breaker State\",\n      targets: [\n        'circuit_breaker_state{service=\"redis\"}',\n      ],\n      visualization: \"stat\",\n      mappings: [\n        { value: 0, text: \"CLOSED\", color: \"green\" },\n        { value: 0.5, text: \"HALF_OPEN\", color: \"yellow\" },\n        { value: 1, text: \"OPEN\", color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Token Validation Latency (p50, p99, p99.9)\",\n      targets: [\n        'histogram_quantile(0.50, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.99, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.999, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Postgres Connection Pool Utilization\",\n      targets: [\n        'pg_stat_activity_count / pg_settings_max_connections',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.80, color: \"red\" },\n        { value: 0.60, color: \"yellow\" },\n        { value: 0.40, color: \"green\" }\n      ]\n    },\n    \n    {\n      title: \"Blacklist Propagation Latency (Cross-Region)\",\n      targets: [\n        'histogram_quantile(0.99, sum(rate(blacklist_cross_region_propagation_bucket[5m])) by (source, target, le))',\n      ],\n      visualization: \"heatmap\"\n    },\n    \n    {\n      title: \"Security Events (Last 1 Hour)\",\n      targets: [\n        'sum(increase(security_events_total[1h])) by (event_type)',\n      ],\n      visualization: \"table\",\n      transformations: [\n        {\n          id: 'sortBy',\n          options: { fields: {}, sort: [{ field: 'Value', desc: true }] }\n        }\n      ]\n    },\n    \n    {\n      title: \"Bloom Filter Statistics\",\n      targets: [\n        'bloom_filter_entries',\n        'bloom_filter_capacity',\n        'bloom_filter_load_factor',\n        'rate(bloom_filter_false_positive_total[5m])',\n      ],\n      visualization: \"stat\"\n    },\n    \n    {\n      title: \"Cross-Region Refresh Success Rate\",\n      targets: [\n        'sum(rate(auth_cross_region_refresh_success_total[5m])) / sum(rate(auth_cross_region_refresh_total[5m]))',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.99, color: \"green\" },\n        { value: 0.95, color: \"yellow\" },\n        { value: 0.90, color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Token Reuse Detection Rate\",\n      targets: [\n        'rate(security_events_total{event_type=\"TOKEN_REUSE_DETECTED\"}[5m])',\n      ],\n      visualization: \"timeseries\",\n      alert: {\n        condition: '> 10',\n        severity: 'critical',\n        message: 'Token reuse attack detected - possible credential compromise'\n      }\n    }\n  ],\n  \n  alerts: [\n    {\n      name: \"RedisCircuitBreakerOpen\",\n      condition: \"circuit_breaker_state{service='redis'} == 1\",\n      for: \"1m\",\n      severity: \"critical\",\n      runbook: \"https://wiki.internal/runbooks/redis-unavailable\"\n    },\n    {\n      name: \"HighPostgresFallback\",\n      condition: \"rate(cache_fallback_postgres_total[5m]) > 100\",\n      for: \"5m\",\n      severity: \"warning\",\n      runbook: \"https://wiki.internal/runbooks/high-postgres-fallback\"\n    },\n    {\n      name: \"TokenValidationLatencyHigh\",\n      condition: \"histogram_quantile(0.99, auth_token_validation_latency_bucket) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"BloomFilterFalsePositiveRateHigh\",\n      condition: \"rate(bloom_filter_false_positive_total[5m]) > 0.0001\",\n      for: \"10m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"PostgresConnectionPoolSaturated\",\n      condition: \"pg_stat_activity_count / pg_settings_max_connections > 0.8\",\n      for: \"5m\",\n      severity: \"critical\",\n      runbook: \"https://wiki.internal/runbooks/postgres-connection-pool-saturated\"\n    },\n    {\n      name: \"CrossRegionBlacklistPropagationSlow\",\n      condition: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) > 1000\",\n      for: \"5m\",\n      severity: \"warning\",\n      runbook: \"https://wiki.internal/runbooks/blacklist-propagation-lag\"\n    },\n    {\n      name: \"SecurityEventSpike\",\n      condition: \"rate(security_events_total{severity='HIGH'}[5m]) > 10\",\n      for: \"2m\",\n      severity: \"critical\",\n      pagerduty: true\n    },\n    {\n      name: \"TokenReuseAttackDetected\",\n      condition: \"rate(security_events_total{event_type='TOKEN_REUSE_DETECTED'}[5m]) > 5\",\n      for: \"1m\",\n      severity: \"critical\",\n      pagerduty: true,\n      runbook: \"https://wiki.internal/runbooks/token-reuse-attack\"\n    },\n    {\n      name: \"CrossRegionRefreshFailureRateHigh\",\n      condition: \"1 - (sum(rate(auth_cross_region_refresh_success_total[5m])) / sum(rate(auth_cross_region_refresh_total[5m]))) > 0.05\",\n      for: \"5m\",\n      severity: \"warning\"\n    }\n  ]\n};\n```\n\n### 3. Health Check Endpoints\n\n```typescript\n// Health check implementation\napp.get('/health', async (req, res) => {\n  const health = {\n    status: 'healthy',\n    timestamp: new Date().toISOString(),\n    version: process.env.APP_VERSION,\n    uptime: process.uptime(),\n    region: getCurrentRegion(),\n    \n    components: {\n      redis: await checkRedisHealth(),\n      postgres: await checkPostgresHealth(),\n      bloomFilter: await checkBloomFilterHealth(),\n      propagationBus: await checkPropagationBusHealth()\n    },\n    \n    metrics: {\n      circuitBreakerState: circuitBreaker.getState(),\n      cacheHitRate: await getCacheHitRate(),\n      requestRate: await getRequestRate(),\n      errorRate: await getErrorRate(),\n      blacklistPropagationLag: await getBlacklistPropagationLag()\n    }\n  };\n  \n  // Overall health based on components\n  if (health.components.redis.status === 'unhealthy' && \n      health.components.postgres.status === 'unhealthy') {\n    health.status = 'unhealthy';\n    return res.status(503).json(health);\n  }\n  \n  if (health.components.redis.status === 'unhealthy' ||\n      health.components.postgres.status === 'degraded') {\n    health.status = 'degraded';\n    return res.status(200).json(health);\n  }\n  \n  res.status(200).json(health);\n});\n\nasync function checkRedisHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await redis.ping();\n    const latency = Date.now() - start;\n    \n    return {\n      status: latency < 10 ? 'healthy' : 'degraded',\n      latency,\n      message: 'Redis operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Redis unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkPostgresHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await db.query('SELECT 1');\n    const latency = Date.now() - start;\n    \n    const poolSize = await db.query('SELECT count(*) FROM pg_stat_activity');\n    const maxConnections = await db.query('SHOW max_connections');\n    const utilization = poolSize.rows[0].count / maxConnections.rows[0].max_connections;\n    \n    return {\n      status: latency < 50 && utilization < 0.8 ? 'healthy' : 'degraded',\n      latency,\n      metadata: {\n        poolUtilization: `${(utilization * 100).toFixed(1)}%`,\n        activeConnections: poolSize.rows[0].count\n      },\n      message: 'Postgres operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Postgres unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkBloomFilterHealth(): Promise<ComponentHealth> {\n  const stats = blacklistBloomFilter.getStats();\n  const loadFactor = stats.entries / stats.capacity;\n  \n  return {\n    status: loadFactor < 0.8 ? 'healthy' : 'degraded',\n    metadata: {\n      entries: stats.entries,\n      capacity: stats.capacity,\n      loadFactor: `${(loadFactor * 100).toFixed(1)}%`,\n      lastSync: stats.lastSync\n    },\n    message: loadFactor < 0.8 ? 'Bloom filter healthy' : 'Bloom filter approaching capacity'\n  };\n}\n\nasync function checkPropagationBusHealth(): Promise<ComponentHealth> {\n  try {\n    const streamInfo = await redis.xinfo('STREAM', 'blacklist:global');\n    const lag = await getBlacklistPropagationLag();\n    \n    return {\n      status: lag < 1000 ? 'healthy' : 'degraded',\n      metadata: {\n        streamLength: streamInfo.length,\n        propagationLagMs: lag,\n        consumerGroups: streamInfo.groups\n      },\n      message: lag < 1000 ? 'Propagation bus healthy' : 'Propagation lag detected'\n    };\n  } catch (error) {\n    return {\n      status: 'degraded',\n      message: `Propagation bus check failed: ${error.message}`\n    };\n  }\n}\n\nasync function getBlacklistPropagationLag(): Promise<number> {\n  const result = await db.query(\n    `SELECT EXTRACT(EPOCH FROM (NOW() - MAX(blacklisted_at))) * 1000 AS lag_ms\n     FROM token_blacklist\n     WHERE source_region != $1\n     LIMIT 1`,\n    [getCurrentRegion()]\n  );\n  \n  return result.rows[0]?.lag_ms || 0;\n}\n\ninterface ComponentHealth {\n  status: 'healthy' | 'degraded' | 'unhealthy';\n  latency?: number | null;\n  metadata?: Record<string, any>;\n  message: string;\n}\n```\n\n## What I Improved\n\n### 1. **Added Postgres Primary Failover Load Test**\nThe original design had 7 load tests but didn't explicitly test Postgres failover during refresh operations\u2014the most critical write path. I added an 8th scenario specifically testing refresh token operations during database failover to validate there are no token reuse bugs introduced during the failover window.\n\n### 2. **Enhanced Network Partition Chaos Test with Security Validation**\nThe original network partition scenario was good, but I added explicit security validation steps:\n- Verify high-risk operations fail safely (not just continue)\n- Confirm no blacklisted tokens are accepted in isolated region\n- Validate token family compromise detection works cross-region during partition\n\nThis ensures the system degrades **securely** during partitions, not just functionally.\n\n### 3. **Added Cross-Region Blacklist Propagation Lag Runbook**\nThe original design had a runbook for Redis failures but not for blacklist propagation lag\u2014a critical multi-region failure mode. I added a complete runbook including:\n- Detection via Prometheus alerts\n- Investigation steps for network vs. Postgres replication lag\n- Emergency mitigation (force Postgres checks globally)\n- Clear rollback procedures\n\n### 4. **Added Token Reuse Attack Incident Response Runbook**\nThis is the highest severity security incident possible in this system. I added a detailed P1 runbook covering:\n- Immediate identification of affected users\n- Verification of automatic family revocation\n- Attack scope analysis (single user vs. mass compromise)\n- Emergency mitigation including JWT key rotation\n- Post-mortem and regulatory notification requirements\n\n### 5. **Enhanced Monitoring Dashboard with Security Metrics**\nAdded two critical panels the original design missed:\n- **Cross-Region Refresh Success Rate**: Gauge showing if cross-region mobility is working\n- **Token Reuse Detection Rate**: Time series showing active attacks in real-time\n\nAlso added sorting/transformation to the Security Events table to prioritize high-severity events.\n\n### 6. **Added Runbook URLs to Prometheus Alerts**\nAll alerts now include direct links to runbooks (`runbook: \"https://wiki.internal/...\"`). This reduces MTTM (mean time to mitigation) by eliminating the \"where's the runbook?\" step during incidents.\n\n### 7. **Added PagerDuty Integration to Critical Security Alerts**\nTwo alerts now have `pagerduty: true`:\n- SecurityEventSpike (multiple high-severity events)\n- TokenReuseAttackDetected (active attack)\n\nThese require immediate human response and shouldn't rely on passive monitoring.\n\n### 8. **Added Health Check for Propagation Bus**\nThe original `/health` endpoint checked Redis, Postgres, and Bloom Filter, but not the propagation bus itself. I added `checkPropagationBusHealth()` which:\n- Verifies Redis Streams is operational\n- Measures current propagation lag\n- Reports consumer group health\n\nThis gives a complete picture of multi-region health.\n\n### 9. **Added Region Metadata to Health Check Response**\nThe health check now includes `region: getCurrentRegion()` and `blacklistPropagationLag` in the metrics. This helps diagnose cross-region issues by making it obvious which region is reporting and how stale its blacklist is.\n\n### 10. **Added Redis Sentinel Failover Chaos Test**\nThe original chaos tests covered network partitions, Postgres failover, and memory pressure, but not Redis Sentinel failover\u2014a common failure mode in production. I added a scenario testing:\n- Circuit breaker behavior during sentinel promotion\n- Fallback to Postgres during ~30s failover window\n- Cache rebuild from Postgres after reconnection\n- No blacklist data loss\n\n## What I Kept\n\nThe architect's design was excellent. I kept 95% of it:\n\n- \u2705 **Complete multi-region architecture** with Redis Streams + Postgres WAL\n- \u2705 **Risk-based blacklist checking** (low/medium/high risk operations)\n- \u2705 **Cross-region token refresh with token family tracking**\n- \u2705 **User travel pattern tracking** to reduce false positives\n- \u2705 **Region-aware schema** with `issued_in_region` and `used_in_region`\n- \u2705 **Distributed locking** with Redis + Postgres advisory lock fallback\n- \u2705 **Comprehensive fingerprint validation** with cross-region awareness\n- \u2705 **Circuit breaker patterns** with multi-layer caching\n- \u2705 **7 core load test scenarios** (I only added 1 more)\n- \u2705 **3 chaos engineering scenarios** (I added 1 more)\n- \u2705 **Detailed database schema** with proper indexes\n- \u2705 **k6 load testing implementation**\n- \u2705 **Grafana dashboard structure** with 8 panels\n- \u2705 **Component health checks**\n\nThe architecture is production-ready. My improvements focused on **operational gaps** (runbooks, security incident response, monitoring blind spots) rather than architectural changes.\n\n## Remaining Concerns\n\n### 1. **Postgres WAL Subscription Reliability**\n\n**Concern**: The `PostgresReplicationClient` subscribing to WAL for blacklist propagation is a single point of failure. If the WAL subscription drops:\n- Blacklists only propagate via Postgres replication (slower)\n- Redis Streams may get out of sync with Postgres\n\n**Questions**:\n- How do we detect WAL subscription failures?\n- Should we add a health check for WAL subscription status?\n- Do we need a reconciliation loop to compare Redis vs. Postgres blacklists?\n\n**My recommendation**: Add a background job that periodically (every 30s) compares a sample of recent Postgres blacklist entries against Redis. If divergence > 1%, alert and force a full resync.\n\n---\n\n### 2. **Emergency JWT Key Rotation Blast Radius**\n\n**Concern**: The Token Reuse Attack runbook includes emergency key rotation as a mitigation, which forces all users to re-login. For a system with 10M+ users:\n- Mass re-login creates huge load spike on auth system\n- Customer support gets flooded with \"why was I logged out?\" tickets\n- Potential revenue impact if users don't complete re-login\n\n**Questions**:\n- Do we have capacity modeling for \"all users re-login in 1 hour\" scenario?\n- Should we implement gradual key rotation (issue tokens with both old and new keys, accept both for 24h)?\n- What's the user communication plan for this scenario?\n\n**My recommendation**: Implement dual-key verification (accept tokens signed with either key for 24h) to allow gradual key rotation without mass logout.\n\n---\n\n### 3. **Cross-Region Fingerprint Validation Bypass**\n\n**Concern**: The design allows fingerprint mismatches for users with established cross-region patterns (`userTravelPattern.crossRegionAllowed`). This could be exploited:\n- Attacker steals token from user with cross-region history\n- Attacker uses token from different device/fingerprint\n- System allows it because user has `crossRegionAllowed = true`\n\n**Questions**:\n- Should we still require fingerprint match within the same region?\n- Should we have a \"trust level\" for cross-region usage (e.g., allow mismatch only if user crossed regions in last 7 days)?\n- Do we need additional compensating controls (device reputation, IP geolocation)?\n\n**My recommendation**: Add time-based decay to `crossRegionAllowed`. If user hasn't traveled in 90 days, reset to `false`. Also, always enforce fingerprint match within same region.\n\n---\n\n### 4. **Bloom Filter Rebuild Strategy**\n\n**Concern**: The design mentions rebuilding the bloom filter when `load_factor > 0.8`, but doesn't specify the strategy:\n- Do all instances rebuild simultaneously (thundering herd on Postgres)?\n- Do we rebuild in-place (briefly higher false positive rate)?\n- How long does rebuild take for 1M entries?\n\n**Questions**:\n- Should we use leader election (only one instance rebuilds, then distributes)?\n- Should we build new filter alongside old filter, then atomic swap?\n- What's the impact on request latency during rebuild?\n\n**My recommendation**: Use blue/green bloom filter deployment: build new filter in background, validate against Postgres, then atomic swap. Stagger rebuilds across instances (1 instance every 30s).\n\n---\n\n### 5. **Token Family Compromise Detection in Postgres Replica Lag**\n\n**Concern**: Token reuse detection relies on checking `used = true` in the database. If cross-region refresh reads from a lagging replica:\n- Token marked `used` in US-EAST\n- Attacker immediately uses same token in EU-WEST\n- EU-WEST reads from replica still showing `used = false`\n- Attack succeeds until replication catches up\n\n**Questions**:\n- Should cross-region refresh ALWAYS read from PRIMARY (not replica)?\n- Do we need to add replica lag detection to the refresh flow?\n- What's acceptable replica lag before we reject cross-region refreshes?\n\n**My recommendation**: The design already specifies reading from PRIMARY for refresh (`await getPrimaryDbClient()`), so this concern is addressed. However, we should add a safeguard: if cross-region refresh is attempted and replica lag > 5s, force read from PRIMARY with higher timeout.\n\n## Convergence Status\n\n**ITERATING** - Design is 95% complete but needs refinement on operational failure modes.\n\nThis is a **very strong design** that's nearly production-ready. The core architecture, security model, and resilience patterns are excellent. My improvements focused on operational gaps:\n\n\u2705 **Completed**:\n- Multi-region architecture\n- Load testing plan\n- Chaos engineering\n- Core monitoring\n\n\u26a0\ufe0f **Needs Refinement**:\n- WAL subscription reliability monitoring\n- Emergency key rotation procedure\n- Bloom filter rebuild strategy\n- Cross-region fingerprint validation edge cases\n\nThe remaining concerns are **operational edge cases**, not architectural flaws. With 1 more iteration focused on these operational details, this design will be ready for implementation.\n\n---\n\n## PROMPT_FOR_ARCHITECT:\n\nYour design is excellent! I found only **5 operational edge cases** that need clarification:\n\n1. **WAL Subscription Monitoring**: How do we detect if the Postgres WAL subscription fails? Should we add a reconciliation loop comparing Redis vs. Postgres blacklists?\n\n2. **Emergency Key Rotation**: The runbook includes JWT key rotation as emergency mitigation, which forces all users to re-login. Should we implement dual-key verification (accept both old and new keys for 24h) to avoid mass logout?\n\n3. **Cross-Region Fingerprint Bypass**: Users with `crossRegionAllowed = true` can bypass fingerprint checks. Should we add time-based decay (reset to false after 90 days of no travel) or additional compensating controls?\n\n4. **Bloom Filter Rebuild Strategy**: When load_factor > 0.8, how do we rebuild without thundering herd? Should we use blue/green deployment (build new filter in background, atomic swap)?\n\n5. **Token Reuse + Replica Lag**: If cross-region refresh happens during Postgres replica lag, could we miss a token reuse attack? (I think you already handle this by reading from PRIMARY, but want to confirm).\n\nPlease address these 5 operational concerns and I'll signal **ACCEPTING_FINAL**."
      }
    }
  ],
  "final_design": "",
  "status": "debating",
  "started_at": "2026-01-17T09:00:21.575466",
  "ended_at": "",
  "max_rounds": 4,
  "working_dir": ".",
  "current_round": 3,
  "round_times": [
    279.943336725235,
    460.25292325019836,
    434.80793857574463
  ]
}