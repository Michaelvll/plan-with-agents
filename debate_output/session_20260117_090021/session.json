{
  "initial_prompt": "Design authentication system with JWT refresh tokens\n\nRequirements:\n- Secure login flow with JWT tokens\n- Refresh token mechanism for session extension\n- Token revocation capability\n- Protection against common attacks (CSRF, XSS, replay)\n- Support for multiple devices/sessions",
  "status": "consensus",
  "rounds": 3,
  "started_at": "2026-01-17T09:00:21.575466",
  "ended_at": "2026-01-17T09:19:56.680025",
  "final_design": "# JWT Authentication System with Global Distribution & Production Hardening\n\n## Architecture Overview\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                        GLOBAL REGIONS                           \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502                                                                 \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510            \u2502\n\u2502  \u2502   US-EAST        \u2502         \u2502   EU-WEST        \u2502            \u2502\n\u2502  \u2502                  \u2502         \u2502                  \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 API Gateway\u2502  \u2502         \u2502  \u2502 API Gateway\u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Redis      \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Redis      \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 Regional   \u2502  \u2502 Sync    \u2502  \u2502 Regional   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 App Nodes  \u2502  \u2502         \u2502  \u2502 App Nodes  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (3+ pods)  \u2502  \u2502         \u2502  \u2502 (3+ pods)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2502        \u2193         \u2502         \u2502        \u2193         \u2502            \u2502\n\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502         \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502            \u2502\n\u2502  \u2502  \u2502 Postgres   \u2502\u2190\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2192\u2502 Postgres   \u2502  \u2502            \u2502\n\u2502  \u2502  \u2502 (Primary)  \u2502  \u2502 Replica \u2502  \u2502 (Replica)  \u2502  \u2502            \u2502\n\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502         \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502            \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518            \u2502\n\u2502           \u2193                             \u2193                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502         Global Blacklist Propagation Bus             \u2502     \u2502\n\u2502  \u2502         (Pub/Sub: Redis Streams + Postgres WAL)     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Multi-Region Strategy with Consistency Guarantees\n\n### 1. Token Types and Regional Behavior\n\n**Access Tokens** (15-minute lifetime):\n- \u2705 **Stateless validation** - verify signature + check local blacklist\n- \u2705 **Regional Redis cache** - 99.9% of validations hit local cache\n- \u26a0\ufe0f **Blacklist propagation delay** - acceptable (see security analysis)\n\n**Refresh Tokens** (30-day lifetime):\n- \u2705 **Always write to PRIMARY Postgres** (strong consistency)\n- \u2705 **Refresh allowed in any region** (global mobility)\n- \u2705 **Token family tracking** - detects cross-region reuse attacks\n\n### 2. Blacklist Propagation Strategy\n\n**Design Principle**: **\"Eventually consistent blacklist with risk-based synchronous verification\"**\n\n```typescript\ninterface BlacklistPropagationConfig {\n  // How fast does blacklist propagate between regions?\n  propagationTarget: {\n    p50: 50,      // 50ms - Redis Streams\n    p99: 500,     // 500ms - includes Postgres replication\n    p99.9: 2000   // 2 seconds - degraded network\n  };\n  \n  // Which operations require synchronous cross-region check?\n  requireSyncCheck: {\n    highRiskOperations: true,    // Admin actions, delete account, payment\n    tokenAge: '<5min',           // Recently issued tokens (likely from logout)\n    userRiskScore: '>0.7',       // Suspicious activity score\n    crossRegionRefresh: true     // User moved to different region\n  };\n  \n  // Fallback behavior during propagation delay\n  acceptanceCriteria: {\n    maxAge: 300000,              // 5 minutes since token issued\n    requireCompensatingControls: true,\n    logSecurityEvent: true\n  };\n}\n\n// Global blacklist propagation bus\nclass GlobalBlacklistBus {\n  private regionalRedis: Map<Region, RedisClient>;\n  private postgresWalSubscription: PostgresReplicationClient;\n  private publishQueue: RedisStream;\n  \n  constructor() {\n    // Subscribe to Postgres WAL for blacklist INSERTs\n    this.postgresWalSubscription = new PostgresReplicationClient({\n      tables: ['token_blacklist'],\n      operations: ['INSERT'],\n      onMessage: (change) => this.propagateBlacklistEntry(change)\n    });\n    \n    // Redis Streams for cross-region pub/sub\n    this.publishQueue = new RedisStream('blacklist:global');\n  }\n  \n  async blacklistToken(\n    jti: string, \n    userId: string, \n    reason: string,\n    metadata: BlacklistMetadata\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    // 1. Write to PRIMARY Postgres immediately (source of truth)\n    const expiresAt = new Date(Date.now() + 15 * 60 * 1000);\n    await db.query(\n      `INSERT INTO token_blacklist \n       (jti, user_id, expires_at, reason, blacklisted_at, source_region, session_id)\n       VALUES ($1, $2, $3, $4, NOW(), $5, $6)`,\n      [jti, userId, expiresAt, reason, getCurrentRegion(), metadata.sessionId]\n    );\n    \n    logger.info('Token blacklisted in primary DB', {\n      jti,\n      userId,\n      reason,\n      latency: performance.now() - startTime\n    });\n    \n    // 2. Publish to Redis Stream for immediate regional propagation\n    await this.publishQueue.add('blacklist', {\n      jti,\n      userId,\n      expiresAt: expiresAt.toISOString(),\n      reason,\n      sourceRegion: getCurrentRegion(),\n      timestamp: Date.now()\n    });\n    \n    // 3. Update local region Redis immediately\n    await this.regionalRedis.get(getCurrentRegion())?.setex(\n      `blacklist:jti:${jti}`,\n      15 * 60,\n      JSON.stringify({ userId, reason, blacklistedAt: new Date() })\n    );\n    \n    // 4. Add to local bloom filter immediately\n    blacklistBloomFilter.add(jti);\n    \n    metrics.histogram('blacklist.propagation.latency', performance.now() - startTime);\n  }\n  \n  async propagateBlacklistEntry(entry: BlacklistEntry): Promise<void> {\n    // Propagate to all regional Redis clusters\n    const propagationPromises = Array.from(this.regionalRedis.entries())\n      .filter(([region]) => region !== entry.sourceRegion) // Skip source\n      .map(([region, redis]) => \n        this.propagateToRegion(region, redis, entry)\n      );\n    \n    await Promise.allSettled(propagationPromises);\n  }\n  \n  private async propagateToRegion(\n    region: Region,\n    redis: RedisClient,\n    entry: BlacklistEntry\n  ): Promise<void> {\n    const startTime = performance.now();\n    \n    try {\n      await redis.setex(\n        `blacklist:jti:${entry.jti}`,\n        entry.ttlSeconds,\n        JSON.stringify(entry)\n      );\n      \n      metrics.histogram('blacklist.cross_region_propagation', \n        performance.now() - startTime,\n        { source: entry.sourceRegion, target: region }\n      );\n      \n    } catch (error) {\n      logger.error('Failed to propagate blacklist to region', {\n        region,\n        jti: entry.jti,\n        error: error.message\n      });\n      \n      metrics.increment('blacklist.propagation.failure', {\n        source: entry.sourceRegion,\n        target: region\n      });\n      \n      // Not critical - Postgres replication will eventually sync\n    }\n  }\n  \n  // Check blacklist with cross-region awareness\n  async checkBlacklist(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): Promise<BlacklistCheckResult> {\n    \n    // 1. Check local Redis first (99.9% of requests)\n    const localResult = await this.regionalRedis.get(currentRegion)?.get(\n      `blacklist:jti:${jti}`\n    );\n    \n    if (localResult !== null) {\n      return { \n        blacklisted: true, \n        source: 'local_redis',\n        latency: 1 \n      };\n    }\n    \n    // 2. Determine if we need synchronous cross-region check\n    const needsSyncCheck = this.shouldCheckAllRegions(\n      jti,\n      riskLevel,\n      tokenIssuedAt,\n      currentRegion,\n      tokenIssuedInRegion\n    );\n    \n    if (!needsSyncCheck) {\n      // Trust local cache (low risk)\n      return { \n        blacklisted: false, \n        source: 'local_redis',\n        latency: 1\n      };\n    }\n    \n    // 3. Check PRIMARY Postgres (authoritative source)\n    // This handles cross-region edge cases\n    const startTime = performance.now();\n    const pgResult = await db.query({\n      name: 'check_blacklist_global',\n      text: `SELECT 1 FROM token_blacklist \n             WHERE jti = $1 AND expires_at > NOW() \n             LIMIT 1`,\n      values: [jti]\n    });\n    \n    const latency = performance.now() - startTime;\n    \n    metrics.histogram('blacklist.cross_region_check.latency', latency, {\n      risk_level: riskLevel,\n      cross_region: tokenIssuedInRegion !== currentRegion\n    });\n    \n    return {\n      blacklisted: pgResult.rowCount > 0,\n      source: 'postgres_primary',\n      latency\n    };\n  }\n  \n  private shouldCheckAllRegions(\n    jti: string,\n    riskLevel: 'low' | 'medium' | 'high',\n    tokenIssuedAt: number,\n    currentRegion: Region,\n    tokenIssuedInRegion?: Region\n  ): boolean {\n    \n    // Always check Postgres for high-risk operations\n    if (riskLevel === 'high') {\n      return true;\n    }\n    \n    // Check if token was issued in different region (possible logout in other region)\n    if (tokenIssuedInRegion && tokenIssuedInRegion !== currentRegion) {\n      return true;\n    }\n    \n    // Check if token is very recently issued (< 5 minutes)\n    // Likely from recent logout that may not have propagated yet\n    const tokenAge = Date.now() - tokenIssuedAt;\n    if (tokenAge < 5 * 60 * 1000) {\n      return true;\n    }\n    \n    return false;\n  }\n}\n\ninterface BlacklistEntry {\n  jti: string;\n  userId: string;\n  expiresAt: string;\n  reason: string;\n  sourceRegion: Region;\n  ttlSeconds: number;\n  timestamp: number;\n  sessionId?: string;\n}\n\ninterface BlacklistMetadata {\n  sessionId?: string;\n  deviceId?: string;\n  ipAddress?: string;\n  userAgent?: string;\n}\n\ninterface BlacklistCheckResult {\n  blacklisted: boolean;\n  source: 'local_redis' | 'remote_redis' | 'postgres_primary' | 'bloom_filter';\n  latency: number;\n}\n\ntype Region = 'us-east' | 'us-west' | 'eu-west' | 'ap-southeast' | 'ap-northeast';\n\nfunction getCurrentRegion(): Region {\n  return process.env.AWS_REGION as Region || 'us-east';\n}\n```\n\n### 3. Cross-Region Token Refresh Strategy\n\n**Challenge**: User logs in US-EAST, travels to EU-WEST, refreshes token.\n\n**Requirements**:\n- \u2705 Refresh must succeed (user mobility)\n- \u2705 Prevent token reuse attacks across regions\n- \u2705 Detect compromised token families\n\n**Solution**: **Regional read replicas + primary write with conflict detection**\n\n```typescript\nasync function performCrossRegionRefresh(\n  refreshToken: string,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<AuthResponse> {\n  \n  const tokenHash = hashToken(refreshToken);\n  const startTime = performance.now();\n  \n  // 1. Acquire distributed lock (prefer local Redis, fallback to Postgres)\n  const lockAcquired = await acquireCrossRegionLock(tokenHash, currentRegion);\n  \n  if (!lockAcquired) {\n    throw new AuthError('CONCURRENT_REFRESH_IN_PROGRESS');\n  }\n  \n  try {\n    // 2. Read token from PRIMARY Postgres (not replica)\n    // This ensures we see the absolute latest state across all regions\n    const client = await getPrimaryDbClient();\n    \n    await client.query('BEGIN TRANSACTION ISOLATION LEVEL SERIALIZABLE');\n    \n    const tokenResult = await client.query(\n      `SELECT * FROM refresh_tokens \n       WHERE token_hash = $1 \n       FOR UPDATE`,\n      [tokenHash]\n    );\n    \n    if (tokenResult.rowCount === 0) {\n      throw new AuthError('INVALID_REFRESH_TOKEN');\n    }\n    \n    const tokenData = tokenResult.rows[0];\n    \n    // 3. Enhanced validation for cross-region refresh\n    await validateRefreshToken(tokenData, fingerprint, currentRegion);\n    \n    // 4. Check for token reuse (CRITICAL for cross-region security)\n    if (tokenData.used) {\n      // Token reuse detected - revoke entire family\n      await handleTokenFamilyCompromise(\n        tokenData.token_family_id,\n        'CROSS_REGION_TOKEN_REUSE',\n        client\n      );\n      \n      throw new AuthError('TOKEN_REUSE_DETECTED');\n    }\n    \n    // 5. Mark token as used (in PRIMARY database)\n    await client.query(\n      `UPDATE refresh_tokens \n       SET used = true, \n           used_at = NOW(),\n           used_in_region = $2\n       WHERE token_hash = $1`,\n      [tokenHash, currentRegion]\n    );\n    \n    // 6. Generate new token pair\n    const user = await getUserById(tokenData.user_id);\n    \n    const newAccessToken = generateAccessToken(user, {\n      sessionId: tokenData.session_id,\n      deviceId: tokenData.device_id,\n      tokenVersion: user.token_version,\n      issuedInRegion: currentRegion,  // NEW: Track issuing region\n      previousRegion: tokenData.issued_in_region\n    });\n    \n    const newRefreshToken = await generateAndStoreRefreshToken(\n      user,\n      {\n        sessionId: tokenData.session_id,\n        deviceId: tokenData.device_id,\n        deviceFingerprint: fingerprint,\n        parentTokenId: tokenData.id,\n        tokenFamilyId: tokenData.token_family_id,\n        timesRefreshed: tokenData.times_refreshed + 1,\n        issuedInRegion: currentRegion,  // NEW: Track issuing region\n        previousRegion: tokenData.issued_in_region\n      },\n      client\n    );\n    \n    await client.query('COMMIT');\n    \n    // 7. Update regional cache (eventual consistency is OK here)\n    try {\n      await cache.set(\n        `refresh:${tokenHash}:used`,\n        true,\n        900\n      );\n    } catch {\n      // Not critical\n    }\n    \n    metrics.histogram('auth.cross_region_refresh.latency', \n      performance.now() - startTime,\n      { \n        from_region: tokenData.issued_in_region,\n        to_region: currentRegion\n      }\n    );\n    \n    return {\n      accessToken: newAccessToken.token,\n      refreshToken: newRefreshToken.token,\n      expiresIn: 900,\n      tokenType: 'Bearer',\n      refreshExpiresIn: 2592000\n    };\n    \n  } finally {\n    await releaseCrossRegionLock(tokenHash, currentRegion);\n  }\n}\n\nasync function validateRefreshToken(\n  tokenData: RefreshTokenRow,\n  fingerprint: string,\n  currentRegion: Region\n): Promise<void> {\n  \n  // 1. Check expiry\n  if (tokenData.expires_at < new Date()) {\n    throw new AuthError('REFRESH_TOKEN_EXPIRED');\n  }\n  \n  // 2. Check revocation\n  if (tokenData.revoked) {\n    throw new AuthError('REFRESH_TOKEN_REVOKED');\n  }\n  \n  // 3. Validate fingerprint\n  const fingerprintHash = hashFingerprint(fingerprint);\n  if (tokenData.device_fingerprint_hash !== fingerprintHash) {\n    \n    // Cross-region fingerprint mismatch could be legitimate (VPN, proxy)\n    // Check if user has history of cross-region usage\n    const userTravelPattern = await getUserTravelPattern(tokenData.user_id);\n    \n    if (!userTravelPattern.crossRegionAllowed) {\n      // User has never used service from multiple regions - suspicious\n      await createSecurityEvent({\n        userId: tokenData.user_id,\n        eventType: 'CROSS_REGION_FINGERPRINT_MISMATCH',\n        severity: 'HIGH',\n        details: {\n          originalRegion: tokenData.issued_in_region,\n          currentRegion,\n          originalFingerprint: tokenData.device_fingerprint_hash.substring(0, 16),\n          currentFingerprint: fingerprintHash.substring(0, 16)\n        }\n      });\n      \n      throw new AuthError('FINGERPRINT_MISMATCH');\n    }\n    \n    // Log but allow (user has established cross-region pattern)\n    logger.warn('Cross-region fingerprint mismatch allowed', {\n      userId: tokenData.user_id,\n      fromRegion: tokenData.issued_in_region,\n      toRegion: currentRegion\n    });\n  }\n  \n  // 4. Check refresh count (detect infinite refresh loops)\n  if (tokenData.times_refreshed > 1000) {\n    // Suspicious - single refresh chain shouldn't exceed 1000 refreshes\n    // (30 days / 15 min = 2880 max normal refreshes, but user should re-login)\n    await createSecurityEvent({\n      userId: tokenData.user_id,\n      eventType: 'EXCESSIVE_REFRESH_COUNT',\n      severity: 'MEDIUM',\n      details: {\n        timesRefreshed: tokenData.times_refreshed,\n        tokenFamilyId: tokenData.token_family_id\n      }\n    });\n    \n    throw new AuthError('REFRESH_LIMIT_EXCEEDED');\n  }\n}\n\n// Cross-region distributed lock using Redis + Postgres fallback\nasync function acquireCrossRegionLock(\n  tokenHash: string,\n  region: Region,\n  timeoutMs: number = 5000\n): Promise<boolean> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  const lockValue = `${region}:${uuidv4()}:${Date.now()}`;\n  \n  // Try regional Redis first\n  const regionalRedis = getRegionalRedis(region);\n  \n  try {\n    const acquired = await regionalRedis.set(\n      lockKey,\n      lockValue,\n      'NX',\n      'PX',\n      timeoutMs\n    );\n    \n    if (acquired) {\n      return true;\n    }\n  } catch (error) {\n    logger.warn('Regional Redis lock failed, trying Postgres', {\n      tokenHash: tokenHash.substring(0, 16),\n      region,\n      error: error.message\n    });\n  }\n  \n  // Fallback to Postgres advisory lock\n  // Use hash of token as lock ID (deterministic across regions)\n  const lockId = hashToInt64(tokenHash);\n  \n  const result = await db.query(\n    'SELECT pg_try_advisory_lock($1) as acquired',\n    [lockId]\n  );\n  \n  return result.rows[0].acquired;\n}\n\nasync function releaseCrossRegionLock(\n  tokenHash: string,\n  region: Region\n): Promise<void> {\n  \n  const lockKey = `lock:refresh:${tokenHash}`;\n  \n  // Try Redis first\n  try {\n    const regionalRedis = getRegionalRedis(region);\n    await regionalRedis.del(lockKey);\n  } catch (error) {\n    // Fallback to Postgres advisory lock release\n    const lockId = hashToInt64(tokenHash);\n    await db.query('SELECT pg_advisory_unlock($1)', [lockId]);\n  }\n}\n```\n\n### 4. Enhanced Schema for Multi-Region Support\n\n```sql\n-- Refresh tokens table with region tracking\nCREATE TABLE refresh_tokens (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  token_hash VARCHAR(64) NOT NULL UNIQUE,\n  user_id UUID NOT NULL REFERENCES users(id),\n  session_id UUID NOT NULL,\n  device_id VARCHAR(255) NOT NULL,\n  device_fingerprint_hash VARCHAR(64) NOT NULL,\n  \n  -- NEW: Region tracking\n  issued_in_region VARCHAR(50) NOT NULL,\n  used_in_region VARCHAR(50),\n  \n  expires_at TIMESTAMP NOT NULL,\n  issued_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  used BOOLEAN DEFAULT FALSE,\n  used_at TIMESTAMP,\n  revoked BOOLEAN DEFAULT FALSE,\n  revoked_at TIMESTAMP,\n  \n  -- Token family tracking\n  token_family_id UUID NOT NULL,\n  parent_token_id UUID REFERENCES refresh_tokens(id),\n  times_refreshed INTEGER DEFAULT 0,\n  \n  -- Audit\n  ip_address INET,\n  user_agent TEXT,\n  created_at TIMESTAMP DEFAULT NOW()\n);\n\n-- Composite index for cross-region lookups\nCREATE INDEX idx_refresh_tokens_hash_region ON refresh_tokens(token_hash, issued_in_region);\nCREATE INDEX idx_refresh_tokens_family ON refresh_tokens(token_family_id, issued_at DESC);\nCREATE INDEX idx_refresh_tokens_user_active ON refresh_tokens(user_id, expires_at) \n  WHERE NOT used AND NOT revoked;\n\n-- Token blacklist with region tracking\nCREATE TABLE token_blacklist (\n  jti VARCHAR(255) PRIMARY KEY,\n  user_id UUID NOT NULL,\n  revoked_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  expires_at TIMESTAMP NOT NULL,\n  reason VARCHAR(100),\n  session_id UUID,\n  \n  -- NEW: Region tracking for propagation monitoring\n  source_region VARCHAR(50) NOT NULL,\n  blacklisted_at TIMESTAMP NOT NULL DEFAULT NOW(),\n  \n  ip_address INET,\n  user_agent TEXT\n);\n\n-- Index for cross-region blacklist checks\nCREATE INDEX idx_blacklist_active ON token_blacklist(jti, expires_at)\n  WHERE expires_at > NOW();\nCREATE INDEX idx_blacklist_user_recent ON token_blacklist(user_id, revoked_at DESC);\nCREATE INDEX idx_blacklist_region ON token_blacklist(source_region, blacklisted_at DESC);\n\n-- User travel patterns (for risk assessment)\nCREATE TABLE user_region_history (\n  id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n  user_id UUID NOT NULL REFERENCES users(id),\n  region VARCHAR(50) NOT NULL,\n  first_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  last_seen TIMESTAMP NOT NULL DEFAULT NOW(),\n  request_count INTEGER DEFAULT 1,\n  \n  UNIQUE(user_id, region)\n);\n\nCREATE INDEX idx_user_region_history_user ON user_region_history(user_id, last_seen DESC);\n```\n\n## Load Testing & Failure Scenarios\n\n### 1. Load Testing Plan\n\n```typescript\n// Load test scenarios\nconst loadTestScenarios = [\n  {\n    name: 'Peak Traffic - Normal Operation',\n    duration: '10min',\n    rps: 100000, // 100K requests/sec\n    operations: {\n      tokenValidation: 0.70,  // 70% reads (API calls)\n      tokenRefresh: 0.25,     // 25% refresh operations\n      logout: 0.05            // 5% logout (blacklist writes)\n    },\n    assertions: {\n      p99Latency: '<50ms',\n      errorRate: '<0.01%',\n      redisHitRate: '>99.5%'\n    }\n  },\n  \n  {\n    name: 'Redis Failure - Circuit Breaker Activation',\n    duration: '5min',\n    setup: 'Stop Redis at t=60s, restart at t=180s',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerOpens: '<10s after Redis down',\n      fallbackLatency: '<100ms p99',\n      errorRate: '<1%',\n      recoveryTime: '<30s after Redis up',\n      noDataLoss: 'All blacklist entries in Postgres'\n    }\n  },\n  \n  {\n    name: 'Postgres Connection Pool Exhaustion',\n    duration: '3min',\n    setup: 'max_connections=100, 20 app instances',\n    rps: 10000,\n    operations: {\n      // Force all requests to Postgres (Redis disabled)\n      tokenValidation: 0.90,\n      tokenRefresh: 0.10\n    },\n    assertions: {\n      connectionPoolUtilization: '<80%',\n      p99Latency: '<200ms',\n      errorRate: '<5%',\n      noConnectionLeaks: 'Pool size stable'\n    }\n  },\n  \n  {\n    name: 'Circuit Breaker Flapping',\n    duration: '10min',\n    setup: 'Intermittent Redis failures (30s down, 30s up)',\n    rps: 50000,\n    operations: {\n      tokenValidation: 0.80,\n      tokenRefresh: 0.15,\n      logout: 0.05\n    },\n    assertions: {\n      circuitBreakerFlaps: '<5 times',\n      halfOpenDuration: '<30s',\n      errorRate: '<2%',\n      cacheCoherence: 'Local cache syncs after recovery'\n    }\n  },\n  \n  {\n    name: 'Cross-Instance Rate Limit Bypass',\n    duration: '5min',\n    setup: '10 app instances, rate limit 100 req/min per user',\n    rps: 5000,\n    operations: {\n      // Single user attacking across all instances\n      singleUserAttack: true,\n      targetRate: 1000 // 10x rate limit\n    },\n    assertions: {\n      effectiveRateLimit: '<150 req/min', // Allow 50% overage\n      postgresSync: 'Within 10 seconds',\n      blockAfterSync: 'Within 1 request'\n    }\n  },\n  \n  {\n    name: 'Cross-Region Token Refresh',\n    duration: '10min',\n    setup: 'Users refresh from different region than login',\n    rps: 10000,\n    operations: {\n      loginRegion: 'us-east',\n      refreshRegion: 'eu-west',\n      percentCrossRegion: 50\n    },\n    assertions: {\n      crossRegionLatency: '<200ms p99',\n      tokenReuseDetection: '100%',\n      blacklistPropagation: '<500ms p99',\n      noFalsePositives: 'Zero legitimate refreshes blocked'\n    }\n  },\n  \n  {\n    name: 'Bloom Filter False Positive Rate',\n    duration: '30min',\n    setup: '1M active tokens, 100K blacklisted',\n    rps: 100000,\n    operations: {\n      tokenValidation: 1.0\n    },\n    assertions: {\n      bloomFalsePositiveRate: '<0.01%',\n      postgresCheckRate: '<10 req/s',\n      noFalseNegatives: 'Zero blacklisted tokens accepted'\n    }\n  }\n];\n\n// Chaos engineering scenarios\nconst chaosScenarios = [\n  {\n    name: 'Network Partition - Region Isolation',\n    setup: 'Isolate EU-WEST region for 60 seconds',\n    expectedBehavior: [\n      'EU-WEST continues serving from local Redis + Postgres replica',\n      'New blacklists in US-EAST do not propagate to EU-WEST',\n      'High-risk operations in EU-WEST check PRIMARY Postgres',\n      'After partition heals, blacklists propagate within 5 seconds'\n    ]\n  },\n  \n  {\n    name: 'Postgres Primary Failover',\n    setup: 'Promote replica to primary (30s downtime)',\n    expectedBehavior: [\n      'Token refreshes fail during failover window',\n      'Token validation continues (cached data)',\n      'After promotion, refreshes resume within 10 seconds',\n      'No token reuse vulnerabilities introduced'\n    ]\n  },\n  \n  {\n    name: 'Memory Pressure - Cache Eviction',\n    setup: 'Fill LRU cache to capacity, continue adding entries',\n    expectedBehavior: [\n      'LRU evicts oldest entries',\n      'Cache hit rate remains >95%',\n      'No memory leak (heap size stable)',\n      'Evicted entries refetched from Redis/Postgres'\n    ]\n  }\n];\n```\n\n### 2. Load Testing Implementation\n\n```typescript\n// Load testing harness using k6 or artillery\nimport { check, group, sleep } from 'k6';\nimport http from 'k6/http';\n\nexport const options = {\n  stages: [\n    { duration: '2m', target: 50000 }, // Ramp up\n    { duration: '5m', target: 100000 }, // Peak\n    { duration: '2m', target: 0 }     // Ramp down\n  ],\n  thresholds: {\n    http_req_duration: ['p(99)<50'],\n    http_req_failed: ['rate<0.0001'],\n    'circuit_breaker_open': ['count<1']\n  }\n};\n\nexport default function() {\n  const scenario = Math.random();\n  \n  if (scenario < 0.70) {\n    // Token validation (70% of traffic)\n    testTokenValidation();\n  } else if (scenario < 0.95) {\n    // Token refresh (25% of traffic)\n    testTokenRefresh();\n  } else {\n    // Logout (5% of traffic)\n    testLogout();\n  }\n  \n  sleep(Math.random() * 2); // Random delay 0-2s\n}\n\nfunction testTokenValidation() {\n  const token = __ENV.TEST_ACCESS_TOKEN;\n  \n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${token}` }\n  });\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'latency < 50ms': (r) => r.timings.duration < 50,\n    'cache hit': (r) => r.headers['X-Cache-Layer'] === 'redis'\n  });\n}\n\nfunction testTokenRefresh() {\n  const refreshToken = __ENV.TEST_REFRESH_TOKEN;\n  \n  const res = http.post('https://api.example.com/auth/refresh', \n    JSON.stringify({ refreshToken }),\n    { headers: { 'Content-Type': 'application/json' } }\n  );\n  \n  check(res, {\n    'status is 200': (r) => r.status === 200,\n    'returns new tokens': (r) => r.json('accessToken') !== undefined,\n    'latency < 100ms': (r) => r.timings.duration < 100\n  });\n}\n\n// Failure injection for chaos testing\nexport function injectRedisFailure() {\n  // Use Kubernetes pod deletion or network policy\n  exec(`kubectl delete pod -l app=redis-cache -n production`);\n  \n  sleep(10); // Wait for circuit breaker to trip\n  \n  // Verify fallback behavior\n  const res = http.get('https://api.example.com/protected', {\n    headers: { 'Authorization': `Bearer ${__ENV.TEST_ACCESS_TOKEN}` }\n  });\n  \n  check(res, {\n    'fallback successful': (r) => r.status === 200,\n    'postgres fallback used': (r) => r.headers['X-Cache-Layer'] === 'postgres'\n  });\n}\n```\n\n## Operational Runbook\n\n### 1. Incident Response Playbooks\n\n```yaml\n# Runbook: Redis Unavailable (Circuit Breaker Open)\n\nincident_type: \"Redis Unavailable\"\nseverity: P2 (High)\ndetection: Alert \"RedisCircuitBreakerOpen\" fires\n\nimmediate_actions:\n  - verify_fallback:\n      cmd: \"curl https://api.example.com/health/fallback\"\n      expected: '{\"status\":\"degraded\",\"fallback\":\"postgres\",\"latency_p99\":150}'\n      \n  - check_postgres_load:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'active'\"\n      threshold: \"< 80% of max_connections\"\n      \n  - verify_no_data_loss:\n      cmd: \"SELECT COUNT(*) FROM token_blacklist WHERE blacklisted_at > NOW() - INTERVAL '5 minutes'\"\n      compare_with: \"Redis blacklist count (should match)\"\n\ninvestigation:\n  - check_redis_health:\n      - \"kubectl get pods -n production -l app=redis\"\n      - \"kubectl logs -n production -l app=redis --tail=100\"\n      - \"redis-cli -h redis.internal PING\"\n      \n  - check_sentinel_status:\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL masters\"\n      - \"redis-cli -h sentinel.internal -p 26379 SENTINEL slaves auth-master\"\n      \n  - check_network:\n      - \"kubectl get networkpolicies -n production\"\n      - \"traceroute redis.internal\"\n\nmitigation:\n  - if_redis_pod_crash:\n      action: \"Kubernetes should auto-restart\"\n      verify: \"Wait 30s, check if circuit breaker closes\"\n      \n  - if_sentinel_failover:\n      action: \"Wait for automatic failover (< 30s)\"\n      verify: \"SENTINEL masters shows new master\"\n      \n  - if_persistent_failure:\n      action: \"Scale Postgres read replicas\"\n      cmd: \"kubectl scale deployment postgres-replica --replicas=5\"\n\nrecovery_verification:\n  - circuit_breaker_closed:\n      query: \"circuit_breaker_state{service='redis'} == 0\"\n      \n  - latency_normal:\n      query: \"histogram_quantile(0.99, cache_get_latency) < 10\"\n      \n  - error_rate_normal:\n      query: \"rate(http_requests_total{status=~'5..'}[5m]) < 0.001\"\n\npost_mortem:\n  - analyze_root_cause: true\n  - update_circuit_breaker_thresholds: \"if flapping observed\"\n  - review_postgres_capacity: \"if connection pool saturated\"\n```\n\n```yaml\n# Runbook: Bloom Filter False Positive Rate Spike\n\nincident_type: \"Bloom Filter False Positive Rate High\"\nseverity: P3 (Medium)\ndetection: Alert \"BloomFilterFalsePositiveRateHigh\" fires\n\nimmediate_actions:\n  - check_metrics:\n      query: \"rate(auth_bloom_filter_false_positive_total[5m])\"\n      threshold: \"> 0.01%\"\n      \n  - verify_no_false_negatives:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE event_type = 'BLACKLISTED_TOKEN_ACCEPTED' AND created_at > NOW() - INTERVAL '5 minutes'\"\n      expected: \"0\"\n\ninvestigation:\n  - check_bloom_filter_size:\n      cmd: \"curl http://api.example.com/internal/bloom-filter/stats\"\n      expected: '{\"entries\":100000,\"capacity\":1000000,\"load_factor\":0.1}'\n      \n  - check_sync_lag:\n      cmd: \"SELECT MAX(NOW() - blacklisted_at) FROM token_blacklist\"\n      threshold: \"< 60 seconds\"\n      \n  - analyze_blacklist_growth:\n      query: \"rate(token_blacklist_inserts_total[1h])\"\n      compare_with: \"Historical average\"\n\nmitigation:\n  - if_overloaded:\n      condition: \"load_factor > 0.8\"\n      action: \"Rebuild bloom filter with larger capacity\"\n      cmd: \"curl -X POST http://api.example.com/internal/bloom-filter/rebuild?capacity=10000000\"\n      \n  - if_sync_lag:\n      condition: \"sync lag > 60s\"\n      action: \"Trigger immediate sync from Postgres\"\n      cmd: \"curl -X POST http://api.example.com/internal/bloom-filter/sync\"\n\nrecovery_verification:\n  - false_positive_rate_normal:\n      query: \"rate(auth_bloom_filter_false_positive_total[5m]) < 0.0001\"\n      \n  - no_security_incidents:\n      cmd: \"SELECT COUNT(*) FROM security_events WHERE severity = 'HIGH' AND created_at > NOW() - INTERVAL '10 minutes'\"\n      expected: \"0\"\n\nprevention:\n  - implement_auto_scaling:\n      description: \"Automatically resize bloom filter when load_factor > 0.7\"\n      \n  - add_capacity_alerts:\n      alert: \"BloomFilterCapacityWarning\"\n      threshold: \"load_factor > 0.6\"\n```\n\n```yaml\n# Runbook: Postgres Fallback Latency High\n\nincident_type: \"Postgres Fallback Latency Exceeds SLA\"\nseverity: P2 (High)\ndetection: Alert \"PostgresFallbackLatencyHigh\" fires\n\nimmediate_actions:\n  - check_sla_breach:\n      query: \"histogram_quantile(0.99, blacklist_check_latency{source='postgres'}) > 200\"\n      \n  - verify_connection_pool:\n      cmd: \"SELECT count(*) FROM pg_stat_activity\"\n      threshold: \"< max_connections * 0.8\"\n      \n  - check_query_performance:\n      cmd: \"SELECT query, mean_exec_time, calls FROM pg_stat_statements ORDER BY mean_exec_time DESC LIMIT 10\"\n\ninvestigation:\n  - analyze_slow_queries:\n      cmd: \"SELECT pid, query_start, state, query FROM pg_stat_activity WHERE state = 'active' AND query_start < NOW() - INTERVAL '1 second'\"\n      \n  - check_index_usage:\n      cmd: \"SELECT schemaname, tablename, indexname, idx_scan FROM pg_stat_user_indexes WHERE schemaname = 'public' AND idx_scan = 0\"\n      \n  - check_table_bloat:\n      cmd: \"SELECT schemaname, tablename, pg_size_pretty(pg_total_relation_size(schemaname||'.'||tablename)) FROM pg_tables WHERE schemaname = 'public' ORDER BY pg_total_relation_size(schemaname||'.'||tablename) DESC\"\n\nmitigation:\n  - if_connection_pool_exhausted:\n      action: \"Increase connection pool size\"\n      cmd: \"kubectl set env deployment/api-server DB_POOL_SIZE=200\"\n      \n  - if_missing_index:\n      action: \"Create missing indexes\"\n      cmd: \"psql -c 'CREATE INDEX CONCURRENTLY idx_missing ON table(column)'\"\n      \n  - if_table_bloat:\n      action: \"Run VACUUM ANALYZE\"\n      cmd: \"psql -c 'VACUUM ANALYZE token_blacklist'\"\n      \n  - if_query_slow:\n      action: \"Optimize query or add caching\"\n      review: \"src/auth/blacklist-check.ts\"\n\nemergency_mitigation:\n  - reduce_postgres_checks:\n      description: \"Temporarily reduce risk threshold for Postgres checks\"\n      cmd: \"kubectl set env deployment/api-server BLACKLIST_CHECK_RISK_THRESHOLD=high\"\n      impact: \"Medium/low risk operations skip Postgres check (accept stale cache)\"\n      rollback: \"After Postgres performance recovers\"\n\nrecovery_verification:\n  - latency_within_sla:\n      query: \"histogram_quantile(0.99, blacklist_check_latency{source='postgres'}) < 100\"\n      \n  - connection_pool_healthy:\n      cmd: \"SELECT count(*) FROM pg_stat_activity WHERE state = 'idle'\"\n      expected: \"> 20% of pool size\"\n\npost_mortem:\n  - review_query_plans: true\n  - optimize_indexes: true\n  - consider_read_replicas: \"if load is consistently high\"\n  - review_cache_ttl: \"increase TTL to reduce Postgres load\"\n```\n\n### 2. Monitoring Dashboard\n\n```typescript\n// Grafana dashboard JSON\nconst authSystemDashboard = {\n  title: \"JWT Auth System - Production Monitoring\",\n  panels: [\n    {\n      title: \"Request Rate by Operation\",\n      targets: [\n        'sum(rate(http_requests_total{service=\"auth\"}[5m])) by (operation)',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Cache Hit Rate by Layer\",\n      targets: [\n        'sum(rate(cache_hit_total[5m])) by (layer) / sum(rate(cache_requests_total[5m])) by (layer)',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.95, color: \"green\" },\n        { value: 0.90, color: \"yellow\" },\n        { value: 0.85, color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Circuit Breaker State\",\n      targets: [\n        'circuit_breaker_state{service=\"redis\"}',\n      ],\n      visualization: \"stat\",\n      mappings: [\n        { value: 0, text: \"CLOSED\", color: \"green\" },\n        { value: 0.5, text: \"HALF_OPEN\", color: \"yellow\" },\n        { value: 1, text: \"OPEN\", color: \"red\" }\n      ]\n    },\n    \n    {\n      title: \"Token Validation Latency (p50, p99, p99.9)\",\n      targets: [\n        'histogram_quantile(0.50, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.99, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n        'histogram_quantile(0.999, sum(rate(auth_token_validation_latency_bucket[5m])) by (le))',\n      ],\n      visualization: \"timeseries\"\n    },\n    \n    {\n      title: \"Postgres Connection Pool Utilization\",\n      targets: [\n        'pg_stat_activity_count / pg_settings_max_connections',\n      ],\n      visualization: \"gauge\",\n      thresholds: [\n        { value: 0.80, color: \"red\" },\n        { value: 0.60, color: \"yellow\" },\n        { value: 0.40, color: \"green\" }\n      ]\n    },\n    \n    {\n      title: \"Blacklist Propagation Latency (Cross-Region)\",\n      targets: [\n        'histogram_quantile(0.99, sum(rate(blacklist_cross_region_propagation_bucket[5m])) by (source, target, le))',\n      ],\n      visualization: \"heatmap\"\n    },\n    \n    {\n      title: \"Security Events (Last 1 Hour)\",\n      targets: [\n        'sum(increase(security_events_total[1h])) by (event_type)',\n      ],\n      visualization: \"table\"\n    },\n    \n    {\n      title: \"Bloom Filter Statistics\",\n      targets: [\n        'bloom_filter_entries',\n        'bloom_filter_capacity',\n        'bloom_filter_load_factor',\n        'rate(bloom_filter_false_positive_total[5m])',\n      ],\n      visualization: \"stat\"\n    }\n  ],\n  \n  alerts: [\n    {\n      name: \"RedisCircuitBreakerOpen\",\n      condition: \"circuit_breaker_state{service='redis'} == 1\",\n      for: \"1m\",\n      severity: \"critical\"\n    },\n    {\n      name: \"HighPostgresFallback\",\n      condition: \"rate(cache_fallback_postgres_total[5m]) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"TokenValidationLatencyHigh\",\n      condition: \"histogram_quantile(0.99, auth_token_validation_latency_bucket) > 100\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"BloomFilterFalsePositiveRateHigh\",\n      condition: \"rate(bloom_filter_false_positive_total[5m]) > 0.0001\",\n      for: \"10m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"PostgresConnectionPoolSaturated\",\n      condition: \"pg_stat_activity_count / pg_settings_max_connections > 0.8\",\n      for: \"5m\",\n      severity: \"critical\"\n    },\n    {\n      name: \"CrossRegionBlacklistPropagationSlow\",\n      condition: \"histogram_quantile(0.99, blacklist_cross_region_propagation_bucket) > 1000\",\n      for: \"5m\",\n      severity: \"warning\"\n    },\n    {\n      name: \"SecurityEventSpike\",\n      condition: \"rate(security_events_total{severity='HIGH'}[5m]) > 10\",\n      for: \"2m\",\n      severity: \"critical\"\n    }\n  ]\n};\n```\n\n### 3. Health Check Endpoints\n\n```typescript\n// Health check implementation\napp.get('/health', async (req, res) => {\n  const health = {\n    status: 'healthy',\n    timestamp: new Date().toISOString(),\n    version: process.env.APP_VERSION,\n    uptime: process.uptime(),\n    \n    components: {\n      redis: await checkRedisHealth(),\n      postgres: await checkPostgresHealth(),\n      bloomFilter: await checkBloomFilterHealth()\n    },\n    \n    metrics: {\n      circuitBreakerState: circuitBreaker.getState(),\n      cacheHitRate: await getCacheHitRate(),\n      requestRate: await getRequestRate(),\n      errorRate: await getErrorRate()\n    }\n  };\n  \n  // Overall health based on components\n  if (health.components.redis.status === 'unhealthy' && \n      health.components.postgres.status === 'unhealthy') {\n    health.status = 'unhealthy';\n    return res.status(503).json(health);\n  }\n  \n  if (health.components.redis.status === 'unhealthy' ||\n      health.components.postgres.status === 'degraded') {\n    health.status = 'degraded';\n    return res.status(200).json(health);\n  }\n  \n  res.status(200).json(health);\n});\n\nasync function checkRedisHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await redis.ping();\n    const latency = Date.now() - start;\n    \n    return {\n      status: latency < 10 ? 'healthy' : 'degraded',\n      latency,\n      message: 'Redis operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Redis unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkPostgresHealth(): Promise<ComponentHealth> {\n  try {\n    const start = Date.now();\n    await db.query('SELECT 1');\n    const latency = Date.now() - start;\n    \n    const poolSize = await db.query('SELECT count(*) FROM pg_stat_activity');\n    const maxConnections = await db.query('SHOW max_connections');\n    const utilization = poolSize.rows[0].count / maxConnections.rows[0].max_connections;\n    \n    return {\n      status: latency < 50 && utilization < 0.8 ? 'healthy' : 'degraded',\n      latency,\n      metadata: {\n        poolUtilization: `${(utilization * 100).toFixed(1)}%`,\n        activeConnections: poolSize.rows[0].count\n      },\n      message: 'Postgres operational'\n    };\n  } catch (error) {\n    return {\n      status: 'unhealthy',\n      latency: null,\n      message: `Postgres unavailable: ${error.message}`\n    };\n  }\n}\n\nasync function checkBloomFilterHealth(): Promise<ComponentHealth> {\n  const stats = blacklistBloomFilter.getStats();\n  const loadFactor = stats.entries / stats.capacity;\n  \n  return {\n    status: loadFactor < 0.8 ? 'healthy' : 'degraded',\n    metadata: {\n      entries: stats.entries,\n      capacity: stats.capacity,\n      loadFactor: `${(loadFactor * 100).toFixed(1)}%`,\n      lastSync: stats.lastSync\n    },\n    message: loadFactor < 0.8 ? 'Bloom filter healthy' : 'Bloom filter approaching capacity'\n  };\n}\n\ninterface ComponentHealth {\n  status: 'healthy' | 'degraded' | 'unhealthy';\n  latency?: number | null;\n  metadata?: Record<string, any>;\n  message: string;\n}\n```"
}