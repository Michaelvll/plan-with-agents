{
  "initial_prompt": "Design schema for e-commerce orders with indexing strategy\n\nRequirements:\n- Support for orders, order items, customers, products\n- Handle order status transitions\n- Efficient queries for common operations\n- Consider read vs write optimization\n- Plan for future scale",
  "status": "debating",
  "rounds": 4,
  "started_at": "2026-01-17T09:20:32.095237",
  "ended_at": "2026-01-17T09:46:58.744679",
  "final_design": "## Design\n\n### E-Commerce Order System - Production Schema with Operational Clarity\n\n#### 1. Core Schema with Refined Operational Controls\n\n```sql\n-- ============================================\n-- CUSTOMERS & AUTHENTICATION\n-- ============================================\n\nCREATE TABLE customers (\n    customer_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    email VARCHAR(255) UNIQUE NOT NULL,\n    phone VARCHAR(50),\n    first_name VARCHAR(100) NOT NULL,\n    last_name VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    status ENUM('active', 'suspended', 'deleted') NOT NULL DEFAULT 'active',\n    version INT NOT NULL DEFAULT 1,\n    \n    INDEX idx_email (email),\n    INDEX idx_status_created (status, created_at)\n) ENGINE=InnoDB;\n\n-- ============================================\n-- WAREHOUSE & INVENTORY MANAGEMENT\n-- ============================================\n\nCREATE TABLE warehouses (\n    warehouse_id INT PRIMARY KEY AUTO_INCREMENT,\n    warehouse_code VARCHAR(20) UNIQUE NOT NULL,\n    name VARCHAR(255) NOT NULL,\n    address_line1 VARCHAR(255) NOT NULL,\n    address_line2 VARCHAR(255),\n    city VARCHAR(100) NOT NULL,\n    state VARCHAR(100) NOT NULL,\n    postal_code VARCHAR(20) NOT NULL,\n    country CHAR(2) NOT NULL,\n    latitude DECIMAL(10,8),\n    longitude DECIMAL(11,8),\n    status ENUM('active', 'inactive', 'maintenance') NOT NULL DEFAULT 'active',\n    priority INT NOT NULL DEFAULT 100,\n    \n    -- Simplified capacity with clear semantics\n    base_daily_capacity INT,\n    current_daily_orders INT NOT NULL DEFAULT 0,\n    capacity_reset_date DATE NOT NULL,\n    \n    shipping_cost_base DECIMAL(8,2),\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    INDEX idx_status_priority (status, priority),\n    INDEX idx_location (latitude, longitude),\n    INDEX idx_capacity_date (capacity_reset_date, current_daily_orders)\n) ENGINE=InnoDB;\n\n-- Capacity overrides with explicit conflict resolution\nCREATE TABLE warehouse_capacity_overrides (\n    override_id INT PRIMARY KEY AUTO_INCREMENT,\n    warehouse_id INT NOT NULL,\n    start_date DATE NOT NULL,\n    end_date DATE NOT NULL,\n    capacity_multiplier DECIMAL(5,2) NOT NULL DEFAULT 1.0,\n    hard_capacity_limit INT,\n    reason VARCHAR(255),\n    priority INT NOT NULL DEFAULT 100,  -- NEW: Explicit priority for overlap resolution\n    created_by VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    FOREIGN KEY (warehouse_id) REFERENCES warehouses(warehouse_id),\n    \n    INDEX idx_warehouse_dates (warehouse_id, start_date, end_date),\n    INDEX idx_date_range (start_date, end_date),\n    INDEX idx_priority (priority DESC),  -- NEW: For conflict resolution\n    \n    CHECK (capacity_multiplier > 0),\n    CHECK (end_date >= start_date),\n    CHECK (hard_capacity_limit IS NULL OR hard_capacity_limit > 0),\n    \n    -- Prevent exact duplicate ranges for same warehouse\n    UNIQUE KEY uk_warehouse_date_range (warehouse_id, start_date, end_date)\n) ENGINE=InnoDB;\n\nCREATE TABLE products (\n    product_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    sku VARCHAR(100) UNIQUE NOT NULL,\n    name VARCHAR(255) NOT NULL,\n    description TEXT,\n    base_price DECIMAL(10,2) NOT NULL,\n    weight_grams INT,\n    status ENUM('active', 'inactive', 'discontinued') NOT NULL DEFAULT 'active',\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    version INT NOT NULL DEFAULT 1,\n    \n    INDEX idx_sku (sku),\n    INDEX idx_status (status)\n) ENGINE=InnoDB;\n\nCREATE TABLE warehouse_inventory (\n    inventory_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    warehouse_id INT NOT NULL,\n    product_id BIGINT NOT NULL,\n    stock_quantity INT NOT NULL DEFAULT 0,\n    reserved_quantity INT NOT NULL DEFAULT 0,\n    safety_stock_level INT NOT NULL DEFAULT 0,\n    last_restock_at TIMESTAMP NULL,\n    version INT NOT NULL DEFAULT 1,\n    \n    UNIQUE KEY uk_warehouse_product (warehouse_id, product_id),\n    FOREIGN KEY (warehouse_id) REFERENCES warehouses(warehouse_id),\n    FOREIGN KEY (product_id) REFERENCES products(product_id),\n    \n    INDEX idx_product_warehouse (product_id, warehouse_id),\n    INDEX idx_low_stock (warehouse_id, stock_quantity),\n    \n    CHECK (stock_quantity >= 0),\n    CHECK (reserved_quantity >= 0),\n    CHECK (reserved_quantity <= stock_quantity)\n) ENGINE=InnoDB;\n\nCREATE TABLE inventory_reservations (\n    reservation_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    warehouse_id INT NOT NULL,\n    product_id BIGINT NOT NULL,\n    order_id BIGINT NULL,\n    customer_id BIGINT NOT NULL,\n    quantity INT NOT NULL CHECK (quantity > 0),\n    status ENUM('active', 'expired', 'completed', 'cancelled', 'reallocated') NOT NULL DEFAULT 'active',\n    expires_at TIMESTAMP NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    completed_at TIMESTAMP NULL,\n    reallocation_reason VARCHAR(255),\n    \n    FOREIGN KEY (warehouse_id) REFERENCES warehouses(warehouse_id),\n    FOREIGN KEY (product_id) REFERENCES products(product_id),\n    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,\n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n    \n    INDEX idx_status_expires (status, expires_at),\n    INDEX idx_warehouse_product_status (warehouse_id, product_id, status),\n    INDEX idx_order_id (order_id),\n    INDEX idx_customer_created (customer_id, created_at)\n) ENGINE=InnoDB;\n\nCREATE TABLE inventory_transfers (\n    transfer_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    product_id BIGINT NOT NULL,\n    from_warehouse_id INT NOT NULL,\n    to_warehouse_id INT NOT NULL,\n    quantity INT NOT NULL CHECK (quantity > 0),\n    status ENUM('pending', 'in_transit', 'completed', 'cancelled') NOT NULL DEFAULT 'pending',\n    reason VARCHAR(255),\n    requested_by VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    shipped_at TIMESTAMP NULL,\n    received_at TIMESTAMP NULL,\n    \n    FOREIGN KEY (product_id) REFERENCES products(product_id),\n    FOREIGN KEY (from_warehouse_id) REFERENCES warehouses(warehouse_id),\n    FOREIGN KEY (to_warehouse_id) REFERENCES warehouses(warehouse_id),\n    \n    INDEX idx_product_status (product_id, status),\n    INDEX idx_from_warehouse (from_warehouse_id, status),\n    INDEX idx_to_warehouse (to_warehouse_id, status),\n    INDEX idx_created_at (created_at DESC)\n) ENGINE=InnoDB;\n\n-- Pre-populated geocode reference data\nCREATE TABLE geocode_reference (\n    reference_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    postal_code VARCHAR(20) NOT NULL,\n    country CHAR(2) NOT NULL,\n    latitude DECIMAL(10,8) NOT NULL,\n    longitude DECIMAL(11,8) NOT NULL,\n    geocode_quality ENUM('rooftop', 'range_interpolated', 'centroid', 'approximate') NOT NULL,\n    data_source ENUM('google', 'usps', 'openstreetmap', 'manual') NOT NULL,\n    last_updated TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    \n    UNIQUE KEY uk_postal_country (postal_code, country),\n    INDEX idx_country (country),\n    INDEX idx_quality (geocode_quality)\n) ENGINE=InnoDB;\n\n-- Runtime geocode cache (Redis-backed, transient)\nCREATE TABLE geocode_cache (\n    cache_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    postal_code VARCHAR(20) NOT NULL,\n    country CHAR(2) NOT NULL,\n    latitude DECIMAL(10,8) NOT NULL,\n    longitude DECIMAL(11,8) NOT NULL,\n    geocode_quality ENUM('rooftop', 'range_interpolated', 'centroid', 'approximate') NOT NULL,\n    geocode_source ENUM('api', 'reference_table', 'fallback') NOT NULL,\n    hit_count INT NOT NULL DEFAULT 1,  -- NEW: Track cache effectiveness\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    last_accessed_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    UNIQUE KEY uk_postal_country (postal_code, country),\n    INDEX idx_country (country),\n    INDEX idx_quality (geocode_quality),\n    INDEX idx_last_accessed (last_accessed_at)  -- NEW: For cache eviction\n) ENGINE=InnoDB;\n\n-- Failed geocode attempts with triage workflow\nCREATE TABLE geocode_failures (\n    failure_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    postal_code VARCHAR(20) NOT NULL,\n    country CHAR(2) NOT NULL,\n    full_address TEXT NOT NULL,\n    order_id BIGINT NULL,\n    failure_reason VARCHAR(255),\n    fallback_strategy ENUM('centroid', 'customer_corrected', 'none') NOT NULL,\n    customer_notified BOOLEAN NOT NULL DEFAULT FALSE,  -- NEW: Track customer communication\n    resolution_status ENUM('pending', 'customer_contacted', 'corrected', 'cancelled') NOT NULL DEFAULT 'pending',  -- NEW\n    resolved_at TIMESTAMP NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    \n    INDEX idx_resolution_status (resolution_status, created_at),\n    INDEX idx_postal_country (postal_code, country),\n    INDEX idx_created_at (created_at DESC)\n) ENGINE=InnoDB;\n\n-- ============================================\n-- ORDERS & FULFILLMENT\n-- ============================================\n\nCREATE TABLE orders (\n    order_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    customer_id BIGINT NOT NULL,\n    order_number VARCHAR(50) UNIQUE NOT NULL,\n    \n    status ENUM(\n        'cart', \n        'pending_payment', \n        'payment_processing',\n        'payment_failed', \n        'payment_confirmed',\n        'address_review',  -- NEW: Explicit state for geocode failures\n        'confirmed', \n        'processing',\n        'partially_shipped',\n        'shipped', \n        'delivered', \n        'cancelled', \n        'refunded',\n        'partially_refunded'\n    ) NOT NULL DEFAULT 'cart',\n    \n    -- Financial details\n    subtotal_amount DECIMAL(12,2) NOT NULL,\n    tax_amount DECIMAL(10,2) NOT NULL DEFAULT 0,\n    shipping_amount DECIMAL(10,2) NOT NULL DEFAULT 0,\n    discount_amount DECIMAL(10,2) NOT NULL DEFAULT 0,\n    total_amount DECIMAL(12,2) NOT NULL,\n    refunded_amount DECIMAL(12,2) NOT NULL DEFAULT 0,\n    currency_code CHAR(3) NOT NULL DEFAULT 'USD',\n    \n    -- Fulfillment details\n    fulfillment_warehouse_id INT NULL,\n    is_split_shipment BOOLEAN NOT NULL DEFAULT FALSE,\n    shipping_preference ENUM('standard', 'expedited', 'economy') NOT NULL DEFAULT 'standard',\n    \n    -- Shipping information (immutable snapshot)\n    shipping_address_line1 VARCHAR(255),\n    shipping_address_line2 VARCHAR(255),\n    shipping_city VARCHAR(100),\n    shipping_state VARCHAR(100),\n    shipping_postal_code VARCHAR(20),\n    shipping_country CHAR(2),\n    shipping_latitude DECIMAL(10,8),\n    shipping_longitude DECIMAL(11,8),\n    geocode_quality ENUM('rooftop', 'range_interpolated', 'centroid', 'approximate'),\n    geocode_requires_review BOOLEAN NOT NULL DEFAULT FALSE,  -- NEW: Flag for ops attention\n    \n    -- Billing information\n    billing_address_line1 VARCHAR(255),\n    billing_address_line2 VARCHAR(255),\n    billing_city VARCHAR(100),\n    billing_state VARCHAR(100),\n    billing_postal_code VARCHAR(20),\n    billing_country CHAR(2),\n    \n    -- Customer notes\n    customer_notes TEXT,\n    internal_notes TEXT,\n    \n    -- Timestamps\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    payment_confirmed_at TIMESTAMP NULL,\n    shipped_at TIMESTAMP NULL,\n    delivered_at TIMESTAMP NULL,\n    cancelled_at TIMESTAMP NULL,\n    \n    -- Concurrency control\n    version INT NOT NULL DEFAULT 1,\n    \n    -- Soft delete\n    deleted_at TIMESTAMP NULL,\n    \n    FOREIGN KEY (customer_id) REFERENCES customers(customer_id),\n    FOREIGN KEY (fulfillment_warehouse_id) REFERENCES warehouses(warehouse_id),\n    \n    INDEX idx_customer_created (customer_id, created_at DESC),\n    INDEX idx_status_created (status, created_at DESC),\n    INDEX idx_order_number (order_number),\n    INDEX idx_created_at (created_at DESC),\n    INDEX idx_customer_status (customer_id, status, created_at DESC),\n    INDEX idx_status_updated (status, updated_at),\n    INDEX idx_warehouse_status (fulfillment_warehouse_id, status, created_at),\n    INDEX idx_deleted_at (deleted_at),\n    INDEX idx_split_shipment (is_split_shipment, status),\n    INDEX idx_shipping_preference (shipping_preference, status),\n    INDEX idx_geocode_review (geocode_requires_review, status)  -- NEW: Ops queue\n) ENGINE=InnoDB ROW_FORMAT=COMPRESSED;\n\nCREATE TABLE order_items (\n    order_item_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    order_id BIGINT NOT NULL,\n    product_id BIGINT NOT NULL,\n    warehouse_id INT NOT NULL,\n    \n    -- Immutable product snapshot\n    product_sku VARCHAR(100) NOT NULL,\n    product_name VARCHAR(255) NOT NULL,\n    unit_price DECIMAL(10,2) NOT NULL,\n    quantity INT NOT NULL CHECK (quantity > 0),\n    subtotal DECIMAL(10,2) NOT NULL,\n    \n    -- Refund tracking\n    refunded_quantity INT NOT NULL DEFAULT 0,\n    refunded_amount DECIMAL(10,2) NOT NULL DEFAULT 0,\n    \n    -- Shipment tracking\n    shipment_id BIGINT NULL,\n    \n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,\n    FOREIGN KEY (product_id) REFERENCES products(product_id),\n    FOREIGN KEY (warehouse_id) REFERENCES warehouses(warehouse_id),\n    \n    INDEX idx_order_id (order_id),\n    INDEX idx_product_id (product_id),\n    INDEX idx_warehouse_created (warehouse_id, created_at),\n    INDEX idx_product_created (product_id, created_at),\n    INDEX idx_shipment_id (shipment_id),\n    \n    CHECK (refunded_quantity <= quantity),\n    CHECK (refunded_amount <= subtotal)\n) ENGINE=InnoDB;\n\nCREATE TABLE shipments (\n    shipment_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    order_id BIGINT NOT NULL,\n    warehouse_id INT NOT NULL,\n    \n    shipment_number VARCHAR(50) UNIQUE NOT NULL,\n    carrier VARCHAR(100),\n    tracking_number VARCHAR(255),\n    tracking_url VARCHAR(500),\n    \n    status ENUM('pending', 'picked', 'packed', 'shipped', 'in_transit', 'delivered', 'failed') \n        NOT NULL DEFAULT 'pending',\n    \n    shipping_cost DECIMAL(10,2),\n    estimated_delivery_date DATE,\n    actual_delivery_date DATE,\n    \n    notification_sent BOOLEAN NOT NULL DEFAULT FALSE,\n    notification_sent_at TIMESTAMP NULL,\n    \n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    shipped_at TIMESTAMP NULL,\n    delivered_at TIMESTAMP NULL,\n    \n    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    FOREIGN KEY (warehouse_id) REFERENCES warehouses(warehouse_id),\n    \n    INDEX idx_order_id (order_id),\n    INDEX idx_warehouse_status (warehouse_id, status),\n    INDEX idx_tracking_number (tracking_number),\n    INDEX idx_status_created (status, created_at),\n    INDEX idx_notification_status (notification_sent, status)\n) ENGINE=InnoDB;\n\n-- ============================================\n-- PAYMENT SYSTEM WITH OPERATIONAL MONITORING\n-- ============================================\n\nCREATE TABLE payments (\n    payment_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    order_id BIGINT NOT NULL,\n    \n    payment_gateway ENUM('stripe', 'paypal', 'square', 'adyen') NOT NULL,\n    gateway_transaction_id VARCHAR(255),\n    gateway_customer_id VARCHAR(255),\n    \n    payment_method ENUM('credit_card', 'debit_card', 'paypal', 'bank_transfer', 'crypto') NOT NULL,\n    card_last_four CHAR(4),\n    card_brand VARCHAR(20),\n    \n    amount DECIMAL(12,2) NOT NULL,\n    currency_code CHAR(3) NOT NULL,\n    \n    status ENUM(\n        'pending',\n        'processing',\n        'authorized',\n        'captured',\n        'failed',\n        'cancelled',\n        'refunded',\n        'partially_refunded'\n    ) NOT NULL DEFAULT 'pending',\n    \n    failure_code VARCHAR(50),\n    failure_message TEXT,\n    retry_count INT NOT NULL DEFAULT 0,\n    \n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    authorized_at TIMESTAMP NULL,\n    captured_at TIMESTAMP NULL,\n    failed_at TIMESTAMP NULL,\n    refunded_at TIMESTAMP NULL,\n    \n    idempotency_key VARCHAR(255) UNIQUE,\n    \n    metadata JSON,\n    \n    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    \n    INDEX idx_order_id (order_id),\n    INDEX idx_gateway_transaction (payment_gateway, gateway_transaction_id),\n    INDEX idx_status_created (status, created_at),\n    INDEX idx_idempotency_key (idempotency_key),\n    INDEX idx_created_at (created_at DESC),\n    INDEX idx_retry_count (retry_count, status)\n) ENGINE=InnoDB;\n\nCREATE TABLE payment_events (\n    event_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    payment_id BIGINT NOT NULL,\n    \n    event_type ENUM(\n        'payment_initiated',\n        'authorization_requested',\n        'authorization_succeeded',\n        'authorization_failed',\n        'capture_requested',\n        'capture_succeeded',\n        'capture_failed',\n        'refund_requested',\n        'refund_succeeded',\n        'refund_failed',\n        'webhook_received',\n        'retry_scheduled',\n        'moved_to_dlq'\n    ) NOT NULL,\n    \n    gateway_event_id VARCHAR(255),\n    gateway_event_type VARCHAR(100),\n    \n    event_payload JSON,\n    \n    processed BOOLEAN NOT NULL DEFAULT FALSE,\n    processed_at TIMESTAMP NULL,\n    processing_error TEXT,\n    retry_count INT NOT NULL DEFAULT 0,\n    next_retry_at TIMESTAMP NULL,\n    \n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    FOREIGN KEY (payment_id) REFERENCES payments(payment_id) ON DELETE CASCADE,\n    \n    UNIQUE KEY uk_gateway_event (gateway_event_id),\n    INDEX idx_payment_created (payment_id, created_at DESC),\n    INDEX idx_processed_created (processed, created_at),\n    INDEX idx_event_type (event_type, created_at),\n    INDEX idx_retry_schedule (processed, next_retry_at),\n    INDEX idx_retry_count (retry_count, processed)\n) ENGINE=InnoDB;\n\n-- Error classification with manual grouping\nCREATE TABLE payment_error_classes (\n    class_id INT PRIMARY KEY AUTO_INCREMENT,\n    class_name VARCHAR(100) UNIQUE NOT NULL,  -- e.g., 'stripe_timeout', 'insufficient_funds'\n    event_type VARCHAR(50) NOT NULL,\n    \n    -- Match criteria (any field can be NULL for wildcard)\n    gateway_pattern VARCHAR(100),  -- SQL LIKE pattern for gateway\n    failure_code_pattern VARCHAR(100),  -- SQL LIKE pattern for failure_code\n    message_keywords JSON,  -- Array of keywords that must appear in error message\n    \n    -- Auto-replay configuration\n    auto_replay_enabled BOOLEAN NOT NULL DEFAULT FALSE,\n    auto_replay_delay_seconds INT NOT NULL DEFAULT 300,  -- 5 minutes default\n    \n    -- Learning metrics\n    successful_replays INT NOT NULL DEFAULT 0,\n    failed_replays INT NOT NULL DEFAULT 0,\n    auto_replay_confidence DECIMAL(5,2) GENERATED ALWAYS AS (\n        CASE \n            WHEN (successful_replays + failed_replays) > 0 \n            THEN (successful_replays * 100.0) / (successful_replays + failed_replays)\n            ELSE 0.0\n        END\n    ) STORED,\n    \n    last_seen_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    created_by VARCHAR(100) NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    \n    INDEX idx_auto_replay (auto_replay_enabled, auto_replay_confidence),\n    INDEX idx_event_type (event_type)\n) ENGINE=InnoDB;\n\nCREATE TABLE payment_events_dlq (\n    dlq_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    original_event_id BIGINT NOT NULL,\n    payment_id BIGINT NOT NULL,\n    event_type VARCHAR(50) NOT NULL,\n    event_payload JSON,\n    failure_reason TEXT NOT NULL,\n    error_class_id INT NULL,  -- NEW: Link to manually-curated error class\n    retry_attempts INT NOT NULL,\n    \n    -- Operational workflow\n    status ENUM('pending_classification', 'pending_review', 'investigating', 'auto_replay_scheduled', 'ready_for_replay', 'replayed', 'discarded') \n        NOT NULL DEFAULT 'pending_classification',\n    assigned_to VARCHAR(100),\n    priority ENUM('low', 'medium', 'high', 'critical') NOT NULL DEFAULT 'medium',\n    \n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    assigned_at TIMESTAMP NULL,\n    resolved_at TIMESTAMP NULL,\n    replayed_at TIMESTAMP NULL,\n    resolution_notes TEXT,\n    \n    FOREIGN KEY (payment_id) REFERENCES payments(payment_id),\n    FOREIGN KEY (error_class_id) REFERENCES payment_error_classes(class_id),\n    \n    INDEX idx_payment_id (payment_id),\n    INDEX idx_status_priority (status, priority, created_at),\n    INDEX idx_assigned_to (assigned_to, status),\n    INDEX idx_created_at (created_at DESC),\n    INDEX idx_resolved (resolved_at),\n    INDEX idx_error_class (error_class_id, status)\n) ENGINE=InnoDB;\n\nCREATE TABLE dlq_metrics_daily (\n    metric_date DATE PRIMARY KEY,\n    total_entries INT NOT NULL DEFAULT 0,\n    pending_classification INT NOT NULL DEFAULT 0,\n    pending_review INT NOT NULL DEFAULT 0,\n    investigating INT NOT NULL DEFAULT 0,\n    auto_replay_scheduled INT NOT NULL DEFAULT 0,\n    ready_for_replay INT NOT NULL DEFAULT 0,\n    replayed_today INT NOT NULL DEFAULT 0,\n    auto_replayed_today INT NOT NULL DEFAULT 0,\n    discarded_today INT NOT NULL DEFAULT 0,\n    avg_resolution_hours DECIMAL(8,2),\n    critical_count INT NOT NULL DEFAULT 0,\n    \n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    \n    INDEX idx_metric_date (metric_date DESC)\n) ENGINE=InnoDB;\n\nCREATE TABLE refunds (\n    refund_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    payment_id BIGINT NOT NULL,\n    order_id BIGINT NOT NULL,\n    \n    refund_amount DECIMAL(12,2) NOT NULL,\n    refund_reason ENUM('customer_request', 'fraud', 'duplicate', 'product_issue', 'damaged', 'other') NOT NULL,\n    refund_notes TEXT,\n    \n    gateway_refund_id VARCHAR(255),\n    \n    status ENUM('pending', 'processing', 'succeeded', 'failed', 'cancelled') NOT NULL DEFAULT 'pending',\n    \n    is_full_refund BOOLEAN NOT NULL DEFAULT TRUE,\n    \n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    updated_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP ON UPDATE CURRENT_TIMESTAMP,\n    completed_at TIMESTAMP NULL,\n    \n    created_by VARCHAR(100),\n    \n    FOREIGN KEY (payment_id) REFERENCES payments(payment_id),\n    FOREIGN KEY (order_id) REFERENCES orders(order_id),\n    \n    INDEX idx_payment_id (payment_id),\n    INDEX idx_order_id (order_id),\n    INDEX idx_status_created (status, created_at),\n    INDEX idx_created_at (created_at DESC),\n    INDEX idx_reason (refund_reason)\n) ENGINE=InnoDB;\n\nCREATE TABLE refund_items (\n    refund_item_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    refund_id BIGINT NOT NULL,\n    order_item_id BIGINT NOT NULL,\n    quantity INT NOT NULL CHECK (quantity > 0),\n    refund_amount DECIMAL(10,2) NOT NULL,\n    restock BOOLEAN NOT NULL DEFAULT TRUE,\n    restock_reason ENUM('resellable', 'damaged', 'customer_kept', 'other') NOT NULL DEFAULT 'resellable',\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    FOREIGN KEY (refund_id) REFERENCES refunds(refund_id) ON DELETE CASCADE,\n    FOREIGN KEY (order_item_id) REFERENCES order_items(order_item_id),\n    \n    INDEX idx_refund_id (refund_id),\n    INDEX idx_order_item_id (order_item_id),\n    INDEX idx_restock (restock, created_at),\n    INDEX idx_restock_reason (restock_reason)\n) ENGINE=InnoDB;\n\n-- ============================================\n-- AUDIT & HISTORY\n-- ============================================\n\nCREATE TABLE order_status_history (\n    history_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    order_id BIGINT NOT NULL,\n    from_status VARCHAR(50),\n    to_status VARCHAR(50) NOT NULL,\n    changed_by VARCHAR(100),\n    notes TEXT,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    FOREIGN KEY (order_id) REFERENCES orders(order_id) ON DELETE CASCADE,\n    \n    INDEX idx_order_created (order_id, created_at DESC),\n    INDEX idx_created_at (created_at)\n) ENGINE=InnoDB;\n\n-- ============================================\n-- ARCHIVE SYSTEM WITH TIERED VERIFICATION\n-- ============================================\n\nCREATE TABLE order_archive_index (\n    order_id BIGINT PRIMARY KEY,\n    location ENUM('active', 'archived', 'cold_storage') NOT NULL DEFAULT 'active',\n    archived_at TIMESTAMP NULL,\n    cold_storage_path VARCHAR(500),\n    \n    -- Tiered verification strategy\n    verification_tier ENUM('standard', 'high_value') NOT NULL DEFAULT 'standard',\n    order_total DECIMAL(12,2),  -- Cached for tier determination\n    \n    last_verified_at TIMESTAMP NULL,\n    verification_status ENUM('ok', 'missing', 'duplicate', 'inconsistent') NOT NULL DEFAULT 'ok',\n    verification_checksum VARCHAR(64),\n    \n    INDEX idx_location (location),\n    INDEX idx_archived_at (archived_at),\n    INDEX idx_verification (verification_status, last_verified_at),\n    INDEX idx_verification_tier (verification_tier)  -- NEW: Tier-based verification\n) ENGINE=InnoDB;\n\nCREATE TABLE orders_archive (\n    LIKE orders\n) ENGINE=InnoDB ROW_FORMAT=COMPRESSED;\n\nCREATE TABLE order_items_archive (\n    LIKE order_items\n) ENGINE=InnoDB;\n\nCREATE TABLE order_status_history_archive (\n    LIKE order_status_history\n) ENGINE=InnoDB;\n\nCREATE TABLE archive_batches (\n    batch_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    archive_date DATE NOT NULL,\n    orders_archived INT NOT NULL,\n    oldest_order_date TIMESTAMP NOT NULL,\n    newest_order_date TIMESTAMP NOT NULL,\n    status ENUM('in_progress', 'completed', 'failed', 'verified') NOT NULL,\n    \n    -- Verification strategy applied\n    verification_strategy ENUM('sampled', 'high_value_only', 'full') NOT NULL DEFAULT 'sampled',\n    sample_percentage DECIMAL(5,2),  -- NULL for full verification\n    high_value_orders_verified INT NOT NULL DEFAULT 0,\n    sampled_orders_verified INT NOT NULL DEFAULT 0,\n    \n    verification_discrepancies INT NOT NULL DEFAULT 0,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    completed_at TIMESTAMP NULL,\n    verified_at TIMESTAMP NULL,\n    \n    INDEX idx_archive_date (archive_date),\n    INDEX idx_status (status),\n    INDEX idx_verification (status, verified_at)\n) ENGINE=InnoDB;\n\nCREATE TABLE archive_reconciliation_log (\n    log_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    batch_id BIGINT NULL,\n    reconciliation_type ENUM('full_scan', 'batch_verification', 'spot_check', 'post_archive') NOT NULL,\n    orders_checked INT NOT NULL,\n    discrepancies_found INT NOT NULL,\n    discrepancies_resolved INT NOT NULL,\n    checksum_failures INT NOT NULL DEFAULT 0,\n    execution_time_ms INT NOT NULL,\n    created_at TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    \n    FOREIGN KEY (batch_id) REFERENCES archive_batches(batch_id),\n    \n    INDEX idx_created_at (created_at DESC),\n    INDEX idx_type_date (reconciliation_type, created_at)\n) ENGINE=InnoDB;\n\n-- ============================================\n-- CAPACITY ALERTING & MONITORING\n-- ============================================\n\nCREATE TABLE capacity_alerts (\n    alert_id BIGINT PRIMARY KEY AUTO_INCREMENT,\n    warehouse_id INT NOT NULL,\n    alert_type ENUM('warning', 'critical', 'overflow') NOT NULL,\n    capacity_utilization DECIMAL(5,2) NOT NULL,  -- Percentage\n    current_orders INT NOT NULL,\n    effective_capacity INT NOT NULL,\n    alert_time TIMESTAMP NOT NULL DEFAULT CURRENT_TIMESTAMP,\n    resolved_at TIMESTAMP NULL,\n    notification_sent BOOLEAN NOT NULL DEFAULT FALSE,\n    \n    FOREIGN KEY (warehouse_id) REFERENCES warehouses(warehouse_id),\n    \n    INDEX idx_warehouse_time (warehouse_id, alert_time DESC),\n    INDEX idx_alert_type_time (alert_type, alert_time DESC),\n    INDEX idx_resolved (resolved_at)\n) ENGINE=InnoDB;\n```\n\n#### 2. Enhanced Multi-Warehouse Service\n\n```python\nfrom typing import List, Optional, Tuple\nfrom dataclasses import dataclass\nfrom decimal import Decimal\nfrom datetime import date, datetime\n\n@dataclass\nclass WarehouseAllocation:\n    warehouse_id: int\n    warehouse_code: str\n    quantity: int\n    distance_km: float\n    shipping_cost: Decimal\n    capacity_utilization: Decimal\n\n@dataclass\nclass AllocationStrategy:\n    shipping_preference: str\n    prefer_single_warehouse: bool = True\n    max_split_shipments: int = 2\n    allow_capacity_overflow: bool = True\n    \n    @property\n    def cost_weight(self) -> float:\n        return {'economy': 0.9, 'standard': 0.7, 'expedited': 0.3}[self.shipping_preference]\n    \n    @property\n    def speed_weight(self) -> float:\n        return {'economy': 0.1, 'standard': 0.3, 'expedited': 0.7}[self.shipping_preference]\n\nclass MultiWarehouseInventoryService:\n    \n    # Configurable thresholds\n    CAPACITY_WARNING_THRESHOLD = 0.80  # 80%\n    CAPACITY_CRITICAL_THRESHOLD = 0.95  # 95%\n    HIGH_VALUE_ORDER_THRESHOLD = Decimal('1000.00')\n    \n    async def reserve_stock_multi_warehouse(\n        self,\n        product_id: int,\n        quantity: int,\n        customer_id: int,\n        shipping_address: Address,\n        order_id: Optional[int] = None,\n        ttl_seconds: int = 900,\n        strategy: Optional[AllocationStrategy] = None\n    ) -> List[WarehouseAllocation]:\n        \"\"\"Intelligently allocate stock with capacity handling.\"\"\"\n        if strategy is None:\n            strategy = AllocationStrategy(\n                shipping_preference='standard',\n                allow_capacity_overflow=True\n            )\n        \n        customer_coords = await self._get_cached_geocode(shipping_address, order_id)\n        \n        async with self.db.transaction():\n            await self._reset_warehouse_capacity_if_needed()\n            \n            available_warehouses = await self._get_available_warehouses_with_capacity(\n                product_id, quantity, date.today()\n            )\n            \n            if not available_warehouses:\n                raise InsufficientStockError(f\"Product {product_id} out of stock\")\n            \n            # Filter warehouses based on capacity policy\n            if not strategy.allow_capacity_overflow:\n                available_warehouses = [\n                    wh for wh in available_warehouses\n                    if wh['current_daily_orders'] < wh['effective_capacity']\n                ]\n                \n                if not available_warehouses:\n                    raise CapacityExceededError(\n                        f\"All warehouses at capacity. Retry later or enable overflow.\"\n                    )\n            \n            warehouses_scored = await self._score_warehouses(\n                available_warehouses,\n                customer_coords,\n                quantity,\n                strategy\n            )\n            \n            # Try single-warehouse fulfillment\n            single_wh_options = [\n                wh for wh in warehouses_scored \n                if wh['available_quantity'] >= quantity\n            ]\n            \n            if single_wh_options:\n                best_single = single_wh_options[0]\n                \n                split_allocation = self._calculate_split_allocation(\n                    warehouses_scored, quantity, strategy\n                )\n                \n                if self._should_prefer_single_warehouse(\n                    best_single, split_allocation, strategy\n                ):\n                    allocations = await self._reserve_from_warehouse(\n                        best_single, product_id, quantity, \n                        customer_id, order_id, ttl_seconds\n                    )\n                    await self._increment_warehouse_capacity(best_single['warehouse_id'])\n                    await self._check_capacity_alert(best_single['warehouse_id'])\n                    return allocations\n            \n            # Fall back to split shipment\n            allocations = await self._reserve_split_shipment(\n                warehouses_scored, product_id, quantity,\n                customer_id, order_id, ttl_seconds,\n                strategy.max_split_shipments\n            )\n            \n            if sum(a.quantity for a in allocations) < quantity:\n                raise InsufficientStockError(\n                    f\"Only {sum(a.quantity for a in allocations)}/{quantity} \"\n                    f\"units available for product {product_id}\"\n                )\n            \n            for alloc in allocations:\n                await self._increment_warehouse_capacity(alloc.warehouse_id)\n                await self._check_capacity_alert(alloc.warehouse_id)\n            \n            return allocations\n    \n    async def _get_available_warehouses_with_capacity(\n        self,\n        product_id: int,\n        quantity: int,\n        check_date: date\n    ) -> List[dict]:\n        \"\"\"\n        Get warehouses with stock and effective capacity.\n        \n        Capacity resolution:\n        1. Find all active overrides for check_date\n        2. Select highest priority override (lowest priority number)\n        3. Apply hard_capacity_limit if set, otherwise base * multiplier\n        \"\"\"\n        return await self.db.execute(\n            \"\"\"\n            WITH active_overrides AS (\n                SELECT \n                    warehouse_id,\n                    hard_capacity_limit,\n                    capacity_multiplier,\n                    priority,\n                    reason,\n                    ROW_NUMBER() OVER (\n                        PARTITION BY warehouse_id \n                        ORDER BY priority ASC, created_at DESC\n                    ) as rn\n                FROM warehouse_capacity_overrides\n                WHERE ? BETWEEN start_date AND end_date\n            )\n            SELECT \n                w.warehouse_id,\n                w.warehouse_code,\n                w.latitude,\n                w.longitude,\n                w.shipping_cost_base,\n                w.priority,\n                w.base_daily_capacity,\n                w.current_daily_orders,\n                wi.stock_quantity - wi.reserved_quantity as available_quantity,\n                \n                -- Calculate effective capacity (priority-based override resolution)\n                COALESCE(\n                    (SELECT hard_capacity_limit FROM active_overrides ao \n                     WHERE ao.warehouse_id = w.warehouse_id AND ao.rn = 1),\n                    FLOOR(w.base_daily_capacity * COALESCE(\n                        (SELECT capacity_multiplier FROM active_overrides ao \n                         WHERE ao.warehouse_id = w.warehouse_id AND ao.rn = 1),\n                        1.0\n                    ))\n                ) as effective_capacity,\n                \n                (SELECT capacity_multiplier FROM active_overrides ao \n                 WHERE ao.warehouse_id = w.warehouse_id AND ao.rn = 1) as capacity_multiplier,\n                \n                (SELECT reason FROM active_overrides ao \n                 WHERE ao.warehouse_id = w.warehouse_id AND ao.rn = 1) as override_reason,\n                \n                -- Capacity utilization ratio\n                CASE \n                    WHEN COALESCE(\n                        (SELECT hard_capacity_limit FROM active_overrides ao \n                         WHERE ao.warehouse_id = w.warehouse_id AND ao.rn = 1),\n                        w.base_daily_capacity\n                    ) > 0\n                    THEN w.current_daily_orders / COALESCE(\n                        (SELECT hard_capacity_limit FROM active_overrides ao \n                         WHERE ao.warehouse_id = w.warehouse_id AND ao.rn = 1),\n                        FLOOR(w.base_daily_capacity * COALESCE(\n                            (SELECT capacity_multiplier FROM active_overrides ao \n                             WHERE ao.warehouse_id = w.warehouse_id AND ao.rn = 1),\n                            1.0\n                        ))\n                    )\n                    ELSE 0.0\n                END as capacity_utilization\n                \n            FROM warehouses w\n            JOIN warehouse_inventory wi \n                ON w.warehouse_id = wi.warehouse_id\n            WHERE w.status = 'active'\n                AND wi.product_id = ?\n                AND (wi.stock_quantity - wi.reserved_quantity) > 0\n            ORDER BY \n                capacity_utilization ASC,\n                w.priority ASC\n            \"\"\",\n            (check_date, product_id)\n        )\n    \n    async def _check_capacity_alert(self, warehouse_id: int):\n        \"\"\"Check capacity thresholds and create alerts if needed.\"\"\"\n        wh_stats = await self.db.execute(\n            \"\"\"\n            SELECT \n                w.warehouse_id,\n                w.current_daily_orders,\n                w.base_daily_capacity,\n                COALESCE(\n                    (SELECT hard_capacity_limit \n                     FROM warehouse_capacity_overrides \n                     WHERE warehouse_id = w.warehouse_id \n                       AND CURDATE() BETWEEN start_date AND end_date\n                     ORDER BY priority ASC, created_at DESC\n                     LIMIT 1),\n                    FLOOR(w.base_daily_capacity * COALESCE(\n                        (SELECT capacity_multiplier \n                         FROM warehouse_capacity_overrides \n                         WHERE warehouse_id = w.warehouse_id \n                           AND CURDATE() BETWEEN start_date AND end_date\n                         ORDER BY priority ASC, created_at DESC\n                         LIMIT 1),\n                        1.0\n                    ))\n                ) as effective_capacity\n            FROM warehouses w\n            WHERE w.warehouse_id = ?\n            \"\"\",\n            (warehouse_id,)\n        )\n        \n        utilization = wh_stats.current_daily_orders / wh_stats.effective_capacity\n        \n        alert_type = None\n        if utilization >= 1.0:\n            alert_type = 'overflow'\n        elif utilization >= self.CAPACITY_CRITICAL_THRESHOLD:\n            alert_type = 'critical'\n        elif utilization >= self.CAPACITY_WARNING_THRESHOLD:\n            alert_type = 'warning'\n        \n        if alert_type:\n            # Check if alert already exists and unresolved\n            existing_alert = await self.db.execute(\n                \"\"\"\n                SELECT alert_id FROM capacity_alerts\n                WHERE warehouse_id = ? \n                  AND alert_type = ?\n                  AND resolved_at IS NULL\n                  AND alert_time > DATE_SUB(NOW(), INTERVAL 1 HOUR)\n                \"\"\",\n                (warehouse_id, alert_type)\n            )\n            \n            if not existing_alert:\n                await self.db.execute(\n                    \"\"\"\n                    INSERT INTO capacity_alerts (\n                        warehouse_id, alert_type, capacity_utilization,\n                        current_orders, effective_capacity\n                    )\n                    VALUES (?, ?, ?, ?, ?)\n                    \"\"\",\n                    (warehouse_id, alert_type, utilization * 100,\n                     wh_stats.current_daily_orders, wh_stats.effective_capacity)\n                )\n                \n                # Send notification for critical/overflow\n                if alert_type in ['critical', 'overflow']:\n                    await self.alerting_service.send_alert(\n                        severity=alert_type,\n                        message=f\"Warehouse {warehouse_id} at {utilization:.1%} capacity\",\n                        details={\n                            'warehouse_id': warehouse_id,\n                            'current_orders': wh_stats.current_daily_orders,\n                            'effective_capacity': wh_stats.effective_capacity,\n                            'utilization': f\"{utilization:.1%}\"\n                        }\n                    )\n    \n    async def _get_cached_geocode(\n        self, \n        address: Address,\n        order_id: Optional[int] = None\n    ) -> Tuple[float, float]:\n        \"\"\"\n        Geocode with three-tier lookup and failure handling.\n        \n        Lookup order:\n        1. Redis cache (hot, <1ms)\n        2. Database cache (warm, ~5ms)\n        3. Reference table (pre-populated centroids)\n        4. External API call (cold, ~200ms)\n        5. Fallback to approximate centroid\n        \"\"\"\n        cache_key = f\"{address.postal_code}:{address.country}\"\n        \n        # Tier 1: Redis\n        cached = await self.redis.get(cache_key)\n        if cached:\n            return tuple(map(float, cached.split(',')))\n        \n        # Tier 2: Database cache\n        db_cached = await self.db.execute(\n            \"\"\"\n            UPDATE geocode_cache\n            SET hit_count = hit_count + 1,\n                last_accessed_at = NOW()\n            WHERE postal_code = ? AND country = ?\n            RETURNING latitude, longitude, geocode_quality\n            \"\"\",\n            (address.postal_code, address.country)\n        )\n        \n        if db_cached:\n            await self.redis.setex(cache_key, 86400, f\"{db_cached.latitude},{db_cached.longitude}\")\n            return (db_cached.latitude, db_cached.longitude)\n        \n        # Tier 3: Reference table (pre-populated)\n        reference = await self.db.execute(\n            \"\"\"\n            SELECT latitude, longitude, geocode_quality\n            FROM geocode_reference\n            WHERE postal_code = ? AND country = ?\n            \"\"\",\n            (address.postal_code, address.country)\n        )\n        \n        if reference:\n            # Promote to cache\n            await self.db.execute(\n                \"\"\"\n                INSERT INTO geocode_cache (\n                    postal_code, country, latitude, longitude,\n                    geocode_quality, geocode_source\n                )\n                VALUES (?, ?, ?, ?, ?, 'reference_table')\n                \"\"\",\n                (address.postal_code, address.country, \n                 reference.latitude, reference.longitude, reference.geocode_quality)\n            )\n            await self.redis.setex(cache_key, 86400, f\"{reference.latitude},{reference.longitude}\")\n            return (reference.latitude, reference.longitude)\n        \n        # Tier 4: External API\n        try:\n            coords, quality, source = await self.geocoding_service.geocode_with_quality(\n                f\"{address.line1}, {address.city}, {address.state} {address.postal_code}, {address.country}\"\n            )\n            \n            # Store in cache\n            await self.db.execute(\n                \"\"\"\n                INSERT INTO geocode_cache (\n                    postal_code, country, latitude, longitude, \n                    geocode_quality, geocode_source\n                )\n                VALUES (?, ?, ?, ?, ?, 'api')\n                \"\"\",\n                (address.postal_code, address.country, coords[0], coords[1], quality)\n            )\n            \n            await self.redis.setex(cache_key, 86400, f\"{coords[0]},{coords[1]}\")\n            return coords\n            \n        except GeocodingError as e:\n            # Tier 5: Fallback to approximate centroid\n            await self._handle_geocode_failure(\n                address, order_id, str(e), 'centroid'\n            )\n            \n            # Use country-level centroid as last resort\n            country_centroid = await self._get_country_centroid(address.country)\n            \n            await self.redis.setex(cache_key, 3600, f\"{country_centroid[0]},{country_centroid[1]}\")\n            return country_centroid\n    \n    async def _handle_geocode_failure(\n        self,\n        address: Address,\n        order_id: Optional[int],\n        error_message: str,\n        fallback_strategy: str\n    ):\n        \"\"\"\n        Log geocode failure and trigger customer notification workflow.\n        \"\"\"\n        failure_id = await self.db.execute(\n            \"\"\"\n            INSERT INTO geocode_failures (\n                postal_code, country, full_address, order_id,\n                failure_reason, fallback_strategy, resolution_status\n            )\n            VALUES (?, ?, ?, ?, ?, ?, 'pending')\n            \"\"\",\n            (\n                address.postal_code,\n                address.country,\n                f\"{address.line1}, {address.city}, {address.state}\",\n                order_id,\n                error_message,\n                fallback_strategy\n            )\n        )\n        \n        if order_id:\n            # Update order to require address review\n            await self.db.execute(\n                \"\"\"\n                UPDATE orders\n                SET status = 'address_review',\n                    geocode_requires_review = TRUE,\n                    geocode_quality = 'approximate',\n                    internal_notes = CONCAT(\n                        COALESCE(internal_notes, ''),\n                        '\\nGeocoding failed (failure_id: ', ?, '). Customer notification required.'\n                    )\n                WHERE order_id = ?\n                \"\"\",\n                (failure_id, order_id)\n            )\n            \n            # Trigger async customer notification\n            await self.task_queue.enqueue(\n                task='notify_customer_address_verification',\n                args={\n                    'order_id': order_id,\n                    'failure_id': failure_id\n                }\n            )\n    \n    async def notify_customer_address_verification(self, order_id: int, failure_id: int):\n        \"\"\"\n        Send customer email requesting address verification.\n        \n        Email includes:\n        - Current address on file\n        - Link to update shipping address\n        - 48-hour deadline before order auto-cancels\n        \"\"\"\n        order = await self.db.execute(\n            \"SELECT customer_id, order_number, shipping_address_line1, shipping_city, shipping_state, shipping_postal_code FROM orders WHERE order_id = ?\",\n            (order_id,)\n        )\n        \n        customer = await self.db.execute(\n            \"SELECT email, first_name FROM customers WHERE customer_id = ?\",\n            (order.customer_id,)\n        )\n        \n        verification_link = f\"{self.config.base_url}/orders/{order.order_number}/verify-address?token={self._generate_verification_token(order_id)}\"\n        \n        await self.email_service.send(\n            to=customer.email,\n            subject=f\"Address verification needed for order {order.order_number}\",\n            template='address_verification',\n            context={\n                'customer_name': customer.first_name,\n                'order_number': order.order_number,\n                'address': f\"{order.shipping_address_line1}, {order.shipping_city}, {order.shipping_state} {order.shipping_postal_code}\",\n                'verification_link': verification_link,\n                'deadline_hours': 48\n            }\n        )\n        \n        await self.db.execute(\n            \"\"\"\n            UPDATE geocode_failures\n            SET customer_notified = TRUE,\n                resolution_status = 'customer_contacted'\n            WHERE failure_id = ?\n            \"\"\",\n            (failure_id,)\n        )\n        \n        # Schedule auto-cancel job\n        await self.task_queue.enqueue(\n            task='auto_cancel_unverified_order',\n            args={'order_id': order_id},\n            execute_at=datetime.utcnow() + timedelta(hours=48)\n        )\n```\n\n#### 3. Payment Service with Manual Error Classification\n\n```python\nfrom typing import Optional\nimport asyncio\nfrom datetime import datetime, timedelta\nfrom enum import Enum\n\nclass PaymentService:\n    \n    MAX_RETRY_ATTEMPTS = 5\n    BASE_RETRY_DELAY = 30\n    \n    async def process_payment_event(self, event_id: int):\n        \"\"\"Process payment event with automatic retry and DLQ.\"\"\"\n        async with self.db.transaction():\n            event = await self.db.execute(\n                \"\"\"\n                SELECT \n                    pe.event_id,\n                    pe.payment_id,\n                    pe.event_type,\n                    pe.event_payload,\n                    pe.processed,\n                    pe.retry_count,\n                    p.order_id,\n                    p.payment_gateway,\n                    p.status as payment_status,\n                    p.amount,\n                    p.failure_code\n                FROM payment_events pe\n                JOIN payments p ON pe.payment_id = p.payment_id\n                WHERE pe.event_id = ?\n                FOR UPDATE\n                \"\"\",\n                (event_id,)\n            )\n            \n            if not event or event.processed:\n                return\n            \n            try:\n                if event.event_type == 'authorization_succeeded':\n                    await self._handle_authorization_success(event)\n                elif event.event_type == 'capture_succeeded':\n                    await self._handle_capture_success(event)\n                elif event.event_type == 'authorization_failed':\n                    await self._handle_authorization_failure(event)\n                elif event.event_type == 'refund_succeeded':\n                    await self._handle_refund_success(event)\n                \n                await self.db.execute(\n                    \"\"\"\n                    UPDATE payment_events\n                    SET processed = TRUE, processed_at = NOW()\n                    WHERE event_id = ?\n                    \"\"\",\n                    (event_id,)\n                )\n                \n            except Exception as e:\n                await self._handle_event_failure(event, e)\n                raise\n    \n    async def _handle_event_failure(self, event, error: Exception):\n        \"\"\"Handle failed event with DLQ routing and classification.\"\"\"\n        retry_count = event.retry_count + 1\n        \n        if retry_count >= self.MAX_RETRY_ATTEMPTS:\n            # Check if error matches known class\n            error_class = await self._classify_error(\n                event.event_type,\n                event.payment_gateway,\n                event.failure_code,\n                str(error)\n            )\n            \n            priority = self._calculate_dlq_priority(event)\n            \n            # Determine initial status based on classification\n            if error_class and error_class.auto_replay_enabled:\n                initial_status = 'auto_replay_scheduled'\n            elif error_class:\n                initial_status = 'pending_review'\n            else:\n                initial_status = 'pending_classification'\n            \n            dlq_id = await self.db.execute(\n                \"\"\"\n                INSERT INTO payment_events_dlq (\n                    original_event_id,\n                    payment_id,\n                    event_type,\n                    event_payload,\n                    failure_reason,\n                    error_class_id,\n                    retry_attempts,\n                    status,\n                    priority\n                )\n                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\",\n                (event.event_id, event.payment_id, event.event_type,\n                 event.event_payload, str(error), \n                 error_class.class_id if error_class else None,\n                 retry_count, initial_status, priority)\n            )\n            \n            await self.db.execute(\n                \"\"\"\n                UPDATE payment_events\n                SET event_type = 'moved_to_dlq',\n                    processing_error = ?\n                WHERE event_id = ?\n                \"\"\",\n                (f\"Max retries exceeded: {error}\", event.event_id)\n            )\n            \n            await self._update_dlq_metrics()\n            \n            # Schedule auto-replay if enabled\n            if error_class and error_class.auto_replay_enabled:\n                await self._schedule_auto_replay(\n                    dlq_id, \n                    error_class.class_id,\n                    error_class.auto_replay_delay_seconds\n                )\n            elif priority in ['high', 'critical']:\n                await self.alerting_service.send_alert(\n                    severity=priority,\n                    message=f\"Payment event {event.event_id} moved to DLQ (needs classification)\",\n                    details={\n                        'event_id': event.event_id,\n                        'payment_id': event.payment_id,\n                        'dlq_id': dlq_id,\n                        'amount': event.amount,\n                        'event_type': event.event_type,\n                        'needs_classification': error_class is None\n                    }\n                )\n        else:\n            # Schedule retry\n            delay = self._calculate_retry_delay(retry_count)\n            next_retry_at = datetime.utcnow() + timedelta(seconds=delay)\n            \n            await self.db.execute(\n                \"\"\"\n                UPDATE payment_events\n                SET retry_count = ?,\n                    next_retry_at = ?,\n                    processing_error = ?\n                WHERE event_id = ?\n                \"\"\",\n                (retry_count, next_retry_at, str(error), event.event_id)\n            )\n    \n    async def _classify_error(\n        self,\n        event_type: str,\n        gateway: str,\n        failure_code: Optional[str],\n        error_message: str\n    ) -> Optional[dict]:\n        \"\"\"\n        Match error against manually-curated error classes.\n        \n        Returns matching error class or None if unclassified.\n        \"\"\"\n        error_classes = await self.db.execute(\n            \"\"\"\n            SELECT \n                class_id, class_name, auto_replay_enabled,\n                auto_replay_delay_seconds, auto_replay_confidence\n            FROM payment_error_classes\n            WHERE event_type = ?\n            ORDER BY class_id\n            \"\"\",\n            (event_type,)\n        )\n        \n        for error_class in error_classes:\n            # Check gateway pattern\n            if error_class.gateway_pattern:\n                import re\n                pattern = error_class.gateway_pattern.replace('%', '.*')\n                if not re.match(pattern, gateway, re.IGNORECASE):\n                    continue\n            \n            # Check failure code pattern\n            if error_class.failure_code_pattern and failure_code:\n                pattern = error_class.failure_code_pattern.replace('%', '.*')\n                if not re.match(pattern, failure_code, re.IGNORECASE):\n                    continue\n            \n            # Check message keywords\n            if error_class.message_keywords:\n                import json\n                keywords = json.loads(error_class.message_keywords)\n                if not all(kw.lower() in error_message.lower() for kw in keywords):\n                    continue\n            \n            # Match found\n            await self.db.execute(\n                \"\"\"\n                UPDATE payment_error_classes\n                SET last_seen_at = NOW()\n                WHERE class_id = ?\n                \"\"\",\n                (error_class.class_id,)\n            )\n            \n            return error_class\n        \n        return None\n    \n    async def create_error_class(\n        self,\n        class_name: str,\n        event_type: str,\n        gateway_pattern: Optional[str] = None,\n        failure_code_pattern: Optional[str] = None,\n        message_keywords: Optional[List[str]] = None,\n        operator: str = 'system'\n    ) -> int:\n        \"\"\"\n        Manually create error class for DLQ categorization.\n        \n        Example:\n        - class_name: 'stripe_timeout'\n        - gateway_pattern: 'stripe'\n        - message_keywords: ['timeout', 'connection']\n        \"\"\"\n        import json\n        \n        class_id = await self.db.execute(\n            \"\"\"\n            INSERT INTO payment_error_classes (\n                class_name, event_type, gateway_pattern,\n                failure_code_pattern, message_keywords, created_by\n            )\n            VALUES (?, ?, ?, ?, ?, ?)\n            \"\"\",\n            (class_name, event_type, gateway_pattern, failure_code_pattern,\n             json.dumps(message_keywords) if message_keywords else None, operator)\n        )\n        \n        # Reclassify pending DLQ entries\n        await self.task_queue.enqueue(\n            task='reclassify_pending_dlq',\n            args={'error_class_id': class_id}\n        )\n        \n        return class_id\n    \n    async def enable_auto_replay_for_class(\n        self,\n        class_id: int,\n        delay_seconds: int = 300,\n        operator: str = 'system'\n    ) -> bool:\n        \"\"\"\n        Enable auto-replay for error class after validation.\n        \n        Requirements:\n        - At least 10 successful manual replays\n        - Success rate >= 85%\n        \"\"\"\n        error_class = await self.db.execute(\n            \"\"\"\n            SELECT \n                class_name, successful_replays, failed_replays,\n                auto_replay_confidence\n            FROM payment_error_classes\n            WHERE class_id = ?\n            \"\"\",\n            (class_id,)\n        )\n        \n        if not error_class:\n            return False\n        \n        if error_class.successful_replays < 10:\n            raise ValueError(\n                f\"Class '{error_class.class_name}' needs \"\n                f\"{10 - error_class.successful_replays} more successful replays\"\n            )\n        \n        if error_class.auto_replay_confidence < 85.0:\n            raise ValueError(\n                f\"Class confidence ({error_class.auto_replay_confidence:.1f}%) \"\n                f\"below 85% threshold\"\n            )\n        \n        await self.db.execute(\n            \"\"\"\n            UPDATE payment_error_classes\n            SET auto_replay_enabled = TRUE,\n                auto_replay_delay_seconds = ?\n            WHERE class_id = ?\n            \"\"\",\n            (delay_seconds, class_id)\n        )\n        \n        await self.logger.info(\n            f\"Auto-replay enabled for class {error_class.class_name} \"\n            f\"by {operator} (confidence: {error_class.auto_replay_confidence:.1f}%)\"\n        )\n        \n        return True\n    \n    async def replay_dlq_entry(self, dlq_id: int, operator: str) -> dict:\n        \"\"\"Manual replay with learning feedback.\"\"\"\n        async with self.db.transaction():\n            dlq_entry = await self.db.execute(\n                \"\"\"\n                SELECT \n                    dlq_id, original_event_id, payment_id,\n                    event_type, event_payload, status, error_class_id\n                FROM payment_events_dlq\n                WHERE dlq_id = ?\n                FOR UPDATE\n                \"\"\",\n                (dlq_id,)\n            )\n            \n            if not dlq_entry or dlq_entry.status not in ['ready_for_replay', 'pending_review']:\n                return {'success': False, 'reason': 'invalid_status'}\n            \n            try:\n                new_event_id = await self.db.execute(\n                    \"\"\"\n                    INSERT INTO payment_events (\n                        payment_id, event_type, event_payload, processed\n                    )\n                    VALUES (?, ?, ?, FALSE)\n                    \"\"\",\n                    (dlq_entry.payment_id, dlq_entry.event_type, dlq_entry.event_payload)\n                )\n                \n                await self.process_payment_event(new_event_id)\n                \n                await self.db.execute(\n                    \"\"\"\n                    UPDATE payment_events_dlq\n                    SET status = 'replayed',\n                        replayed_at = NOW(),\n                        resolution_notes = ?\n                    WHERE dlq_id = ?\n                    \"\"\",\n                    (f\"Manually replayed by {operator}\", dlq_id)\n                )\n                \n                # Update error class learning\n                if dlq_entry.error_class_id:\n                    await self.db.execute(\n                        \"\"\"\n                        UPDATE payment_error_classes\n                        SET successful_replays = successful_replays + 1\n                        WHERE class_id = ?\n                        \"\"\",\n                        (dlq_entry.error_class_id,)\n                    )\n                    \n                    # Check if qualifies for auto-replay\n                    updated_class = await self.db.execute(\n                        \"\"\"\n                        SELECT \n                            class_name, successful_replays, \n                            auto_replay_confidence, auto_replay_enabled\n                        FROM payment_error_classes\n                        WHERE class_id = ?\n                        \"\"\",\n                        (dlq_entry.error_class_id,)\n                    )\n                    \n                    if (not updated_class.auto_replay_enabled and\n                        updated_class.successful_replays >= 10 and\n                        updated_class.auto_replay_confidence >= 85.0):\n                        return {\n                            'success': True,\n                            'ready_for_auto_replay': True,\n                            'class_name': updated_class.class_name,\n                            'confidence': updated_class.auto_replay_confidence\n                        }\n                \n                return {'success': True}\n                \n            except Exception as e:\n                await self.db.execute(\n                    \"\"\"\n                    UPDATE payment_events_dlq\n                    SET resolution_notes = CONCAT(\n                        COALESCE(resolution_notes, ''),\n                        '\\nReplay failed (', ?, '): ', ?\n                    )\n                    WHERE dlq_id = ?\n                    \"\"\",\n                    (operator, str(e), dlq_id)\n                )\n                \n                # Update error class failure\n                if dlq_entry.error_class_id:\n                    await self.db.execute(\n                        \"\"\"\n                        UPDATE payment_error_classes\n                        SET failed_replays = failed_replays + 1\n                        WHERE class_id = ?\n                        \"\"\",\n                        (dlq_entry.error_class_id,)\n                    )\n                \n                return {'success': False, 'error': str(e)}\n```\n\n#### 4. Archive Service with Tiered Verification\n\n```python\nclass OrderArchiveService:\n    \n    HIGH_VALUE_THRESHOLD = Decimal('1000.00')\n    STANDARD_SAMPLE_RATE = 0.10  # 10% sampling for standard orders\n    \n    async def archive_old_orders(\n        self,\n        cutoff_date: datetime,\n        batch_size: int = 1000,\n        verification_strategy: str = 'sampled'\n    ) -> int:\n        \"\"\"\n        Archive with configurable verification strategy.\n        \n        Strategies:\n        - 'sampled': 10% random sample + all high-value orders\n        - 'high_value_only': Only verify orders >= $1000\n        - 'full': Verify every order (slow, high integrity)\n        \"\"\"\n        terminal_states = ['delivered', 'cancelled', 'refunded', 'partially_refunded']\n        total_archived = 0\n        \n        batch_id = await self._create_archive_batch(cutoff_date, verification_strategy)\n        \n        try:\n            while True:\n                async with self.db.transaction():\n                    orders = await self.db.execute(\n                        \"\"\"\n                        SELECT order_id, total_amount\n                        FROM orders\n                        WHERE created_at < ?\n                            AND status IN (?, ?, ?, ?)\n                            AND deleted_at IS NULL\n                        LIMIT ?\n                        FOR UPDATE SKIP LOCKED\n                        \"\"\",\n                        (cutoff_date, *terminal_states, batch_size)\n                    )\n                    \n                    if not orders:\n                        break\n                    \n                    order_ids = [o.order_id for o in orders]\n                    \n                    # Copy to archive\n                    await self._copy_to_archive(order_ids)\n                    \n                    # Determine verification tier and checksum\n                    for order in orders:\n                        is_high_value = order.total_amount >= self.HIGH_VALUE_THRESHOLD\n                        verification_tier = 'high_value' if is_high_value else 'standard'\n                        \n                        # Calculate checksum based on strategy\n                        should_verify = (\n                            verification_strategy == 'full' or\n                            (verification_strategy == 'high_value_only' and is_high_value) or\n                            (verification_strategy == 'sampled' and \n                             (is_high_value or self._should_sample()))\n                        )\n                        \n                        checksum = None\n                        if should_verify:\n                            checksum = await self._calculate_order_checksum(order.order_id)\n                        \n                        await self.db.execute(\n                            \"\"\"\n                            INSERT INTO order_archive_index (\n                                order_id, location, archived_at,\n                                verification_tier, order_total,\n                                last_verified_at, verification_checksum,\n                                verification_status\n                            )\n                            VALUES (?, 'archived', NOW(), ?, ?, ?, ?, 'ok')\n                            ON DUPLICATE KEY UPDATE \n                                location = 'archived',\n                                archived_at = NOW(),\n                                last_verified_at = IF(? IS NOT NULL, NOW(), last_verified_at),\n                                verification_checksum = COALESCE(?, verification_checksum),\n                                verification_status = 'ok'\n                            \"\"\",\n                            (order.order_id, verification_tier, order.total_amount,\n                             checksum if should_verify else None,\n                             checksum,\n                             should_verify, checksum)\n                        )\n                    \n                    # Soft delete from main table\n                    await self.db.execute(\n                        \"\"\"\n                        UPDATE orders\n                        SET deleted_at = NOW()\n                        WHERE order_id IN ({})\n                        \"\"\".format(','.join('?' * len(order_ids))),\n                        order_ids\n                    )\n                    \n                    total_archived += len(order_ids)\n                    await self._update_batch_progress(batch_id, total_archived)\n                \n                await asyncio.sleep(0.5)\n            \n            await self._complete_batch(batch_id)\n            \n            # Post-archive verification\n            await self.verify_archive_batch(batch_id, verification_strategy)\n            \n            return total_archived\n            \n        except Exception as e:\n            await self._fail_batch(batch_id)\n            raise\n    \n    def _should_sample(self) -> bool:\n        \"\"\"Random sampling for standard orders.\"\"\"\n        import random\n        return random.random() < self.STANDARD_SAMPLE_RATE\n    \n    async def verify_archive_batch(\n        self,\n        batch_id: int,\n        strategy: str\n    ) -> dict:\n        \"\"\"\n        Verify archive batch with tiered approach.\n        \"\"\"\n        start_time = datetime.utcnow()\n        \n        batch = await self.db.execute(\n            \"SELECT orders_archived FROM archive_batches WHERE batch_id = ?\",\n            (batch_id,)\n        )\n        \n        # Get orders to verify based on strategy\n        if strategy == 'full':\n            orders_to_verify = await self.db.execute(\n                \"\"\"\n                SELECT order_id, verification_checksum\n                FROM order_archive_index\n                WHERE archived_at >= (SELECT created_at FROM archive_batches WHERE batch_id = ?)\n                  AND archived_at <= (SELECT completed_at FROM archive_batches WHERE batch_id = ?)\n                \"\"\",\n                (batch_id, batch_id)\n            )\n        elif strategy == 'high_value_only':\n            orders_to_verify = await self.db.execute(\n                \"\"\"\n                SELECT order_id, verification_checksum\n                FROM order_archive_index\n                WHERE archived_at >= (SELECT created_at FROM archive_batches WHERE batch_id = ?)\n                  AND archived_at <= (SELECT completed_at FROM archive_batches WHERE batch_id = ?)\n                  AND verification_tier = 'high_value'\n                \"\"\",\n                (batch_id, batch_id)\n            )\n        else:  # sampled\n            orders_to_verify = await self.db.execute(\n                \"\"\"\n                SELECT order_id, verification_checksum\n                FROM order_archive_index\n                WHERE archived_at >= (SELECT created_at FROM archive_batches WHERE batch_id = ?)\n                  AND archived_at <= (SELECT completed_at FROM archive_batches WHERE batch_id = ?)\n                  AND verification_checksum IS NOT NULL\n                \"\"\",\n                (batch_id, batch_id)\n            )\n        \n        discrepancies = 0\n        checksum_failures = 0\n        high_value_verified = 0\n        \n        for entry in orders_to_verify:\n            # Count high-value verifications\n            tier = await self.db.execute(\n                \"SELECT verification_tier FROM order_archive_index WHERE order_id = ?\",\n                (entry.order_id,)\n            )\n            if tier.verification_tier == 'high_value':\n                high_value_verified += 1\n            \n            # Check archive exists\n            archived = await self.db.execute(\n                \"SELECT 1 FROM orders_archive WHERE order_id = ?\",\n                (entry.order_id,)\n            )\n            \n            if not archived:\n                discrepancies += 1\n                await self._mark_index_inconsistent(entry.order_id, 'missing_from_archive')\n                continue\n            \n            # Check removed from active\n            active = await self.db.execute(\n                \"SELECT 1 FROM orders WHERE order_id = ? AND deleted_at IS NULL\",\n                (entry.order_id,)\n            )\n            \n            if active:\n                discrepancies += 1\n                await self._mark_index_inconsistent(entry.order_id, 'still_in_active')\n                continue\n            \n            # Verify checksum if present\n            if entry.verification_checksum:\n                archived_checksum = await self._calculate_archived_order_checksum(entry.order_id)\n                if archived_checksum != entry.verification_checksum:\n                    discrepancies += 1\n                    checksum_failures += 1\n                    await self._mark_index_inconsistent(entry.order_id, 'checksum_mismatch')\n        \n        execution_time_ms = int((datetime.utcnow() - start_time).total_seconds() * 1000)\n        \n        # Log reconciliation\n        await self.db.execute(\n            \"\"\"\n            INSERT INTO archive_reconciliation_log (\n                batch_id, reconciliation_type, orders_checked,\n                discrepancies_found, discrepancies_resolved,\n                checksum_failures, execution_time_ms\n            )\n            VALUES (?, 'post_archive', ?, ?, 0, ?, ?)\n            \"\"\",\n            (batch_id, len(orders_to_verify), discrepancies, checksum_failures, execution_time_ms)\n        )\n        \n        # Update batch with verification details\n        sample_pct = None\n        if strategy == 'sampled' and batch.orders_archived > 0:\n            sample_pct = (len(orders_to_verify) / batch.orders_archived) * 100\n        \n        if discrepancies == 0:\n            await self.db.execute(\n                \"\"\"\n                UPDATE archive_batches\n                SET status = 'verified',\n                    verified_at = NOW(),\n                    verification_discrepancies = 0,\n                    verification_strategy = ?,\n                    sample_percentage = ?,\n                    high_value_orders_verified = ?\n                WHERE batch_id = ?\n                \"\"\",\n                (strategy, sample_pct, high_value_verified, batch_id)\n            )\n        else:\n            await self.db.execute(\n                \"\"\"\n                UPDATE archive_batches\n                SET verification_discrepancies = ?,\n                    verification_strategy = ?,\n                    sample_percentage = ?,\n                    high_value_orders_verified = ?\n                WHERE batch_id = ?\n                \"\"\",\n                (discrepancies, strategy, sample_pct, high_value_verified, batch_id)\n            )\n            \n            # Alert if significant issues\n            error_rate = discrepancies / len(orders_to_verify) if orders_to_verify else 0\n            if error_rate > 0.01:  # >1% error rate\n                await self.alerting_service.send_alert(\n                    severity='high',\n                    message=f\"Archive batch {batch_id} verification failed\",\n                    details={\n                        'batch_id': batch_id,\n                        'discrepancies': discrepancies,\n                        'checksum_failures': checksum_failures,\n                        'orders_checked': len(orders_to_verify),\n                        'error_rate': f\"{error_rate:.2%}\"\n                    }\n                )\n        \n        return {\n            'orders_checked': len(orders_to_verify),\n            'discrepancies_found': discrepancies,\n            'checksum_failures': checksum_failures,\n            'high_value_orders_verified': high_value_verified,\n            'execution_time_ms': execution_time_ms\n        }\n    \n    async def _calculate_order_checksum(self, order_id: int) -> str:\n        \"\"\"Calculate SHA256 checksum of order data.\"\"\"\n        order_data = await self.db.execute(\n            \"\"\"\n            SELECT o.*, \n                   GROUP_CONCAT(oi.order_item_id ORDER BY oi.order_item_id) as items\n            FROM orders o\n            LEFT JOIN order_items oi ON o.order_id = oi.order_id\n            WHERE o.order_id = ?\n            GROUP BY o.order_id\n            \"\"\",\n            (order_id,)\n        )\n        \n        import json\n        import hashlib\n        serialized = json.dumps(dict(order_data), sort_keys=True, default=str)\n        return hashlib.sha256(serialized.encode()).hexdigest()\n```"
}