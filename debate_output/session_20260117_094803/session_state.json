{
  "initial_prompt": "Design a real-time notification system\n\nRequirements:\n- Support push notifications (mobile, web)\n- Handle different notification types (alerts, messages, reminders)\n- Allow user preferences for notification channels\n- Support batching to avoid notification fatigue\n- Consider delivery guarantees and retry logic\n- Plan for scale (millions of users)",
  "rounds": [
    {
      "round_number": 1,
      "agent_a": {
        "content": "## Design\n\n# Real-Time Notification System Architecture\n\n## System Overview\n\nA distributed, event-driven notification system supporting push notifications across mobile (iOS/Android) and web platforms with intelligent batching, user preference management, and guaranteed delivery at scale.\n\n## Architecture Components\n\n### 1. Core Services\n\n#### Notification Gateway Service\n- **Purpose**: Single entry point for all notification requests\n- **Responsibilities**:\n  - Request validation and authentication\n  - Rate limiting (per user, per application)\n  - Initial routing to appropriate handlers\n  - Request deduplication using bloom filters\n- **Technology**: Go/Java with gRPC for internal services, REST for external clients\n- **Scaling**: Stateless, horizontally scalable behind load balancer\n\n#### Notification Processing Engine\n- **Purpose**: Core business logic for notification handling\n- **Responsibilities**:\n  - Apply user preferences and filtering rules\n  - Determine optimal delivery channels\n  - Handle notification priority classification (critical, high, medium, low)\n  - Trigger batching logic for non-critical notifications\n  - Enrich notifications with user context\n- **Technology**: Go/Java with worker pool pattern\n- **Scaling**: Horizontal scaling with message queue consumers\n\n#### Channel Delivery Services (one per channel type)\n- **Mobile Push Service**: Integrates with FCM (Firebase Cloud Messaging) and APNs (Apple Push Notification service)\n- **Web Push Service**: Integrates with Web Push Protocol\n- **Email Service**: Backup/alternative channel\n- **SMS Service**: Critical notifications fallback\n- **Technology**: Specialized microservices per channel\n- **Scaling**: Independent scaling based on channel volume\n\n#### Batching & Scheduling Service\n- **Purpose**: Aggregate notifications to prevent user fatigue\n- **Responsibilities**:\n  - Group related notifications by user and type\n  - Apply time-window batching (configurable per notification type)\n  - Smart scheduling based on user timezone and activity patterns\n  - Digest generation for batched notifications\n- **Batching Strategy**:\n  - Immediate: Critical alerts (system failures, security alerts)\n  - 5-minute window: High priority messages\n  - 1-hour window: Medium priority (social interactions)\n  - Daily digest: Low priority (newsletters, summaries)\n- **Technology**: Redis for temporary storage, scheduled jobs with distributed locking\n- **Scaling**: Sharded by user_id hash\n\n#### User Preference Service\n- **Purpose**: Manage user notification settings\n- **Responsibilities**:\n  - CRUD operations for user preferences\n  - Real-time preference caching\n  - Quiet hours management\n  - Channel priority management\n  - DND (Do Not Disturb) support\n- **Technology**: PostgreSQL for persistence, Redis for caching\n- **Scaling**: Read replicas, cache-aside pattern\n\n#### Delivery Tracking & Retry Service\n- **Purpose**: Ensure guaranteed delivery with retry logic\n- **Responsibilities**:\n  - Track delivery status (queued, sent, delivered, failed)\n  - Exponential backoff retry logic (max 5 retries)\n  - Dead letter queue management for permanent failures\n  - Delivery analytics and monitoring\n- **Retry Strategy**:\n  - Attempt 1: Immediate\n  - Attempt 2: 30 seconds\n  - Attempt 3: 5 minutes\n  - Attempt 4: 30 minutes\n  - Attempt 5: 2 hours\n  - After failure: Move to DLQ, alert ops team, attempt fallback channel\n- **Technology**: Kafka for event streaming, PostgreSQL for state tracking\n- **Scaling**: Partitioned by notification_id\n\n### 2. Data Stores\n\n#### Primary Database (PostgreSQL)\n- User preferences and settings\n- Notification templates\n- Delivery history (partitioned by date, 90-day retention)\n- Device registrations\n\n#### Cache Layer (Redis Cluster)\n- User preference cache (TTL: 5 minutes)\n- Device token cache\n- Rate limiting counters (sliding window)\n- Batching windows (temporary notification storage)\n\n#### Message Queue (Apache Kafka)\n- Topics:\n  - `notifications.incoming`: Raw notification requests\n  - `notifications.processed`: Enriched notifications ready for delivery\n  - `notifications.retry`: Failed deliveries needing retry\n  - `notifications.dlq`: Permanent failures\n  - `notifications.analytics`: Delivery events for analytics\n- Partitioning: By user_id for ordered processing per user\n- Retention: 7 days for replay capability\n\n#### Object Storage (S3/GCS)\n- Notification archives (historical data)\n- Large payload storage (images, rich media)\n- Analytics data exports\n\n### 3. External Integrations\n\n#### Push Notification Providers\n- **FCM (Firebase Cloud Messaging)**: Android and iOS via Firebase\n- **APNs (Apple Push Notification service)**: iOS native\n- **Web Push**: Browser-based notifications via service workers\n\n#### Monitoring & Observability\n- **Prometheus + Grafana**: Metrics and dashboards\n- **ELK Stack**: Centralized logging\n- **Jaeger/Zipkin**: Distributed tracing\n- **PagerDuty**: Alerting for critical failures\n\n## Data Models\n\n### Notification Model\n```json\n{\n  \"notification_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"type\": \"enum[ALERT, MESSAGE, REMINDER, SOCIAL, PROMOTIONAL]\",\n  \"priority\": \"enum[CRITICAL, HIGH, MEDIUM, LOW]\",\n  \"title\": \"string\",\n  \"body\": \"string\",\n  \"data\": {\n    \"action_url\": \"string\",\n    \"image_url\": \"string\",\n    \"custom_fields\": {}\n  },\n  \"channels\": [\"PUSH\", \"EMAIL\", \"SMS\"],\n  \"batch_eligible\": \"boolean\",\n  \"batch_key\": \"string (for grouping related notifications)\",\n  \"ttl\": \"integer (seconds)\",\n  \"created_at\": \"timestamp\",\n  \"scheduled_for\": \"timestamp (null for immediate)\",\n  \"idempotency_key\": \"string\"\n}\n```\n\n### User Preferences Model\n```json\n{\n  \"user_id\": \"string\",\n  \"preferences\": {\n    \"global_enabled\": \"boolean\",\n    \"channels\": {\n      \"push\": {\n        \"enabled\": \"boolean\",\n        \"notification_types\": {\n          \"ALERT\": \"boolean\",\n          \"MESSAGE\": \"boolean\",\n          \"REMINDER\": \"boolean\",\n          \"SOCIAL\": \"boolean\",\n          \"PROMOTIONAL\": \"boolean\"\n        }\n      },\n      \"email\": { /* same structure */ },\n      \"sms\": { /* same structure */ }\n    },\n    \"quiet_hours\": {\n      \"enabled\": \"boolean\",\n      \"start_time\": \"string (HH:mm)\",\n      \"end_time\": \"string (HH:mm)\",\n      \"timezone\": \"string\",\n      \"allow_critical\": \"boolean\"\n    },\n    \"batching\": {\n      \"enabled\": \"boolean\",\n      \"frequency\": \"enum[REALTIME, 5MIN, 1HOUR, DAILY]\"\n    }\n  },\n  \"updated_at\": \"timestamp\"\n}\n```\n\n### Device Registration Model\n```json\n{\n  \"device_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"platform\": \"enum[IOS, ANDROID, WEB]\",\n  \"push_token\": \"string (FCM/APNs token)\",\n  \"endpoint\": \"string (for Web Push)\",\n  \"user_agent\": \"string\",\n  \"is_active\": \"boolean\",\n  \"registered_at\": \"timestamp\",\n  \"last_active_at\": \"timestamp\"\n}\n```\n\n### Delivery Status Model\n```json\n{\n  \"delivery_id\": \"uuid\",\n  \"notification_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"device_id\": \"uuid\",\n  \"channel\": \"enum[PUSH, EMAIL, SMS]\",\n  \"status\": \"enum[QUEUED, SENT, DELIVERED, FAILED, EXPIRED]\",\n  \"attempt_count\": \"integer\",\n  \"provider_response\": \"json\",\n  \"error_message\": \"string (if failed)\",\n  \"sent_at\": \"timestamp\",\n  \"delivered_at\": \"timestamp\",\n  \"failed_at\": \"timestamp\"\n}\n```\n\n## API Interfaces\n\n### Internal gRPC Service Interface\n\n```protobuf\nservice NotificationService {\n  // Send single notification\n  rpc SendNotification(SendNotificationRequest) returns (SendNotificationResponse);\n  \n  // Send batch of notifications\n  rpc SendBulkNotifications(SendBulkNotificationsRequest) returns (SendBulkNotificationsResponse);\n  \n  // Get delivery status\n  rpc GetDeliveryStatus(GetDeliveryStatusRequest) returns (GetDeliveryStatusResponse);\n  \n  // Cancel scheduled notification\n  rpc CancelNotification(CancelNotificationRequest) returns (CancelNotificationResponse);\n}\n\nservice UserPreferenceService {\n  rpc GetUserPreferences(GetUserPreferencesRequest) returns (GetUserPreferencesResponse);\n  rpc UpdateUserPreferences(UpdateUserPreferencesRequest) returns (UpdateUserPreferencesResponse);\n  rpc RegisterDevice(RegisterDeviceRequest) returns (RegisterDeviceResponse);\n  rpc UnregisterDevice(UnregisterDeviceRequest) returns (UnregisterDeviceResponse);\n}\n```\n\n### External REST API\n\n```\nPOST /api/v1/notifications\n  - Send notification\n  - Request: Notification model JSON\n  - Response: {notification_id, status}\n  - Rate limit: 1000 req/min per API key\n\nPOST /api/v1/notifications/bulk\n  - Send multiple notifications\n  - Request: Array of Notification models\n  - Response: {job_id, accepted_count}\n  - Rate limit: 100 req/min per API key\n\nGET /api/v1/notifications/{notification_id}/status\n  - Get delivery status\n  - Response: Delivery status model\n\nDELETE /api/v1/notifications/{notification_id}\n  - Cancel scheduled notification\n  - Response: {cancelled: boolean}\n\nGET /api/v1/users/{user_id}/preferences\n  - Get user preferences\n  - Response: User preferences model\n\nPUT /api/v1/users/{user_id}/preferences\n  - Update user preferences\n  - Request: User preferences model\n  - Response: Updated preferences\n\nPOST /api/v1/devices\n  - Register device\n  - Request: Device registration model\n  - Response: {device_id}\n\nDELETE /api/v1/devices/{device_id}\n  - Unregister device\n  - Response: {success: boolean}\n```\n\n## Error Handling & Resilience\n\n### Error Categories\n\n1. **Transient Errors** (Retry with backoff)\n   - Network timeouts\n   - Provider rate limits (FCM/APNs throttling)\n   - Temporary service unavailability\n   - Database connection issues\n\n2. **Permanent Errors** (No retry, log and alert)\n   - Invalid device tokens\n   - Malformed notification payloads\n   - User not found\n   - Authentication failures\n\n3. **Partial Failures** (Retry failed subset)\n   - Bulk send with some failures\n   - Multi-channel delivery with channel-specific failures\n\n### Resilience Patterns\n\n1. **Circuit Breaker**: Protect downstream services (FCM, APNs) from cascading failures\n   - Open after 10 consecutive failures\n   - Half-open after 30 seconds\n   - Close after 5 successful requests\n\n2. **Bulkhead Isolation**: Separate thread pools per channel to prevent cross-channel impact\n\n3. **Timeout Management**:\n   - Gateway timeout: 5 seconds\n   - Processing timeout: 10 seconds\n   - Provider timeout: 30 seconds (with retries)\n\n4. **Graceful Degradation**:\n   - If batching service fails, deliver immediately\n   - If primary channel fails, attempt fallback channel\n   - If preference service fails, use cached/default preferences\n\n5. **Idempotency**: Use idempotency_key to prevent duplicate deliveries during retries\n\n6. **Dead Letter Queue**: Capture permanent failures for manual review and replay\n\n## Scaling Strategy\n\n### Horizontal Scaling\n- All services are stateless and horizontally scalable\n- Auto-scaling based on:\n  - Queue depth (Kafka lag)\n  - CPU/Memory utilization\n  - Request rate\n\n### Database Scaling\n- **PostgreSQL**:\n  - Master-slave replication (1 writer, multiple readers)\n  - Partitioning: delivery_status table by date (monthly partitions)\n  - Connection pooling: PgBouncer\n  - Archive old data to cold storage after 90 days\n\n- **Redis**:\n  - Cluster mode with 3 masters, 3 replicas\n  - Sharding by user_id hash\n  - Separate clusters for cache vs. queuing\n\n- **Kafka**:\n  - 10 partitions per topic (scale to 50 as needed)\n  - 3 replicas per partition\n  - Consumer groups for parallel processing\n\n### Capacity Planning (for 10M active users)\n- **Notification volume**: 100M notifications/day (~1,200/sec avg, 5,000/sec peak)\n- **Gateway service**: 20 instances (250 req/sec each)\n- **Processing engine**: 50 workers (100 notif/sec each)\n- **Channel services**: 10 instances each\n- **Kafka**: 15-node cluster (3 brokers per AZ)\n- **Redis**: 6-node cluster (100GB total memory)\n- **PostgreSQL**: 1 master (32 cores, 128GB RAM) + 3 read replicas\n\n### Geographic Distribution\n- Multi-region deployment (US-East, US-West, EU-West, Asia-Pacific)\n- User data sharded by region\n- Cross-region replication for user preferences (eventual consistency)\n- Regional Kafka clusters with cross-region mirroring for disaster recovery\n\n## Monitoring & Observability\n\n### Key Metrics\n- **Throughput**: Notifications sent/delivered per second\n- **Latency**: p50, p95, p99 for end-to-end delivery time\n- **Success Rate**: Delivery success rate per channel\n- **Error Rate**: Failed deliveries per error type\n- **Queue Depth**: Kafka lag per consumer group\n- **Retry Rate**: Percentage of notifications requiring retry\n\n### Alerts\n- **Critical**: \n  - Service availability < 99.9%\n  - Error rate > 5%\n  - Queue lag > 10,000 messages\n- **Warning**:\n  - Latency p99 > 5 seconds\n  - Retry rate > 10%\n  - Provider rate limit approaching\n\n### Logging\n- Structured JSON logs with correlation IDs\n- Log levels: DEBUG, INFO, WARN, ERROR\n- Sensitive data (push tokens) redacted in logs\n\n## Security Considerations\n\n1. **Authentication & Authorization**:\n   - API key-based auth for service-to-service\n   - OAuth 2.0 for user-facing APIs\n   - JWT tokens for device registration\n\n2. **Data Encryption**:\n   - TLS 1.3 for all network communication\n   - At-rest encryption for databases (AES-256)\n   - Push token encryption in database\n\n3. **Rate Limiting**:\n   - Per-API key limits\n   - Per-user limits to prevent abuse\n   - DDoS protection at load balancer level\n\n4. **PII Handling**:\n   - GDPR compliance: user data deletion within 30 days\n   - Data retention policies enforced\n   - Audit logs for preference changes\n\n## Rationale\n\n1. **Event-driven architecture with Kafka**: Provides durability, replay capability, and decoupling between services. Critical for guaranteed delivery and system resilience.\n\n2. **Separate channel services**: Allows independent scaling and failure isolation. FCM and APNs have different characteristics and rate limits.\n\n3. **Batching service**: Addresses notification fatigue directly - users prefer consolidated notifications over constant interruptions. Smart batching based on priority prevents critical alerts from being delayed.\n\n4. **Multi-layer caching**: User preferences are read-heavy, caching dramatically reduces database load at scale.\n\n5. **PostgreSQL + Redis + Kafka**: Each serves a specific purpose - PostgreSQL for transactional data, Redis for high-speed access, Kafka for event streaming and guaranteed message delivery.\n\n6. **Exponential backoff retry**: Balances delivery guarantees with system load. Fallback channels ensure critical notifications get through.\n\n7. **Regional deployment**: Reduces latency for global users and provides disaster recovery capability.\n\n## What I Changed\nFirst iteration - no previous version to compare against.\n\n## What I Kept\nFirst iteration - establishing baseline design.\n\n## Open Questions\n\n1. **Batching complexity**: How do we handle cases where a user has multiple device types (phone + web) with different batching preferences?\n\n2. **Cross-notification intelligence**: Should we implement ML-based notification relevance scoring to further reduce fatigue?\n\n3. **Real-time preferences**: If a user changes preferences while notifications are in the batching queue, do we retroactively apply the new preferences or honor the settings at notification creation time?\n\n4. **Cost optimization**: For lower-priority notifications, should we implement a \"time-based auction\" where notifications compete for delivery slots to reduce provider costs?\n\n## Convergence Status\n**ITERATING** - This is the first iteration and requires peer review to identify weaknesses, edge cases, and areas for improvement.",
        "prompt_for_other": "Focus on the delivery guarantees and retry logic - are there edge cases where notifications could be lost or duplicated? Also examine the batching strategy for potential user experience issues.",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Real-Time Notification System Architecture\n\n## System Overview\n\nA distributed, event-driven notification system supporting push notifications across mobile (iOS/Android) and web platforms with intelligent batching, user preference management, and guaranteed delivery at scale.\n\n## Architecture Components\n\n### 1. Core Services\n\n#### Notification Gateway Service\n- **Purpose**: Single entry point for all notification requests\n- **Responsibilities**:\n  - Request validation and authentication\n  - Rate limiting (per user, per application)\n  - Initial routing to appropriate handlers\n  - Request deduplication using bloom filters\n- **Technology**: Go/Java with gRPC for internal services, REST for external clients\n- **Scaling**: Stateless, horizontally scalable behind load balancer\n\n#### Notification Processing Engine\n- **Purpose**: Core business logic for notification handling\n- **Responsibilities**:\n  - Apply user preferences and filtering rules\n  - Determine optimal delivery channels\n  - Handle notification priority classification (critical, high, medium, low)\n  - Trigger batching logic for non-critical notifications\n  - Enrich notifications with user context\n- **Technology**: Go/Java with worker pool pattern\n- **Scaling**: Horizontal scaling with message queue consumers\n\n#### Channel Delivery Services (one per channel type)\n- **Mobile Push Service**: Integrates with FCM (Firebase Cloud Messaging) and APNs (Apple Push Notification service)\n- **Web Push Service**: Integrates with Web Push Protocol\n- **Email Service**: Backup/alternative channel\n- **SMS Service**: Critical notifications fallback\n- **Technology**: Specialized microservices per channel\n- **Scaling**: Independent scaling based on channel volume\n\n#### Batching & Scheduling Service\n- **Purpose**: Aggregate notifications to prevent user fatigue\n- **Responsibilities**:\n  - Group related notifications by user and type\n  - Apply time-window batching (configurable per notification type)\n  - Smart scheduling based on user timezone and activity patterns\n  - Digest generation for batched notifications\n- **Batching Strategy**:\n  - Immediate: Critical alerts (system failures, security alerts)\n  - 5-minute window: High priority messages\n  - 1-hour window: Medium priority (social interactions)\n  - Daily digest: Low priority (newsletters, summaries)\n- **Technology**: Redis for temporary storage, scheduled jobs with distributed locking\n- **Scaling**: Sharded by user_id hash\n\n#### User Preference Service\n- **Purpose**: Manage user notification settings\n- **Responsibilities**:\n  - CRUD operations for user preferences\n  - Real-time preference caching\n  - Quiet hours management\n  - Channel priority management\n  - DND (Do Not Disturb) support\n- **Technology**: PostgreSQL for persistence, Redis for caching\n- **Scaling**: Read replicas, cache-aside pattern\n\n#### Delivery Tracking & Retry Service\n- **Purpose**: Ensure guaranteed delivery with retry logic\n- **Responsibilities**:\n  - Track delivery status (queued, sent, delivered, failed)\n  - Exponential backoff retry logic (max 5 retries)\n  - Dead letter queue management for permanent failures\n  - Delivery analytics and monitoring\n- **Retry Strategy**:\n  - Attempt 1: Immediate\n  - Attempt 2: 30 seconds\n  - Attempt 3: 5 minutes\n  - Attempt 4: 30 minutes\n  - Attempt 5: 2 hours\n  - After failure: Move to DLQ, alert ops team, attempt fallback channel\n- **Technology**: Kafka for event streaming, PostgreSQL for state tracking\n- **Scaling**: Partitioned by notification_id\n\n### 2. Data Stores\n\n#### Primary Database (PostgreSQL)\n- User preferences and settings\n- Notification templates\n- Delivery history (partitioned by date, 90-day retention)\n- Device registrations\n\n#### Cache Layer (Redis Cluster)\n- User preference cache (TTL: 5 minutes)\n- Device token cache\n- Rate limiting counters (sliding window)\n- Batching windows (temporary notification storage)\n\n#### Message Queue (Apache Kafka)\n- Topics:\n  - `notifications.incoming`: Raw notification requests\n  - `notifications.processed`: Enriched notifications ready for delivery\n  - `notifications.retry`: Failed deliveries needing retry\n  - `notifications.dlq`: Permanent failures\n  - `notifications.analytics`: Delivery events for analytics\n- Partitioning: By user_id for ordered processing per user\n- Retention: 7 days for replay capability\n\n#### Object Storage (S3/GCS)\n- Notification archives (historical data)\n- Large payload storage (images, rich media)\n- Analytics data exports\n\n### 3. External Integrations\n\n#### Push Notification Providers\n- **FCM (Firebase Cloud Messaging)**: Android and iOS via Firebase\n- **APNs (Apple Push Notification service)**: iOS native\n- **Web Push**: Browser-based notifications via service workers\n\n#### Monitoring & Observability\n- **Prometheus + Grafana**: Metrics and dashboards\n- **ELK Stack**: Centralized logging\n- **Jaeger/Zipkin**: Distributed tracing\n- **PagerDuty**: Alerting for critical failures\n\n## Data Models\n\n### Notification Model\n```json\n{\n  \"notification_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"type\": \"enum[ALERT, MESSAGE, REMINDER, SOCIAL, PROMOTIONAL]\",\n  \"priority\": \"enum[CRITICAL, HIGH, MEDIUM, LOW]\",\n  \"title\": \"string\",\n  \"body\": \"string\",\n  \"data\": {\n    \"action_url\": \"string\",\n    \"image_url\": \"string\",\n    \"custom_fields\": {}\n  },\n  \"channels\": [\"PUSH\", \"EMAIL\", \"SMS\"],\n  \"batch_eligible\": \"boolean\",\n  \"batch_key\": \"string (for grouping related notifications)\",\n  \"ttl\": \"integer (seconds)\",\n  \"created_at\": \"timestamp\",\n  \"scheduled_for\": \"timestamp (null for immediate)\",\n  \"idempotency_key\": \"string\"\n}\n```\n\n### User Preferences Model\n```json\n{\n  \"user_id\": \"string\",\n  \"preferences\": {\n    \"global_enabled\": \"boolean\",\n    \"channels\": {\n      \"push\": {\n        \"enabled\": \"boolean\",\n        \"notification_types\": {\n          \"ALERT\": \"boolean\",\n          \"MESSAGE\": \"boolean\",\n          \"REMINDER\": \"boolean\",\n          \"SOCIAL\": \"boolean\",\n          \"PROMOTIONAL\": \"boolean\"\n        }\n      },\n      \"email\": { /* same structure */ },\n      \"sms\": { /* same structure */ }\n    },\n    \"quiet_hours\": {\n      \"enabled\": \"boolean\",\n      \"start_time\": \"string (HH:mm)\",\n      \"end_time\": \"string (HH:mm)\",\n      \"timezone\": \"string\",\n      \"allow_critical\": \"boolean\"\n    },\n    \"batching\": {\n      \"enabled\": \"boolean\",\n      \"frequency\": \"enum[REALTIME, 5MIN, 1HOUR, DAILY]\"\n    }\n  },\n  \"updated_at\": \"timestamp\"\n}\n```\n\n### Device Registration Model\n```json\n{\n  \"device_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"platform\": \"enum[IOS, ANDROID, WEB]\",\n  \"push_token\": \"string (FCM/APNs token)\",\n  \"endpoint\": \"string (for Web Push)\",\n  \"user_agent\": \"string\",\n  \"is_active\": \"boolean\",\n  \"registered_at\": \"timestamp\",\n  \"last_active_at\": \"timestamp\"\n}\n```\n\n### Delivery Status Model\n```json\n{\n  \"delivery_id\": \"uuid\",\n  \"notification_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"device_id\": \"uuid\",\n  \"channel\": \"enum[PUSH, EMAIL, SMS]\",\n  \"status\": \"enum[QUEUED, SENT, DELIVERED, FAILED, EXPIRED]\",\n  \"attempt_count\": \"integer\",\n  \"provider_response\": \"json\",\n  \"error_message\": \"string (if failed)\",\n  \"sent_at\": \"timestamp\",\n  \"delivered_at\": \"timestamp\",\n  \"failed_at\": \"timestamp\"\n}\n```\n\n## API Interfaces\n\n### Internal gRPC Service Interface\n\n```protobuf\nservice NotificationService {\n  // Send single notification\n  rpc SendNotification(SendNotificationRequest) returns (SendNotificationResponse);\n  \n  // Send batch of notifications\n  rpc SendBulkNotifications(SendBulkNotificationsRequest) returns (SendBulkNotificationsResponse);\n  \n  // Get delivery status\n  rpc GetDeliveryStatus(GetDeliveryStatusRequest) returns (GetDeliveryStatusResponse);\n  \n  // Cancel scheduled notification\n  rpc CancelNotification(CancelNotificationRequest) returns (CancelNotificationResponse);\n}\n\nservice UserPreferenceService {\n  rpc GetUserPreferences(GetUserPreferencesRequest) returns (GetUserPreferencesResponse);\n  rpc UpdateUserPreferences(UpdateUserPreferencesRequest) returns (UpdateUserPreferencesResponse);\n  rpc RegisterDevice(RegisterDeviceRequest) returns (RegisterDeviceResponse);\n  rpc UnregisterDevice(UnregisterDeviceRequest) returns (UnregisterDeviceResponse);\n}\n```\n\n### External REST API\n\n```\nPOST /api/v1/notifications\n  - Send notification\n  - Request: Notification model JSON\n  - Response: {notification_id, status}\n  - Rate limit: 1000 req/min per API key\n\nPOST /api/v1/notifications/bulk\n  - Send multiple notifications\n  - Request: Array of Notification models\n  - Response: {job_id, accepted_count}\n  - Rate limit: 100 req/min per API key\n\nGET /api/v1/notifications/{notification_id}/status\n  - Get delivery status\n  - Response: Delivery status model\n\nDELETE /api/v1/notifications/{notification_id}\n  - Cancel scheduled notification\n  - Response: {cancelled: boolean}\n\nGET /api/v1/users/{user_id}/preferences\n  - Get user preferences\n  - Response: User preferences model\n\nPUT /api/v1/users/{user_id}/preferences\n  - Update user preferences\n  - Request: User preferences model\n  - Response: Updated preferences\n\nPOST /api/v1/devices\n  - Register device\n  - Request: Device registration model\n  - Response: {device_id}\n\nDELETE /api/v1/devices/{device_id}\n  - Unregister device\n  - Response: {success: boolean}\n```\n\n## Error Handling & Resilience\n\n### Error Categories\n\n1. **Transient Errors** (Retry with backoff)\n   - Network timeouts\n   - Provider rate limits (FCM/APNs throttling)\n   - Temporary service unavailability\n   - Database connection issues\n\n2. **Permanent Errors** (No retry, log and alert)\n   - Invalid device tokens\n   - Malformed notification payloads\n   - User not found\n   - Authentication failures\n\n3. **Partial Failures** (Retry failed subset)\n   - Bulk send with some failures\n   - Multi-channel delivery with channel-specific failures\n\n### Resilience Patterns\n\n1. **Circuit Breaker**: Protect downstream services (FCM, APNs) from cascading failures\n   - Open after 10 consecutive failures\n   - Half-open after 30 seconds\n   - Close after 5 successful requests\n\n2. **Bulkhead Isolation**: Separate thread pools per channel to prevent cross-channel impact\n\n3. **Timeout Management**:\n   - Gateway timeout: 5 seconds\n   - Processing timeout: 10 seconds\n   - Provider timeout: 30 seconds (with retries)\n\n4. **Graceful Degradation**:\n   - If batching service fails, deliver immediately\n   - If primary channel fails, attempt fallback channel\n   - If preference service fails, use cached/default preferences\n\n5. **Idempotency**: Use idempotency_key to prevent duplicate deliveries during retries\n\n6. **Dead Letter Queue**: Capture permanent failures for manual review and replay\n\n## Scaling Strategy\n\n### Horizontal Scaling\n- All services are stateless and horizontally scalable\n- Auto-scaling based on:\n  - Queue depth (Kafka lag)\n  - CPU/Memory utilization\n  - Request rate\n\n### Database Scaling\n- **PostgreSQL**:\n  - Master-slave replication (1 writer, multiple readers)\n  - Partitioning: delivery_status table by date (monthly partitions)\n  - Connection pooling: PgBouncer\n  - Archive old data to cold storage after 90 days\n\n- **Redis**:\n  - Cluster mode with 3 masters, 3 replicas\n  - Sharding by user_id hash\n  - Separate clusters for cache vs. queuing\n\n- **Kafka**:\n  - 10 partitions per topic (scale to 50 as needed)\n  - 3 replicas per partition\n  - Consumer groups for parallel processing\n\n### Capacity Planning (for 10M active users)\n- **Notification volume**: 100M notifications/day (~1,200/sec avg, 5,000/sec peak)\n- **Gateway service**: 20 instances (250 req/sec each)\n- **Processing engine**: 50 workers (100 notif/sec each)\n- **Channel services**: 10 instances each\n- **Kafka**: 15-node cluster (3 brokers per AZ)\n- **Redis**: 6-node cluster (100GB total memory)\n- **PostgreSQL**: 1 master (32 cores, 128GB RAM) + 3 read replicas\n\n### Geographic Distribution\n- Multi-region deployment (US-East, US-West, EU-West, Asia-Pacific)\n- User data sharded by region\n- Cross-region replication for user preferences (eventual consistency)\n- Regional Kafka clusters with cross-region mirroring for disaster recovery\n\n## Monitoring & Observability\n\n### Key Metrics\n- **Throughput**: Notifications sent/delivered per second\n- **Latency**: p50, p95, p99 for end-to-end delivery time\n- **Success Rate**: Delivery success rate per channel\n- **Error Rate**: Failed deliveries per error type\n- **Queue Depth**: Kafka lag per consumer group\n- **Retry Rate**: Percentage of notifications requiring retry\n\n### Alerts\n- **Critical**: \n  - Service availability < 99.9%\n  - Error rate > 5%\n  - Queue lag > 10,000 messages\n- **Warning**:\n  - Latency p99 > 5 seconds\n  - Retry rate > 10%\n  - Provider rate limit approaching\n\n### Logging\n- Structured JSON logs with correlation IDs\n- Log levels: DEBUG, INFO, WARN, ERROR\n- Sensitive data (push tokens) redacted in logs\n\n## Security Considerations\n\n1. **Authentication & Authorization**:\n   - API key-based auth for service-to-service\n   - OAuth 2.0 for user-facing APIs\n   - JWT tokens for device registration\n\n2. **Data Encryption**:\n   - TLS 1.3 for all network communication\n   - At-rest encryption for databases (AES-256)\n   - Push token encryption in database\n\n3. **Rate Limiting**:\n   - Per-API key limits\n   - Per-user limits to prevent abuse\n   - DDoS protection at load balancer level\n\n4. **PII Handling**:\n   - GDPR compliance: user data deletion within 30 days\n   - Data retention policies enforced\n   - Audit logs for preference changes\n\n## Rationale\n\n1. **Event-driven architecture with Kafka**: Provides durability, replay capability, and decoupling between services. Critical for guaranteed delivery and system resilience.\n\n2. **Separate channel services**: Allows independent scaling and failure isolation. FCM and APNs have different characteristics and rate limits.\n\n3. **Batching service**: Addresses notification fatigue directly - users prefer consolidated notifications over constant interruptions. Smart batching based on priority prevents critical alerts from being delayed.\n\n4. **Multi-layer caching**: User preferences are read-heavy, caching dramatically reduces database load at scale.\n\n5. **PostgreSQL + Redis + Kafka**: Each serves a specific purpose - PostgreSQL for transactional data, Redis for high-speed access, Kafka for event streaming and guaranteed message delivery.\n\n6. **Exponential backoff retry**: Balances delivery guarantees with system load. Fallback channels ensure critical notifications get through.\n\n7. **Regional deployment**: Reduces latency for global users and provides disaster recovery capability.\n\n## What I Changed\nFirst iteration - no previous version to compare against.\n\n## What I Kept\nFirst iteration - establishing baseline design.\n\n## Open Questions\n\n1. **Batching complexity**: How do we handle cases where a user has multiple device types (phone + web) with different batching preferences?\n\n2. **Cross-notification intelligence**: Should we implement ML-based notification relevance scoring to further reduce fatigue?\n\n3. **Real-time preferences**: If a user changes preferences while notifications are in the batching queue, do we retroactively apply the new preferences or honor the settings at notification creation time?\n\n4. **Cost optimization**: For lower-priority notifications, should we implement a \"time-based auction\" where notifications compete for delivery slots to reduce provider costs?\n\n## Convergence Status\n**ITERATING** - This is the first iteration and requires peer review to identify weaknesses, edge cases, and areas for improvement.\n\nPROMPT_FOR_CRITIC:\nFocus on the delivery guarantees and retry logic - are there edge cases where notifications could be lost or duplicated? Also examine the batching strategy for potential user experience issues."
      },
      "agent_b": {
        "content": "## Design\n\n# Real-Time Notification System Architecture\n\n## System Overview\n\nA distributed, event-driven notification system supporting push notifications across mobile (iOS/Android) and web platforms with **exactly-once delivery semantics**, intelligent batching, user preference management, and guaranteed delivery at scale.\n\n## Architecture Components\n\n### 1. Core Services\n\n#### Notification Gateway Service\n- **Purpose**: Single entry point for all notification requests\n- **Responsibilities**:\n  - Request validation and authentication\n  - Rate limiting (per user, per application) using token bucket algorithm\n  - Initial routing to appropriate handlers\n  - **Idempotency enforcement**: Store idempotency_key with 24-hour TTL in Redis before accepting request\n  - **Synchronous validation**: Return 400 for invalid requests, 409 for duplicate idempotency_key, 202 for accepted\n- **Technology**: Go with gRPC for internal services, REST for external clients\n- **Scaling**: Stateless, horizontally scalable behind load balancer\n- **New**: Immediate idempotency check prevents duplicate entry into system\n\n#### Notification Processing Engine\n- **Purpose**: Core business logic with transactional guarantees\n- **Responsibilities**:\n  - Apply user preferences and filtering rules\n  - Determine optimal delivery channels with fallback ordering\n  - Handle notification priority classification (critical, high, medium, low)\n  - Route to batching service OR direct delivery queue based on priority + user preferences\n  - Enrich notifications with user context\n  - **Write notification state to database BEFORE publishing to Kafka** (transaction log pattern)\n  - Generate deterministic notification_id from (user_id + idempotency_key + timestamp)\n- **Technology**: Go with worker pool pattern\n- **Scaling**: Horizontal scaling with message queue consumers (consumer group per service instance)\n- **New**: Database write happens BEFORE Kafka publish to create audit trail\n\n#### Channel Delivery Services (one per channel type)\n- **Mobile Push Service**: Integrates with FCM and APNs with **response validation**\n- **Web Push Service**: Integrates with Web Push Protocol\n- **Email Service**: Backup/alternative channel\n- **SMS Service**: Critical notifications fallback\n- **Key Enhancement**: Each service validates provider response and publishes to `notifications.delivery-events` topic with status\n- **Deduplication**: Before sending, check `delivery_status` table for existing successful delivery within TTL window\n- **Technology**: Specialized microservices per channel with provider circuit breakers\n- **Scaling**: Independent scaling based on channel volume\n- **New**: Pre-send deduplication check + validated delivery confirmation\n\n#### Batching & Scheduling Service\n- **Purpose**: Aggregate notifications with consistency guarantees\n- **Responsibilities**:\n  - Group related notifications by user and type in Redis sorted sets (score = scheduled_time)\n  - Apply time-window batching (configurable per notification type)\n  - **Smart scheduling**: Check user preference + timezone + historical activity patterns\n  - **Batch commit**: When window closes, atomically move batch to delivery queue with batch_id\n  - **Batch escape hatch**: Allow users to \"unbatch\" via real-time preference update (flushes pending batch immediately)\n  - Digest generation for batched notifications\n- **Batching Strategy**:\n  - **Immediate**: Critical alerts (system failures, security alerts) - bypass batching entirely\n  - **5-minute window**: High priority messages - user can configure 0-15 min range\n  - **1-hour window**: Medium priority (social interactions) - user can configure 15min-2hr range\n  - **Daily digest**: Low priority (newsletters, summaries) - configurable time slot per user\n- **Edge Case Handling**:\n  - If preference changes mid-batch: Apply new preference, flush batch if changed from batched\u2192immediate\n  - Cross-device batching: Create separate batches per device but with synchronized delivery timestamps\n  - Batch size limits: Maximum 50 notifications per batch, auto-flush on limit\n- **Technology**: Redis sorted sets for temporary storage, distributed lock (Redlock) for batch commit\n- **Scaling**: Sharded by user_id hash\n- **New**: Preference change handling, cross-device coordination, size limits\n\n#### User Preference Service\n- **Purpose**: Manage user notification settings with real-time propagation\n- **Responsibilities**:\n  - CRUD operations for user preferences\n  - **Write-through cache**: Update PostgreSQL + Redis atomically\n  - **Cache invalidation broadcast**: Publish preference change events to `user.preference-updates` topic\n  - Quiet hours management with timezone awareness\n  - Channel priority management (ordered list of fallback channels)\n  - DND (Do Not Disturb) support with critical-only override\n  - **Preference versioning**: Track version number to detect conflicts during concurrent updates\n- **Technology**: PostgreSQL for persistence with row-level locking, Redis for caching\n- **Scaling**: Read replicas for queries, write-through cache for consistency\n- **New**: Versioning, cache invalidation broadcast, atomic write-through\n\n#### Delivery Tracking & Retry Service with Exactly-Once Semantics\n- **Purpose**: Ensure guaranteed delivery without duplicates\n- **Responsibilities**:\n  - Track delivery status with state machine: PENDING \u2192 IN_FLIGHT \u2192 DELIVERED/FAILED\n  - **State transitions are transactional**: Update database + publish event atomically\n  - Exponential backoff retry logic with jitter (max 5 retries)\n  - **Idempotent retry**: Before retry, check if notification was delivered by another worker (race condition handling)\n  - Dead letter queue management for permanent failures\n  - Delivery analytics and monitoring\n  - **Delivery deduplication window**: 24-hour bloom filter per (user_id, notification_id) to catch any duplicates\n  - **Provider idempotency tokens**: Pass unique token to FCM/APNs for provider-level deduplication\n- **Retry Strategy**:\n  - Attempt 1: Immediate\n  - Attempt 2: 30 seconds + random(0-5s) jitter\n  - Attempt 3: 5 minutes + random(0-30s) jitter\n  - Attempt 4: 30 minutes + random(0-2min) jitter\n  - Attempt 5: 2 hours + random(0-10min) jitter\n  - **Between retries**: Re-fetch user preferences (may have changed)\n  - After failure: Move to DLQ with full context, alert ops team, **check if fallback channel configured**\n  - **New**: If fallback channel exists and original channel exhausted retries, create new notification for fallback (with reference to original)\n- **Race Condition Handling**:\n  - Use database SELECT FOR UPDATE SKIP LOCKED for retry job claims\n  - Worker acquires lock on specific delivery record before processing\n  - If delivery completed by another worker, current worker releases and moves to next job\n- **Technology**: Kafka for event streaming, PostgreSQL for authoritative state with optimistic locking\n- **Scaling**: Partitioned by notification_id, consumer group coordinates work distribution\n- **New**: Bloom filter, provider idempotency tokens, database locking for claim semantics\n\n### 2. Data Stores\n\n#### Primary Database (PostgreSQL)\n- User preferences and settings with version column\n- Notification templates\n- **Notification audit log**: Immutable record of all notifications created (never deleted)\n- **Delivery status table**: State machine tracking with composite index on (notification_id, device_id, status)\n- Device registrations with token refresh timestamps\n- **Partitioning strategy**:\n  - Delivery status: Partitioned by date (monthly), archived after 90 days\n  - Audit log: Partitioned by date (monthly), retained indefinitely for compliance\n- **Indexes**:\n  - (user_id, created_at) for user notification history\n  - (notification_id, status) for delivery queries\n  - (idempotency_key) unique index with 24-hour partial index (WHERE created_at > NOW() - INTERVAL '24 hours')\n\n#### Cache Layer (Redis Cluster)\n- User preference cache with version tags (TTL: 5 minutes, invalidate on update)\n- Device token cache (TTL: 1 hour)\n- Rate limiting counters using sliding window algorithm (TTL: 1 minute)\n- Batching windows using sorted sets (TTL: 48 hours)\n- **Idempotency key store** (TTL: 24 hours) - tracks accepted requests\n- **Delivery deduplication bloom filter** (TTL: 24 hours, 0.01% false positive rate)\n- **Distributed locks** (Redlock algorithm) for batch commits\n\n#### Message Queue (Apache Kafka)\n- Topics with **compaction and retention policies**:\n  - `notifications.incoming`: Raw notification requests (7-day retention)\n  - `notifications.processed`: Enriched notifications ready for delivery (7-day retention)\n  - `notifications.batched`: Batch-ready notifications (7-day retention)\n  - `notifications.immediate`: High-priority bypass batching (7-day retention)\n  - `notifications.retry`: Failed deliveries needing retry (30-day retention for forensics)\n  - `notifications.dlq`: Permanent failures (90-day retention, compacted by notification_id)\n  - `notifications.delivery-events`: Delivery confirmations from providers (7-day retention)\n  - `user.preference-updates`: Cache invalidation events (compacted by user_id, 1-day retention)\n- **Partitioning**: By user_id hash for ordered processing per user\n- **Consumer groups**: Each service instance joins consumer group with unique consumer_id\n- **Exactly-once semantics**: Enable Kafka transactions + idempotent producer + read_committed isolation\n- **New**: Delivery-events topic for confirmations, preference-updates for invalidation, transactions enabled\n\n#### Object Storage (S3/GCS)\n- Notification archives (historical data, Parquet format for analytics)\n- Large payload storage (images, rich media) with CDN fronting\n- Analytics data exports (daily aggregations)\n- DLQ snapshots for forensics\n\n### 3. External Integrations\n\n#### Push Notification Providers\n- **FCM (Firebase Cloud Messaging)**: Android and iOS via Firebase\n  - Use FCM batch API (500 notifications per request) for efficiency\n  - **Validate response**: Extract success/failure per device token\n  - **Handle token refresh**: Update device registry on invalid token errors\n- **APNs (Apple Push Notification service)**: iOS native\n  - Use HTTP/2 connection pooling\n  - **Validate response**: Check status code per notification\n  - **Handle token expiration**: Update device registry on 410 Gone responses\n- **Web Push**: Browser-based notifications via service workers\n  - Handle 410 Gone for expired subscriptions\n- **Provider-level deduplication**: Pass `apns-collapse-id` (APNs) and `collapse_key` (FCM) for provider deduplication\n\n#### Monitoring & Observability\n- **Prometheus + Grafana**: Metrics and dashboards\n  - Custom metrics: duplicate_blocked_count, delivery_latency_by_channel, batch_size_distribution\n- **ELK Stack**: Centralized logging with correlation_id propagation\n- **Jaeger**: Distributed tracing with spans for each service hop\n- **PagerDuty**: Alerting for critical failures\n- **DataDog/New Relic**: End-to-end delivery tracking dashboards\n\n## Data Models\n\n### Notification Model (Enhanced)\n```json\n{\n  \"notification_id\": \"uuid (deterministic from user_id + idempotency_key + timestamp)\",\n  \"user_id\": \"string\",\n  \"type\": \"enum[ALERT, MESSAGE, REMINDER, SOCIAL, PROMOTIONAL]\",\n  \"priority\": \"enum[CRITICAL, HIGH, MEDIUM, LOW]\",\n  \"title\": \"string\",\n  \"body\": \"string\",\n  \"data\": {\n    \"action_url\": \"string\",\n    \"image_url\": \"string\",\n    \"custom_fields\": {}\n  },\n  \"channels\": [\"PUSH\", \"EMAIL\", \"SMS\"],\n  \"fallback_channels\": [\"EMAIL\", \"SMS\"],\n  \"batch_eligible\": \"boolean\",\n  \"batch_key\": \"string (for grouping related notifications)\",\n  \"batch_id\": \"uuid (assigned when batched, null for immediate)\",\n  \"ttl\": \"integer (seconds, default 86400)\",\n  \"created_at\": \"timestamp\",\n  \"scheduled_for\": \"timestamp (null for immediate)\",\n  \"expires_at\": \"timestamp (computed from created_at + ttl)\",\n  \"idempotency_key\": \"string (required, unique per 24 hours)\",\n  \"correlation_id\": \"uuid (for tracing)\",\n  \"preference_version\": \"integer (version at notification creation time)\"\n}\n```\n\n### User Preferences Model (Enhanced)\n```json\n{\n  \"user_id\": \"string\",\n  \"version\": \"integer (incremented on every update, for optimistic locking)\",\n  \"preferences\": {\n    \"global_enabled\": \"boolean\",\n    \"channels\": {\n      \"push\": {\n        \"enabled\": \"boolean\",\n        \"priority\": 1,\n        \"notification_types\": {\n          \"ALERT\": \"boolean\",\n          \"MESSAGE\": \"boolean\",\n          \"REMINDER\": \"boolean\",\n          \"SOCIAL\": \"boolean\",\n          \"PROMOTIONAL\": \"boolean\"\n        }\n      },\n      \"email\": { \n        \"enabled\": \"boolean\",\n        \"priority\": 2,\n        \"notification_types\": { /* same */ }\n      },\n      \"sms\": { \n        \"enabled\": \"boolean\",\n        \"priority\": 3,\n        \"notification_types\": { /* same */ }\n      }\n    },\n    \"channel_fallback_order\": [\"push\", \"email\", \"sms\"],\n    \"quiet_hours\": {\n      \"enabled\": \"boolean\",\n      \"start_time\": \"string (HH:mm)\",\n      \"end_time\": \"string (HH:mm)\",\n      \"timezone\": \"string (IANA timezone)\",\n      \"allow_critical\": \"boolean\",\n      \"buffer_notifications\": \"boolean (queue for delivery after quiet hours)\"\n    },\n    \"batching\": {\n      \"enabled\": \"boolean\",\n      \"per_type_settings\": {\n        \"ALERT\": \"REALTIME\",\n        \"MESSAGE\": \"5MIN\",\n        \"SOCIAL\": \"1HOUR\",\n        \"PROMOTIONAL\": \"DAILY\"\n      },\n      \"daily_digest_time\": \"string (HH:mm in user timezone)\",\n      \"max_batch_size\": \"integer (default 50)\"\n    }\n  },\n  \"updated_at\": \"timestamp\",\n  \"updated_by\": \"string (user/system)\"\n}\n```\n\n### Device Registration Model (Enhanced)\n```json\n{\n  \"device_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"platform\": \"enum[IOS, ANDROID, WEB]\",\n  \"push_token\": \"string (encrypted, FCM/APNs token)\",\n  \"token_refreshed_at\": \"timestamp\",\n  \"endpoint\": \"string (for Web Push)\",\n  \"user_agent\": \"string\",\n  \"is_active\": \"boolean\",\n  \"last_successful_delivery_at\": \"timestamp\",\n  \"consecutive_failures\": \"integer (reset on success, deactivate after 10)\",\n  \"registered_at\": \"timestamp\",\n  \"last_active_at\": \"timestamp\",\n  \"device_preferences_override\": {\n    \"batching_enabled\": \"boolean (null = inherit from user preferences)\"\n  }\n}\n```\n\n### Delivery Status Model (Enhanced - State Machine)\n```json\n{\n  \"delivery_id\": \"uuid\",\n  \"notification_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"device_id\": \"uuid\",\n  \"channel\": \"enum[PUSH, EMAIL, SMS]\",\n  \"status\": \"enum[PENDING, IN_FLIGHT, DELIVERED, FAILED, EXPIRED, CANCELLED]\",\n  \"attempt_count\": \"integer\",\n  \"provider_message_id\": \"string (FCM/APNs message ID for tracking)\",\n  \"provider_idempotency_token\": \"uuid (sent to provider for their deduplication)\",\n  \"provider_response\": \"json\",\n  \"error_code\": \"string (standardized error codes)\",\n  \"error_message\": \"string (if failed)\",\n  \"is_fallback\": \"boolean (true if this is fallback channel delivery)\",\n  \"original_delivery_id\": \"uuid (if fallback, reference to original)\",\n  \"queued_at\": \"timestamp\",\n  \"in_flight_at\": \"timestamp\",\n  \"sent_at\": \"timestamp\",\n  \"delivered_at\": \"timestamp\",\n  \"failed_at\": \"timestamp\",\n  \"expires_at\": \"timestamp\",\n  \"next_retry_at\": \"timestamp (null if no retry scheduled)\",\n  \"last_updated_at\": \"timestamp\",\n  \"lock_acquired_by\": \"string (worker_id for retry job claiming)\"\n}\n```\n\n### Batch Model (New)\n```json\n{\n  \"batch_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"device_id\": \"uuid\",\n  \"notification_type\": \"enum[MESSAGE, SOCIAL, PROMOTIONAL]\",\n  \"notification_ids\": [\"uuid[]\"],\n  \"status\": \"enum[ACCUMULATING, COMMITTED, DELIVERED, FAILED]\",\n  \"batch_window_start\": \"timestamp\",\n  \"batch_window_end\": \"timestamp\",\n  \"scheduled_delivery_time\": \"timestamp\",\n  \"notification_count\": \"integer\",\n  \"created_at\": \"timestamp\",\n  \"committed_at\": \"timestamp\",\n  \"delivered_at\": \"timestamp\"\n}\n```\n\n## API Interfaces\n\n### Internal gRPC Service Interface\n\n```protobuf\nservice NotificationService {\n  // Send single notification with idempotency\n  rpc SendNotification(SendNotificationRequest) returns (SendNotificationResponse);\n  \n  // Send batch of notifications (each with idempotency_key)\n  rpc SendBulkNotifications(SendBulkNotificationsRequest) returns (SendBulkNotificationsResponse);\n  \n  // Get delivery status with full history\n  rpc GetDeliveryStatus(GetDeliveryStatusRequest) returns (GetDeliveryStatusResponse);\n  \n  // Cancel scheduled notification (only if not yet delivered)\n  rpc CancelNotification(CancelNotificationRequest) returns (CancelNotificationResponse);\n  \n  // Flush pending batch for user (manual unbatch)\n  rpc FlushUserBatch(FlushUserBatchRequest) returns (FlushUserBatchResponse);\n}\n\nservice UserPreferenceService {\n  rpc GetUserPreferences(GetUserPreferencesRequest) returns (GetUserPreferencesResponse);\n  \n  // Returns new version number, fails with ABORTED if version mismatch\n  rpc UpdateUserPreferences(UpdateUserPreferencesRequest) returns (UpdateUserPreferencesResponse);\n  \n  rpc RegisterDevice(RegisterDeviceRequest) returns (RegisterDeviceResponse);\n  rpc UnregisterDevice(UnregisterDeviceRequest) returns (UnregisterDeviceResponse);\n  \n  // Mark device as inactive after repeated failures\n  rpc DeactivateDevice(DeactivateDeviceRequest) returns (DeactivateDeviceResponse);\n}\n\nservice DeliveryTrackingService {\n  // Query delivery history for notification\n  rpc GetDeliveryHistory(GetDeliveryHistoryRequest) returns (GetDeliveryHistoryResponse);\n  \n  // Get aggregated delivery metrics\n  rpc GetDeliveryMetrics(GetDeliveryMetricsRequest) returns (GetDeliveryMetricsResponse);\n}\n```\n\n### External REST API\n\n```\nPOST /api/v1/notifications\n  - Send notification with idempotency\n  - Request: Notification model JSON (idempotency_key required)\n  - Response: {notification_id, status: \"accepted\", correlation_id}\n  - Rate limit: 1000 req/min per API key\n  - Idempotent: Returns same notification_id for duplicate idempotency_key within 24h\n\nPOST /api/v1/notifications/bulk\n  - Send multiple notifications (each with idempotency_key)\n  - Request: Array of Notification models\n  - Response: {job_id, accepted: [{notification_id, idempotency_key}], rejected: [{idempotency_key, reason}]}\n  - Rate limit: 100 req/min per API key\n  - Partial success supported\n\nGET /api/v1/notifications/{notification_id}/status\n  - Get delivery status for all devices/channels\n  - Response: {notification_id, deliveries: [Delivery status models], overall_status}\n\nGET /api/v1/notifications/{notification_id}/history\n  - Get full delivery history including retries\n  - Response: {notification_id, timeline: [{timestamp, status, attempt, details}]}\n\nDELETE /api/v1/notifications/{notification_id}\n  - Cancel scheduled notification (idempotent, fails if already delivered)\n  - Response: {cancelled: boolean, reason: string}\n\nPOST /api/v1/users/{user_id}/batches/flush\n  - Flush pending batches immediately\n  - Response: {flushed_count, notification_ids}\n\nGET /api/v1/users/{user_id}/preferences\n  - Get user preferences with version\n  - Response: User preferences model\n\nPUT /api/v1/users/{user_id}/preferences\n  - Update user preferences with optimistic locking\n  - Request: User preferences model with version\n  - Response: Updated preferences with new version\n  - Returns 409 if version mismatch\n\nPOST /api/v1/devices\n  - Register device\n  - Request: Device registration model\n  - Response: {device_id, push_token_encrypted}\n\nPUT /api/v1/devices/{device_id}/token\n  - Refresh device push token\n  - Request: {push_token}\n  - Response: {updated: boolean}\n\nDELETE /api/v1/devices/{device_id}\n  - Unregister device (idempotent)\n  - Response: {success: boolean}\n```\n\n## Error Handling & Resilience\n\n### Error Categories\n\n1. **Transient Errors** (Retry with exponential backoff + jitter)\n   - Network timeouts\n   - Provider rate limits (FCM/APNs throttling) - respect Retry-After header\n   - Temporary service unavailability (503)\n   - Database connection issues\n   - Kafka broker unavailable\n\n2. **Permanent Errors** (No retry, move to DLQ, attempt fallback)\n   - Invalid device tokens (FCM: InvalidRegistration, APNs: BadDeviceToken) \u2192 deactivate device\n   - Malformed notification payloads (400) \u2192 log and alert\n   - User not found \u2192 log and drop\n   - Authentication failures (401/403) \u2192 alert operations\n   - Notification expired (TTL exceeded) \u2192 mark as EXPIRED\n   - Device unregistered by user (FCM: NotRegistered, APNs: 410 Gone) \u2192 deactivate device\n\n3. **Partial Failures** (Retry failed subset, track individually)\n   - Bulk send with some failures \u2192 track each notification separately\n   - Multi-channel delivery with channel-specific failures \u2192 retry only failed channels\n   - Batch delivery where some devices fail \u2192 mark successful deliveries, retry failures\n\n### Resilience Patterns\n\n1. **Circuit Breaker** (per external provider)\n   - FCM circuit breaker: Open after 10 consecutive failures, half-open after 30 seconds, close after 5 successes\n   - APNs circuit breaker: Same thresholds\n   - When open: Immediately fail requests, move to retry queue, alert operations\n   - Separate circuit breakers per provider to prevent cross-contamination\n\n2. **Bulkhead Isolation**\n   - Separate thread pools per channel (Push: 100 threads, Email: 50, SMS: 20)\n   - Separate Kafka consumer groups per service instance\n   - Independent rate limiters per channel\n\n3. **Timeout Management** (with context propagation)\n   - Gateway timeout: 5 seconds (synchronous validation only)\n   - Processing timeout: 10 seconds (preference lookup + routing decision)\n   - Provider timeout: 30 seconds (FCM/APNs API call)\n   - Batch commit timeout: 2 seconds (Redis transaction)\n   - Database query timeout: 5 seconds\n   - Overall correlation_id timeout: 5 minutes (end-to-end delivery attempt)\n\n4. **Graceful Degradation**\n   - If batching service fails: Deliver immediately via `notifications.immediate` topic\n   - If primary channel fails: Attempt fallback channel from user's fallback_order\n   - If preference service fails: Use cached preferences (stale-while-revalidate), fallback to system defaults if cache miss\n   - If Redis fails: Disable batching, operate in pass-through mode\n   - If Kafka fails: Write to database, rely on change data capture (CDC) for eventual message publishing\n\n5. **Idempotency at Every Layer**\n   - Gateway: idempotency_key check in Redis before accepting\n   - Processing: deterministic notification_id generation\n   - Delivery: provider_idempotency_token passed to FCM/APNs\n   - Retry: Check delivery_status before sending (another worker may have succeeded)\n   - Database: Use INSERT ... ON CONFLICT DO NOTHING for delivery records\n\n6. **Dead Letter Queue with Forensics**\n   - Capture full notification payload + all retry attempts + error context\n   - Automated analysis: Group by error_code, identify systemic issues\n   - Manual replay capability: Operations can trigger re-delivery after fixing root cause\n   - Alert thresholds: Page on-call if DLQ depth > 1000 messages\n\n7. **Distributed Lock Management**\n   - Use Redlock algorithm for batch commits (requires majority of Redis nodes)\n   - Lock timeout: 10 seconds max\n   - Lock refresh: Not needed for short-lived batch commits\n   - Failure handling: If lock acquisition fails, retry batch commit after 1 second\n\n## Exactly-Once Delivery Guarantees\n\n### Prevention of Duplicates\n\n1. **Entry Point Deduplication** (Gateway)\n   - Client provides idempotency_key (UUID or content-based hash)\n   - Gateway checks Redis: `idempotency:{key}` \u2192 `{notification_id, status, created_at}`\n   - If exists: Return existing notification_id with 200 (not 409)\n   - If new: Store in Redis with 24-hour TTL, proceed with processing\n   - Edge case: If Redis write succeeds but processing fails, retry with same key is safe\n\n2. **Processing Deduplication**\n   - Deterministic notification_id = SHA256(user_id || idempotency_key || timestamp_bucket)\n   - Database has unique constraint on (notification_id, device_id) in delivery_status table\n   - INSERT with ON CONFLICT DO NOTHING prevents duplicate state tracking\n\n3. **Delivery Deduplication**\n   - Before calling FCM/APNs, check bloom filter: `delivered:{user_id}:{notification_id}`\n   - If exists: Skip send, mark as DELIVERED (another worker already succeeded)\n   - If not exists: Send with provider_idempotency_token, add to bloom filter on success\n   - Provider tokens: APNs uses apns-id header, FCM uses messageId\n   - 24-hour TTL on bloom filter entries\n\n4. **Retry Deduplication**\n   - Worker claims retry job using SELECT FOR UPDATE SKIP LOCKED\n   - Before processing: Re-check delivery_status (status = DELIVERED?)\n   - If delivered: Skip retry, release lock\n   - If still FAILED: Proceed with retry attempt\n\n5. **Cross-Device Deduplication**\n   - User may have multiple devices (phone, tablet, web)\n   - Each device gets separate delivery record (different device_id)\n   - Bloom filter is per (notification_id, device_id) tuple\n   - Trade-off: User may receive on multiple devices (by design), but never duplicate on same device\n\n### Handling of Message Loss\n\n1. **Kafka Durability**\n   - min.insync.replicas = 2 (message written to 2 brokers before ack)\n   - acks = all (wait for all in-sync replicas)\n   - Kafka transactions enabled for exactly-once semantics\n   - Consumer commits offsets only after successful database write\n\n2. **Database as Source of Truth**\n   - Notification audit log is immutable and never deleted\n   - Delivery status table tracks all attempts\n   - If Kafka consumer crashes mid-processing: Offset not committed, message reprocessed\n   - Idempotency prevents duplicate delivery despite reprocessing\n\n3. **Dead Letter Queue as Safety Net**\n   - After 5 failed attempts, move to DLQ (Kafka topic with 90-day retention)\n   - DLQ messages are never deleted automatically\n   - Daily job scans DLQ for retriable errors (e.g., provider outage resolved)\n   - Operations dashboard shows DLQ trends\n\n4. **Change Data Capture (CDC) Backup**\n   - PostgreSQL logical replication streams changes to Kafka (Debezium)\n   - If direct Kafka publishing fails, CDC ensures eventual message availability\n   - Secondary consumer processes CDC stream and publishes to notification topics\n   - Idempotency prevents duplicates from dual-path publishing\n\n## Batching Strategy - Edge Cases\n\n### Preference Changes Mid-Batch\n\n**Scenario**: User has 5 notifications batching (1-hour window), after 30 minutes changes preference from batched to immediate.\n\n**Solution**:\n1. Preference update publishes to `user.preference-updates` topic\n2. Batching service subscribes to preference updates\n3. On preference change event:\n   - Lookup pending batches for user_id in Redis\n   - If batching disabled or type-specific frequency changed:\n     - Atomically move batch from `batching:{user_id}:{type}` to `notifications.immediate` topic\n     - Delete batch from Redis\n     - Publish each notification individually with `batch_flushed: true` flag\n4. If change is within same batching tier (e.g., 1hr \u2192 2hr): Keep accumulating, adjust scheduled_delivery_time\n\n**Trade-off**: Slight delay (up to 5 seconds) between preference update and batch flush due to eventual consistency.\n\n### Cross-Device Batching\n\n**Scenario**: User has phone (batching enabled) and web browser (batching disabled for real-time updates).\n\n**Solution**:\n1. Device registration includes `device_preferences_override.batching_enabled`\n2. When routing notification:\n   - Lookup all user devices\n   - For each device, compute effective batching preference: `device_override ?? user_preference`\n   - Create separate batch contexts per (user_id, device_id, batching_preference) tuple\n3. Same notification may be batched for phone, delivered immediately to web\n4. Batch digest indicates \"also delivered to other devices\" to prevent confusion\n\n**Benefit**: Per-device control without complex cross-device coordination.\n\n### Batch Size Limits\n\n**Scenario**: User receives 100 social notifications in 1-hour window (e.g., viral post).\n\n**Solution**:\n1. Batch accumulation checks `notification_count` before adding\n2. If `notification_count >= max_batch_size` (default 50):\n   - Commit current batch immediately\n   - Start new batch for remaining notifications\n3. Digest message: \"You have 50 new likes and comments. View all \u2192\"\n4. Subsequent batch (if window still open): \"You have 25 more likes and comments since your last digest.\"\n\n**User Experience**: Prevents overwhelming digests, provides progressive updates.\n\n### Expired Notifications in Batch\n\n**Scenario**: Notification with 30-minute TTL sits in 1-hour batch, expires before delivery.\n\n**Solution**:\n1. Batch commit job checks `expires_at` for each notification\n2. If `expires_at < scheduled_delivery_time`:\n   - Remove from batch\n   - Mark delivery_status as EXPIRED\n   - Do not include in digest\n3. If entire batch expired: Skip delivery\n4. If partial expiration: Deliver remaining notifications in digest\n\n**Metric**: Track `expired_in_batch_count` to identify TTL vs. batching window mismatches.\n\n### Batching During Quiet Hours\n\n**Scenario**: User has 10pm-8am quiet hours + 1-hour batching for social notifications. Notification arrives at 9:30pm.\n\n**Solution**:\n1. Quiet hours check happens BEFORE batching decision\n2. If `current_time` in quiet_hours AND `allow_critical = false`:\n   - If `buffer_notifications = true`: Add to batch with scheduled_delivery_time = quiet_hours.end_time\n   - If `buffer_notifications = false`: Drop notification (mark as SUPPRESSED in audit log)\n3. Batch scheduled for 8am delivery with digest: \"Here's what happened overnight\"\n\n**User Control**: `buffer_notifications` preference allows users to choose suppression vs. delayed delivery.\n\n### Concurrent Batch Operations\n\n**Scenario**: Two notifications arrive simultaneously for same user, both trying to create/update batch.\n\n**Solution**:\n1. Redis sorted set operations are atomic (ZADD is thread-safe)\n2. Batch commit uses distributed lock (Redlock):\n   ```\n   LOCK batch_commit:{user_id}:{type} (timeout: 10s)\n   IF batch_window_end <= current_time:\n     notification_ids = ZRANGEBYSCORE batching:{user_id}:{type} -inf +inf\n     DELETE batching:{user_id}:{type}\n     CREATE batch record in PostgreSQL\n     PUBLISH to notifications.immediate topic\n   UNLOCK\n   ```\n3. Lock ensures only one worker commits batch, others skip if batch already committed\n\n## Scaling Strategy\n\n### Horizontal Scaling\n- All services are stateless (session state in Redis, durable state in PostgreSQL/Kafka)\n- Auto-scaling based on:\n  - Kafka consumer lag (target: < 1000 messages per partition)\n  - CPU utilization (target: 70%)\n  - Request rate (gateway: scale at 80% of rate limit capacity)\n- Kubernetes HPA (Horizontal Pod Autoscaler) with custom metrics from Prometheus\n\n### Database Scaling\n- **PostgreSQL**:\n  - Primary-replica topology: 1 writer, 3 read replicas (round-robin for read queries)\n  - Partitioning:\n    - `delivery_status`: Monthly partitions with CHECK constraint on queued_at\n    - `notification_audit_log`: Monthly partitions\n    - Automated partition creation via cron job (create next 3 months)\n    - Automated archival: Detach partitions older than 90 days, move to object storage\n  - Connection pooling: PgBouncer in transaction mode (1000 max connections)\n  - Write scaling: For massive write spikes, use write-through cache (write to Redis, async flush to PostgreSQL)\n\n- **Redis**:\n  - Cluster mode: 6 nodes (3 masters, 3 replicas)\n  - Sharding: Hash slot partitioning by user_id (16384 slots)\n  - Separate clusters for:\n    - Cache data (user preferences, device tokens)\n    - Batching data (sorted sets for accumulation)\n    - Distributed locks (Redlock requires 3+ independent instances)\n  - Memory: 100GB per cluster, eviction policy: allkeys-lru for cache cluster, noeviction for batching/locks\n\n- **Kafka**:\n  - 15-node cluster (5 brokers per availability zone)\n  - Partitions per topic:\n    - `notifications.incoming`: 30 partitions (for write throughput)\n    - `notifications.processed`: 30 partitions (parallel processing)\n    - `notifications.retry`: 10 partitions (lower volume)\n  - Replication factor: 3 (cross-AZ)\n  - Consumer scaling: Consumer group with N consumers = min(N, partition_count)\n  - Partition rebalancing: Cooperative sticky assignor for minimal disruption\n\n### Capacity Planning (for 10M active users)\n- **Notification volume**: 100M notifications/day (~1,200/sec avg, 5,000/sec peak during events)\n- **Batching impact**: Assuming 40% of notifications are batched, reduces delivery operations to ~60M/day\n- **Gateway service**: 20 instances @ 4 cores each (250 req/sec per instance) = 5,000 req/sec capacity\n- **Processing engine**: 50 workers @ 2 cores each (100 notif/sec each) = 5,000 notif/sec capacity\n- **Channel services**: \n  - Push: 15 instances (FCM/APNs have high throughput)\n  - Email: 10 instances (slower delivery)\n  - SMS: 5 instances (lowest volume)\n- **Kafka**: 15-node cluster handling ~10K msg/sec write throughput\n- **Redis**: 6-node cluster with 100GB total memory (~100M keys cached)\n- **PostgreSQL**: \n  - Primary: 32 cores, 128GB RAM, 2TB SSD (handles ~5K writes/sec)\n  - Replicas: 3x (16 cores, 64GB RAM each)\n- **Cost optimization**: Using batching reduces provider API costs by ~30-40%, compute costs by ~25%\n\n### Geographic Distribution\n- **Multi-region deployment** (US-East, US-West, EU-West, Asia-Pacific)\n- **User data sharding**: Users assigned to home region based on registration location\n- **Read-your-writes consistency**: User preference updates routed to home region, replicated to others\n- **Cross-region notification delivery**: Notification created in any region, routed to user's home region for preference lookup, then to nearest channel service for delivery\n- **Kafka mirroring**: MirrorMaker 2.0 for cross-region topic replication (for disaster recovery, not active-active)\n- **Failover strategy**: \n  - If home region unavailable: Route to backup region\n  - Eventual consistency: Preference updates may be stale (max 30 seconds)\n  - Batch coordination: Batches are region-local (cross-region batch merging not supported)\n\n## Monitoring & Observability\n\n### Key Metrics\n\n**Delivery Metrics**:\n- `notifications_sent_total{channel, status}`: Counter of delivery attempts\n- `notifications_delivered_total{channel}`: Counter of successful deliveries\n- `notifications_failed_total{channel, error_code}`: Counter of failures by category\n- `delivery_latency_seconds{channel, priority, p50/p95/p99}`: End-to-end latency from creation to delivery\n- `duplicate_blocked_count`: Notifications blocked by idempotency/bloom filter\n- `batch_size_distribution`: Histogram of notifications per batch\n- `batch_flush_reason{reason}`: Counter of why batches were flushed (window_closed, preference_change, size_limit)\n\n**System Metrics**:\n- `kafka_consumer_lag{topic, consumer_group}`: Messages behind in queue\n- `circuit_breaker_state{provider}`: Open/half-open/closed\n- `redis_memory_usage{cluster}`: Memory consumption\n- `postgres_replication_lag{replica}`: Seconds behind primary\n- `rate_limit_exceeded_total{api_key}`: Rate limit violations\n\n**Business Metrics**:\n- `notifications_created_total{type, priority}`: Total notifications by category\n- `user_preferences_updated_total`: Preference change frequency\n- `device_registrations_total{platform}`: New device registrations\n- `device_deactivations_total{reason}`: Devices marked inactive\n\n### Alerts\n\n**Critical** (Page on-call):\n- Service availability < 99.9% over 5-minute window\n- Error rate > 5% over 5-minute window\n- Kafka consumer lag > 10,000 messages for > 5 minutes\n- DLQ depth > 1000 messages\n- Circuit breaker open for > 2 minutes\n- PostgreSQL replication lag > 60 seconds\n- Duplicate notification sent (bloom filter failed)\n\n**Warning** (Slack notification):\n- Latency p99 > 5 seconds over 10-minute window\n- Retry rate > 10% over 10-minute window\n- Provider rate limit approaching (> 80% of quota)\n- Redis memory usage > 85%\n- Device deactivation rate spike (> 2x baseline)\n- Unusual batch size (> 100 notifications in single batch)\n\n### Logging\n- **Structured JSON logs** with fields: `timestamp`, `level`, `service`, `correlation_id`, `user_id`, `notification_id`, `message`, `context`\n- **Sensitive data redaction**: Push tokens, email addresses, phone numbers \u2192 hash in logs\n- **Log levels**: DEBUG (disabled in production), INFO (request/response), WARN (retries, degraded mode), ERROR (failures)\n- **Correlation ID propagation**: Generated at gateway, passed through all services, included in all logs and traces\n- **Log sampling**: DEBUG and INFO sampled at 1% in production, WARN and ERROR always logged\n\n### Distributed Tracing\n- **Jaeger spans**:\n  - `notification.gateway.accept`: Request validation and idempotency check\n  - `notification.processing.enrich`: Preference lookup and routing\n  - `notification.batching.accumulate`: Adding to batch\n  - `notification.delivery.send`: External provider API call\n  - `notification.delivery.confirm`: Provider response validation\n- **Span attributes**: `user_id`, `notification_id`, `channel`, `batch_id`, `attempt_count`, `error_code`\n- **Trace sampling**: 100% for failed deliveries, 10% for successful deliveries, 1% for batched notifications\n\n### Dashboards\n1. **Real-time Delivery Dashboard**:\n   - Throughput by channel (req/sec)\n   - Success/failure rates\n   - Latency heatmap (p50, p95, p99)\n   - Circuit breaker status\n2. **Batching Dashboard**:\n   - Active batches by type\n   - Average batch size\n   - Batch flush reasons\n   - Batching efficiency (notifications saved)\n3. **Operations Dashboard**:\n   - Kafka consumer lag\n   - DLQ depth and trends\n   - Retry queue depth\n   - Provider rate limit usage\n4. **Business Dashboard**:\n   - Notifications by type and priority\n   - User engagement (delivery \u2192 action conversion)\n   - Device platform distribution\n   - Preference update trends\n\n## Security Considerations\n\n1. **Authentication & Authorization**:\n   - **API key-based auth** for service-to-service (internal services)\n   - **OAuth 2.0 + JWT** for user-facing APIs (user preference management, device registration)\n   - **mTLS** for internal gRPC communication\n   - **Scope-based authorization**: API keys have scopes (SEND_NOTIFICATION, MANAGE_PREFERENCES), enforced at gateway\n   - **Rate limiting by identity**: Per API key, per user_id, per IP address (layered defense)\n\n2. **Data Encryption**:\n   - **In-transit**: TLS 1.3 for all HTTP/gRPC, no plaintext communication\n   - **At-rest**: \n     - PostgreSQL transparent data encryption (TDE) with AES-256\n     - Redis encrypted persistence (snapshot + AOF)\n     - Kafka encryption at rest for all topics\n     - S3/GCS server-side encryption (SSE-KMS)\n   - **Push token encryption**: Device push tokens encrypted with application-level keys (AES-256-GCM), separate key per environment\n\n3. **Rate Limiting** (Token Bucket Algorithm):\n   - **Per API key**: 1000 req/min (burst: 1500)\n   - **Per user_id**: 100 notifications/hour (prevents spam)\n   - **Per IP**: 5000 req/min (DDoS protection)\n   - **Global**: 100K req/min (cluster capacity)\n   - Rate limit state stored in Redis with sliding window counters\n   - 429 response includes Retry-After header\n\n4. **PII Handling** (GDPR/CCPA Compliance):\n   - **Data minimization**: Store only necessary PII (user_id reference, not full profile)\n   - **User data deletion**: On user request, within 30 days:\n     - Delete from PostgreSQL (user preferences, device registrations)\n     - Delete from Redis cache\n     - Anonymize audit logs (replace user_id with hashed token)\n     - Delete from object storage archives\n   - **Audit trail**: Log all preference changes with timestamp and initiator\n   - **Data portability**: API endpoint to export user's notification history (JSON format)\n\n5. **Injection Prevention**:\n   - **Input validation**: All API requests validated against JSON schema\n   - **SQL injection**: Use parameterized queries exclusively (no string concatenation)\n   - **NoSQL injection**: Validate Redis keys, escape special characters\n\n6. **Secrets Management**:\n   - **External secrets store**: HashiCorp Vault or AWS Secrets Manager\n   - **Rotation policy**: FCM/APNs credentials rotated every 90 days\n   - **Least privilege**: Service accounts have minimal IAM permissions\n\n7. **Compliance**:\n   - **SOC 2 Type II**: Audit logging, access controls, encryption\n   - **GDPR**: Right to deletion, data portability, consent management\n   - **CCPA**: Data deletion, opt-out of notifications\n\n## What I Improved\n\n### 1. **Exactly-Once Delivery Semantics**\n- **Added**: Idempotency at every layer (gateway, processing, delivery, retry)\n- **Added**: Bloom filter for delivery deduplication (prevents duplicates from race conditions)\n- **Added**: Provider-level idempotency tokens (FCM messageId, APNs apns-id)\n- **Added**: Database locking for retry job claiming (SELECT FOR UPDATE SKIP LOCKED)\n- **Impact**: Eliminates duplicate notifications even during retries, network issues, or concurrent processing\n\n### 2. **Notification Loss Prevention**\n- **Added**: Database write BEFORE Kafka publish (transaction log pattern)\n- **Added**: Immutable audit log (never deleted, source of truth)\n- **Added**: Change Data Capture (CDC) as backup publishing path\n- **Added**: Kafka transactions + exactly-once semantics\n- **Impact**: Zero message loss even during Kafka outages or consumer crashes\n\n### 3. **Batching Edge Case Handling**\n- **Added**: Real-time preference change propagation to batching service\n- **Added**: Batch flush on preference change (user changes mind mid-batch)\n- **Added**: Per-device batching preferences (phone can batch, web gets real-time)\n- **Added**: Batch size limits with auto-flush (prevents overwhelming digests)\n- **Added**: TTL-aware batch commit (expired notifications removed before delivery)\n- **Added**: Quiet hours + batching coordination (smart scheduling for overnight digests)\n- **Impact**: Better user experience, respects user intent, prevents notification fatigue\n\n### 4. **Retry Logic Robustness**\n- **Added**: Jitter in exponential backoff (prevents thundering herd)\n- **Added**: Re-fetch user preferences between retries (preferences may have changed)\n- **Added**: Idempotent retry checks (verify not already delivered by another worker)\n- **Added**: Fallback channel handling (automatic escalation after original channel exhausts retries)\n- **Added**: Provider-specific error categorization (permanent vs. transient)\n- **Impact**: Higher delivery success rate, respects user preference changes, avoids wasted retry attempts\n\n### 5. **State Machine Clarity**\n- **Enhanced**: Delivery status with explicit state transitions (PENDING \u2192 IN_FLIGHT \u2192 DELIVERED/FAILED)\n- **Added**: Transactional state updates (database + Kafka event atomically)\n- **Added**: Lock ownership tracking (which worker is processing which retry)\n- **Impact**: Clear audit trail, prevents race conditions, enables debugging\n\n### 6. **Cross-Device User Experience**\n- **Added**: Per-device preference overrides (batching settings can differ by device)\n- **Added**: Device deactivation after repeated failures (prevents battery drain)\n- **Added**: Token refresh tracking (know when device tokens need updating)\n- **Impact**: Users get real-time on web, batched on mobile (flexible control)\n\n### 7. **Observability for Duplicates**\n- **Added**: `duplicate_blocked_count` metric (tracks idempotency effectiveness)\n- **Added**: Alert on duplicate sent (bloom filter failure)\n- **Added**: Correlation ID propagation through all services\n- **Impact**: Can detect and investigate any duplicate delivery incidents\n\n### 8. **Security Hardening**\n- **Added**: Push token encryption at rest (AES-256-GCM)\n- **Added**: Scope-based API key authorization\n- **Added**: mTLS for internal gRPC communication\n- **Added**: GDPR compliance details (data deletion, portability, anonymization)\n- **Impact**: Stronger security posture, regulatory compliance\n\n## What I Kept\n\n### Excellent Design Decisions from Original:\n1. **Event-driven architecture with Kafka** - Perfect for durability and replay\n2. **Separate channel services** - Independent scaling and failure isolation\n3. **Batching service as first-class component** - Directly addresses notification fatigue\n4. **Multi-layer caching strategy** - Reduces database load effectively\n5. **Priority-based routing** - Critical alerts bypass batching\n6. **Geographic distribution** - Low latency for global users\n7. **Circuit breaker per provider** - Prevents cascading failures\n8. **Dead letter queue** - Safety net for permanent failures\n9. **Comprehensive monitoring strategy** - Strong observability foundation\n10. **Rate limiting at multiple layers** - Defense in depth\n\n## Remaining Concerns\n\n### 1. **Batching Complexity vs. User Expectation**\n- **Trade-off**: Batching improves engagement but adds complexity\n- **Risk**: Users may not understand why some notifications are delayed\n- **Mitigation**: Clear UI communication about batching settings, \"Deliver Now\" button in app\n- **Question**: Should we A/B test batching effectiveness per user cohort?\n\n### 2. **Bloom Filter False Positives**\n- **Risk**: 0.01% false positive rate means ~100 notifications/day blocked incorrectly at scale\n- **Mitigation**: Track false positive metric, investigate spikes, tune filter size\n- **Alternative**: Use database check instead of bloom filter (higher latency, guaranteed accuracy)\n- **Recommendation**: Start with bloom filter, fallback to database if false positive rate > 0.1%\n\n### 3. **Cross-Region Consistency**\n- **Trade-off**: Eventual consistency for preferences (up to 30 seconds lag)\n- **Risk**: User changes preference in EU region, receives notification via US region with old preference\n- **Mitigation**: Route preference reads to home region (adds latency), or accept stale reads\n- **Question**: Is 30-second lag acceptable for preference updates?\n\n### 4. **CDC as Backup Publishing Path**\n- **Complexity**: Requires Debezium setup, schema registry, separate consumer pipeline\n- **Risk**: Increases operational overhead, potential for CDC lag\n- **Benefit**: True guarantee against message loss\n- **Recommendation**: Implement for critical notification types (ALERT, MESSAGE), optional for PROMOTIONAL\n\n### 5. **Cost at Scale**\n- **Concern**: 10M active users, 100M notifications/day = significant provider API costs\n- **Mitigation**: Batching reduces costs by 30-40%, but still expensive\n- **Optimization**: Implement notification priority auction (lower priority notifications wait for cheaper delivery slots)\n- **Question**: Should we prioritize cost or delivery speed for low-priority notifications?\n\n### 6. **Device Token Refresh Workflow**\n- **Gap**: Design doesn't specify how clients are notified to refresh tokens\n- **Solution needed**: When FCM/APNs returns InvalidRegistration, how do we tell app to re-register?\n- **Recommendation**: Return 410 on next API call from that device, client re-registers on 410 response\n\n### 7. **Batch Cancellation**\n- **Gap**: User deletes original content (e.g., post that generated notifications) while notifications are batching\n- **Solution needed**: Cancel API should also remove from pending batches\n- **Recommendation**: Add `cancel_from_batches` flag to CancelNotification API\n\n## Convergence Status\n**ITERATING** - First review round. Design is significantly improved with exactly-once semantics and edge case handling, but several concerns remain around operational complexity, cross-region consistency, and cost optimization trade-offs. Need architect's input on trade-offs and open questions.\n\n---",
        "prompt_for_other": "Please address the **cross-region consistency trade-off** for user preferences (30-second lag risk) and the **bloom filter false positive handling** strategy. Also, clarify the **device token refresh workflow** - how should clients be notified when their token becomes invalid? Finally, consider the **CDC backup publishing path** - is the operational complexity justified for all notification types, or should we scope it to critical notifications only?",
        "convergence_signal": "ITERATING",
        "raw_response": "## Design\n\n# Real-Time Notification System Architecture\n\n## System Overview\n\nA distributed, event-driven notification system supporting push notifications across mobile (iOS/Android) and web platforms with **exactly-once delivery semantics**, intelligent batching, user preference management, and guaranteed delivery at scale.\n\n## Architecture Components\n\n### 1. Core Services\n\n#### Notification Gateway Service\n- **Purpose**: Single entry point for all notification requests\n- **Responsibilities**:\n  - Request validation and authentication\n  - Rate limiting (per user, per application) using token bucket algorithm\n  - Initial routing to appropriate handlers\n  - **Idempotency enforcement**: Store idempotency_key with 24-hour TTL in Redis before accepting request\n  - **Synchronous validation**: Return 400 for invalid requests, 409 for duplicate idempotency_key, 202 for accepted\n- **Technology**: Go with gRPC for internal services, REST for external clients\n- **Scaling**: Stateless, horizontally scalable behind load balancer\n- **New**: Immediate idempotency check prevents duplicate entry into system\n\n#### Notification Processing Engine\n- **Purpose**: Core business logic with transactional guarantees\n- **Responsibilities**:\n  - Apply user preferences and filtering rules\n  - Determine optimal delivery channels with fallback ordering\n  - Handle notification priority classification (critical, high, medium, low)\n  - Route to batching service OR direct delivery queue based on priority + user preferences\n  - Enrich notifications with user context\n  - **Write notification state to database BEFORE publishing to Kafka** (transaction log pattern)\n  - Generate deterministic notification_id from (user_id + idempotency_key + timestamp)\n- **Technology**: Go with worker pool pattern\n- **Scaling**: Horizontal scaling with message queue consumers (consumer group per service instance)\n- **New**: Database write happens BEFORE Kafka publish to create audit trail\n\n#### Channel Delivery Services (one per channel type)\n- **Mobile Push Service**: Integrates with FCM and APNs with **response validation**\n- **Web Push Service**: Integrates with Web Push Protocol\n- **Email Service**: Backup/alternative channel\n- **SMS Service**: Critical notifications fallback\n- **Key Enhancement**: Each service validates provider response and publishes to `notifications.delivery-events` topic with status\n- **Deduplication**: Before sending, check `delivery_status` table for existing successful delivery within TTL window\n- **Technology**: Specialized microservices per channel with provider circuit breakers\n- **Scaling**: Independent scaling based on channel volume\n- **New**: Pre-send deduplication check + validated delivery confirmation\n\n#### Batching & Scheduling Service\n- **Purpose**: Aggregate notifications with consistency guarantees\n- **Responsibilities**:\n  - Group related notifications by user and type in Redis sorted sets (score = scheduled_time)\n  - Apply time-window batching (configurable per notification type)\n  - **Smart scheduling**: Check user preference + timezone + historical activity patterns\n  - **Batch commit**: When window closes, atomically move batch to delivery queue with batch_id\n  - **Batch escape hatch**: Allow users to \"unbatch\" via real-time preference update (flushes pending batch immediately)\n  - Digest generation for batched notifications\n- **Batching Strategy**:\n  - **Immediate**: Critical alerts (system failures, security alerts) - bypass batching entirely\n  - **5-minute window**: High priority messages - user can configure 0-15 min range\n  - **1-hour window**: Medium priority (social interactions) - user can configure 15min-2hr range\n  - **Daily digest**: Low priority (newsletters, summaries) - configurable time slot per user\n- **Edge Case Handling**:\n  - If preference changes mid-batch: Apply new preference, flush batch if changed from batched\u2192immediate\n  - Cross-device batching: Create separate batches per device but with synchronized delivery timestamps\n  - Batch size limits: Maximum 50 notifications per batch, auto-flush on limit\n- **Technology**: Redis sorted sets for temporary storage, distributed lock (Redlock) for batch commit\n- **Scaling**: Sharded by user_id hash\n- **New**: Preference change handling, cross-device coordination, size limits\n\n#### User Preference Service\n- **Purpose**: Manage user notification settings with real-time propagation\n- **Responsibilities**:\n  - CRUD operations for user preferences\n  - **Write-through cache**: Update PostgreSQL + Redis atomically\n  - **Cache invalidation broadcast**: Publish preference change events to `user.preference-updates` topic\n  - Quiet hours management with timezone awareness\n  - Channel priority management (ordered list of fallback channels)\n  - DND (Do Not Disturb) support with critical-only override\n  - **Preference versioning**: Track version number to detect conflicts during concurrent updates\n- **Technology**: PostgreSQL for persistence with row-level locking, Redis for caching\n- **Scaling**: Read replicas for queries, write-through cache for consistency\n- **New**: Versioning, cache invalidation broadcast, atomic write-through\n\n#### Delivery Tracking & Retry Service with Exactly-Once Semantics\n- **Purpose**: Ensure guaranteed delivery without duplicates\n- **Responsibilities**:\n  - Track delivery status with state machine: PENDING \u2192 IN_FLIGHT \u2192 DELIVERED/FAILED\n  - **State transitions are transactional**: Update database + publish event atomically\n  - Exponential backoff retry logic with jitter (max 5 retries)\n  - **Idempotent retry**: Before retry, check if notification was delivered by another worker (race condition handling)\n  - Dead letter queue management for permanent failures\n  - Delivery analytics and monitoring\n  - **Delivery deduplication window**: 24-hour bloom filter per (user_id, notification_id) to catch any duplicates\n  - **Provider idempotency tokens**: Pass unique token to FCM/APNs for provider-level deduplication\n- **Retry Strategy**:\n  - Attempt 1: Immediate\n  - Attempt 2: 30 seconds + random(0-5s) jitter\n  - Attempt 3: 5 minutes + random(0-30s) jitter\n  - Attempt 4: 30 minutes + random(0-2min) jitter\n  - Attempt 5: 2 hours + random(0-10min) jitter\n  - **Between retries**: Re-fetch user preferences (may have changed)\n  - After failure: Move to DLQ with full context, alert ops team, **check if fallback channel configured**\n  - **New**: If fallback channel exists and original channel exhausted retries, create new notification for fallback (with reference to original)\n- **Race Condition Handling**:\n  - Use database SELECT FOR UPDATE SKIP LOCKED for retry job claims\n  - Worker acquires lock on specific delivery record before processing\n  - If delivery completed by another worker, current worker releases and moves to next job\n- **Technology**: Kafka for event streaming, PostgreSQL for authoritative state with optimistic locking\n- **Scaling**: Partitioned by notification_id, consumer group coordinates work distribution\n- **New**: Bloom filter, provider idempotency tokens, database locking for claim semantics\n\n### 2. Data Stores\n\n#### Primary Database (PostgreSQL)\n- User preferences and settings with version column\n- Notification templates\n- **Notification audit log**: Immutable record of all notifications created (never deleted)\n- **Delivery status table**: State machine tracking with composite index on (notification_id, device_id, status)\n- Device registrations with token refresh timestamps\n- **Partitioning strategy**:\n  - Delivery status: Partitioned by date (monthly), archived after 90 days\n  - Audit log: Partitioned by date (monthly), retained indefinitely for compliance\n- **Indexes**:\n  - (user_id, created_at) for user notification history\n  - (notification_id, status) for delivery queries\n  - (idempotency_key) unique index with 24-hour partial index (WHERE created_at > NOW() - INTERVAL '24 hours')\n\n#### Cache Layer (Redis Cluster)\n- User preference cache with version tags (TTL: 5 minutes, invalidate on update)\n- Device token cache (TTL: 1 hour)\n- Rate limiting counters using sliding window algorithm (TTL: 1 minute)\n- Batching windows using sorted sets (TTL: 48 hours)\n- **Idempotency key store** (TTL: 24 hours) - tracks accepted requests\n- **Delivery deduplication bloom filter** (TTL: 24 hours, 0.01% false positive rate)\n- **Distributed locks** (Redlock algorithm) for batch commits\n\n#### Message Queue (Apache Kafka)\n- Topics with **compaction and retention policies**:\n  - `notifications.incoming`: Raw notification requests (7-day retention)\n  - `notifications.processed`: Enriched notifications ready for delivery (7-day retention)\n  - `notifications.batched`: Batch-ready notifications (7-day retention)\n  - `notifications.immediate`: High-priority bypass batching (7-day retention)\n  - `notifications.retry`: Failed deliveries needing retry (30-day retention for forensics)\n  - `notifications.dlq`: Permanent failures (90-day retention, compacted by notification_id)\n  - `notifications.delivery-events`: Delivery confirmations from providers (7-day retention)\n  - `user.preference-updates`: Cache invalidation events (compacted by user_id, 1-day retention)\n- **Partitioning**: By user_id hash for ordered processing per user\n- **Consumer groups**: Each service instance joins consumer group with unique consumer_id\n- **Exactly-once semantics**: Enable Kafka transactions + idempotent producer + read_committed isolation\n- **New**: Delivery-events topic for confirmations, preference-updates for invalidation, transactions enabled\n\n#### Object Storage (S3/GCS)\n- Notification archives (historical data, Parquet format for analytics)\n- Large payload storage (images, rich media) with CDN fronting\n- Analytics data exports (daily aggregations)\n- DLQ snapshots for forensics\n\n### 3. External Integrations\n\n#### Push Notification Providers\n- **FCM (Firebase Cloud Messaging)**: Android and iOS via Firebase\n  - Use FCM batch API (500 notifications per request) for efficiency\n  - **Validate response**: Extract success/failure per device token\n  - **Handle token refresh**: Update device registry on invalid token errors\n- **APNs (Apple Push Notification service)**: iOS native\n  - Use HTTP/2 connection pooling\n  - **Validate response**: Check status code per notification\n  - **Handle token expiration**: Update device registry on 410 Gone responses\n- **Web Push**: Browser-based notifications via service workers\n  - Handle 410 Gone for expired subscriptions\n- **Provider-level deduplication**: Pass `apns-collapse-id` (APNs) and `collapse_key` (FCM) for provider deduplication\n\n#### Monitoring & Observability\n- **Prometheus + Grafana**: Metrics and dashboards\n  - Custom metrics: duplicate_blocked_count, delivery_latency_by_channel, batch_size_distribution\n- **ELK Stack**: Centralized logging with correlation_id propagation\n- **Jaeger**: Distributed tracing with spans for each service hop\n- **PagerDuty**: Alerting for critical failures\n- **DataDog/New Relic**: End-to-end delivery tracking dashboards\n\n## Data Models\n\n### Notification Model (Enhanced)\n```json\n{\n  \"notification_id\": \"uuid (deterministic from user_id + idempotency_key + timestamp)\",\n  \"user_id\": \"string\",\n  \"type\": \"enum[ALERT, MESSAGE, REMINDER, SOCIAL, PROMOTIONAL]\",\n  \"priority\": \"enum[CRITICAL, HIGH, MEDIUM, LOW]\",\n  \"title\": \"string\",\n  \"body\": \"string\",\n  \"data\": {\n    \"action_url\": \"string\",\n    \"image_url\": \"string\",\n    \"custom_fields\": {}\n  },\n  \"channels\": [\"PUSH\", \"EMAIL\", \"SMS\"],\n  \"fallback_channels\": [\"EMAIL\", \"SMS\"],\n  \"batch_eligible\": \"boolean\",\n  \"batch_key\": \"string (for grouping related notifications)\",\n  \"batch_id\": \"uuid (assigned when batched, null for immediate)\",\n  \"ttl\": \"integer (seconds, default 86400)\",\n  \"created_at\": \"timestamp\",\n  \"scheduled_for\": \"timestamp (null for immediate)\",\n  \"expires_at\": \"timestamp (computed from created_at + ttl)\",\n  \"idempotency_key\": \"string (required, unique per 24 hours)\",\n  \"correlation_id\": \"uuid (for tracing)\",\n  \"preference_version\": \"integer (version at notification creation time)\"\n}\n```\n\n### User Preferences Model (Enhanced)\n```json\n{\n  \"user_id\": \"string\",\n  \"version\": \"integer (incremented on every update, for optimistic locking)\",\n  \"preferences\": {\n    \"global_enabled\": \"boolean\",\n    \"channels\": {\n      \"push\": {\n        \"enabled\": \"boolean\",\n        \"priority\": 1,\n        \"notification_types\": {\n          \"ALERT\": \"boolean\",\n          \"MESSAGE\": \"boolean\",\n          \"REMINDER\": \"boolean\",\n          \"SOCIAL\": \"boolean\",\n          \"PROMOTIONAL\": \"boolean\"\n        }\n      },\n      \"email\": { \n        \"enabled\": \"boolean\",\n        \"priority\": 2,\n        \"notification_types\": { /* same */ }\n      },\n      \"sms\": { \n        \"enabled\": \"boolean\",\n        \"priority\": 3,\n        \"notification_types\": { /* same */ }\n      }\n    },\n    \"channel_fallback_order\": [\"push\", \"email\", \"sms\"],\n    \"quiet_hours\": {\n      \"enabled\": \"boolean\",\n      \"start_time\": \"string (HH:mm)\",\n      \"end_time\": \"string (HH:mm)\",\n      \"timezone\": \"string (IANA timezone)\",\n      \"allow_critical\": \"boolean\",\n      \"buffer_notifications\": \"boolean (queue for delivery after quiet hours)\"\n    },\n    \"batching\": {\n      \"enabled\": \"boolean\",\n      \"per_type_settings\": {\n        \"ALERT\": \"REALTIME\",\n        \"MESSAGE\": \"5MIN\",\n        \"SOCIAL\": \"1HOUR\",\n        \"PROMOTIONAL\": \"DAILY\"\n      },\n      \"daily_digest_time\": \"string (HH:mm in user timezone)\",\n      \"max_batch_size\": \"integer (default 50)\"\n    }\n  },\n  \"updated_at\": \"timestamp\",\n  \"updated_by\": \"string (user/system)\"\n}\n```\n\n### Device Registration Model (Enhanced)\n```json\n{\n  \"device_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"platform\": \"enum[IOS, ANDROID, WEB]\",\n  \"push_token\": \"string (encrypted, FCM/APNs token)\",\n  \"token_refreshed_at\": \"timestamp\",\n  \"endpoint\": \"string (for Web Push)\",\n  \"user_agent\": \"string\",\n  \"is_active\": \"boolean\",\n  \"last_successful_delivery_at\": \"timestamp\",\n  \"consecutive_failures\": \"integer (reset on success, deactivate after 10)\",\n  \"registered_at\": \"timestamp\",\n  \"last_active_at\": \"timestamp\",\n  \"device_preferences_override\": {\n    \"batching_enabled\": \"boolean (null = inherit from user preferences)\"\n  }\n}\n```\n\n### Delivery Status Model (Enhanced - State Machine)\n```json\n{\n  \"delivery_id\": \"uuid\",\n  \"notification_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"device_id\": \"uuid\",\n  \"channel\": \"enum[PUSH, EMAIL, SMS]\",\n  \"status\": \"enum[PENDING, IN_FLIGHT, DELIVERED, FAILED, EXPIRED, CANCELLED]\",\n  \"attempt_count\": \"integer\",\n  \"provider_message_id\": \"string (FCM/APNs message ID for tracking)\",\n  \"provider_idempotency_token\": \"uuid (sent to provider for their deduplication)\",\n  \"provider_response\": \"json\",\n  \"error_code\": \"string (standardized error codes)\",\n  \"error_message\": \"string (if failed)\",\n  \"is_fallback\": \"boolean (true if this is fallback channel delivery)\",\n  \"original_delivery_id\": \"uuid (if fallback, reference to original)\",\n  \"queued_at\": \"timestamp\",\n  \"in_flight_at\": \"timestamp\",\n  \"sent_at\": \"timestamp\",\n  \"delivered_at\": \"timestamp\",\n  \"failed_at\": \"timestamp\",\n  \"expires_at\": \"timestamp\",\n  \"next_retry_at\": \"timestamp (null if no retry scheduled)\",\n  \"last_updated_at\": \"timestamp\",\n  \"lock_acquired_by\": \"string (worker_id for retry job claiming)\"\n}\n```\n\n### Batch Model (New)\n```json\n{\n  \"batch_id\": \"uuid\",\n  \"user_id\": \"string\",\n  \"device_id\": \"uuid\",\n  \"notification_type\": \"enum[MESSAGE, SOCIAL, PROMOTIONAL]\",\n  \"notification_ids\": [\"uuid[]\"],\n  \"status\": \"enum[ACCUMULATING, COMMITTED, DELIVERED, FAILED]\",\n  \"batch_window_start\": \"timestamp\",\n  \"batch_window_end\": \"timestamp\",\n  \"scheduled_delivery_time\": \"timestamp\",\n  \"notification_count\": \"integer\",\n  \"created_at\": \"timestamp\",\n  \"committed_at\": \"timestamp\",\n  \"delivered_at\": \"timestamp\"\n}\n```\n\n## API Interfaces\n\n### Internal gRPC Service Interface\n\n```protobuf\nservice NotificationService {\n  // Send single notification with idempotency\n  rpc SendNotification(SendNotificationRequest) returns (SendNotificationResponse);\n  \n  // Send batch of notifications (each with idempotency_key)\n  rpc SendBulkNotifications(SendBulkNotificationsRequest) returns (SendBulkNotificationsResponse);\n  \n  // Get delivery status with full history\n  rpc GetDeliveryStatus(GetDeliveryStatusRequest) returns (GetDeliveryStatusResponse);\n  \n  // Cancel scheduled notification (only if not yet delivered)\n  rpc CancelNotification(CancelNotificationRequest) returns (CancelNotificationResponse);\n  \n  // Flush pending batch for user (manual unbatch)\n  rpc FlushUserBatch(FlushUserBatchRequest) returns (FlushUserBatchResponse);\n}\n\nservice UserPreferenceService {\n  rpc GetUserPreferences(GetUserPreferencesRequest) returns (GetUserPreferencesResponse);\n  \n  // Returns new version number, fails with ABORTED if version mismatch\n  rpc UpdateUserPreferences(UpdateUserPreferencesRequest) returns (UpdateUserPreferencesResponse);\n  \n  rpc RegisterDevice(RegisterDeviceRequest) returns (RegisterDeviceResponse);\n  rpc UnregisterDevice(UnregisterDeviceRequest) returns (UnregisterDeviceResponse);\n  \n  // Mark device as inactive after repeated failures\n  rpc DeactivateDevice(DeactivateDeviceRequest) returns (DeactivateDeviceResponse);\n}\n\nservice DeliveryTrackingService {\n  // Query delivery history for notification\n  rpc GetDeliveryHistory(GetDeliveryHistoryRequest) returns (GetDeliveryHistoryResponse);\n  \n  // Get aggregated delivery metrics\n  rpc GetDeliveryMetrics(GetDeliveryMetricsRequest) returns (GetDeliveryMetricsResponse);\n}\n```\n\n### External REST API\n\n```\nPOST /api/v1/notifications\n  - Send notification with idempotency\n  - Request: Notification model JSON (idempotency_key required)\n  - Response: {notification_id, status: \"accepted\", correlation_id}\n  - Rate limit: 1000 req/min per API key\n  - Idempotent: Returns same notification_id for duplicate idempotency_key within 24h\n\nPOST /api/v1/notifications/bulk\n  - Send multiple notifications (each with idempotency_key)\n  - Request: Array of Notification models\n  - Response: {job_id, accepted: [{notification_id, idempotency_key}], rejected: [{idempotency_key, reason}]}\n  - Rate limit: 100 req/min per API key\n  - Partial success supported\n\nGET /api/v1/notifications/{notification_id}/status\n  - Get delivery status for all devices/channels\n  - Response: {notification_id, deliveries: [Delivery status models], overall_status}\n\nGET /api/v1/notifications/{notification_id}/history\n  - Get full delivery history including retries\n  - Response: {notification_id, timeline: [{timestamp, status, attempt, details}]}\n\nDELETE /api/v1/notifications/{notification_id}\n  - Cancel scheduled notification (idempotent, fails if already delivered)\n  - Response: {cancelled: boolean, reason: string}\n\nPOST /api/v1/users/{user_id}/batches/flush\n  - Flush pending batches immediately\n  - Response: {flushed_count, notification_ids}\n\nGET /api/v1/users/{user_id}/preferences\n  - Get user preferences with version\n  - Response: User preferences model\n\nPUT /api/v1/users/{user_id}/preferences\n  - Update user preferences with optimistic locking\n  - Request: User preferences model with version\n  - Response: Updated preferences with new version\n  - Returns 409 if version mismatch\n\nPOST /api/v1/devices\n  - Register device\n  - Request: Device registration model\n  - Response: {device_id, push_token_encrypted}\n\nPUT /api/v1/devices/{device_id}/token\n  - Refresh device push token\n  - Request: {push_token}\n  - Response: {updated: boolean}\n\nDELETE /api/v1/devices/{device_id}\n  - Unregister device (idempotent)\n  - Response: {success: boolean}\n```\n\n## Error Handling & Resilience\n\n### Error Categories\n\n1. **Transient Errors** (Retry with exponential backoff + jitter)\n   - Network timeouts\n   - Provider rate limits (FCM/APNs throttling) - respect Retry-After header\n   - Temporary service unavailability (503)\n   - Database connection issues\n   - Kafka broker unavailable\n\n2. **Permanent Errors** (No retry, move to DLQ, attempt fallback)\n   - Invalid device tokens (FCM: InvalidRegistration, APNs: BadDeviceToken) \u2192 deactivate device\n   - Malformed notification payloads (400) \u2192 log and alert\n   - User not found \u2192 log and drop\n   - Authentication failures (401/403) \u2192 alert operations\n   - Notification expired (TTL exceeded) \u2192 mark as EXPIRED\n   - Device unregistered by user (FCM: NotRegistered, APNs: 410 Gone) \u2192 deactivate device\n\n3. **Partial Failures** (Retry failed subset, track individually)\n   - Bulk send with some failures \u2192 track each notification separately\n   - Multi-channel delivery with channel-specific failures \u2192 retry only failed channels\n   - Batch delivery where some devices fail \u2192 mark successful deliveries, retry failures\n\n### Resilience Patterns\n\n1. **Circuit Breaker** (per external provider)\n   - FCM circuit breaker: Open after 10 consecutive failures, half-open after 30 seconds, close after 5 successes\n   - APNs circuit breaker: Same thresholds\n   - When open: Immediately fail requests, move to retry queue, alert operations\n   - Separate circuit breakers per provider to prevent cross-contamination\n\n2. **Bulkhead Isolation**\n   - Separate thread pools per channel (Push: 100 threads, Email: 50, SMS: 20)\n   - Separate Kafka consumer groups per service instance\n   - Independent rate limiters per channel\n\n3. **Timeout Management** (with context propagation)\n   - Gateway timeout: 5 seconds (synchronous validation only)\n   - Processing timeout: 10 seconds (preference lookup + routing decision)\n   - Provider timeout: 30 seconds (FCM/APNs API call)\n   - Batch commit timeout: 2 seconds (Redis transaction)\n   - Database query timeout: 5 seconds\n   - Overall correlation_id timeout: 5 minutes (end-to-end delivery attempt)\n\n4. **Graceful Degradation**\n   - If batching service fails: Deliver immediately via `notifications.immediate` topic\n   - If primary channel fails: Attempt fallback channel from user's fallback_order\n   - If preference service fails: Use cached preferences (stale-while-revalidate), fallback to system defaults if cache miss\n   - If Redis fails: Disable batching, operate in pass-through mode\n   - If Kafka fails: Write to database, rely on change data capture (CDC) for eventual message publishing\n\n5. **Idempotency at Every Layer**\n   - Gateway: idempotency_key check in Redis before accepting\n   - Processing: deterministic notification_id generation\n   - Delivery: provider_idempotency_token passed to FCM/APNs\n   - Retry: Check delivery_status before sending (another worker may have succeeded)\n   - Database: Use INSERT ... ON CONFLICT DO NOTHING for delivery records\n\n6. **Dead Letter Queue with Forensics**\n   - Capture full notification payload + all retry attempts + error context\n   - Automated analysis: Group by error_code, identify systemic issues\n   - Manual replay capability: Operations can trigger re-delivery after fixing root cause\n   - Alert thresholds: Page on-call if DLQ depth > 1000 messages\n\n7. **Distributed Lock Management**\n   - Use Redlock algorithm for batch commits (requires majority of Redis nodes)\n   - Lock timeout: 10 seconds max\n   - Lock refresh: Not needed for short-lived batch commits\n   - Failure handling: If lock acquisition fails, retry batch commit after 1 second\n\n## Exactly-Once Delivery Guarantees\n\n### Prevention of Duplicates\n\n1. **Entry Point Deduplication** (Gateway)\n   - Client provides idempotency_key (UUID or content-based hash)\n   - Gateway checks Redis: `idempotency:{key}` \u2192 `{notification_id, status, created_at}`\n   - If exists: Return existing notification_id with 200 (not 409)\n   - If new: Store in Redis with 24-hour TTL, proceed with processing\n   - Edge case: If Redis write succeeds but processing fails, retry with same key is safe\n\n2. **Processing Deduplication**\n   - Deterministic notification_id = SHA256(user_id || idempotency_key || timestamp_bucket)\n   - Database has unique constraint on (notification_id, device_id) in delivery_status table\n   - INSERT with ON CONFLICT DO NOTHING prevents duplicate state tracking\n\n3. **Delivery Deduplication**\n   - Before calling FCM/APNs, check bloom filter: `delivered:{user_id}:{notification_id}`\n   - If exists: Skip send, mark as DELIVERED (another worker already succeeded)\n   - If not exists: Send with provider_idempotency_token, add to bloom filter on success\n   - Provider tokens: APNs uses apns-id header, FCM uses messageId\n   - 24-hour TTL on bloom filter entries\n\n4. **Retry Deduplication**\n   - Worker claims retry job using SELECT FOR UPDATE SKIP LOCKED\n   - Before processing: Re-check delivery_status (status = DELIVERED?)\n   - If delivered: Skip retry, release lock\n   - If still FAILED: Proceed with retry attempt\n\n5. **Cross-Device Deduplication**\n   - User may have multiple devices (phone, tablet, web)\n   - Each device gets separate delivery record (different device_id)\n   - Bloom filter is per (notification_id, device_id) tuple\n   - Trade-off: User may receive on multiple devices (by design), but never duplicate on same device\n\n### Handling of Message Loss\n\n1. **Kafka Durability**\n   - min.insync.replicas = 2 (message written to 2 brokers before ack)\n   - acks = all (wait for all in-sync replicas)\n   - Kafka transactions enabled for exactly-once semantics\n   - Consumer commits offsets only after successful database write\n\n2. **Database as Source of Truth**\n   - Notification audit log is immutable and never deleted\n   - Delivery status table tracks all attempts\n   - If Kafka consumer crashes mid-processing: Offset not committed, message reprocessed\n   - Idempotency prevents duplicate delivery despite reprocessing\n\n3. **Dead Letter Queue as Safety Net**\n   - After 5 failed attempts, move to DLQ (Kafka topic with 90-day retention)\n   - DLQ messages are never deleted automatically\n   - Daily job scans DLQ for retriable errors (e.g., provider outage resolved)\n   - Operations dashboard shows DLQ trends\n\n4. **Change Data Capture (CDC) Backup**\n   - PostgreSQL logical replication streams changes to Kafka (Debezium)\n   - If direct Kafka publishing fails, CDC ensures eventual message availability\n   - Secondary consumer processes CDC stream and publishes to notification topics\n   - Idempotency prevents duplicates from dual-path publishing\n\n## Batching Strategy - Edge Cases\n\n### Preference Changes Mid-Batch\n\n**Scenario**: User has 5 notifications batching (1-hour window), after 30 minutes changes preference from batched to immediate.\n\n**Solution**:\n1. Preference update publishes to `user.preference-updates` topic\n2. Batching service subscribes to preference updates\n3. On preference change event:\n   - Lookup pending batches for user_id in Redis\n   - If batching disabled or type-specific frequency changed:\n     - Atomically move batch from `batching:{user_id}:{type}` to `notifications.immediate` topic\n     - Delete batch from Redis\n     - Publish each notification individually with `batch_flushed: true` flag\n4. If change is within same batching tier (e.g., 1hr \u2192 2hr): Keep accumulating, adjust scheduled_delivery_time\n\n**Trade-off**: Slight delay (up to 5 seconds) between preference update and batch flush due to eventual consistency.\n\n### Cross-Device Batching\n\n**Scenario**: User has phone (batching enabled) and web browser (batching disabled for real-time updates).\n\n**Solution**:\n1. Device registration includes `device_preferences_override.batching_enabled`\n2. When routing notification:\n   - Lookup all user devices\n   - For each device, compute effective batching preference: `device_override ?? user_preference`\n   - Create separate batch contexts per (user_id, device_id, batching_preference) tuple\n3. Same notification may be batched for phone, delivered immediately to web\n4. Batch digest indicates \"also delivered to other devices\" to prevent confusion\n\n**Benefit**: Per-device control without complex cross-device coordination.\n\n### Batch Size Limits\n\n**Scenario**: User receives 100 social notifications in 1-hour window (e.g., viral post).\n\n**Solution**:\n1. Batch accumulation checks `notification_count` before adding\n2. If `notification_count >= max_batch_size` (default 50):\n   - Commit current batch immediately\n   - Start new batch for remaining notifications\n3. Digest message: \"You have 50 new likes and comments. View all \u2192\"\n4. Subsequent batch (if window still open): \"You have 25 more likes and comments since your last digest.\"\n\n**User Experience**: Prevents overwhelming digests, provides progressive updates.\n\n### Expired Notifications in Batch\n\n**Scenario**: Notification with 30-minute TTL sits in 1-hour batch, expires before delivery.\n\n**Solution**:\n1. Batch commit job checks `expires_at` for each notification\n2. If `expires_at < scheduled_delivery_time`:\n   - Remove from batch\n   - Mark delivery_status as EXPIRED\n   - Do not include in digest\n3. If entire batch expired: Skip delivery\n4. If partial expiration: Deliver remaining notifications in digest\n\n**Metric**: Track `expired_in_batch_count` to identify TTL vs. batching window mismatches.\n\n### Batching During Quiet Hours\n\n**Scenario**: User has 10pm-8am quiet hours + 1-hour batching for social notifications. Notification arrives at 9:30pm.\n\n**Solution**:\n1. Quiet hours check happens BEFORE batching decision\n2. If `current_time` in quiet_hours AND `allow_critical = false`:\n   - If `buffer_notifications = true`: Add to batch with scheduled_delivery_time = quiet_hours.end_time\n   - If `buffer_notifications = false`: Drop notification (mark as SUPPRESSED in audit log)\n3. Batch scheduled for 8am delivery with digest: \"Here's what happened overnight\"\n\n**User Control**: `buffer_notifications` preference allows users to choose suppression vs. delayed delivery.\n\n### Concurrent Batch Operations\n\n**Scenario**: Two notifications arrive simultaneously for same user, both trying to create/update batch.\n\n**Solution**:\n1. Redis sorted set operations are atomic (ZADD is thread-safe)\n2. Batch commit uses distributed lock (Redlock):\n   ```\n   LOCK batch_commit:{user_id}:{type} (timeout: 10s)\n   IF batch_window_end <= current_time:\n     notification_ids = ZRANGEBYSCORE batching:{user_id}:{type} -inf +inf\n     DELETE batching:{user_id}:{type}\n     CREATE batch record in PostgreSQL\n     PUBLISH to notifications.immediate topic\n   UNLOCK\n   ```\n3. Lock ensures only one worker commits batch, others skip if batch already committed\n\n## Scaling Strategy\n\n### Horizontal Scaling\n- All services are stateless (session state in Redis, durable state in PostgreSQL/Kafka)\n- Auto-scaling based on:\n  - Kafka consumer lag (target: < 1000 messages per partition)\n  - CPU utilization (target: 70%)\n  - Request rate (gateway: scale at 80% of rate limit capacity)\n- Kubernetes HPA (Horizontal Pod Autoscaler) with custom metrics from Prometheus\n\n### Database Scaling\n- **PostgreSQL**:\n  - Primary-replica topology: 1 writer, 3 read replicas (round-robin for read queries)\n  - Partitioning:\n    - `delivery_status`: Monthly partitions with CHECK constraint on queued_at\n    - `notification_audit_log`: Monthly partitions\n    - Automated partition creation via cron job (create next 3 months)\n    - Automated archival: Detach partitions older than 90 days, move to object storage\n  - Connection pooling: PgBouncer in transaction mode (1000 max connections)\n  - Write scaling: For massive write spikes, use write-through cache (write to Redis, async flush to PostgreSQL)\n\n- **Redis**:\n  - Cluster mode: 6 nodes (3 masters, 3 replicas)\n  - Sharding: Hash slot partitioning by user_id (16384 slots)\n  - Separate clusters for:\n    - Cache data (user preferences, device tokens)\n    - Batching data (sorted sets for accumulation)\n    - Distributed locks (Redlock requires 3+ independent instances)\n  - Memory: 100GB per cluster, eviction policy: allkeys-lru for cache cluster, noeviction for batching/locks\n\n- **Kafka**:\n  - 15-node cluster (5 brokers per availability zone)\n  - Partitions per topic:\n    - `notifications.incoming`: 30 partitions (for write throughput)\n    - `notifications.processed`: 30 partitions (parallel processing)\n    - `notifications.retry`: 10 partitions (lower volume)\n  - Replication factor: 3 (cross-AZ)\n  - Consumer scaling: Consumer group with N consumers = min(N, partition_count)\n  - Partition rebalancing: Cooperative sticky assignor for minimal disruption\n\n### Capacity Planning (for 10M active users)\n- **Notification volume**: 100M notifications/day (~1,200/sec avg, 5,000/sec peak during events)\n- **Batching impact**: Assuming 40% of notifications are batched, reduces delivery operations to ~60M/day\n- **Gateway service**: 20 instances @ 4 cores each (250 req/sec per instance) = 5,000 req/sec capacity\n- **Processing engine**: 50 workers @ 2 cores each (100 notif/sec each) = 5,000 notif/sec capacity\n- **Channel services**: \n  - Push: 15 instances (FCM/APNs have high throughput)\n  - Email: 10 instances (slower delivery)\n  - SMS: 5 instances (lowest volume)\n- **Kafka**: 15-node cluster handling ~10K msg/sec write throughput\n- **Redis**: 6-node cluster with 100GB total memory (~100M keys cached)\n- **PostgreSQL**: \n  - Primary: 32 cores, 128GB RAM, 2TB SSD (handles ~5K writes/sec)\n  - Replicas: 3x (16 cores, 64GB RAM each)\n- **Cost optimization**: Using batching reduces provider API costs by ~30-40%, compute costs by ~25%\n\n### Geographic Distribution\n- **Multi-region deployment** (US-East, US-West, EU-West, Asia-Pacific)\n- **User data sharding**: Users assigned to home region based on registration location\n- **Read-your-writes consistency**: User preference updates routed to home region, replicated to others\n- **Cross-region notification delivery**: Notification created in any region, routed to user's home region for preference lookup, then to nearest channel service for delivery\n- **Kafka mirroring**: MirrorMaker 2.0 for cross-region topic replication (for disaster recovery, not active-active)\n- **Failover strategy**: \n  - If home region unavailable: Route to backup region\n  - Eventual consistency: Preference updates may be stale (max 30 seconds)\n  - Batch coordination: Batches are region-local (cross-region batch merging not supported)\n\n## Monitoring & Observability\n\n### Key Metrics\n\n**Delivery Metrics**:\n- `notifications_sent_total{channel, status}`: Counter of delivery attempts\n- `notifications_delivered_total{channel}`: Counter of successful deliveries\n- `notifications_failed_total{channel, error_code}`: Counter of failures by category\n- `delivery_latency_seconds{channel, priority, p50/p95/p99}`: End-to-end latency from creation to delivery\n- `duplicate_blocked_count`: Notifications blocked by idempotency/bloom filter\n- `batch_size_distribution`: Histogram of notifications per batch\n- `batch_flush_reason{reason}`: Counter of why batches were flushed (window_closed, preference_change, size_limit)\n\n**System Metrics**:\n- `kafka_consumer_lag{topic, consumer_group}`: Messages behind in queue\n- `circuit_breaker_state{provider}`: Open/half-open/closed\n- `redis_memory_usage{cluster}`: Memory consumption\n- `postgres_replication_lag{replica}`: Seconds behind primary\n- `rate_limit_exceeded_total{api_key}`: Rate limit violations\n\n**Business Metrics**:\n- `notifications_created_total{type, priority}`: Total notifications by category\n- `user_preferences_updated_total`: Preference change frequency\n- `device_registrations_total{platform}`: New device registrations\n- `device_deactivations_total{reason}`: Devices marked inactive\n\n### Alerts\n\n**Critical** (Page on-call):\n- Service availability < 99.9% over 5-minute window\n- Error rate > 5% over 5-minute window\n- Kafka consumer lag > 10,000 messages for > 5 minutes\n- DLQ depth > 1000 messages\n- Circuit breaker open for > 2 minutes\n- PostgreSQL replication lag > 60 seconds\n- Duplicate notification sent (bloom filter failed)\n\n**Warning** (Slack notification):\n- Latency p99 > 5 seconds over 10-minute window\n- Retry rate > 10% over 10-minute window\n- Provider rate limit approaching (> 80% of quota)\n- Redis memory usage > 85%\n- Device deactivation rate spike (> 2x baseline)\n- Unusual batch size (> 100 notifications in single batch)\n\n### Logging\n- **Structured JSON logs** with fields: `timestamp`, `level`, `service`, `correlation_id`, `user_id`, `notification_id`, `message`, `context`\n- **Sensitive data redaction**: Push tokens, email addresses, phone numbers \u2192 hash in logs\n- **Log levels**: DEBUG (disabled in production), INFO (request/response), WARN (retries, degraded mode), ERROR (failures)\n- **Correlation ID propagation**: Generated at gateway, passed through all services, included in all logs and traces\n- **Log sampling**: DEBUG and INFO sampled at 1% in production, WARN and ERROR always logged\n\n### Distributed Tracing\n- **Jaeger spans**:\n  - `notification.gateway.accept`: Request validation and idempotency check\n  - `notification.processing.enrich`: Preference lookup and routing\n  - `notification.batching.accumulate`: Adding to batch\n  - `notification.delivery.send`: External provider API call\n  - `notification.delivery.confirm`: Provider response validation\n- **Span attributes**: `user_id`, `notification_id`, `channel`, `batch_id`, `attempt_count`, `error_code`\n- **Trace sampling**: 100% for failed deliveries, 10% for successful deliveries, 1% for batched notifications\n\n### Dashboards\n1. **Real-time Delivery Dashboard**:\n   - Throughput by channel (req/sec)\n   - Success/failure rates\n   - Latency heatmap (p50, p95, p99)\n   - Circuit breaker status\n2. **Batching Dashboard**:\n   - Active batches by type\n   - Average batch size\n   - Batch flush reasons\n   - Batching efficiency (notifications saved)\n3. **Operations Dashboard**:\n   - Kafka consumer lag\n   - DLQ depth and trends\n   - Retry queue depth\n   - Provider rate limit usage\n4. **Business Dashboard**:\n   - Notifications by type and priority\n   - User engagement (delivery \u2192 action conversion)\n   - Device platform distribution\n   - Preference update trends\n\n## Security Considerations\n\n1. **Authentication & Authorization**:\n   - **API key-based auth** for service-to-service (internal services)\n   - **OAuth 2.0 + JWT** for user-facing APIs (user preference management, device registration)\n   - **mTLS** for internal gRPC communication\n   - **Scope-based authorization**: API keys have scopes (SEND_NOTIFICATION, MANAGE_PREFERENCES), enforced at gateway\n   - **Rate limiting by identity**: Per API key, per user_id, per IP address (layered defense)\n\n2. **Data Encryption**:\n   - **In-transit**: TLS 1.3 for all HTTP/gRPC, no plaintext communication\n   - **At-rest**: \n     - PostgreSQL transparent data encryption (TDE) with AES-256\n     - Redis encrypted persistence (snapshot + AOF)\n     - Kafka encryption at rest for all topics\n     - S3/GCS server-side encryption (SSE-KMS)\n   - **Push token encryption**: Device push tokens encrypted with application-level keys (AES-256-GCM), separate key per environment\n\n3. **Rate Limiting** (Token Bucket Algorithm):\n   - **Per API key**: 1000 req/min (burst: 1500)\n   - **Per user_id**: 100 notifications/hour (prevents spam)\n   - **Per IP**: 5000 req/min (DDoS protection)\n   - **Global**: 100K req/min (cluster capacity)\n   - Rate limit state stored in Redis with sliding window counters\n   - 429 response includes Retry-After header\n\n4. **PII Handling** (GDPR/CCPA Compliance):\n   - **Data minimization**: Store only necessary PII (user_id reference, not full profile)\n   - **User data deletion**: On user request, within 30 days:\n     - Delete from PostgreSQL (user preferences, device registrations)\n     - Delete from Redis cache\n     - Anonymize audit logs (replace user_id with hashed token)\n     - Delete from object storage archives\n   - **Audit trail**: Log all preference changes with timestamp and initiator\n   - **Data portability**: API endpoint to export user's notification history (JSON format)\n\n5. **Injection Prevention**:\n   - **Input validation**: All API requests validated against JSON schema\n   - **SQL injection**: Use parameterized queries exclusively (no string concatenation)\n   - **NoSQL injection**: Validate Redis keys, escape special characters\n\n6. **Secrets Management**:\n   - **External secrets store**: HashiCorp Vault or AWS Secrets Manager\n   - **Rotation policy**: FCM/APNs credentials rotated every 90 days\n   - **Least privilege**: Service accounts have minimal IAM permissions\n\n7. **Compliance**:\n   - **SOC 2 Type II**: Audit logging, access controls, encryption\n   - **GDPR**: Right to deletion, data portability, consent management\n   - **CCPA**: Data deletion, opt-out of notifications\n\n## What I Improved\n\n### 1. **Exactly-Once Delivery Semantics**\n- **Added**: Idempotency at every layer (gateway, processing, delivery, retry)\n- **Added**: Bloom filter for delivery deduplication (prevents duplicates from race conditions)\n- **Added**: Provider-level idempotency tokens (FCM messageId, APNs apns-id)\n- **Added**: Database locking for retry job claiming (SELECT FOR UPDATE SKIP LOCKED)\n- **Impact**: Eliminates duplicate notifications even during retries, network issues, or concurrent processing\n\n### 2. **Notification Loss Prevention**\n- **Added**: Database write BEFORE Kafka publish (transaction log pattern)\n- **Added**: Immutable audit log (never deleted, source of truth)\n- **Added**: Change Data Capture (CDC) as backup publishing path\n- **Added**: Kafka transactions + exactly-once semantics\n- **Impact**: Zero message loss even during Kafka outages or consumer crashes\n\n### 3. **Batching Edge Case Handling**\n- **Added**: Real-time preference change propagation to batching service\n- **Added**: Batch flush on preference change (user changes mind mid-batch)\n- **Added**: Per-device batching preferences (phone can batch, web gets real-time)\n- **Added**: Batch size limits with auto-flush (prevents overwhelming digests)\n- **Added**: TTL-aware batch commit (expired notifications removed before delivery)\n- **Added**: Quiet hours + batching coordination (smart scheduling for overnight digests)\n- **Impact**: Better user experience, respects user intent, prevents notification fatigue\n\n### 4. **Retry Logic Robustness**\n- **Added**: Jitter in exponential backoff (prevents thundering herd)\n- **Added**: Re-fetch user preferences between retries (preferences may have changed)\n- **Added**: Idempotent retry checks (verify not already delivered by another worker)\n- **Added**: Fallback channel handling (automatic escalation after original channel exhausts retries)\n- **Added**: Provider-specific error categorization (permanent vs. transient)\n- **Impact**: Higher delivery success rate, respects user preference changes, avoids wasted retry attempts\n\n### 5. **State Machine Clarity**\n- **Enhanced**: Delivery status with explicit state transitions (PENDING \u2192 IN_FLIGHT \u2192 DELIVERED/FAILED)\n- **Added**: Transactional state updates (database + Kafka event atomically)\n- **Added**: Lock ownership tracking (which worker is processing which retry)\n- **Impact**: Clear audit trail, prevents race conditions, enables debugging\n\n### 6. **Cross-Device User Experience**\n- **Added**: Per-device preference overrides (batching settings can differ by device)\n- **Added**: Device deactivation after repeated failures (prevents battery drain)\n- **Added**: Token refresh tracking (know when device tokens need updating)\n- **Impact**: Users get real-time on web, batched on mobile (flexible control)\n\n### 7. **Observability for Duplicates**\n- **Added**: `duplicate_blocked_count` metric (tracks idempotency effectiveness)\n- **Added**: Alert on duplicate sent (bloom filter failure)\n- **Added**: Correlation ID propagation through all services\n- **Impact**: Can detect and investigate any duplicate delivery incidents\n\n### 8. **Security Hardening**\n- **Added**: Push token encryption at rest (AES-256-GCM)\n- **Added**: Scope-based API key authorization\n- **Added**: mTLS for internal gRPC communication\n- **Added**: GDPR compliance details (data deletion, portability, anonymization)\n- **Impact**: Stronger security posture, regulatory compliance\n\n## What I Kept\n\n### Excellent Design Decisions from Original:\n1. **Event-driven architecture with Kafka** - Perfect for durability and replay\n2. **Separate channel services** - Independent scaling and failure isolation\n3. **Batching service as first-class component** - Directly addresses notification fatigue\n4. **Multi-layer caching strategy** - Reduces database load effectively\n5. **Priority-based routing** - Critical alerts bypass batching\n6. **Geographic distribution** - Low latency for global users\n7. **Circuit breaker per provider** - Prevents cascading failures\n8. **Dead letter queue** - Safety net for permanent failures\n9. **Comprehensive monitoring strategy** - Strong observability foundation\n10. **Rate limiting at multiple layers** - Defense in depth\n\n## Remaining Concerns\n\n### 1. **Batching Complexity vs. User Expectation**\n- **Trade-off**: Batching improves engagement but adds complexity\n- **Risk**: Users may not understand why some notifications are delayed\n- **Mitigation**: Clear UI communication about batching settings, \"Deliver Now\" button in app\n- **Question**: Should we A/B test batching effectiveness per user cohort?\n\n### 2. **Bloom Filter False Positives**\n- **Risk**: 0.01% false positive rate means ~100 notifications/day blocked incorrectly at scale\n- **Mitigation**: Track false positive metric, investigate spikes, tune filter size\n- **Alternative**: Use database check instead of bloom filter (higher latency, guaranteed accuracy)\n- **Recommendation**: Start with bloom filter, fallback to database if false positive rate > 0.1%\n\n### 3. **Cross-Region Consistency**\n- **Trade-off**: Eventual consistency for preferences (up to 30 seconds lag)\n- **Risk**: User changes preference in EU region, receives notification via US region with old preference\n- **Mitigation**: Route preference reads to home region (adds latency), or accept stale reads\n- **Question**: Is 30-second lag acceptable for preference updates?\n\n### 4. **CDC as Backup Publishing Path**\n- **Complexity**: Requires Debezium setup, schema registry, separate consumer pipeline\n- **Risk**: Increases operational overhead, potential for CDC lag\n- **Benefit**: True guarantee against message loss\n- **Recommendation**: Implement for critical notification types (ALERT, MESSAGE), optional for PROMOTIONAL\n\n### 5. **Cost at Scale**\n- **Concern**: 10M active users, 100M notifications/day = significant provider API costs\n- **Mitigation**: Batching reduces costs by 30-40%, but still expensive\n- **Optimization**: Implement notification priority auction (lower priority notifications wait for cheaper delivery slots)\n- **Question**: Should we prioritize cost or delivery speed for low-priority notifications?\n\n### 6. **Device Token Refresh Workflow**\n- **Gap**: Design doesn't specify how clients are notified to refresh tokens\n- **Solution needed**: When FCM/APNs returns InvalidRegistration, how do we tell app to re-register?\n- **Recommendation**: Return 410 on next API call from that device, client re-registers on 410 response\n\n### 7. **Batch Cancellation**\n- **Gap**: User deletes original content (e.g., post that generated notifications) while notifications are batching\n- **Solution needed**: Cancel API should also remove from pending batches\n- **Recommendation**: Add `cancel_from_batches` flag to CancelNotification API\n\n## Convergence Status\n**ITERATING** - First review round. Design is significantly improved with exactly-once semantics and edge case handling, but several concerns remain around operational complexity, cross-region consistency, and cost optimization trade-offs. Need architect's input on trade-offs and open questions.\n\n---\n\nPROMPT_FOR_ARCHITECT:\nPlease address the **cross-region consistency trade-off** for user preferences (30-second lag risk) and the **bloom filter false positive handling** strategy. Also, clarify the **device token refresh workflow** - how should clients be notified when their token becomes invalid? Finally, consider the **CDC backup publishing path** - is the operational complexity justified for all notification types, or should we scope it to critical notifications only?"
      }
    }
  ],
  "final_design": "",
  "status": "debating",
  "started_at": "2026-01-17T09:48:03.255981",
  "ended_at": "",
  "max_rounds": 4,
  "working_dir": ".",
  "current_round": 1,
  "round_times": [
    288.8201630115509
  ]
}