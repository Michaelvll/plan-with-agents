{
  "initial_prompt": "Design a simple task queue system with priority support",
  "status": "debating",
  "rounds": 2,
  "started_at": "2026-01-17T09:14:03.427099",
  "ended_at": "2026-01-17T09:19:33.059719",
  "final_design": "## Design\n\n# Priority Task Queue System with Bounded Memory & Smart Cleanup\n\n## Architecture Overview\n\nA production-ready priority task queue system with in-memory storage, automatic memory management, and flexible retry policies. Key improvements: bounded memory through automatic cleanup, heap health monitoring, and simplified worker API.\n\n## Core Components\n\n### 1. Task Model\n```python\nfrom enum import Enum\nfrom dataclasses import dataclass, field\nfrom typing import Optional, Any, Dict\nfrom datetime import datetime, timedelta\nimport uuid\n\nclass TaskStatus(Enum):\n    PENDING = \"pending\"\n    IN_PROGRESS = \"in_progress\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\nclass TaskPriority(Enum):\n    LOW = 1\n    MEDIUM = 2\n    HIGH = 3\n    CRITICAL = 4\n\n@dataclass\nclass Task:\n    id: str\n    payload: Any\n    priority: TaskPriority\n    status: TaskStatus\n    created_at: datetime\n    updated_at: datetime\n    started_at: Optional[datetime] = None\n    completed_at: Optional[datetime] = None\n    error_message: Optional[str] = None\n    retry_count: int = 0\n    max_retries: int = 3\n    sequence: int = 0\n    worker_id: Optional[str] = None\n    # Retry policy metadata\n    retry_policy: Optional[Dict[str, Any]] = None\n    \n    def is_terminal(self) -> bool:\n        \"\"\"Check if task is in a terminal state\"\"\"\n        return self.status in {TaskStatus.COMPLETED, TaskStatus.FAILED, \n                               TaskStatus.CANCELLED}\n    \n    def age_seconds(self) -> float:\n        \"\"\"Get task age in seconds\"\"\"\n        return (datetime.utcnow() - self.created_at).total_seconds()\n    \n    def time_in_progress_seconds(self) -> Optional[float]:\n        \"\"\"Get time spent in progress (None if not started)\"\"\"\n        if not self.started_at:\n            return None\n        end_time = self.completed_at or datetime.utcnow()\n        return (end_time - self.started_at).total_seconds()\n```\n\n### 2. Heap Health Monitor\n```python\n@dataclass\nclass HeapHealth:\n    \"\"\"Track heap efficiency for automatic maintenance\"\"\"\n    total_entries: int = 0\n    valid_entries: int = 0\n    invalid_entries: int = 0\n    last_rebuild: datetime = field(default_factory=datetime.utcnow)\n    \n    @property\n    def efficiency(self) -> float:\n        \"\"\"Percentage of valid entries in heap\"\"\"\n        if self.total_entries == 0:\n            return 1.0\n        return self.valid_entries / self.total_entries\n    \n    @property\n    def should_rebuild(self) -> bool:\n        \"\"\"Rebuild if efficiency drops below 50% and heap has 100+ entries\"\"\"\n        return (self.efficiency < 0.5 and self.total_entries >= 100)\n```\n\n### 3. Priority Queue Storage with Auto-Cleanup\n```python\nimport heapq\nimport threading\nfrom collections import defaultdict\nfrom typing import Set, Optional, Tuple\n\nclass PriorityTaskQueue:\n    def __init__(self, \n                 max_queue_size: Optional[int] = None,\n                 task_retention_seconds: int = 3600,\n                 cleanup_interval_seconds: int = 300):\n        \"\"\"\n        Args:\n            max_queue_size: Max pending tasks (None = unlimited)\n            task_retention_seconds: How long to keep completed/failed tasks (default 1hr)\n            cleanup_interval_seconds: How often to run cleanup (default 5min)\n        \"\"\"\n        self._lock = threading.RLock()\n        self._heap = []\n        self._tasks: Dict[str, Task] = {}\n        self._heap_task_ids: Set[str] = set()\n        self._counter = 0\n        self._max_queue_size = max_queue_size\n        self._task_retention = timedelta(seconds=task_retention_seconds)\n        self._cleanup_interval = timedelta(seconds=cleanup_interval_seconds)\n        self._last_cleanup = datetime.utcnow()\n        self._heap_health = HeapHealth()\n        \n    def enqueue(self, payload: Any, priority: TaskPriority = TaskPriority.MEDIUM,\n                max_retries: int = 3, retry_policy: Optional[Dict] = None) -> str:\n        \"\"\"Add task to queue, return task ID\"\"\"\n        with self._lock:\n            # Run cleanup opportunistically\n            self._maybe_cleanup()\n            \n            # Check queue size limit (only count PENDING)\n            pending_count = sum(1 for t in self._tasks.values() \n                              if t.status == TaskStatus.PENDING)\n            if self._max_queue_size and pending_count >= self._max_queue_size:\n                raise QueueFullError(f\"Queue at capacity: {self._max_queue_size}\")\n            \n            task = Task(\n                id=str(uuid.uuid4()),\n                payload=payload,\n                priority=priority,\n                status=TaskStatus.PENDING,\n                created_at=datetime.utcnow(),\n                updated_at=datetime.utcnow(),\n                max_retries=max_retries,\n                sequence=self._counter,\n                retry_policy=retry_policy\n            )\n            self._tasks[task.id] = task\n            self._add_to_heap(task)\n            return task.id\n    \n    def _add_to_heap(self, task: Task):\n        \"\"\"Add task to heap with health tracking\"\"\"\n        heapq.heappush(self._heap, (-task.priority.value, task.sequence, task.id))\n        self._heap_task_ids.add(task.id)\n        self._counter += 1\n        self._heap_health.total_entries += 1\n        self._heap_health.valid_entries += 1\n    \n    def dequeue(self, worker_id: Optional[str] = None) -> Optional[Task]:\n        \"\"\"Get highest priority pending task\"\"\"\n        with self._lock:\n            self._maybe_cleanup()\n            \n            found_valid = False\n            while self._heap:\n                _, _, task_id = heapq.heappop(self._heap)\n                self._heap_task_ids.discard(task_id)\n                self._heap_health.total_entries -= 1\n                \n                task = self._tasks.get(task_id)\n                \n                if not task or task.status != TaskStatus.PENDING:\n                    # Invalid entry, continue searching\n                    continue\n                \n                # Found valid task\n                found_valid = True\n                self._heap_health.valid_entries -= 1\n                task.status = TaskStatus.IN_PROGRESS\n                task.started_at = datetime.utcnow()\n                task.updated_at = datetime.utcnow()\n                task.worker_id = worker_id\n                \n                # Check if heap needs rebuilding\n                if self._heap_health.should_rebuild:\n                    self._rebuild_heap()\n                \n                return task\n            \n            # Heap exhausted\n            if not found_valid and self._heap_health.total_entries > 0:\n                # All remaining entries are invalid, rebuild\n                self._rebuild_heap()\n            \n            return None\n    \n    def _rebuild_heap(self):\n        \"\"\"Rebuild heap with only valid pending tasks\"\"\"\n        with self._lock:\n            valid_tasks = [t for t in self._tasks.values() \n                          if t.status == TaskStatus.PENDING]\n            \n            self._heap.clear()\n            self._heap_task_ids.clear()\n            \n            for task in valid_tasks:\n                heapq.heappush(self._heap, \n                             (-task.priority.value, task.sequence, task.id))\n                self._heap_task_ids.add(task.id)\n            \n            self._heap_health = HeapHealth(\n                total_entries=len(self._heap),\n                valid_entries=len(self._heap),\n                invalid_entries=0,\n                last_rebuild=datetime.utcnow()\n            )\n    \n    def _maybe_cleanup(self):\n        \"\"\"Run cleanup if enough time has passed\"\"\"\n        now = datetime.utcnow()\n        if now - self._last_cleanup > self._cleanup_interval:\n            self._cleanup_old_tasks()\n            self._last_cleanup = now\n    \n    def _cleanup_old_tasks(self):\n        \"\"\"Remove old terminal tasks to prevent unbounded memory growth\"\"\"\n        with self._lock:\n            now = datetime.utcnow()\n            cutoff = now - self._task_retention\n            \n            to_delete = []\n            for task_id, task in self._tasks.items():\n                if task.is_terminal() and task.updated_at < cutoff:\n                    to_delete.append(task_id)\n            \n            for task_id in to_delete:\n                del self._tasks[task_id]\n                # Note: task might still be in heap, but will be skipped during dequeue\n    \n    def get_task(self, task_id: str) -> Optional[Task]:\n        \"\"\"Retrieve task by ID (returns reference for internal use)\"\"\"\n        with self._lock:\n            return self._tasks.get(task_id)\n    \n    def update_status(self, task_id: str, status: TaskStatus, \n                     error_message: Optional[str] = None) -> Tuple[bool, bool]:\n        \"\"\"\n        Update task status with state validation.\n        Returns: (success, was_retried)\n        \"\"\"\n        with self._lock:\n            task = self._tasks.get(task_id)\n            if not task:\n                raise TaskNotFoundError(f\"Task {task_id} not found\")\n            \n            if not self._is_valid_transition(task.status, status):\n                raise InvalidStateTransitionError(\n                    f\"Cannot transition from {task.status.value} to {status.value}\"\n                )\n            \n            task.status = status\n            task.updated_at = datetime.utcnow()\n            \n            if status == TaskStatus.COMPLETED:\n                task.completed_at = datetime.utcnow()\n                return (True, False)\n            \n            elif status == TaskStatus.FAILED:\n                task.error_message = error_message\n                task.completed_at = datetime.utcnow()\n                task.retry_count += 1\n                \n                # Check if should retry (automatic based on max_retries)\n                if task.retry_count < task.max_retries:\n                    # Reset for retry\n                    task.status = TaskStatus.PENDING\n                    task.started_at = None\n                    task.completed_at = None\n                    task.worker_id = None\n                    \n                    # Add back to heap if not already present\n                    if task_id not in self._heap_task_ids:\n                        self._add_to_heap(task)\n                    \n                    return (True, True)\n                return (True, False)\n            \n            elif status == TaskStatus.CANCELLED:\n                task.completed_at = datetime.utcnow()\n                return (True, False)\n            \n            return (True, False)\n    \n    def _is_valid_transition(self, from_status: TaskStatus, \n                            to_status: TaskStatus) -> bool:\n        \"\"\"Validate state machine transitions\"\"\"\n        valid_transitions = {\n            TaskStatus.PENDING: {TaskStatus.IN_PROGRESS, TaskStatus.CANCELLED},\n            TaskStatus.IN_PROGRESS: {TaskStatus.COMPLETED, TaskStatus.FAILED, \n                                    TaskStatus.CANCELLED},\n            TaskStatus.FAILED: set(),\n            TaskStatus.COMPLETED: set(),\n            TaskStatus.CANCELLED: set(),\n        }\n        return to_status in valid_transitions.get(from_status, set())\n    \n    def cancel_task(self, task_id: str) -> bool:\n        \"\"\"Cancel a pending task\"\"\"\n        with self._lock:\n            task = self._tasks.get(task_id)\n            if not task:\n                return False\n            if task.status == TaskStatus.PENDING:\n                try:\n                    self.update_status(task_id, TaskStatus.CANCELLED)\n                    return True\n                except (TaskNotFoundError, InvalidStateTransitionError):\n                    return False\n            return False\n    \n    def requeue_task(self, task_id: str, reset_retries: bool = False) -> bool:\n        \"\"\"\n        Manually requeue a task (for worker failures or manual retry).\n        Only works for IN_PROGRESS tasks.\n        \"\"\"\n        with self._lock:\n            task = self._tasks.get(task_id)\n            if not task or task.status != TaskStatus.IN_PROGRESS:\n                return False\n            \n            task.status = TaskStatus.PENDING\n            task.started_at = None\n            task.worker_id = None\n            task.updated_at = datetime.utcnow()\n            \n            if reset_retries:\n                task.retry_count = 0\n                task.error_message = None\n            \n            if task_id not in self._heap_task_ids:\n                self._add_to_heap(task)\n            \n            return True\n    \n    def get_queue_stats(self) -> dict:\n        \"\"\"Get comprehensive queue statistics\"\"\"\n        with self._lock:\n            status_counts = defaultdict(int)\n            priority_counts = defaultdict(int)\n            \n            for task in self._tasks.values():\n                status_counts[task.status.value] += 1\n                if task.status == TaskStatus.PENDING:\n                    priority_counts[task.priority.name] += 1\n            \n            return {\n                \"total_tasks\": len(self._tasks),\n                \"pending\": status_counts[TaskStatus.PENDING.value],\n                \"in_progress\": status_counts[TaskStatus.IN_PROGRESS.value],\n                \"completed\": status_counts[TaskStatus.COMPLETED.value],\n                \"failed\": status_counts[TaskStatus.FAILED.value],\n                \"cancelled\": status_counts[TaskStatus.CANCELLED.value],\n                \"pending_by_priority\": dict(priority_counts),\n                \"heap_size\": len(self._heap),\n                \"heap_efficiency\": self._heap_health.efficiency,\n                \"last_cleanup\": self._last_cleanup.isoformat(),\n                \"last_heap_rebuild\": self._heap_health.last_rebuild.isoformat()\n            }\n    \n    def force_cleanup(self) -> dict:\n        \"\"\"Manually trigger cleanup and return results\"\"\"\n        with self._lock:\n            before_count = len(self._tasks)\n            self._cleanup_old_tasks()\n            after_count = len(self._tasks)\n            \n            before_heap = len(self._heap)\n            self._rebuild_heap()\n            after_heap = len(self._heap)\n            \n            return {\n                \"tasks_removed\": before_count - after_count,\n                \"heap_entries_removed\": before_heap - after_heap,\n                \"tasks_remaining\": after_count,\n                \"heap_size\": after_heap\n            }\n\n# Custom exceptions\nclass QueueFullError(Exception):\n    pass\n\nclass TaskNotFoundError(Exception):\n    pass\n\nclass InvalidStateTransitionError(Exception):\n    pass\n```\n\n### 4. Task Queue Manager Interface\n```python\nclass TaskQueueManager:\n    \"\"\"High-level interface for task queue operations\"\"\"\n    \n    def __init__(self, \n                 max_queue_size: Optional[int] = None,\n                 task_retention_seconds: int = 3600,\n                 cleanup_interval_seconds: int = 300):\n        \"\"\"\n        Args:\n            max_queue_size: Max pending tasks (None = unlimited)\n            task_retention_seconds: Keep terminal tasks for this long (default 1hr)\n            cleanup_interval_seconds: Run cleanup every N seconds (default 5min)\n        \"\"\"\n        self.queue = PriorityTaskQueue(\n            max_queue_size=max_queue_size,\n            task_retention_seconds=task_retention_seconds,\n            cleanup_interval_seconds=cleanup_interval_seconds\n        )\n    \n    def submit_task(self, \n                   payload: Any, \n                   priority: str = \"medium\",\n                   max_retries: int = 3,\n                   retry_policy: Optional[Dict] = None) -> dict:\n        \"\"\"\n        Submit a new task.\n        \n        Args:\n            payload: Task data\n            priority: \"low\", \"medium\", \"high\", or \"critical\"\n            max_retries: Max automatic retries (0 = no retry)\n            retry_policy: Optional metadata for worker-side retry logic\n                         e.g., {\"backoff\": \"exponential\", \"max_delay\": 300}\n        \"\"\"\n        try:\n            priority_enum = TaskPriority[priority.upper()]\n        except KeyError:\n            raise ValueError(f\"Invalid priority: {priority}. \"\n                           f\"Valid: {[p.name.lower() for p in TaskPriority]}\")\n        \n        if max_retries < 0:\n            raise ValueError(\"max_retries must be non-negative\")\n        \n        try:\n            task_id = self.queue.enqueue(payload, priority_enum, max_retries, \n                                        retry_policy)\n            return {\n                \"task_id\": task_id, \n                \"status\": \"pending\",\n                \"priority\": priority\n            }\n        except QueueFullError as e:\n            return {\"error\": str(e), \"status\": \"rejected\"}\n    \n    def get_next_task(self, worker_id: Optional[str] = None) -> Optional[dict]:\n        \"\"\"\n        Worker polls for next task.\n        Returns None if queue is empty.\n        \"\"\"\n        task = self.queue.dequeue(worker_id=worker_id)\n        if task:\n            return {\n                \"task_id\": task.id,\n                \"payload\": task.payload,\n                \"priority\": task.priority.name,\n                \"retry_count\": task.retry_count,\n                \"max_retries\": task.max_retries,\n                \"retry_policy\": task.retry_policy\n            }\n        return None\n    \n    def complete_task(self, task_id: str, success: bool = True, \n                     error: Optional[str] = None) -> dict:\n        \"\"\"\n        Mark task as completed or failed.\n        Automatic retry happens based on max_retries.\n        \"\"\"\n        try:\n            status = TaskStatus.COMPLETED if success else TaskStatus.FAILED\n            success_flag, was_retried = self.queue.update_status(\n                task_id, status, error\n            )\n            \n            return {\n                \"task_id\": task_id,\n                \"status\": status.value,\n                \"retried\": was_retried\n            }\n        except (TaskNotFoundError, InvalidStateTransitionError) as e:\n            return {\"error\": str(e)}\n    \n    def get_task_status(self, task_id: str) -> Optional[dict]:\n        \"\"\"Get detailed task status\"\"\"\n        task = self.queue.get_task(task_id)\n        if not task:\n            return None\n        \n        return {\n            \"task_id\": task.id,\n            \"status\": task.status.value,\n            \"priority\": task.priority.name,\n            \"created_at\": task.created_at.isoformat(),\n            \"started_at\": task.started_at.isoformat() if task.started_at else None,\n            \"completed_at\": task.completed_at.isoformat() if task.completed_at else None,\n            \"error_message\": task.error_message,\n            \"retry_count\": task.retry_count,\n            \"max_retries\": task.max_retries,\n            \"worker_id\": task.worker_id,\n            \"age_seconds\": task.age_seconds(),\n            \"time_in_progress\": task.time_in_progress_seconds()\n        }\n    \n    def cancel_task(self, task_id: str) -> dict:\n        \"\"\"Cancel a pending task\"\"\"\n        success = self.queue.cancel_task(task_id)\n        return {\n            \"task_id\": task_id,\n            \"cancelled\": success,\n            \"message\": \"Task cancelled\" if success \n                      else \"Task not pending or not found\"\n        }\n    \n    def requeue_task(self, task_id: str, reset_retries: bool = False) -> dict:\n        \"\"\"\n        Manually requeue an in-progress task.\n        Useful for worker failure recovery.\n        \"\"\"\n        success = self.queue.requeue_task(task_id, reset_retries)\n        return {\n            \"task_id\": task_id,\n            \"requeued\": success,\n            \"reset_retries\": reset_retries,\n            \"message\": \"Task requeued\" if success \n                      else \"Task not in-progress or not found\"\n        }\n    \n    def get_statistics(self) -> dict:\n        \"\"\"Get comprehensive queue statistics\"\"\"\n        return self.queue.get_queue_stats()\n    \n    def force_maintenance(self) -> dict:\n        \"\"\"\n        Manually trigger cleanup and heap rebuild.\n        Useful for testing or emergency maintenance.\n        \"\"\"\n        return self.queue.force_cleanup()\n```\n\n### 5. Health Monitor & Supervisor Utilities\n```python\nclass TaskSupervisor:\n    \"\"\"Monitor and recover stuck tasks\"\"\"\n    \n    def __init__(self, manager: TaskQueueManager, \n                 stuck_threshold_seconds: int = 300):\n        self.manager = manager\n        self.stuck_threshold = stuck_threshold_seconds\n    \n    def check_stuck_tasks(self) -> list[dict]:\n        \"\"\"Find tasks stuck in IN_PROGRESS state\"\"\"\n        stats = self.manager.get_statistics()\n        stuck_tasks = []\n        \n        # Would need to iterate tasks - simplified here\n        # In production, maintain a separate index of in-progress tasks\n        \n        return stuck_tasks\n    \n    def requeue_stuck_tasks(self) -> dict:\n        \"\"\"Automatically requeue stuck tasks\"\"\"\n        stuck = self.check_stuck_tasks()\n        requeued = []\n        \n        for task_info in stuck:\n            result = self.manager.requeue_task(task_info['task_id'])\n            if result['requeued']:\n                requeued.append(task_info['task_id'])\n        \n        return {\n            \"stuck_tasks_found\": len(stuck),\n            \"tasks_requeued\": len(requeued),\n            \"task_ids\": requeued\n        }\n```\n\n## Usage Examples\n\n### Basic Worker (Automatic Retry)\n```python\n# Initialize with bounded memory\nmanager = TaskQueueManager(\n    max_queue_size=10000,\n    task_retention_seconds=3600,  # Keep completed tasks for 1 hour\n    cleanup_interval_seconds=300   # Cleanup every 5 minutes\n)\n\n# Submit tasks with automatic retry\ntask1 = manager.submit_task(\n    {\"action\": \"send_email\", \"to\": \"user@example.com\"}, \n    priority=\"high\",\n    max_retries=3  # Will retry up to 3 times automatically\n)\n\n# Simple worker loop - retry is automatic\nworker_id = \"worker-1\"\nwhile True:\n    task = manager.get_next_task(worker_id=worker_id)\n    if task:\n        try:\n            result = process(task['payload'])\n            manager.complete_task(task['task_id'], success=True)\n        except Exception as e:\n            # Automatic retry based on max_retries\n            manager.complete_task(task['task_id'], success=False, error=str(e))\n    else:\n        time.sleep(1)\n```\n\n### Advanced Worker with Retry Policy Metadata\n```python\n# Submit task with retry policy hints for worker\ntask = manager.submit_task(\n    {\"action\": \"api_call\", \"url\": \"https://api.example.com\"},\n    priority=\"medium\",\n    max_retries=5,\n    retry_policy={\n        \"backoff\": \"exponential\",\n        \"base_delay\": 2,\n        \"max_delay\": 60,\n        \"retryable_errors\": [\"timeout\", \"rate_limit\"]\n    }\n)\n\n# Worker interprets retry_policy for smart delays\ndef smart_worker():\n    while True:\n        task = manager.get_next_task(worker_id=\"smart-worker\")\n        if not task:\n            time.sleep(1)\n            continue\n        \n        try:\n            result = process(task['payload'])\n            manager.complete_task(task['task_id'], success=True)\n        except Exception as e:\n            # Mark as failed (automatic retry happens in queue)\n            manager.complete_task(task['task_id'], success=False, error=str(e))\n            \n            # Worker can implement smart delay before next poll\n            if task.get('retry_policy'):\n                policy = task['retry_policy']\n                if policy.get('backoff') == 'exponential':\n                    delay = min(\n                        policy['base_delay'] ** task['retry_count'],\n                        policy['max_delay']\n                    )\n                    time.sleep(delay)\n```\n\n### Monitoring & Maintenance\n```python\n# Check queue health\nstats = manager.get_statistics()\nprint(f\"Heap efficiency: {stats['heap_efficiency']:.2%}\")\nprint(f\"Pending tasks: {stats['pending']}\")\nprint(f\"Tasks in progress: {stats['in_progress']}\")\n\n# Force maintenance if needed\nif stats['heap_efficiency'] < 0.3:\n    result = manager.force_maintenance()\n    print(f\"Cleaned up {result['tasks_removed']} old tasks\")\n    print(f\"Removed {result['heap_entries_removed']} stale heap entries\")\n\n# Supervisor for stuck task recovery\nsupervisor = TaskSupervisor(manager, stuck_threshold_seconds=300)\nrecovery = supervisor.requeue_stuck_tasks()\nprint(f\"Requeued {recovery['tasks_requeued']} stuck tasks\")\n```"
}