{
  "initial_prompt": "Design a rate limiting system using token bucket algorithm",
  "status": "debating",
  "rounds": 2,
  "started_at": "2026-01-17T10:06:48.271068",
  "ended_at": "2026-01-17T10:13:59.049774",
  "final_design": "## Design\n\n# Production-Grade Rate Limiting System Using Token Bucket Algorithm\n\n## Architecture Overview\n\nA distributed rate limiting system with three tiers optimized for accuracy, performance, and operational simplicity:\n1. **Client Library Layer**: Lightweight SDK with intelligent retry and header parsing\n2. **Rate Limiter Service Layer**: Stateless service with Redis-backed atomic operations\n3. **Storage Layer**: Redis (standalone or cluster) with carefully tuned consistency guarantees\n\n## Core Components\n\n### 1. Simplified Token Bucket with Redis-Only State\n\n```python\nclass TokenBucketState:\n    \"\"\"\n    Immutable representation of bucket state returned from Redis.\n    No local state management - Redis is the single source of truth.\n    \"\"\"\n    def __init__(\n        self,\n        tokens_available: float,\n        capacity: int,\n        refill_rate: float,\n        last_refill_time: float,\n        reset_time: float\n    ):\n        self.tokens_available = tokens_available\n        self.capacity = capacity\n        self.refill_rate = refill_rate\n        self.last_refill_time = last_refill_time\n        self.reset_time = reset_time\n    \n    def calculate_retry_after(self, tokens_requested: int) -> Optional[float]:\n        \"\"\"Calculate retry delay for denied requests.\"\"\"\n        if self.tokens_available >= tokens_requested:\n            return None\n        \n        tokens_needed = tokens_requested - self.tokens_available\n        return tokens_needed / self.refill_rate if self.refill_rate > 0 else None\n\n@dataclass\nclass BucketConfig:\n    \"\"\"Immutable bucket configuration with validation.\"\"\"\n    capacity: int\n    refill_rate: float  # tokens per second\n    min_refill_interval_ms: int = 10\n    \n    def __post_init__(self):\n        if self.capacity <= 0:\n            raise ConfigurationError(\"Capacity must be positive\")\n        if self.refill_rate < 0:\n            raise ConfigurationError(\"Refill rate cannot be negative\")\n        # Allow high refill rates - precision issues handled in Lua\n        if self.refill_rate > self.capacity * 1000:\n            raise ConfigurationError(\n                f\"Refill rate {self.refill_rate} exceeds capacity*1000 safety limit\"\n            )\n```\n\n### 2. Rate Limiter Service with Selective Caching\n\n```python\nclass RateLimiterService:\n    \"\"\"\n    Main service handling rate limit requests.\n    Uses selective caching only for reads (check without consume).\n    \"\"\"\n    def __init__(\n        self,\n        storage: StorageBackend,\n        config_provider: ConfigProvider,\n        fail_open: bool = True,\n        enable_read_cache: bool = True,\n        read_cache_ttl_ms: int = 50\n    ):\n        self.storage = storage\n        self.config_provider = config_provider\n        self.metrics = MetricsCollector()\n        self.fail_open = fail_open\n        \n        # Selective caching: ONLY for read-only checks (preview mode)\n        self.enable_read_cache = enable_read_cache\n        self.read_cache_ttl_ms = read_cache_ttl_ms\n        self.read_cache = TTLCache(maxsize=5000, ttl=read_cache_ttl_ms / 1000.0)\n        \n        # Simplified circuit breaker (per-instance is correct)\n        self.redis_healthy = True\n        self.last_health_check = time.time()\n        self.consecutive_failures = 0\n        self.failure_threshold = 5\n    \n    async def check_rate_limit(\n        self,\n        request: RateLimitRequest,\n        consume: bool = True  # False = preview mode (check only)\n    ) -> RateLimitResponse:\n        \"\"\"\n        Primary endpoint: check and optionally consume tokens.\n        \n        Args:\n            request: Rate limit request\n            consume: If False, only check limits without consuming (cacheable)\n        \"\"\"\n        start_time = time.time()\n        bucket_key = self._generate_key(request)\n        config = self.config_provider.get_config(request.scope, request.resource)\n        \n        # Check cache only for non-consuming reads\n        if not consume and self.enable_read_cache:\n            cached = self.read_cache.get(bucket_key)\n            if cached and cached['config_hash'] == self._hash_config(config):\n                self.metrics.cache_hit.inc()\n                return self._build_response_from_cache(cached, request)\n        \n        try:\n            # Check circuit breaker\n            if not self._is_redis_healthy():\n                return self._handle_storage_unavailable(request, config)\n            \n            # Execute atomic operation in Redis\n            result = await self.storage.refill_and_consume(\n                key=bucket_key,\n                config=config,\n                tokens_requested=request.tokens if consume else 0,\n                current_time=time.time()\n            )\n            \n            self._record_success()\n            \n            # Update cache only for non-consuming reads\n            if not consume and self.enable_read_cache:\n                self._update_read_cache(bucket_key, result, config)\n            \n            response = self._build_response(result, request, consume)\n            self.metrics.record_request(request, response, time.time() - start_time)\n            \n            return response\n            \n        except StorageError as e:\n            self._record_failure()\n            self.metrics.record_error(request, e)\n            return self._handle_storage_unavailable(request, config)\n    \n    async def check_hierarchical_limits(\n        self,\n        request: RateLimitRequest,\n        limit_hierarchy: List[Tuple[str, str]]  # [(scope, resource), ...]\n    ) -> HierarchicalRateLimitResponse:\n        \"\"\"\n        Check multiple rate limits in parallel, consume only if all pass.\n        \n        Example:\n            limit_hierarchy = [\n                ('user', 'api.search'),\n                ('ip', 'global'),\n                ('global', 'api.search')\n            ]\n        \n        This enables:\n        - User-specific limit: 1000 req/hour\n        - IP-based limit: 100 req/minute (DDoS protection)\n        - Global API limit: 1M req/hour (system protection)\n        \"\"\"\n        # Phase 1: Check all limits in parallel (no consumption)\n        check_tasks = []\n        for scope, resource in limit_hierarchy:\n            check_request = RateLimitRequest(\n                identifier=self._get_identifier_for_scope(request, scope),\n                resource=resource,\n                tokens=request.tokens,\n                scope=scope,\n                metadata=request.metadata\n            )\n            check_tasks.append(\n                self.check_rate_limit(check_request, consume=False)\n            )\n        \n        check_results = await asyncio.gather(*check_tasks)\n        \n        # Find first blocking limit\n        blocking_limit = None\n        for i, result in enumerate(check_results):\n            if not result.allowed:\n                blocking_limit = (limit_hierarchy[i], result)\n                break\n        \n        # Phase 2: If all pass, consume from all limits in parallel\n        if not blocking_limit:\n            consume_tasks = []\n            for scope, resource in limit_hierarchy:\n                consume_request = RateLimitRequest(\n                    identifier=self._get_identifier_for_scope(request, scope),\n                    resource=resource,\n                    tokens=request.tokens,\n                    scope=scope,\n                    metadata=request.metadata\n                )\n                consume_tasks.append(\n                    self.check_rate_limit(consume_request, consume=True)\n                )\n            \n            consume_results = await asyncio.gather(*consume_tasks)\n            \n            # Return most restrictive limit info\n            most_restrictive = min(\n                consume_results,\n                key=lambda r: r.tokens_remaining / r.tokens_capacity\n            )\n            \n            return HierarchicalRateLimitResponse(\n                allowed=True,\n                limit_results=dict(zip(limit_hierarchy, consume_results)),\n                blocking_limit=None,\n                most_restrictive_limit=most_restrictive\n            )\n        else:\n            return HierarchicalRateLimitResponse(\n                allowed=False,\n                limit_results=dict(zip(limit_hierarchy, check_results)),\n                blocking_limit=blocking_limit,\n                most_restrictive_limit=blocking_limit[1]\n            )\n    \n    def _is_redis_healthy(self) -> bool:\n        \"\"\"Simple per-instance health tracking.\"\"\"\n        # Fast path: check cached health status\n        if time.time() - self.last_health_check < 5.0:\n            return self.redis_healthy\n        \n        # Periodic health check (every 5 seconds)\n        self.last_health_check = time.time()\n        return self.redis_healthy\n    \n    def _record_success(self):\n        \"\"\"Record successful Redis operation.\"\"\"\n        self.consecutive_failures = 0\n        if not self.redis_healthy:\n            self.redis_healthy = True\n            self.metrics.redis_recovered.inc()\n    \n    def _record_failure(self):\n        \"\"\"Record failed Redis operation.\"\"\"\n        self.consecutive_failures += 1\n        if self.consecutive_failures >= self.failure_threshold:\n            if self.redis_healthy:\n                self.redis_healthy = False\n                self.metrics.redis_unhealthy.inc()\n    \n    def _handle_storage_unavailable(\n        self,\n        request: RateLimitRequest,\n        config: BucketConfig\n    ) -> RateLimitResponse:\n        \"\"\"Fallback when Redis is unavailable.\"\"\"\n        self.metrics.storage_fallback.inc()\n        \n        if self.fail_open:\n            return RateLimitResponse(\n                allowed=True,\n                tokens_remaining=config.capacity,\n                tokens_capacity=config.capacity,\n                retry_after_seconds=None,\n                reset_at=datetime.now() + timedelta(seconds=60),\n                degraded=True,\n                degraded_reason=\"storage_unavailable\"\n            )\n        else:\n            return RateLimitResponse(\n                allowed=False,\n                tokens_remaining=0,\n                tokens_capacity=config.capacity,\n                retry_after_seconds=60.0,\n                reset_at=datetime.now() + timedelta(seconds=60),\n                degraded=True,\n                degraded_reason=\"storage_unavailable\"\n            )\n    \n    def _hash_config(self, config: BucketConfig) -> str:\n        \"\"\"Generate stable hash of bucket configuration.\"\"\"\n        return hashlib.sha256(\n            f\"{config.capacity}:{config.refill_rate}\".encode()\n        ).hexdigest()[:16]\n    \n    def _update_read_cache(\n        self,\n        bucket_key: str,\n        result: TokenBucketState,\n        config: BucketConfig\n    ):\n        \"\"\"Update read-only cache.\"\"\"\n        self.read_cache[bucket_key] = {\n            'tokens_available': result.tokens_available,\n            'capacity': result.capacity,\n            'reset_time': result.reset_time,\n            'config_hash': self._hash_config(config),\n            'cached_at': time.time()\n        }\n```\n\n### 3. Enhanced Redis Storage with Separate Config Keys\n\n```python\nclass RedisStorageBackend:\n    \"\"\"\n    Redis storage backend using Lua scripts for atomicity.\n    Stores configuration separately from bucket state.\n    \"\"\"\n    \n    REFILL_AND_CONSUME_SCRIPT = \"\"\"\n    local bucket_key = KEYS[1]\n    local config_key = KEYS[2]\n    local tokens_requested = tonumber(ARGV[1])\n    local capacity = tonumber(ARGV[2])\n    local refill_rate = tonumber(ARGV[3])\n    local current_time = tonumber(ARGV[4])\n    local min_refill_interval_ms = tonumber(ARGV[5]) / 1000.0\n    local config_hash = ARGV[6]\n    \n    -- Get or initialize configuration\n    local stored_config_hash = redis.call('GET', config_key)\n    \n    if stored_config_hash and stored_config_hash ~= config_hash then\n        -- Config changed - reset bucket\n        redis.call('DEL', bucket_key)\n        stored_config_hash = nil\n    end\n    \n    if not stored_config_hash then\n        redis.call('SET', config_key, config_hash)\n        redis.call('EXPIRE', config_key, 86400)  -- 24 hours\n    end\n    \n    -- Get bucket state\n    local bucket = redis.call('HMGET', bucket_key, 'tokens', 'last_refill')\n    local tokens = tonumber(bucket[1])\n    local last_refill = tonumber(bucket[2])\n    \n    -- Initialize if bucket doesn't exist\n    if not tokens or not last_refill then\n        tokens = capacity\n        last_refill = current_time\n    end\n    \n    -- Clock skew protection\n    local time_elapsed = current_time - last_refill\n    if time_elapsed < -60 then\n        return redis.error_reply(\"CLOCK_SKEW:\" .. time_elapsed)\n    end\n    \n    -- Cap forward time leaps (prevent exploitation)\n    if time_elapsed > 3600 then\n        time_elapsed = 3600\n    end\n    \n    -- Refill tokens\n    local new_tokens = tokens\n    local new_last_refill = last_refill\n    \n    if time_elapsed >= min_refill_interval_ms then\n        local tokens_to_add = time_elapsed * refill_rate\n        new_tokens = math.min(capacity, tokens + tokens_to_add)\n        new_last_refill = current_time\n    end\n    \n    -- Consume tokens\n    local success = 0\n    local remaining_tokens = new_tokens\n    \n    if tokens_requested > 0 then\n        if new_tokens >= tokens_requested then\n            remaining_tokens = new_tokens - tokens_requested\n            success = 1\n        end\n    else\n        -- Read-only check (tokens_requested = 0)\n        success = 1\n    end\n    \n    -- Update state only if consuming\n    if tokens_requested > 0 then\n        redis.call('HMSET', bucket_key, \n            'tokens', remaining_tokens,\n            'last_refill', new_last_refill\n        )\n        \n        -- Dynamic TTL: time to refill to capacity + buffer\n        local ttl\n        if refill_rate > 0 then\n            local time_to_full = (capacity - remaining_tokens) / refill_rate\n            ttl = math.ceil(math.max(time_to_full * 2, 300))\n        else\n            ttl = 3600\n        end\n        redis.call('EXPIRE', bucket_key, ttl)\n    end\n    \n    -- Calculate retry_after\n    local retry_after = 0\n    if success == 0 and refill_rate > 0 then\n        local tokens_needed = tokens_requested - new_tokens\n        retry_after = math.ceil(tokens_needed / refill_rate * 1000) / 1000\n    end\n    \n    -- Calculate reset time\n    local reset_time = new_last_refill\n    if refill_rate > 0 then\n        reset_time = new_last_refill + (capacity - remaining_tokens) / refill_rate\n    else\n        reset_time = new_last_refill + 3600\n    end\n    \n    return {success, remaining_tokens, retry_after, capacity, new_last_refill, reset_time}\n    \"\"\"\n    \n    def __init__(self, redis_client: Redis):\n        self.redis = redis_client\n        self.script_sha = None\n    \n    async def refill_and_consume(\n        self,\n        key: str,\n        config: BucketConfig,\n        tokens_requested: int,\n        current_time: float\n    ) -> TokenBucketState:\n        \"\"\"Execute atomic refill and consume operation.\"\"\"\n        if not self.script_sha:\n            self.script_sha = await self.redis.script_load(\n                self.REFILL_AND_CONSUME_SCRIPT\n            )\n        \n        config_key = f\"{key}:config\"\n        config_hash = hashlib.sha256(\n            f\"{config.capacity}:{config.refill_rate}\".encode()\n        ).hexdigest()[:16]\n        \n        try:\n            result = await self.redis.evalsha(\n                self.script_sha,\n                2,  # number of keys\n                key,\n                config_key,\n                tokens_requested,\n                config.capacity,\n                config.refill_rate,\n                current_time,\n                config.min_refill_interval_ms,\n                config_hash\n            )\n            \n            success, tokens_remaining, retry_after, capacity, last_refill, reset_time = result\n            \n            return TokenBucketState(\n                tokens_available=tokens_remaining,\n                capacity=capacity,\n                refill_rate=config.refill_rate,\n                last_refill_time=last_refill,\n                reset_time=reset_time\n            )\n            \n        except redis.exceptions.ResponseError as e:\n            if \"CLOCK_SKEW\" in str(e):\n                raise ClockSkewError(f\"Clock skew detected: {e}\")\n            raise StorageError(f\"Redis script error: {e}\")\n        except redis.exceptions.RedisError as e:\n            raise StorageError(f\"Redis connection error: {e}\")\n```\n\n### 4. Configuration Provider with Hot Reload\n\n```python\nclass ConfigProvider:\n    \"\"\"\n    Provides rate limit configurations with hot reload support.\n    Configurations stored separately from runtime state.\n    \"\"\"\n    def __init__(self, config_source: str = \"config/rate_limits.yaml\"):\n        self.config_source = config_source\n        self.configs: Dict[Tuple[str, str], BucketConfig] = {}\n        self.last_reload = 0\n        self.reload_interval = 60  # seconds\n        self._load_configs()\n    \n    def get_config(self, scope: str, resource: str) -> BucketConfig:\n        \"\"\"Get configuration for scope and resource.\"\"\"\n        # Periodic reload\n        if time.time() - self.last_reload > self.reload_interval:\n            self._load_configs()\n        \n        key = (scope, resource)\n        if key not in self.configs:\n            # Fallback to default config\n            return BucketConfig(\n                capacity=1000,\n                refill_rate=10.0,\n                min_refill_interval_ms=10\n            )\n        \n        return self.configs[key]\n    \n    def _load_configs(self):\n        \"\"\"Load configurations from YAML file.\"\"\"\n        try:\n            with open(self.config_source) as f:\n                data = yaml.safe_load(f)\n            \n            new_configs = {}\n            for limit_config in data.get('rate_limits', []):\n                scope = limit_config['scope']\n                resource = limit_config.get('resource', 'default')\n                \n                config = BucketConfig(\n                    capacity=limit_config['capacity'],\n                    refill_rate=limit_config['refill_rate'],\n                    min_refill_interval_ms=limit_config.get('min_refill_interval_ms', 10)\n                )\n                \n                new_configs[(scope, resource)] = config\n            \n            self.configs = new_configs\n            self.last_reload = time.time()\n            \n        except Exception as e:\n            # Keep existing configs on error\n            logging.error(f\"Failed to reload configs: {e}\")\n```\n\n## Data Models\n\n```python\n@dataclass\nclass RateLimitRequest:\n    identifier: str  # user_id, ip_address, api_key\n    resource: str  # API endpoint or resource name\n    tokens: int = 1\n    scope: str = \"user\"  # user, ip, api_key, global\n    metadata: Dict[str, Any] = field(default_factory=dict)\n    \n    def __post_init__(self):\n        if self.tokens <= 0:\n            raise InvalidRequestError(\"Tokens must be positive\")\n        if self.tokens > 100000:\n            raise InvalidRequestError(\"Token request too large (max 100k)\")\n\n@dataclass\nclass RateLimitResponse:\n    allowed: bool\n    tokens_remaining: float\n    tokens_capacity: int\n    retry_after_seconds: Optional[float]\n    reset_at: datetime\n    degraded: bool = False\n    degraded_reason: Optional[str] = None\n    \n    def to_headers(self) -> Dict[str, str]:\n        \"\"\"Generate RFC 6585 compliant headers.\"\"\"\n        headers = {\n            \"X-RateLimit-Limit\": str(self.tokens_capacity),\n            \"X-RateLimit-Remaining\": str(int(self.tokens_remaining)),\n            \"X-RateLimit-Reset\": str(int(self.reset_at.timestamp())),\n        }\n        \n        if self.retry_after_seconds:\n            headers[\"Retry-After\"] = str(max(1, int(math.ceil(self.retry_after_seconds))))\n        \n        if self.degraded:\n            headers[\"X-RateLimit-Degraded\"] = \"true\"\n            if self.degraded_reason:\n                headers[\"X-RateLimit-Degraded-Reason\"] = self.degraded_reason\n        \n        return headers\n\n@dataclass\nclass HierarchicalRateLimitResponse:\n    \"\"\"Response for hierarchical rate limit checks.\"\"\"\n    allowed: bool\n    limit_results: Dict[Tuple[str, str], RateLimitResponse]\n    blocking_limit: Optional[Tuple[Tuple[str, str], RateLimitResponse]]\n    most_restrictive_limit: RateLimitResponse\n    \n    def to_headers(self) -> Dict[str, str]:\n        \"\"\"Return headers from most restrictive limit.\"\"\"\n        return self.most_restrictive_limit.to_headers()\n```\n\n## Storage Schema\n\n**Bucket State Keys:**\n```\nKey: ratelimit:{scope}:{identifier}:{resource}\nType: Hash\nFields:\n  - tokens: <float>         # Current available tokens\n  - last_refill: <float>    # Unix timestamp with decimals\nTTL: Dynamic (2x time_to_full, min 300s)\n```\n\n**Configuration Keys (Separate):**\n```\nKey: ratelimit:{scope}:{identifier}:{resource}:config\nType: String\nValue: <config_hash>        # SHA256 of capacity:refill_rate\nTTL: 86400 (24 hours)\n```\n\n## Deployment Architecture\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Client Apps    \u2502\n\u2502  (SDK with      \u2502\n\u2502   retry logic)  \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    HTTP/gRPC + TLS\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Load Balancer  \u2502\n\u2502  (L7 - Envoy)   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n    \u250c\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2510\n    \u2502         \u2502\n    \u25bc         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502RateLimit\u2502 \u2502RateLimit\u2502  (Stateless pods)\n\u2502Service  \u2502 \u2502Service  \u2502  (Selective read cache)\n\u2502  Pod 1  \u2502 \u2502  Pod 2  \u2502  (Per-instance health tracking)\n\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518\n     \u2502           \u2502\n     \u2514\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502 Redis         \u2502  (Standalone or Cluster)\n   \u2502 (AOF enabled) \u2502  (Strong consistency)\n   \u2502 (No Sentinel) \u2502  (Simple ops)\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n           \u2502\n           \u25bc\n   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n   \u2502  Prometheus   \u2502  (Metrics)\n   \u2502  + Grafana    \u2502  (Dashboards)\n   \u2502  + Alerts     \u2502  (On-call)\n   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n**Deployment Decisions:**\n\n1. **Standalone Redis First**: Start with standalone Redis (AOF persistence)\n   - Simpler operations, fewer failure modes\n   - 99.9% uptime with proper monitoring\n   - Scale to Redis Cluster only when needed (multi-region, >100k RPS)\n\n2. **Per-Instance Circuit Breaking**: Each pod tracks Redis health independently\n   - Simpler than shared circuit breaker (no Redis dependency)\n   - Fast local decisions (no network hop)\n   - Acceptable: different pods may disagree briefly\n\n3. **Selective Caching**: Only cache read-only checks (preview mode)\n   - 50ms TTL (tighter than 100ms)\n   - Never cache consuming operations\n   - Bounded inaccuracy: max 50ms stale data\n\n4. **Stateless Services**: Zero local state except read cache\n   - Easy horizontal scaling\n   - No state synchronization needed\n   - Clean pod restarts\n\n## Configuration Example\n\n```yaml\n# rate_limits.yaml\nrate_limits:\n  # User-level limits\n  - name: api_standard_user\n    scope: user\n    resource: api.search\n    capacity: 1000\n    refill_rate: 10.0  # 10/sec = 36k/hour\n    min_refill_interval_ms: 10\n    \n  - name: api_premium_user\n    scope: user\n    resource: api.search\n    capacity: 10000\n    refill_rate: 100.0\n    min_refill_interval_ms: 10\n    \n  # IP-based limits (DDoS protection)\n  - name: ip_global_limit\n    scope: ip\n    resource: global\n    capacity: 100\n    refill_rate: 5.0  # 5/sec = 300/min\n    min_refill_interval_ms: 50\n    \n  # Global system limits\n  - name: global_api_limit\n    scope: global\n    resource: api.search\n    capacity: 1000000\n    refill_rate: 10000.0  # 10k/sec = 36M/hour\n    min_refill_interval_ms: 10\n\n# Runtime configuration\nruntime:\n  fail_open: true\n  enable_read_cache: true\n  read_cache_ttl_ms: 50\n  redis:\n    mode: standalone  # Start simple\n    host: redis.default.svc.cluster.local\n    port: 6379\n    db: 0\n    connection_pool_size: 50\n    socket_timeout_ms: 1000\n    socket_connect_timeout_ms: 1000\n  health_check:\n    failure_threshold: 5\n    check_interval_seconds: 5\n```\n\n## API Usage Example\n\n### Hierarchical Rate Limiting\n\n```python\n# Example: Check user + IP + global limits\nresponse = await rate_limiter.check_hierarchical_limits(\n    request=RateLimitRequest(\n        identifier=\"user_12345\",\n        resource=\"api.search\",\n        tokens=1,\n        scope=\"user\",\n        metadata={\"ip_address\": \"203.0.113.42\"}\n    ),\n    limit_hierarchy=[\n        ('user', 'api.search'),      # User-specific: 1000/hour\n        ('ip', 'global'),             # IP-based: 100/min\n        ('global', 'api.search')      # Global: 1M/hour\n    ]\n)\n\nif response.allowed:\n    # Process request\n    return response.most_restrictive_limit.to_headers()\nelse:\n    # Deny request with info about blocking limit\n    scope, resource = response.blocking_limit[0]\n    limit_response = response.blocking_limit[1]\n    \n    return {\n        \"error\": \"rate_limit_exceeded\",\n        \"blocking_limit\": f\"{scope}:{resource}\",\n        \"retry_after\": limit_response.retry_after_seconds\n    }, 429, limit_response.to_headers()\n```"
}