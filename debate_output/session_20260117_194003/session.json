{
  "initial_prompt": "Design a simple task queue system for background job processing\n\nRequirements:\n- Support job submission with priority levels\n- Handle retries on failure with exponential backoff\n- Provide job status tracking (pending, running, completed, failed)\n- Single-server deployment (no distributed requirements)\n- Use SQLite for persistence\n- Python implementation",
  "status": "debating",
  "rounds": 4,
  "started_at": "2026-01-17T19:40:03.257414",
  "ended_at": "2026-01-17T20:10:23.270303",
  "final_design": "## Design\n\n# Task Queue System for Background Job Processing\n\n## Architecture Overview\n\n**Three-Layer Architecture with Version-Based Lease Authority:**\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502   Client Code   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  TaskQueue API  \u2502\n\u2502  (Public Facade)\u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 Queue Manager   \u2502\u2500\u2500\u2500\u2500\u25b6\u2502   SQLite DB      \u2502\n\u2502 (Job Lifecycle) \u2502     \u2502 (WAL Mode)       \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n         \u2502\n         \u25bc\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502  Worker Pool    \u2502\n\u2502  (Thread-based) \u2502\n\u2502  + Lease Auth   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Data Models\n\n### Database Schema (SQLite with WAL)\n\n```sql\n-- Enable WAL mode for concurrent reads/writes\nPRAGMA journal_mode=WAL;\nPRAGMA busy_timeout=5000;\nPRAGMA synchronous=NORMAL;\n\nCREATE TABLE jobs (\n    job_id TEXT PRIMARY KEY,\n    task_name TEXT NOT NULL,\n    priority INTEGER NOT NULL DEFAULT 0,\n    status TEXT NOT NULL CHECK(status IN ('pending', 'running', 'completed', 'failed', 'cancelled')),\n    payload BLOB NOT NULL,\n    result BLOB,\n    error_message TEXT,\n    created_at REAL NOT NULL,\n    started_at REAL,\n    completed_at REAL,\n    retry_count INTEGER DEFAULT 0,\n    max_retries INTEGER DEFAULT 3,\n    next_retry_at REAL,\n    timeout_seconds REAL,\n    \n    -- Lease authority with monotonic version counter\n    claimed_by TEXT,\n    claimed_at REAL,\n    lease_expires_at REAL,\n    lease_token TEXT,\n    lease_version INTEGER DEFAULT 0,  -- Source of truth for lease ownership\n    last_heartbeat_at REAL,\n    \n    CONSTRAINT valid_priority CHECK(priority BETWEEN 0 AND 10)\n);\n\n-- Composite index for efficient job claiming\nCREATE INDEX idx_claimable_jobs ON jobs(status, priority DESC, created_at ASC)\n    WHERE status IN ('pending', 'failed');\n\n-- Index for lease expiration checks\nCREATE INDEX idx_lease_expiration ON jobs(lease_expires_at)\n    WHERE status = 'running' AND lease_expires_at IS NOT NULL;\n\n-- Index for retry scheduling\nCREATE INDEX idx_retry_ready ON jobs(next_retry_at)\n    WHERE status = 'failed' AND next_retry_at IS NOT NULL;\n\nCREATE TABLE job_history (\n    id INTEGER PRIMARY KEY AUTOINCREMENT,\n    job_id TEXT NOT NULL,\n    timestamp REAL NOT NULL,\n    old_status TEXT,\n    new_status TEXT NOT NULL,\n    event_type TEXT NOT NULL,  -- 'created', 'claimed', 'completed', 'failed', 'heartbeat', 'recovered', 'version_mismatch', 'abandoned'\n    message TEXT,\n    worker_name TEXT,\n    lease_version INTEGER,\n    FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE\n);\n\nCREATE INDEX idx_job_history_lookup ON job_history(job_id, timestamp DESC);\n\n-- Weekly archive partitions (optimal for 7-90 day retention)\nCREATE TABLE job_archive (\n    job_id TEXT PRIMARY KEY,\n    task_name TEXT NOT NULL,\n    status TEXT NOT NULL,\n    created_at REAL NOT NULL,\n    completed_at REAL NOT NULL,\n    archive_partition TEXT NOT NULL,  -- Format: YYYY-WW for weekly partitions\n    result_summary TEXT,\n    error_message TEXT,\n    retry_count INTEGER,\n    execution_duration_seconds REAL,\n    FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE\n);\n\n-- Weekly partitions balance precision vs overhead\nCREATE INDEX idx_archive_partition ON job_archive(archive_partition, completed_at);\nCREATE INDEX idx_archive_task_performance ON job_archive(task_name, execution_duration_seconds);\n\nCREATE TABLE dead_letter_queue (\n    job_id TEXT PRIMARY KEY,\n    task_name TEXT NOT NULL,\n    priority INTEGER NOT NULL,\n    payload BLOB NOT NULL,\n    error_message TEXT,\n    failure_count INTEGER NOT NULL,\n    first_failed_at REAL NOT NULL,\n    last_failed_at REAL NOT NULL,\n    moved_at REAL NOT NULL,\n    FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE\n);\n\nCREATE TABLE cleanup_metadata (\n    key TEXT PRIMARY KEY,\n    last_cleanup_at REAL NOT NULL,\n    records_cleaned INTEGER NOT NULL,\n    cleanup_duration_seconds REAL,\n    partition_cleaned TEXT\n);\n\n-- NEW: Throughput monitoring table\nCREATE TABLE performance_metrics (\n    metric_date TEXT PRIMARY KEY,  -- YYYY-MM-DD\n    jobs_submitted INTEGER DEFAULT 0,\n    jobs_completed INTEGER DEFAULT 0,\n    jobs_failed INTEGER DEFAULT 0,\n    avg_execution_time REAL,\n    p95_execution_time REAL,\n    max_queue_depth INTEGER DEFAULT 0,\n    last_updated REAL NOT NULL\n);\n```\n\n### Python Data Classes\n\n```python\nfrom dataclasses import dataclass, field\nfrom enum import Enum\nfrom typing import Any, Callable, Optional, Dict\nimport time\nimport uuid\n\nclass JobStatus(Enum):\n    PENDING = \"pending\"\n    RUNNING = \"running\"\n    COMPLETED = \"completed\"\n    FAILED = \"failed\"\n    CANCELLED = \"cancelled\"\n\n@dataclass\nclass Job:\n    job_id: str\n    task_name: str\n    func: Callable\n    args: tuple\n    kwargs: dict\n    priority: int = 0\n    status: JobStatus = JobStatus.PENDING\n    result: Optional[Any] = None\n    error_message: Optional[str] = None\n    created_at: float = field(default_factory=time.time)\n    started_at: Optional[float] = None\n    completed_at: Optional[float] = None\n    retry_count: int = 0\n    max_retries: int = 3\n    next_retry_at: Optional[float] = None\n    timeout_seconds: Optional[float] = None\n    claimed_by: Optional[str] = None\n    claimed_at: Optional[float] = None\n    lease_expires_at: Optional[float] = None\n    lease_token: Optional[str] = None\n    lease_version: int = 0\n    last_heartbeat_at: Optional[float] = None\n    \n    def __post_init__(self):\n        if not (0 <= self.priority <= 10):\n            raise ValueError(\"Priority must be between 0 and 10\")\n\n@dataclass\nclass JobResult:\n    job_id: str\n    status: JobStatus\n    result: Optional[Any] = None\n    error_message: Optional[str] = None\n    created_at: float = None\n    completed_at: Optional[float] = None\n    retry_count: int = 0\n    next_retry_at: Optional[float] = None\n    \n    @property\n    def is_done(self) -> bool:\n        return self.status in (JobStatus.COMPLETED, JobStatus.CANCELLED)\n    \n    @property\n    def is_terminal(self) -> bool:\n        return self.status in (JobStatus.COMPLETED, JobStatus.CANCELLED) or \\\n               (self.status == JobStatus.FAILED and self.next_retry_at is None)\n    \n    @property\n    def will_retry(self) -> bool:\n        return self.status == JobStatus.FAILED and self.next_retry_at is not None\n\n@dataclass\nclass LeaseContext:\n    \"\"\"\n    Lease authority token with version-based ownership verification.\n    \n    The lease_version is the SOURCE OF TRUTH for ownership. SQLite's\n    atomic UPDATE WHERE lease_version = ? ensures only one worker\n    can successfully write results, even under network partitions.\n    \"\"\"\n    job_id: str\n    lease_token: str\n    lease_version: int  # Monotonic counter, never reused\n    worker_name: str\n    claimed_at: float = field(default_factory=time.time)\n    last_heartbeat_success: float = field(default_factory=time.time)\n    consecutive_heartbeat_failures: int = 0\n    is_valid: bool = True\n    abandon_reason: Optional[str] = None\n    \n    def record_heartbeat_success(self):\n        \"\"\"Reset failure counter on successful heartbeat.\"\"\"\n        self.last_heartbeat_success = time.time()\n        self.consecutive_heartbeat_failures = 0\n    \n    def record_heartbeat_failure(self, threshold: int) -> bool:\n        \"\"\"\n        Increment failure counter and check if threshold exceeded.\n        \n        Returns True if lease should be abandoned.\n        \"\"\"\n        self.consecutive_heartbeat_failures += 1\n        return self.consecutive_heartbeat_failures >= threshold\n    \n    def abandon(self, reason: str):\n        \"\"\"Mark lease as abandoned (worker stops trying to write results).\"\"\"\n        self.is_valid = False\n        self.abandon_reason = reason\n```\n\n## Core Interfaces\n\n### 1. TaskQueue API (Public Interface)\n\n```python\nimport threading\nimport logging\nfrom typing import Callable, Optional, Any, Dict\n\nclass TaskQueue:\n    \"\"\"Main interface for background job processing.\"\"\"\n    \n    def __init__(self, \n                 db_path: str = \"taskqueue.db\",\n                 num_workers: int = 4,\n                 poll_interval: float = 0.5,\n                 lease_duration: float = 300.0,\n                 heartbeat_interval: float = 30.0,\n                 heartbeat_failure_threshold: int = 3,\n                 enable_recovery: bool = True,\n                 retention_days: int = 7,\n                 cleanup_interval_hours: int = 12,\n                 cleanup_batch_size: int = 500,\n                 max_write_retries: int = 3):\n        \"\"\"\n        Args:\n            db_path: Path to SQLite database\n            num_workers: Number of worker threads\n            poll_interval: Seconds between queue polls\n            lease_duration: Seconds before a claimed job lease expires\n            heartbeat_interval: Seconds between lease renewals (30s)\n            heartbeat_failure_threshold: Consecutive heartbeat failures before abandoning (3 = ~90s tolerance)\n            enable_recovery: Reset expired leases on startup\n            retention_days: Days to retain completed/failed jobs (0=disable cleanup)\n            cleanup_interval_hours: Hours between cleanup runs (12h = twice daily)\n            cleanup_batch_size: Number of jobs to delete per transaction (500 = balance throughput vs blocking)\n            max_write_retries: Times to retry on SQLITE_BUSY (3 = handle brief contention)\n        \"\"\"\n        self.db = DatabaseManager(db_path, max_write_retries=max_write_retries)\n        self.queue_manager = QueueManager(\n            self.db, \n            lease_duration=lease_duration,\n            heartbeat_interval=heartbeat_interval,\n            heartbeat_failure_threshold=heartbeat_failure_threshold\n        )\n        self.worker_pool = WorkerPool(\n            num_workers, \n            self.queue_manager, \n            poll_interval\n        )\n        \n        if enable_recovery:\n            recovered = self.queue_manager.recover_expired_leases()\n            if recovered > 0:\n                logging.warning(f\"Recovered {recovered} jobs with expired leases on startup\")\n        \n        self.worker_pool.start()\n        self._shutdown = False\n        self._shutdown_lock = threading.Lock()\n        \n        # Background cleanup thread\n        if retention_days > 0:\n            self._cleanup_thread = threading.Thread(\n                target=self._cleanup_loop,\n                args=(retention_days, cleanup_interval_hours, cleanup_batch_size),\n                daemon=True,\n                name=\"TaskQueue-Cleanup\"\n            )\n            self._cleanup_thread.start()\n    \n    def submit(self,\n               func: Callable,\n               *args,\n               priority: int = 0,\n               max_retries: int = 3,\n               timeout_seconds: Optional[float] = None,\n               task_name: Optional[str] = None,\n               job_id: Optional[str] = None,\n               **kwargs) -> str:\n        \"\"\"Submit a job to the queue.\"\"\"\n        with self._shutdown_lock:\n            if self._shutdown:\n                raise RuntimeError(\"Queue is shut down\")\n        \n        if job_id is None:\n            job_id = str(uuid.uuid4())\n        \n        if task_name is None:\n            task_name = func.__name__\n        \n        job = Job(\n            job_id=job_id,\n            task_name=task_name,\n            func=func,\n            args=args,\n            kwargs=kwargs,\n            priority=priority,\n            max_retries=max_retries,\n            timeout_seconds=timeout_seconds\n        )\n        \n        return self.queue_manager.enqueue(job)\n    \n    def get_status(self, job_id: str) -> JobResult:\n        \"\"\"Get current job status and result.\"\"\"\n        return self.queue_manager.get_job_status(job_id)\n    \n    def get_result(self, job_id: str, timeout: Optional[float] = None) -> Any:\n        \"\"\"Block until job completes and return result.\"\"\"\n        start_time = time.time()\n        poll_interval = 0.1\n        \n        while True:\n            status = self.get_status(job_id)\n            \n            if status.status == JobStatus.COMPLETED:\n                return status.result\n            elif status.status == JobStatus.CANCELLED:\n                raise JobCancelledError(f\"Job {job_id} was cancelled\")\n            elif status.is_terminal and status.status == JobStatus.FAILED:\n                raise JobFailedError(f\"Job {job_id} failed: {status.error_message}\")\n            \n            if timeout and (time.time() - start_time) >= timeout:\n                raise TimeoutError(f\"Job {job_id} did not complete within {timeout}s\")\n            \n            time.sleep(poll_interval)\n    \n    def cancel(self, job_id: str) -> bool:\n        \"\"\"Cancel a pending/failed job (not running).\"\"\"\n        return self.queue_manager.cancel_job(job_id)\n    \n    def get_queue_stats(self) -> Dict[str, int]:\n        \"\"\"Get queue statistics by status.\"\"\"\n        return self.queue_manager.get_stats()\n    \n    def shutdown(self, wait: bool = True, timeout: float = 30.0):\n        \"\"\"Stop accepting jobs and shut down workers.\"\"\"\n        with self._shutdown_lock:\n            if self._shutdown:\n                return\n            self._shutdown = True\n        \n        self.worker_pool.shutdown(wait, timeout)\n        self.db.close_all_connections()\n    \n    def _cleanup_loop(self, retention_days: int, cleanup_interval_hours: int, batch_size: int):\n        \"\"\"Background thread for cleaning old jobs.\"\"\"\n        cleanup_interval = cleanup_interval_hours * 3600\n        \n        while not self._shutdown:\n            try:\n                time.sleep(cleanup_interval)\n                if not self._shutdown:\n                    self.queue_manager.cleanup_old_jobs(retention_days, batch_size)\n            except Exception as e:\n                logging.error(f\"Cleanup error: {e}\", exc_info=True)\n```\n\n### 2. QueueManager (Internal)\n\n```python\nimport pickle\nimport random\nimport time\nfrom typing import Optional\nfrom datetime import datetime\n\nclass QueueManager:\n    \"\"\"Manages job lifecycle and database operations.\"\"\"\n    \n    def __init__(self, db: 'DatabaseManager', \n                 lease_duration: float = 300.0,\n                 heartbeat_interval: float = 30.0,\n                 heartbeat_failure_threshold: int = 3):\n        self.db = db\n        self.lease_duration = lease_duration\n        self.heartbeat_interval = heartbeat_interval\n        self.heartbeat_failure_threshold = heartbeat_failure_threshold\n    \n    def enqueue(self, job: Job) -> str:\n        \"\"\"Persist job to database.\"\"\"\n        try:\n            self.db.insert_job(job)\n            return job.job_id\n        except Exception as e:\n            raise ValueError(f\"Failed to enqueue job: {e}\")\n    \n    def get_next_job(self, worker_name: str) -> Optional[Job]:\n        \"\"\"\n        Claim next available job using atomic version-increment mechanism.\n        \n        CRITICAL CORRECTNESS PROPERTY:\n        The lease_version increment happens inside a BEGIN IMMEDIATE transaction,\n        ensuring that:\n        1. Only ONE worker can hold version N at any time\n        2. Once version increments to N+1, version N is permanently invalidated\n        3. Any writes using version N will fail atomically (WHERE clause mismatch)\n        \n        This provides TOTAL ORDERING of lease generations.\n        \"\"\"\n        lease_token = str(uuid.uuid4())\n        now = time.time()\n        lease_expires_at = now + self.lease_duration\n        \n        with self.db.get_connection(timeout=2.0) as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                # Find claimable job (prioritize: pending > expired leases > failed ready for retry)\n                cursor = conn.execute(\"\"\"\n                    SELECT job_id, task_name, priority, status, payload,\n                           retry_count, max_retries, timeout_seconds,\n                           created_at, error_message, next_retry_at,\n                           lease_expires_at, lease_version\n                    FROM jobs\n                    WHERE (\n                        (status = 'pending')\n                        OR (status = 'failed' AND next_retry_at IS NOT NULL AND next_retry_at <= ?)\n                        OR (status = 'running' AND lease_expires_at IS NOT NULL AND lease_expires_at < ?)\n                    )\n                    ORDER BY \n                        CASE \n                            WHEN status = 'pending' THEN 0\n                            WHEN status = 'running' THEN 1\n                            ELSE 2\n                        END,\n                        priority DESC,\n                        created_at ASC\n                    LIMIT 1\n                \"\"\", (now, now))\n                \n                row = cursor.fetchone()\n                if not row:\n                    conn.execute(\"ROLLBACK\")\n                    return None\n                \n                job_id = row['job_id']\n                old_status = row['status']\n                old_version = row['lease_version'] or 0\n                new_version = old_version + 1  # CRITICAL: Increment creates new lease generation\n                \n                # Atomic claim with version increment (source of truth for ownership)\n                cursor = conn.execute(\"\"\"\n                    UPDATE jobs\n                    SET status = 'running',\n                        started_at = ?,\n                        claimed_by = ?,\n                        claimed_at = ?,\n                        lease_expires_at = ?,\n                        lease_token = ?,\n                        lease_version = ?,\n                        last_heartbeat_at = ?\n                    WHERE job_id = ?\n                      AND (\n                          status = 'pending'\n                          OR (status = 'failed' AND next_retry_at <= ?)\n                          OR (status = 'running' AND lease_expires_at < ?)\n                      )\n                \"\"\", (now, worker_name, now, lease_expires_at, \n                      lease_token, new_version, now, job_id, now, now))\n                \n                if cursor.rowcount == 0:\n                    conn.execute(\"ROLLBACK\")\n                    return None\n                \n                # Fetch full updated job state\n                cursor = conn.execute(\"SELECT * FROM jobs WHERE job_id = ?\", (job_id,))\n                job_row = cursor.fetchone()\n                \n                event_type = 'recovered' if old_status == 'running' else 'claimed'\n                self.db._add_history(\n                    conn, job_id, old_status, JobStatus.RUNNING,\n                    event_type,\n                    f\"{'Recovered' if old_status == 'running' else 'Claimed'} by {worker_name} (v{old_version}\u2192v{new_version})\",\n                    worker_name,\n                    new_version\n                )\n                \n                conn.execute(\"COMMIT\")\n                \n                # Reconstruct Job object\n                func, args, kwargs = pickle.loads(job_row['payload'])\n                return Job(\n                    job_id=job_row['job_id'],\n                    task_name=job_row['task_name'],\n                    func=func,\n                    args=args,\n                    kwargs=kwargs,\n                    priority=job_row['priority'],\n                    status=JobStatus.RUNNING,\n                    retry_count=job_row['retry_count'],\n                    max_retries=job_row['max_retries'],\n                    timeout_seconds=job_row['timeout_seconds'],\n                    created_at=job_row['created_at'],\n                    started_at=job_row['started_at'],\n                    claimed_by=job_row['claimed_by'],\n                    claimed_at=job_row['claimed_at'],\n                    lease_token=job_row['lease_token'],\n                    lease_version=job_row['lease_version'],\n                    lease_expires_at=job_row['lease_expires_at'],\n                    last_heartbeat_at=job_row['last_heartbeat_at']\n                )\n                \n            except sqlite3.OperationalError as e:\n                conn.execute(\"ROLLBACK\")\n                return None\n            except Exception as e:\n                conn.execute(\"ROLLBACK\")\n                logging.error(f\"Error claiming job: {e}\", exc_info=True)\n                return None\n    \n    def extend_lease(self, lease_ctx: LeaseContext) -> bool:\n        \"\"\"\n        Extend lease for long-running jobs with version verification.\n        \n        CORRECTNESS: Checks lease_version to ensure heartbeat is from\n        current lease holder. Stale workers fail silently here.\n        \"\"\"\n        with self.db.get_connection(timeout=5.0) as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                now = time.time()\n                new_expiration = now + self.lease_duration\n                \n                # Verify version AND token together (defense in depth)\n                cursor = conn.execute(\"\"\"\n                    UPDATE jobs\n                    SET lease_expires_at = ?,\n                        last_heartbeat_at = ?\n                    WHERE job_id = ?\n                      AND lease_token = ?\n                      AND lease_version = ?\n                      AND status = 'running'\n                \"\"\", (new_expiration, now, lease_ctx.job_id, \n                      lease_ctx.lease_token, lease_ctx.lease_version))\n                \n                if cursor.rowcount == 0:\n                    conn.execute(\"ROLLBACK\")\n                    return False\n                \n                self.db._add_history(\n                    conn, lease_ctx.job_id, 'running', JobStatus.RUNNING,\n                    'heartbeat',\n                    f\"Lease extended by {lease_ctx.worker_name} (v{lease_ctx.lease_version})\",\n                    lease_ctx.worker_name,\n                    lease_ctx.lease_version\n                )\n                \n                conn.execute(\"COMMIT\")\n                return True\n                \n            except sqlite3.OperationalError as e:\n                conn.execute(\"ROLLBACK\")\n                logging.warning(f\"Heartbeat failed for job {lease_ctx.job_id}: database locked\")\n                return False\n            except Exception as e:\n                conn.execute(\"ROLLBACK\")\n                logging.error(f\"Heartbeat error for job {lease_ctx.job_id}: {e}\", exc_info=True)\n                return False\n    \n    def mark_completed(self, job_id: str, result: Any, \n                       lease_ctx: LeaseContext) -> bool:\n        \"\"\"\n        Mark job as completed with lease version validation.\n        \n        CRITICAL RACE CONDITION PREVENTION:\n        \n        Scenario: Network partition causes Worker A to lose heartbeat\n        1. Worker A (v5): Executing job, network partition occurs\n        2. Worker A (v5): Heartbeat failures accumulate, lease expires\n        3. Worker B claims job \u2192 version increments to v6\n        4. Worker B (v6): Completes job, writes result with v6\n        5. Worker A (v5): Network recovers, attempts to write result\n        \n        Step 5 trace:\n        - Worker A calls mark_completed with lease_ctx.lease_version = 5\n        - UPDATE jobs SET ... WHERE lease_version = 5\n        - SQLite evaluates WHERE clause:\n          - Current DB state: lease_version = 6 (Worker B updated it)\n          - WHERE 6 = 5 \u2192 FALSE\n          - cursor.rowcount = 0 (no rows matched)\n        - Worker A's write is REJECTED atomically\n        \n        KEY INSIGHT: The version check happens INSIDE the atomic UPDATE,\n        not as a separate SELECT. This eliminates TOCTOU race conditions.\n        \"\"\"\n        with self.db.get_connection() as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                now = time.time()\n                \n                # Version check is the AUTHORITATIVE protection\n                cursor = conn.execute(\"\"\"\n                    UPDATE jobs\n                    SET status = 'completed',\n                        completed_at = ?,\n                        result = ?,\n                        lease_expires_at = NULL,\n                        lease_token = NULL\n                    WHERE job_id = ?\n                      AND lease_token = ?\n                      AND lease_version = ?\n                      AND status = 'running'\n                \"\"\", (now, pickle.dumps(result), job_id, \n                      lease_ctx.lease_token, lease_ctx.lease_version))\n                \n                if cursor.rowcount == 0:\n                    # Diagnostic: Determine why write was rejected\n                    cursor = conn.execute(\"\"\"\n                        SELECT status, lease_version, lease_token \n                        FROM jobs \n                        WHERE job_id = ?\n                    \"\"\", (job_id,))\n                    row = cursor.fetchone()\n                    \n                    if row:\n                        if row['lease_version'] != lease_ctx.lease_version:\n                            logging.warning(\n                                f\"Worker {lease_ctx.worker_name} lost lease for job {job_id}: \"\n                                f\"version mismatch (had v{lease_ctx.lease_version}, \"\n                                f\"DB now has v{row['lease_version']})\"\n                            )\n                            self.db._add_history(\n                                conn, job_id, row['status'], JobStatus(row['status']),\n                                'version_mismatch',\n                                f\"Stale worker v{lease_ctx.lease_version} rejected, current v{row['lease_version']}\",\n                                lease_ctx.worker_name,\n                                lease_ctx.lease_version\n                            )\n                        elif row['status'] != 'running':\n                            logging.warning(\n                                f\"Worker {lease_ctx.worker_name} attempted to complete \"\n                                f\"job {job_id} but status is {row['status']}\"\n                            )\n                    \n                    conn.execute(\"ROLLBACK\")\n                    return False\n                \n                # Calculate execution duration for monitoring\n                cursor = conn.execute(\"\"\"\n                    SELECT started_at FROM jobs WHERE job_id = ?\n                \"\"\", (job_id,))\n                row = cursor.fetchone()\n                duration = now - row['started_at'] if row else None\n                \n                self.db._add_history(\n                    conn, job_id, 'running', JobStatus.COMPLETED,\n                    'completed',\n                    f\"Completed by {lease_ctx.worker_name} in {duration:.2f}s (v{lease_ctx.lease_version})\" if duration else f\"Completed by {lease_ctx.worker_name}\",\n                    lease_ctx.worker_name,\n                    lease_ctx.lease_version\n                )\n                \n                conn.execute(\"COMMIT\")\n                return True\n                \n            except Exception as e:\n                conn.execute(\"ROLLBACK\")\n                logging.error(f\"Failed to mark job {job_id} completed: {e}\", exc_info=True)\n                return False\n    \n    def mark_failed(self, job_id: str, error: Exception, \n                    lease_ctx: LeaseContext) -> bool:\n        \"\"\"\n        Mark job as failed with retry logic and version validation.\n        \"\"\"\n        with self.db.get_connection() as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                # Verify lease ownership with version\n                cursor = conn.execute(\"\"\"\n                    SELECT lease_token, lease_version, retry_count, max_retries, status \n                    FROM jobs WHERE job_id = ?\n                \"\"\", (job_id,))\n                row = cursor.fetchone()\n                \n                if not row:\n                    conn.execute(\"ROLLBACK\")\n                    return False\n                \n                # Check version match\n                if (row['lease_token'] != lease_ctx.lease_token or \n                    row['lease_version'] != lease_ctx.lease_version):\n                    logging.warning(\n                        f\"Worker {lease_ctx.worker_name} lost lease for job {job_id} \"\n                        f\"before recording failure (v{lease_ctx.lease_version} != v{row['lease_version']})\"\n                    )\n                    conn.execute(\"ROLLBACK\")\n                    return False\n                \n                retry_count = row['retry_count']\n                max_retries = row['max_retries']\n                \n                should_retry_job = (\n                    retry_count < max_retries and \n                    self._should_retry_error(error)\n                )\n                \n                if should_retry_job:\n                    next_retry = time.time() + self._calculate_backoff(retry_count)\n                    \n                    conn.execute(\"\"\"\n                        UPDATE jobs\n                        SET status = 'failed',\n                            retry_count = retry_count + 1,\n                            error_message = ?,\n                            next_retry_at = ?,\n                            lease_expires_at = NULL,\n                            lease_token = NULL\n                        WHERE job_id = ?\n                    \"\"\", (str(error), next_retry, job_id))\n                    \n                    message = f\"Failed, will retry at {next_retry} (attempt {retry_count + 1}/{max_retries})\"\n                else:\n                    conn.execute(\"\"\"\n                        UPDATE jobs\n                        SET status = 'failed',\n                            retry_count = retry_count + 1,\n                            error_message = ?,\n                            completed_at = ?,\n                            next_retry_at = NULL,\n                            lease_expires_at = NULL,\n                            lease_token = NULL\n                        WHERE job_id = ?\n                    \"\"\", (str(error), time.time(), job_id))\n                    \n                    self._move_to_dlq(conn, job_id, retry_count + 1)\n                    message = f\"Failed permanently after {retry_count + 1} attempts\"\n                \n                self.db._add_history(\n                    conn, job_id, 'running', JobStatus.FAILED,\n                    'failed',\n                    message, lease_ctx.worker_name,\n                    lease_ctx.lease_version\n                )\n                \n                conn.execute(\"COMMIT\")\n                return True\n                \n            except Exception as e:\n                conn.execute(\"ROLLBACK\")\n                logging.error(f\"Failed to mark job {job_id} failed: {e}\", exc_info=True)\n                return False\n    \n    def cancel_job(self, job_id: str) -> bool:\n        \"\"\"Cancel a job if not running.\"\"\"\n        with self.db.get_connection() as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                cursor = conn.execute(\"SELECT status FROM jobs WHERE job_id = ?\", (job_id,))\n                row = cursor.fetchone()\n                \n                if not row:\n                    conn.execute(\"ROLLBACK\")\n                    return False\n                \n                if row['status'] in ('running', 'completed', 'cancelled'):\n                    conn.execute(\"ROLLBACK\")\n                    return False\n                \n                conn.execute(\"\"\"\n                    UPDATE jobs\n                    SET status = 'cancelled',\n                        completed_at = ?,\n                        lease_expires_at = NULL,\n                        lease_token = NULL,\n                        next_retry_at = NULL\n                    WHERE job_id = ?\n                \"\"\", (time.time(), job_id))\n                \n                self.db._add_history(\n                    conn, job_id, row['status'], JobStatus.CANCELLED,\n                    'cancelled',\n                    \"Cancelled by user\", None, None\n                )\n                \n                conn.execute(\"COMMIT\")\n                return True\n            except Exception:\n                conn.execute(\"ROLLBACK\")\n                raise\n    \n    def get_job_status(self, job_id: str) -> JobResult:\n        \"\"\"Retrieve job status.\"\"\"\n        with self.db.get_connection() as conn:\n            cursor = conn.execute(\"\"\"\n                SELECT job_id, status, result, error_message, \n                       created_at, completed_at, retry_count, next_retry_at\n                FROM jobs\n                WHERE job_id = ?\n            \"\"\", (job_id,))\n            \n            row = cursor.fetchone()\n            if not row:\n                raise JobNotFoundError(f\"Job {job_id} not found\")\n            \n            result = pickle.loads(row['result']) if row['result'] else None\n            \n            return JobResult(\n                job_id=row['job_id'],\n                status=JobStatus(row['status']),\n                result=result,\n                error_message=row['error_message'],\n                created_at=row['created_at'],\n                completed_at=row['completed_at'],\n                retry_count=row['retry_count'],\n                next_retry_at=row['next_retry_at']\n            )\n    \n    def get_stats(self) -> Dict[str, int]:\n        \"\"\"Get queue statistics.\"\"\"\n        with self.db.get_connection() as conn:\n            cursor = conn.execute(\"\"\"\n                SELECT status, COUNT(*) as count\n                FROM jobs\n                GROUP BY status\n            \"\"\")\n            \n            stats = {status.value: 0 for status in JobStatus}\n            for row in cursor:\n                stats[row['status']] = row['count']\n            \n            return stats\n    \n    def recover_expired_leases(self) -> int:\n        \"\"\"Reset jobs with expired leases to pending.\"\"\"\n        with self.db.get_connection() as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                now = time.time()\n                \n                # Get job IDs before reset for history tracking\n                cursor = conn.execute(\"\"\"\n                    SELECT job_id, lease_version \n                    FROM jobs \n                    WHERE status = 'running' \n                      AND lease_expires_at < ?\n                \"\"\", (now,))\n                \n                expired_jobs = cursor.fetchall()\n                \n                cursor = conn.execute(\"\"\"\n                    UPDATE jobs\n                    SET status = 'pending',\n                        claimed_by = NULL,\n                        claimed_at = NULL,\n                        lease_expires_at = NULL,\n                        lease_token = NULL,\n                        last_heartbeat_at = NULL\n                    WHERE status = 'running' \n                      AND lease_expires_at < ?\n                \"\"\", (now,))\n                \n                count = cursor.rowcount\n                \n                # Record history for each recovered job\n                for row in expired_jobs:\n                    self.db._add_history(\n                        conn, row['job_id'], 'running', JobStatus.PENDING,\n                        'recovered',\n                        f\"Recovered from expired lease on startup (was v{row['lease_version']})\",\n                        None,\n                        row['lease_version']\n                    )\n                \n                conn.execute(\"COMMIT\")\n                return count\n                \n            except Exception:\n                conn.execute(\"ROLLBACK\")\n                raise\n    \n    def cleanup_old_jobs(self, retention_days: int, batch_size: int):\n        \"\"\"\n        Archive and delete old jobs with weekly partition granularity.\n        \n        IMPROVEMENT: Weekly partitions (vs daily/monthly) provide:\n        - Sufficient precision for 7-90 day retention policies\n        - Lower partition overhead than daily\n        - Cleaner deletion (whole weeks at a time)\n        \"\"\"\n        cutoff_time = time.time() - (retention_days * 86400)\n        start_time = time.time()\n        total_archived = 0\n        total_deleted = 0\n        \n        # Determine partition to clean (YYYY-WW format for ISO week)\n        cutoff_date = datetime.fromtimestamp(cutoff_time)\n        cutoff_partition = f\"{cutoff_date.year}-{cutoff_date.isocalendar()[1]:02d}\"\n        \n        # Phase 1: Archive terminal jobs in batches\n        while True:\n            with self.db.get_connection() as conn:\n                try:\n                    conn.execute(\"BEGIN IMMEDIATE\")\n                    \n                    cursor = conn.execute(\"\"\"\n                        INSERT OR IGNORE INTO job_archive \n                        (job_id, task_name, status, created_at, completed_at, \n                         archive_partition, result_summary, error_message, retry_count,\n                         execution_duration_seconds)\n                        SELECT \n                            job_id, \n                            task_name, \n                            status, \n                            created_at, \n                            completed_at,\n                            strftime('%Y-', completed_at, 'unixepoch') || \n                            CAST(strftime('%W', completed_at, 'unixepoch') AS INTEGER),\n                            substr(CAST(result AS TEXT), 1, 200),\n                            error_message,\n                            retry_count,\n                            completed_at - started_at\n                        FROM jobs\n                        WHERE completed_at IS NOT NULL\n                          AND completed_at < ?\n                          AND status IN ('completed', 'cancelled', 'failed')\n                          AND job_id NOT IN (SELECT job_id FROM job_archive)\n                        LIMIT ?\n                    \"\"\", (cutoff_time, batch_size))\n                    \n                    archived = cursor.rowcount\n                    total_archived += archived\n                    \n                    conn.execute(\"COMMIT\")\n                    \n                    if archived == 0:\n                        break\n                    \n                    time.sleep(0.05)  # Brief yield to other operations\n                    \n                except Exception as e:\n                    conn.execute(\"ROLLBACK\")\n                    logging.error(f\"Archival batch failed: {e}\", exc_info=True)\n                    break\n        \n        # Phase 2: Delete archived jobs in batches\n        while True:\n            with self.db.get_connection() as conn:\n                try:\n                    conn.execute(\"BEGIN IMMEDIATE\")\n                    \n                    cursor = conn.execute(\"\"\"\n                        DELETE FROM jobs\n                        WHERE job_id IN (\n                            SELECT job_id FROM job_archive\n                            WHERE archive_partition <= ?\n                            LIMIT ?\n                        )\n                    \"\"\", (cutoff_partition, batch_size))\n                    \n                    deleted = cursor.rowcount\n                    total_deleted += deleted\n                    \n                    conn.execute(\"COMMIT\")\n                    \n                    if deleted == 0:\n                        break\n                    \n                    time.sleep(0.05)\n                    \n                except Exception as e:\n                    conn.execute(\"ROLLBACK\")\n                    logging.error(f\"Batch deletion failed: {e}\", exc_info=True)\n                    break\n        \n        # Phase 3: Cleanup history (keep last 100k entries)\n        with self.db.get_connection() as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                cursor = conn.execute(\"\"\"\n                    DELETE FROM job_history\n                    WHERE id NOT IN (\n                        SELECT id FROM job_history\n                        ORDER BY timestamp DESC\n                        LIMIT 100000\n                    )\n                \"\"\")\n                \n                history_deleted = cursor.rowcount\n                \n                conn.execute(\"COMMIT\")\n                \n            except Exception as e:\n                conn.execute(\"ROLLBACK\")\n                logging.error(f\"History cleanup failed: {e}\", exc_info=True)\n                history_deleted = 0\n        \n        # Record metrics\n        duration = time.time() - start_time\n        \n        with self.db.get_connection() as conn:\n            try:\n                conn.execute(\"\"\"\n                    INSERT OR REPLACE INTO cleanup_metadata \n                    (key, last_cleanup_at, records_cleaned, cleanup_duration_seconds, partition_cleaned)\n                    VALUES ('last_cleanup', ?, ?, ?, ?)\n                \"\"\", (time.time(), total_deleted + history_deleted, duration, cutoff_partition))\n                conn.commit()\n            except Exception:\n                pass\n        \n        if total_deleted > 0 or history_deleted > 0:\n            logging.info(\n                f\"Cleanup: archived {total_archived}, deleted {total_deleted} jobs, \"\n                f\"{history_deleted} history entries in {duration:.2f}s (partition: {cutoff_partition})\"\n            )\n        \n        # Alert if cleanup is taking too long (sign of contention or undersized batch)\n        if duration > 180.0:  # 3 minutes\n            logging.warning(\n                f\"Cleanup took {duration:.2f}s - consider increasing cleanup_interval_hours \"\n                f\"or reducing retention_days\"\n            )\n    \n    def _should_retry_error(self, error: Exception) -> bool:\n        \"\"\"Determine if error is retryable.\"\"\"\n        if isinstance(error, PermanentError):\n            return False\n        \n        if isinstance(error, RetryableError):\n            return True\n        \n        non_retryable = (\n            TypeError, ValueError, AttributeError,\n            KeyError, ImportError, SyntaxError, AssertionError\n        )\n        if isinstance(error, non_retryable):\n            return False\n        \n        retryable = (ConnectionError, TimeoutError, IOError, OSError)\n        if isinstance(error, retryable):\n            return True\n        \n        return True\n    \n    def _calculate_backoff(self, retry_count: int) -> float:\n        \"\"\"Calculate exponential backoff with jitter.\"\"\"\n        base_delay = 1.0\n        max_delay = 300.0\n        \n        delay = base_delay * (2 ** retry_count)\n        jitter = random.uniform(0, delay * 0.1)\n        \n        return min(delay + jitter, max_delay)\n    \n    def _move_to_dlq(self, conn, job_id: str, failure_count: int):\n        \"\"\"Move permanently failed job to dead letter queue.\"\"\"\n        cursor = conn.execute(\"\"\"\n            SELECT task_name, priority, payload, error_message, created_at\n            FROM jobs\n            WHERE job_id = ?\n        \"\"\", (job_id,))\n        \n        row = cursor.fetchone()\n        if not row:\n            return\n        \n        conn.execute(\"\"\"\n            INSERT OR REPLACE INTO dead_letter_queue\n            (job_id, task_name, priority, payload, error_message,\n             failure_count, first_failed_at, last_failed_at, moved_at)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\", (\n            job_id, row['task_name'], row['priority'], row['payload'],\n            row['error_message'], failure_count,\n            row['created_at'], time.time(), time.time()\n        ))\n```\n\n### 3. WorkerPool (Internal)\n\n```python\nimport threading\nimport logging\nfrom typing import List, Dict\n\nclass WorkerPool:\n    \"\"\"Manages worker threads with lease-context execution.\"\"\"\n    \n    def __init__(self, \n                 num_workers: int,\n                 queue_manager: QueueManager,\n                 poll_interval: float):\n        self.num_workers = num_workers\n        self.queue_manager = queue_manager\n        self.poll_interval = poll_interval\n        self.workers: List[threading.Thread] = []\n        self._shutdown_event = threading.Event()\n        self._active_jobs: Dict[str, Dict] = {}\n        self._active_jobs_lock = threading.Lock()\n    \n    def start(self):\n        \"\"\"Start all worker threads.\"\"\"\n        for i in range(self.num_workers):\n            worker = threading.Thread(\n                target=self._worker_loop,\n                name=f\"TaskQueue-Worker-{i}\",\n                daemon=False\n            )\n            worker.start()\n            self.workers.append(worker)\n    \n    def _worker_loop(self):\n        \"\"\"Main worker loop with lease context management.\"\"\"\n        worker_name = threading.current_thread().name\n        \n        while not self._shutdown_event.is_set():\n            try:\n                job = self.queue_manager.get_next_job(worker_name)\n                \n                if job is None:\n                    time.sleep(self.poll_interval)\n                    continue\n                \n                # Create lease context with version\n                lease_ctx = LeaseContext(\n                    job_id=job.job_id,\n                    lease_token=job.lease_token,\n                    lease_version=job.lease_version,\n                    worker_name=worker_name\n                )\n                \n                # Track active job\n                with self._active_jobs_lock:\n                    self._active_jobs[job.job_id] = {\n                        'job': job,\n                        'lease_ctx': lease_ctx,\n                        'thread': threading.current_thread(),\n                        'started_at': time.time()\n                    }\n                \n                try:\n                    # Start heartbeat thread\n                    heartbeat_thread = threading.Thread(\n                        target=self._heartbeat_loop,\n                        args=(job, lease_ctx),\n                        daemon=True\n                    )\n                    heartbeat_thread.start()\n                    \n                    self._execute_job(job, lease_ctx)\n                    \n                finally:\n                    with self._active_jobs_lock:\n                        self._active_jobs.pop(job.job_id, None)\n                        \n            except Exception as e:\n                logging.error(f\"{worker_name} error: {e}\", exc_info=True)\n                time.sleep(self.poll_interval)\n    \n    def _heartbeat_loop(self, job: Job, lease_ctx: LeaseContext):\n        \"\"\"\n        Periodically extend lease with failure tracking.\n        \n        DESIGN RATIONALE:\n        - 30s interval: Responsive to issues without excessive DB load\n        - Threshold=3: Tolerates ~90s of transient database locks\n        - Abandons lease locally (doesn't kill thread): Simpler than cooperative cancellation\n        \"\"\"\n        heartbeat_interval = self.queue_manager.heartbeat_interval\n        failure_threshold = self.queue_manager.heartbeat_failure_threshold\n        \n        while job.job_id in self._active_jobs and lease_ctx.is_valid:\n            time.sleep(heartbeat_interval)\n            \n            if job.job_id not in self._active_jobs:\n                break\n            \n            success = self.queue_manager.extend_lease(lease_ctx)\n            \n            if success:\n                lease_ctx.record_heartbeat_success()\n            else:\n                should_abandon = lease_ctx.record_heartbeat_failure(failure_threshold)\n                \n                logging.warning(\n                    f\"Heartbeat failed for job {job.job_id} \"\n                    f\"({lease_ctx.consecutive_heartbeat_failures}/{failure_threshold})\"\n                )\n                \n                if should_abandon:\n                    lease_ctx.abandon(\"persistent_heartbeat_failure\")\n                    logging.error(\n                        f\"Abandoning lease for job {job.job_id} after \"\n                        f\"{lease_ctx.consecutive_heartbeat_failures} heartbeat failures \"\n                        f\"(~{heartbeat_interval * failure_threshold:.0f}s of issues)\"\n                    )\n                    break\n    \n    def _execute_job(self, job: Job, lease_ctx: LeaseContext):\n        \"\"\"\n        Execute job with lease context awareness.\n        \n        SIMPLIFICATION: No cooperative cancellation required.\n        The version check in mark_completed() is the ONLY necessary guard.\n        Thread continues executing even after abandon, but results are discarded.\n        \"\"\"\n        try:\n            # Execute with optional timeout\n            if job.timeout_seconds:\n                result = self._execute_with_timeout(\n                    job.func, job.args, job.kwargs, job.timeout_seconds,\n                    lease_ctx\n                )\n            else:\n                result = job.func(*job.args, **job.kwargs)\n            \n            # Check if we abandoned the lease during execution\n            if not lease_ctx.is_valid:\n                logging.warning(\n                    f\"Job {job.job_id} completed but lease was abandoned \"\n                    f\"({lease_ctx.abandon_reason}), discarding result\"\n                )\n                return\n            \n            # Attempt to mark completed (version check is the authoritative guard)\n            success = self.queue_manager.mark_completed(\n                job.job_id, result, lease_ctx\n            )\n            \n            if not success:\n                logging.warning(\n                    f\"Job {job.job_id} completed but failed to record result \"\n                    \"(lease version mismatch - another worker claimed this job)\"\n                )\n            \n        except TimeoutError as e:\n            if lease_ctx.is_valid:\n                self.queue_manager.mark_failed(\n                    job.job_id,\n                    PermanentError(f\"Timeout after {job.timeout_seconds}s\"),\n                    lease_ctx\n                )\n        except PermanentError as e:\n            if lease_ctx.is_valid:\n                self.queue_manager.mark_failed(job.job_id, e, lease_ctx)\n        except Exception as e:\n            if lease_ctx.is_valid:\n                self.queue_manager.mark_failed(job.job_id, e, lease_ctx)\n    \n    def _execute_with_timeout(self, func: Callable, args: tuple,\n                             kwargs: dict, timeout: float,\n                             lease_ctx: LeaseContext) -> Any:\n        \"\"\"Execute function with timeout (daemon thread approach).\"\"\"\n        result_container = []\n        exception_container = []\n        \n        def target():\n            try:\n                result = func(*args, **kwargs)\n                result_container.append(result)\n            except Exception as e:\n                exception_container.append(e)\n        \n        thread = threading.Thread(target=target, daemon=True)\n        thread.start()\n        thread.join(timeout)\n        \n        if thread.is_alive():\n            lease_ctx.abandon(\"timeout\")\n            raise TimeoutError(f\"Job exceeded timeout of {timeout}s\")\n        \n        if exception_container:\n            raise exception_container[0]\n        \n        return result_container[0] if result_container else None\n    \n    def shutdown(self, wait: bool, timeout: float):\n        \"\"\"Shutdown worker pool gracefully.\"\"\"\n        self._shutdown_event.set()\n        \n        if not wait:\n            return\n        \n        start_time = time.time()\n        for worker in self.workers:\n            remaining = timeout - (time.time() - start_time)\n            if remaining <= 0:\n                logging.warning(\"Worker shutdown timeout exceeded\")\n                break\n            worker.join(timeout=remaining)\n        \n        still_running = [w for w in self.workers if w.is_alive()]\n        if still_running:\n            logging.warning(f\"{len(still_running)} workers did not shutdown gracefully\")\n```\n\n### 4. DatabaseManager (Internal)\n\n```python\nimport sqlite3\nimport threading\nimport pickle\nimport logging\nfrom contextlib import contextmanager\nfrom typing import Optional\n\nclass DatabaseManager:\n    \"\"\"Thread-safe SQLite operations with configurable timeouts and retry logic.\"\"\"\n    \n    def __init__(self, db_path: str, max_write_retries: int = 3):\n        self.db_path = db_path\n        self.max_write_retries = max_write_retries\n        self._local = threading.local()\n        self._init_lock = threading.Lock()\n        self._init_db()\n    \n    def _get_connection(self, timeout: float = 5.0) -> sqlite3.Connection:\n        \"\"\"Get thread-local connection with specified timeout.\"\"\"\n        if not hasattr(self._local, 'conn') or self._local.conn is None:\n            self._local.conn = self._create_connection(timeout)\n        else:\n            self._local.conn.execute(f\"PRAGMA busy_timeout={int(timeout * 1000)}\")\n        \n        return self._local.conn\n    \n    def _create_connection(self, timeout: float = 5.0) -> sqlite3.Connection:\n        \"\"\"Create new connection with proper SQLite settings.\"\"\"\n        conn = sqlite3.connect(\n            self.db_path,\n            isolation_level=None,\n            check_same_thread=False,\n            timeout=timeout\n        )\n        conn.row_factory = sqlite3.Row\n        \n        conn.execute(\"PRAGMA journal_mode=WAL\")\n        conn.execute(f\"PRAGMA busy_timeout={int(timeout * 1000)}\")\n        conn.execute(\"PRAGMA synchronous=NORMAL\")\n        \n        return conn\n    \n    @contextmanager\n    def get_connection(self, timeout: float = 5.0):\n        \"\"\"\n        Context manager for connection access with configurable timeout.\n        \n        IMPROVEMENT: Automatically retries on SQLITE_BUSY for write operations.\n        \"\"\"\n        conn = self._get_connection(timeout)\n        retries = 0\n        last_error = None\n        \n        while retries <= self.max_write_retries:\n            try:\n                yield conn\n                return\n            except sqlite3.OperationalError as e:\n                if \"database is locked\" in str(e) and retries < self.max_write_retries:\n                    retries += 1\n                    last_error = e\n                    time.sleep(0.1 * retries)  # Exponential backoff\n                    continue\n                else:\n                    raise\n        \n        if last_error:\n            raise last_error\n    \n    def _init_db(self):\n        \"\"\"Initialize database schema (idempotent).\"\"\"\n        with self._init_lock:\n            conn = self._create_connection()\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                # Create jobs table with version column\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS jobs (\n                        job_id TEXT PRIMARY KEY,\n                        task_name TEXT NOT NULL,\n                        priority INTEGER NOT NULL DEFAULT 0,\n                        status TEXT NOT NULL CHECK(status IN ('pending', 'running', 'completed', 'failed', 'cancelled')),\n                        payload BLOB NOT NULL,\n                        result BLOB,\n                        error_message TEXT,\n                        created_at REAL NOT NULL,\n                        started_at REAL,\n                        completed_at REAL,\n                        retry_count INTEGER DEFAULT 0,\n                        max_retries INTEGER DEFAULT 3,\n                        next_retry_at REAL,\n                        timeout_seconds REAL,\n                        claimed_by TEXT,\n                        claimed_at REAL,\n                        lease_expires_at REAL,\n                        lease_token TEXT,\n                        lease_version INTEGER DEFAULT 0,\n                        last_heartbeat_at REAL,\n                        CONSTRAINT valid_priority CHECK(priority BETWEEN 0 AND 10)\n                    )\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_claimable_jobs \n                    ON jobs(status, priority DESC, created_at ASC)\n                    WHERE status IN ('pending', 'failed')\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_lease_expiration\n                    ON jobs(lease_expires_at)\n                    WHERE status = 'running' AND lease_expires_at IS NOT NULL\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_retry_ready\n                    ON jobs(next_retry_at)\n                    WHERE status = 'failed' AND next_retry_at IS NOT NULL\n                \"\"\")\n                \n                # Create history table with version tracking\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS job_history (\n                        id INTEGER PRIMARY KEY AUTOINCREMENT,\n                        job_id TEXT NOT NULL,\n                        timestamp REAL NOT NULL,\n                        old_status TEXT,\n                        new_status TEXT NOT NULL,\n                        event_type TEXT NOT NULL,\n                        message TEXT,\n                        worker_name TEXT,\n                        lease_version INTEGER,\n                        FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE\n                    )\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_job_history_lookup\n                    ON job_history(job_id, timestamp DESC)\n                \"\"\")\n                \n                # Archive table with weekly partitions\n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS job_archive (\n                        job_id TEXT PRIMARY KEY,\n                        task_name TEXT NOT NULL,\n                        status TEXT NOT NULL,\n                        created_at REAL NOT NULL,\n                        completed_at REAL NOT NULL,\n                        archive_partition TEXT NOT NULL,\n                        result_summary TEXT,\n                        error_message TEXT,\n                        retry_count INTEGER,\n                        execution_duration_seconds REAL,\n                        FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE\n                    )\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_archive_partition\n                    ON job_archive(archive_partition, completed_at)\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE INDEX IF NOT EXISTS idx_archive_task_performance\n                    ON job_archive(task_name, execution_duration_seconds)\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS dead_letter_queue (\n                        job_id TEXT PRIMARY KEY,\n                        task_name TEXT NOT NULL,\n                        priority INTEGER NOT NULL,\n                        payload BLOB NOT NULL,\n                        error_message TEXT,\n                        failure_count INTEGER NOT NULL,\n                        first_failed_at REAL NOT NULL,\n                        last_failed_at REAL NOT NULL,\n                        moved_at REAL NOT NULL,\n                        FOREIGN KEY (job_id) REFERENCES jobs(job_id) ON DELETE CASCADE\n                    )\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS cleanup_metadata (\n                        key TEXT PRIMARY KEY,\n                        last_cleanup_at REAL NOT NULL,\n                        records_cleaned INTEGER NOT NULL,\n                        cleanup_duration_seconds REAL,\n                        partition_cleaned TEXT\n                    )\n                \"\"\")\n                \n                conn.execute(\"\"\"\n                    CREATE TABLE IF NOT EXISTS performance_metrics (\n                        metric_date TEXT PRIMARY KEY,\n                        jobs_submitted INTEGER DEFAULT 0,\n                        jobs_completed INTEGER DEFAULT 0,\n                        jobs_failed INTEGER DEFAULT 0,\n                        avg_execution_time REAL,\n                        p95_execution_time REAL,\n                        max_queue_depth INTEGER DEFAULT 0,\n                        last_updated REAL NOT NULL\n                    )\n                \"\"\")\n                \n                conn.execute(\"COMMIT\")\n            except Exception:\n                conn.execute(\"ROLLBACK\")\n                raise\n            finally:\n                conn.close()\n    \n    def insert_job(self, job: Job):\n        \"\"\"Insert new job into database.\"\"\"\n        with self.get_connection() as conn:\n            try:\n                conn.execute(\"BEGIN IMMEDIATE\")\n                \n                conn.execute(\"\"\"\n                    INSERT INTO jobs (\n                        job_id, task_name, priority, status, payload,\n                        created_at, max_retries, timeout_seconds\n                    ) VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n                \"\"\", (\n                    job.job_id, job.task_name, job.priority,\n                    job.status.value,\n                    pickle.dumps((job.func, job.args, job.kwargs)),\n                    job.created_at, job.max_retries, job.timeout_seconds\n                ))\n                \n                self._add_history(\n                    conn, job.job_id, None, JobStatus.PENDING,\n                    'created', \"Job created\", None, None\n                )\n                \n                conn.execute(\"COMMIT\")\n            except Exception:\n                conn.execute(\"ROLLBACK\")\n                raise\n    \n    def _add_history(self, conn: sqlite3.Connection, job_id: str,\n                     old_status: Optional[str], new_status: JobStatus,\n                     event_type: str, message: str, worker_name: Optional[str],\n                     lease_version: Optional[int]):\n        \"\"\"Add job history entry with version tracking.\"\"\"\n        conn.execute(\"\"\"\n            INSERT INTO job_history\n            (job_id, timestamp, old_status, new_status, event_type, message, worker_name, lease_version)\n            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n        \"\"\", (\n            job_id, time.time(),\n            old_status.value if isinstance(old_status, JobStatus) else old_status,\n            new_status.value, event_type, message, worker_name, lease_version\n        ))\n    \n    def close_all_connections(self):\n        \"\"\"Close thread-local connections.\"\"\"\n        pass\n```\n\n## Exception Classes\n\n```python\nclass JobError(Exception):\n    \"\"\"Base exception for job errors.\"\"\"\n    pass\n\nclass RetryableError(JobError):\n    \"\"\"Error that should trigger retry.\"\"\"\n    pass\n\nclass PermanentError(JobError):\n    \"\"\"Error that should not be retried.\"\"\"\n    pass\n\nclass JobNotFoundError(JobError):\n    \"\"\"Job ID does not exist.\"\"\"\n    pass\n\nclass JobFailedError(JobError):\n    \"\"\"Job failed after all retries.\"\"\"\n    pass\n\nclass JobCancelledError(JobError):\n    \"\"\"Job was cancelled.\"\"\"\n    pass\n```\n\n## Usage Examples\n\n```python\n# Initialize with version-based lease protection\nqueue = TaskQueue(\n    db_path=\"jobs.db\",\n    num_workers=4,\n    poll_interval=0.5,\n    lease_duration=300.0,\n    heartbeat_interval=30.0,\n    heartbeat_failure_threshold=3,\n    retention_days=7,\n    cleanup_interval_hours=12,  # Twice daily\n    cleanup_batch_size=500,  # Smaller batches = less blocking\n    max_write_retries=3  # Handle brief database contention\n)\n\n# Submit long-running job\ndef process_large_dataset(dataset_id: str):\n    for chunk in load_dataset(dataset_id):\n        process_chunk(chunk)\n    return \"processed\"\n\njob_id = queue.submit(\n    process_large_dataset,\n    \"dataset-123\",\n    priority=5,\n    max_retries=3,\n    timeout_seconds=3600.0\n)\n\n# Shutdown gracefully\nqueue.shutdown(wait=True, timeout=30.0)\n```\n\n---"
}