{
  "initial_prompt": "Design a rate limiting system for a REST API\n\nRequirements:\n- Multiple rate limit tiers (free, pro, enterprise)\n- Support both user-level and IP-level limiting\n- Handle burst traffic gracefully\n- Provide clear feedback in headers\n- Support distributed deployment\n- Consider Redis-based implementation",
  "status": "debating",
  "rounds": 4,
  "started_at": "2026-01-17T09:56:50.123599",
  "ended_at": "2026-01-17T10:13:27.340882",
  "final_design": "## Design\n\n# Rate Limiting System for REST API\n\n## Architecture Overview\n\n**Three-tier architecture with adaptive resilience:**\n\n```\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                    API Gateway Layer                         \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n\u2502  \u2502 Rate Limit     \u2502\u2192 \u2502 Identifier   \u2502\u2192 \u2502 Tier        \u2502    \u2502\n\u2502  \u2502 Middleware     \u2502  \u2502 Resolver     \u2502  \u2502 Resolver    \u2502    \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502                Rate Limiting Core                            \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Rate Limiter Service                                \u2502   \u2502\n\u2502  \u2502  \u2022 Token bucket with atomic Redis operations         \u2502   \u2502\n\u2502  \u2502  \u2022 Multi-tier limit checking (global + endpoint)     \u2502   \u2502\n\u2502  \u2502  \u2022 Cost calculation per operation                    \u2502   \u2502\n\u2502  \u2502  \u2022 Adaptive caching with health-based policy        \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2502         \u2193 (tiered cache)                \u2193 (authoritative)   \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502  Tiered Cache        \u2502      \u2502  Redis Sentinel     \u2502     \u2502\n\u2502  \u2502  \u2022 L1: In-memory     \u2502      \u2502  Primary + Replicas \u2502     \u2502\n\u2502  \u2502  \u2022 L2: Read replicas \u2502      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                    \u2502\n\u2502         \u2193 (on failure)                                       \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n\u2502  \u2502  Degradation Handler with Exponential Recovery       \u2502   \u2502\n\u2502  \u2502  \u2022 Tier-based quotas with cost awareness            \u2502   \u2502\n\u2502  \u2502  \u2022 Exponential backoff recovery tracking            \u2502   \u2502\n\u2502  \u2502  \u2022 Jittered retry windows                           \u2502   \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n               \u2193\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502              Observability & Operations                      \u2502\n\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510     \u2502\n\u2502  \u2502  Metrics     \u2502  \u2502  Audit       \u2502  \u2502  Config      \u2502     \u2502\n\u2502  \u2502  Collector   \u2502  \u2502  Logger      \u2502  \u2502  Manager     \u2502     \u2502\n\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518     \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n```\n\n## Core Components\n\n### 1. Rate Limiter Service with Tiered Caching\n\n```typescript\nclass TokenBucketRateLimiter implements RateLimiter {\n  constructor(\n    private redis: RedisClient,\n    private l1Cache: AdaptiveLRUCache,\n    private l2Strategy: ReadReplicaStrategy,\n    private config: RateLimitConfig,\n    private circuitBreaker: CircuitBreaker,\n    private degradationHandler: DegradationHandler\n  ) {}\n\n  async checkAndConsume(\n    identifier: Identifier,\n    tier: Tier,\n    path: string,\n    cost: number\n  ): Promise<RateLimitResult> {\n    const limits = this.config.getLimits(tier, path);\n    \n    const results = await Promise.all(\n      limits.map(limit => this.checkSingleLimit(identifier, limit, cost, path, tier))\n    );\n    \n    const blocked = results.find(r => !r.allowed);\n    if (blocked) return blocked;\n    \n    return results.reduce((most, current) => \n      current.remaining < most.remaining ? current : most\n    );\n  }\n\n  private async checkSingleLimit(\n    identifier: Identifier,\n    limit: LimitConfig,\n    cost: number,\n    path: string,\n    tier: string\n  ): Promise<RateLimitResult> {\n    const cacheKey = this.buildCacheKey(identifier, limit);\n    const policy = this.getCachePolicy(limit, tier);\n    \n    // L1: In-memory cache with policy-based headroom\n    const cached = this.l1Cache.get(cacheKey);\n    if (cached && cached.expiresAt > Date.now()) {\n      if (cached.remaining >= cost * policy.headroom) {\n        return {\n          ...cached,\n          remaining: cached.remaining - cost,\n          source: 'l1-cache'\n        };\n      }\n    }\n    \n    // L2: Read replicas for non-critical reads\n    if (policy.allowL2 && !limit.requiresPrimary) {\n      try {\n        const l2Result = await this.l2Strategy.checkReplica(identifier, limit, cost);\n        if (l2Result && l2Result.remaining >= cost * 1.1) {\n          this.l1Cache.set(cacheKey, l2Result, policy.l1TTL);\n          return { ...l2Result, source: 'l2-replica' };\n        }\n      } catch (error) {\n        // L2 failure, fall through to primary\n        logger.debug('L2 cache miss or failure', { error: error.message });\n      }\n    }\n    \n    // Primary: Redis check (authoritative)\n    try {\n      return await this.circuitBreaker.execute(async () => {\n        const result = await this.checkRedis(identifier, limit, cost, true);\n        \n        if (policy.l1TTL > 0) {\n          this.l1Cache.set(cacheKey, result, policy.l1TTL);\n        }\n        \n        return result;\n      });\n    } catch (error) {\n      logger.error('Rate limit check failed', { \n        identifier: `${identifier.type}:${this.maskKey(identifier.key)}`, \n        limit: limit.name, \n        error: error.message,\n        path\n      });\n      \n      return this.degradationHandler.handleFailure(\n        identifier, \n        limit, \n        cost, \n        tier, \n        path\n      );\n    }\n  }\n\n  private getCachePolicy(limit: LimitConfig, tier: string): CachePolicy {\n    // Critical endpoints: minimal caching\n    if (limit.criticality === 'critical') {\n      return {\n        l1TTL: 500,        // 500ms only\n        headroom: 1.0,     // No headroom tolerance\n        allowL2: false     // Never use replicas\n      };\n    }\n    \n    // High-value tiers: conservative caching\n    if (tier === 'enterprise' || tier === 'pro') {\n      return {\n        l1TTL: 1000,       // 1s cache\n        headroom: 1.2,     // 20% headroom\n        allowL2: true\n      };\n    }\n    \n    // Free tier: aggressive caching to reduce load\n    return {\n      l1TTL: 2000,         // 2s cache\n      headroom: 1.5,       // 50% headroom\n      allowL2: true\n    };\n  }\n\n  private async checkRedis(\n    identifier: Identifier,\n    limit: LimitConfig,\n    cost: number,\n    isPrimary: boolean\n  ): Promise<RateLimitResult> {\n    const key = this.buildRedisKey(identifier, limit);\n    const now = await this.getRedisTime(isPrimary);\n    \n    const script = `\n      local key = KEYS[1]\n      local capacity = tonumber(ARGV[1])\n      local refill_rate = tonumber(ARGV[2])\n      local cost = tonumber(ARGV[3])\n      local now = tonumber(ARGV[4])\n      local ttl = tonumber(ARGV[5])\n      local is_primary = tonumber(ARGV[6])\n      \n      local state = redis.call('HMGET', key, 'tokens', 'last_refill')\n      local tokens = tonumber(state[1]) or capacity\n      local last_refill = tonumber(state[2]) or now\n      \n      local elapsed = math.max(0, now - last_refill)\n      local tokens_to_add = elapsed * refill_rate\n      tokens = math.min(capacity, tokens + tokens_to_add)\n      \n      -- Only consume on primary\n      if is_primary == 1 and tokens >= cost then\n        local new_tokens = tokens - cost\n        redis.call('HSET', key, 'tokens', new_tokens, 'last_refill', now)\n        redis.call('EXPIRE', key, ttl)\n        \n        local reset_time = now + ((capacity - new_tokens) / refill_rate)\n        return {1, new_tokens, capacity, reset_time}\n      elseif is_primary == 0 then\n        -- Read-only check for replicas\n        return {tokens >= cost and 1 or 0, tokens, capacity, now}\n      else\n        redis.call('HSET', key, 'tokens', tokens, 'last_refill', now)\n        redis.call('EXPIRE', key, ttl)\n        \n        local deficit = cost - tokens\n        local retry_after = math.ceil(deficit / refill_rate)\n        \n        return {0, tokens, capacity, now + retry_after, retry_after}\n      }\n    `;\n    \n    const result = await this.redis.eval(\n      script,\n      [key],\n      [\n        limit.capacity,\n        limit.refillRate,\n        cost,\n        now,\n        limit.windowSeconds + 60,\n        isPrimary ? 1 : 0\n      ]\n    );\n    \n    return {\n      allowed: result[0] === 1,\n      remaining: result[1],\n      limit: result[2],\n      resetAt: result[3],\n      retryAfter: result[4] || null,\n      source: isPrimary ? 'redis-primary' : 'redis-replica',\n      expiresAt: now + this.l1Cache.getBaseTTL()\n    };\n  }\n  \n  private async getRedisTime(isPrimary: boolean): Promise<number> {\n    const client = isPrimary ? this.redis : this.l2Strategy.getReplicaClient();\n    const [seconds, microseconds] = await client.time();\n    return parseInt(seconds) * 1000 + Math.floor(parseInt(microseconds) / 1000);\n  }\n  \n  private buildCacheKey(identifier: Identifier, limit: LimitConfig): string {\n    return `rl:${identifier.type}:${identifier.key}:${limit.name}`;\n  }\n  \n  private buildRedisKey(identifier: Identifier, limit: LimitConfig): string {\n    return `ratelimit:${identifier.type}:${identifier.key}:${limit.name}`;\n  }\n  \n  private maskKey(key: string): string {\n    if (key.length <= 4) return '****';\n    return key.slice(0, 4) + '****';\n  }\n}\n```\n\n### 2. Tiered Cache Architecture\n\n```typescript\nclass AdaptiveLRUCache {\n  private cache: LRUCache<string, RateLimitResult>;\n  private healthState: HealthState = { healthy: true, lastCheck: Date.now() };\n  \n  constructor(private config: CacheConfig) {\n    this.cache = new LRUCache<string, RateLimitResult>({\n      max: config.maxEntries || 100000,\n      ttl: config.baseTTL,\n      ttlAutopurge: true,\n      updateAgeOnGet: false,\n      updateAgeOnHas: false,\n      maxSize: config.maxMemoryBytes || 256 * 1024 * 1024,\n      sizeCalculation: () => 2048\n    });\n  }\n  \n  getBaseTTL(): number {\n    return this.config.baseTTL;\n  }\n  \n  setHealthState(healthy: boolean): void {\n    if (this.healthState.healthy !== healthy) {\n      this.healthState = { healthy, lastCheck: Date.now() };\n      logger.info('Cache health state changed', { healthy });\n    }\n  }\n  \n  get(key: string): RateLimitResult | undefined {\n    return this.cache.get(key);\n  }\n  \n  set(key: string, value: RateLimitResult, ttl: number): void {\n    this.cache.set(key, value, { ttl });\n  }\n  \n  clear(): void {\n    this.cache.clear();\n  }\n  \n  getMetrics(): CacheMetrics {\n    return {\n      size: this.cache.size,\n      memoryUsage: this.cache.calculatedSize || 0,\n      healthy: this.healthState.healthy\n    };\n  }\n}\n\nclass ReadReplicaStrategy {\n  private replicaClients: RedisClient[];\n  private currentReplicaIndex = 0;\n  \n  constructor(\n    private config: ReplicaConfig,\n    private primaryClient: RedisClient\n  ) {\n    this.replicaClients = config.replicaHosts.map(host =>\n      new Redis({\n        host,\n        port: config.port,\n        password: config.password,\n        db: 0,\n        connectTimeout: 1000,\n        commandTimeout: 1000,\n        maxRetriesPerRequest: 1,\n        enableReadyCheck: false,\n        lazyConnect: false,\n        readOnly: true\n      })\n    );\n  }\n  \n  getReplicaClient(): RedisClient {\n    if (this.replicaClients.length === 0) {\n      return this.primaryClient;\n    }\n    \n    // Round-robin replica selection\n    const client = this.replicaClients[this.currentReplicaIndex];\n    this.currentReplicaIndex = (this.currentReplicaIndex + 1) % this.replicaClients.length;\n    return client;\n  }\n  \n  async checkReplica(\n    identifier: Identifier,\n    limit: LimitConfig,\n    cost: number\n  ): Promise<RateLimitResult | null> {\n    const client = this.getReplicaClient();\n    const key = `ratelimit:${identifier.type}:${identifier.key}:${limit.name}`;\n    \n    try {\n      const [seconds] = await client.time();\n      const now = parseInt(seconds) * 1000;\n      \n      const state = await client.hmget(key, 'tokens', 'last_refill');\n      const tokens = parseInt(state[0]) || limit.capacity;\n      const lastRefill = parseInt(state[1]) || now;\n      \n      const elapsed = Math.max(0, now - lastRefill);\n      const tokensToAdd = elapsed * limit.refillRate;\n      const currentTokens = Math.min(limit.capacity, tokens + tokensToAdd);\n      \n      return {\n        allowed: currentTokens >= cost,\n        remaining: currentTokens,\n        limit: limit.capacity,\n        resetAt: now + ((limit.capacity - currentTokens) / limit.refillRate),\n        retryAfter: null,\n        source: 'redis-replica',\n        expiresAt: now + 1000\n      };\n    } catch (error) {\n      logger.debug('Replica check failed', { error: error.message });\n      return null;\n    }\n  }\n}\n\ninterface CachePolicy {\n  l1TTL: number;      // L1 cache TTL in ms\n  headroom: number;   // Token headroom multiplier (1.0 = exact, 1.5 = 50% buffer)\n  allowL2: boolean;   // Whether to use L2 read replicas\n}\n```\n\n### 3. Degradation Handler with Exponential Recovery\n\n```typescript\nclass DegradationHandler {\n  private readonly DEGRADATION_QUOTAS: Record<string, DegradationQuota> = {\n    enterprise: {\n      requestsPerMinute: 100,\n      costMultiplier: 1.0\n    },\n    pro: {\n      requestsPerMinute: 50,\n      costMultiplier: 1.0\n    },\n    free: {\n      requestsPerMinute: 10,\n      costMultiplier: 1.5\n    },\n    ip: {\n      requestsPerMinute: 5,\n      costMultiplier: 2.0\n    }\n  };\n  \n  private degradationState = new LRUCache<string, DegradationState>({\n    max: 10000,\n    ttl: 120000, // 2 minutes TTL (extended from 1min)\n  });\n  \n  private recoveryTracker = new LRUCache<string, RecoveryState>({\n    max: 10000,\n    ttl: 180000, // 3 minutes TTL (reduced from 5min)\n  });\n  \n  private globalRecoveryPhase: RecoveryPhase = RecoveryPhase.NORMAL;\n  \n  handleFailure(\n    identifier: Identifier,\n    limit: LimitConfig,\n    cost: number,\n    tier: string,\n    path: string\n  ): RateLimitResult {\n    const stateKey = this.buildStateKey(identifier, limit);\n    const quota = this.DEGRADATION_QUOTAS[tier] || this.DEGRADATION_QUOTAS.ip;\n    \n    let state = this.degradationState.get(stateKey);\n    const now = Date.now();\n    \n    if (!state || now - state.windowStart >= 60000) {\n      state = {\n        windowStart: now,\n        tokensUsed: 0,\n        requestCount: 0,\n        failureTime: now\n      };\n    }\n    \n    const adjustedCost = cost * quota.costMultiplier;\n    const capacityInWindow = quota.requestsPerMinute;\n    const wouldExceed = (state.tokensUsed + adjustedCost) > capacityInWindow;\n    \n    if (!wouldExceed) {\n      state.tokensUsed += adjustedCost;\n      state.requestCount++;\n      this.degradationState.set(stateKey, state);\n      \n      return {\n        allowed: true,\n        remaining: Math.max(0, capacityInWindow - state.tokensUsed),\n        limit: capacityInWindow,\n        resetAt: state.windowStart + 60000,\n        retryAfter: null,\n        source: 'degraded',\n        expiresAt: now + 1000 // Shorter cache during degradation\n      };\n    } else {\n      const timeUntilReset = (state.windowStart + 60000) - now;\n      \n      return {\n        allowed: false,\n        remaining: 0,\n        limit: capacityInWindow,\n        resetAt: state.windowStart + 60000,\n        retryAfter: Math.ceil(timeUntilReset / 1000),\n        source: 'degraded',\n        expiresAt: now + 1000\n      };\n    }\n  }\n  \n  recordSuccess(identifier: Identifier, limit: LimitConfig): void {\n    const recoveryKey = this.buildStateKey(identifier, limit);\n    let recovery = this.recoveryTracker.get(recoveryKey);\n    \n    if (!recovery) {\n      recovery = {\n        successCount: 0,\n        totalCount: 0,\n        lastSuccessTime: Date.now(),\n        consecutiveSuccesses: 0,\n        phase: RecoveryPhase.TESTING\n      };\n    }\n    \n    recovery.successCount++;\n    recovery.totalCount++;\n    recovery.consecutiveSuccesses++;\n    recovery.lastSuccessTime = Date.now();\n    \n    this.recoveryTracker.set(recoveryKey, recovery);\n    \n    // Exponential recovery thresholds\n    const shouldProgress = this.shouldProgressRecovery(recovery);\n    \n    if (shouldProgress) {\n      this.progressRecoveryPhase(identifier, limit, recovery);\n    }\n  }\n  \n  recordFailure(identifier: Identifier, limit: LimitConfig): void {\n    const recoveryKey = this.buildStateKey(identifier, limit);\n    let recovery = this.recoveryTracker.get(recoveryKey);\n    \n    if (!recovery) {\n      recovery = {\n        successCount: 0,\n        totalCount: 0,\n        lastSuccessTime: Date.now(),\n        consecutiveSuccesses: 0,\n        phase: RecoveryPhase.TESTING\n      };\n    }\n    \n    recovery.totalCount++;\n    recovery.consecutiveSuccesses = 0; // Reset on failure\n    this.recoveryTracker.set(recoveryKey, recovery);\n    \n    // Regression: move back a phase\n    if (recovery.phase !== RecoveryPhase.TESTING) {\n      recovery.phase = Math.max(RecoveryPhase.TESTING, recovery.phase - 1);\n      logger.warn('Recovery regression', {\n        identifier: `${identifier.type}:${identifier.key}`,\n        limit: limit.name,\n        phase: RecoveryPhase[recovery.phase]\n      });\n    }\n  }\n  \n  private shouldProgressRecovery(recovery: RecoveryState): boolean {\n    // Phase-based thresholds with exponential backoff\n    switch (recovery.phase) {\n      case RecoveryPhase.TESTING:\n        // Initial testing: need 5 consecutive successes\n        return recovery.consecutiveSuccesses >= 5;\n      \n      case RecoveryPhase.PARTIAL:\n        // Partial recovery: need 15 consecutive successes\n        return recovery.consecutiveSuccesses >= 15;\n      \n      case RecoveryPhase.STABILIZING:\n        // Stabilizing: need 30 consecutive successes + 95% success rate\n        return recovery.consecutiveSuccesses >= 30 && \n               (recovery.successCount / recovery.totalCount) >= 0.95;\n      \n      case RecoveryPhase.NORMAL:\n        return false; // Already fully recovered\n      \n      default:\n        return false;\n    }\n  }\n  \n  private progressRecoveryPhase(\n    identifier: Identifier,\n    limit: LimitConfig,\n    recovery: RecoveryState\n  ): void {\n    const oldPhase = recovery.phase;\n    recovery.phase = Math.min(RecoveryPhase.NORMAL, recovery.phase + 1);\n    \n    logger.info('Recovery phase progression', {\n      identifier: `${identifier.type}:${identifier.key}`,\n      limit: limit.name,\n      oldPhase: RecoveryPhase[oldPhase],\n      newPhase: RecoveryPhase[recovery.phase],\n      consecutiveSuccesses: recovery.consecutiveSuccesses,\n      successRate: (recovery.successCount / recovery.totalCount).toFixed(3)\n    });\n    \n    // Fully recovered\n    if (recovery.phase === RecoveryPhase.NORMAL) {\n      this.exitDegradedMode(identifier, limit);\n    }\n    \n    metrics.recoveryPhaseTransitions.inc({\n      tier: identifier.tier || 'unknown',\n      limit: limit.name,\n      phase: RecoveryPhase[recovery.phase]\n    });\n  }\n  \n  private exitDegradedMode(identifier: Identifier, limit: LimitConfig): void {\n    const stateKey = this.buildStateKey(identifier, limit);\n    \n    this.degradationState.delete(stateKey);\n    this.recoveryTracker.delete(stateKey);\n    \n    logger.info('Fully recovered from degraded mode', {\n      identifier: `${identifier.type}:${identifier.key}`,\n      limit: limit.name\n    });\n    \n    metrics.degradationRecoveries.inc({\n      tier: identifier.tier || 'unknown',\n      limit: limit.name\n    });\n    \n    // Update global recovery phase\n    if (this.degradationState.size === 0) {\n      this.globalRecoveryPhase = RecoveryPhase.NORMAL;\n      logger.info('All rate limiters fully recovered');\n    }\n  }\n  \n  private buildStateKey(identifier: Identifier, limit: LimitConfig): string {\n    return `degraded:${identifier.type}:${identifier.key}:${limit.name}`;\n  }\n  \n  isInDegradedMode(): boolean {\n    return this.degradationState.size > 0;\n  }\n  \n  getRecoveryPhase(): RecoveryPhase {\n    return this.globalRecoveryPhase;\n  }\n}\n\nenum RecoveryPhase {\n  TESTING = 0,      // Initial recovery attempts (5 successes)\n  PARTIAL = 1,      // Partial recovery (15 successes)\n  STABILIZING = 2,  // Stabilizing (30 successes + 95% rate)\n  NORMAL = 3        // Fully recovered\n}\n\ninterface DegradationState {\n  windowStart: number;\n  tokensUsed: number;\n  requestCount: number;\n  failureTime: number;\n}\n\ninterface RecoveryState {\n  successCount: number;\n  totalCount: number;\n  lastSuccessTime: number;\n  consecutiveSuccesses: number;\n  phase: RecoveryPhase;\n}\n```\n\n### 4. Configuration Model with Criticality Levels\n\n```typescript\ninterface RateLimitConfig {\n  version: string;\n  tiers: {\n    [tier: string]: TierConfig;\n  };\n  endpoints: EndpointConfig[];\n  cacheConfig: CacheConfig;\n  replicaConfig?: ReplicaConfig;\n}\n\ninterface LimitConfig {\n  name: string;\n  scope: 'read' | 'write';\n  capacity: number;\n  refillRate: number;\n  windowSeconds: number;\n  appliesTo?: string[];\n  criticality?: 'normal' | 'critical'; // NEW: Endpoint criticality\n  requiresPrimary?: boolean;           // NEW: Force primary checks\n}\n\ninterface EndpointConfig {\n  pattern: string;\n  method?: string;\n  cost: number;\n  criticality?: 'normal' | 'critical'; // NEW: Per-endpoint criticality\n}\n\nconst config: RateLimitConfig = {\n  version: '1.0.0',\n  cacheConfig: {\n    baseTTL: 1500,\n    maxEntries: 100000,\n    maxMemoryBytes: 256 * 1024 * 1024\n  },\n  replicaConfig: {\n    replicaHosts: ['replica-1.example.com', 'replica-2.example.com'],\n    port: 6379,\n    password: process.env.REDIS_PASSWORD || ''\n  },\n  tiers: {\n    free: {\n      name: 'free',\n      limits: [\n        {\n          name: 'global',\n          scope: 'read',\n          capacity: 100,\n          refillRate: 100 / 3600,\n          windowSeconds: 3600,\n          criticality: 'normal'\n        },\n        {\n          name: 'write',\n          scope: 'write',\n          capacity: 20,\n          refillRate: 20 / 3600,\n          windowSeconds: 3600,\n          appliesTo: ['/api/create', '/api/update', '/api/delete'],\n          criticality: 'normal'\n        }\n      ]\n    },\n    pro: {\n      name: 'pro',\n      limits: [\n        {\n          name: 'global',\n          scope: 'read',\n          capacity: 1000,\n          refillRate: 1000 / 3600,\n          windowSeconds: 3600,\n          criticality: 'normal'\n        },\n        {\n          name: 'write',\n          scope: 'write',\n          capacity: 500,\n          refillRate: 500 / 3600,\n          windowSeconds: 3600,\n          appliesTo: ['/api/create', '/api/update', '/api/delete'],\n          criticality: 'normal'\n        },\n        {\n          name: 'payment',\n          scope: 'write',\n          capacity: 20,\n          refillRate: 20 / 300,\n          windowSeconds: 300,\n          appliesTo: ['/api/payment/*'],\n          criticality: 'critical',\n          requiresPrimary: true\n        }\n      ]\n    },\n    enterprise: {\n      name: 'enterprise',\n      limits: [\n        {\n          name: 'global',\n          scope: 'read',\n          capacity: 10000,\n          refillRate: 10000 / 3600,\n          windowSeconds: 3600,\n          criticality: 'normal'\n        },\n        {\n          name: 'payment',\n          scope: 'write',\n          capacity: 100,\n          refillRate: 100 / 300,\n          windowSeconds: 300,\n          appliesTo: ['/api/payment/*'],\n          criticality: 'critical',\n          requiresPrimary: true\n        }\n      ]\n    }\n  },\n  endpoints: [\n    { pattern: '/api/search', method: 'POST', cost: 3, criticality: 'normal' },\n    { pattern: '/api/analyze', method: 'POST', cost: 5, criticality: 'normal' },\n    { pattern: '/api/export', method: 'GET', cost: 10, criticality: 'normal' },\n    { pattern: '/api/payment/*', method: 'POST', cost: 1, criticality: 'critical' }\n  ]\n};\n```\n\n### 5. Enhanced Circuit Breaker with Exponential Backoff\n\n```typescript\nclass CircuitBreaker {\n  private failures = 0;\n  private successes = 0;\n  private lastFailureTime = 0;\n  private state: CircuitState = CircuitState.CLOSED;\n  private openDuration = 10000; // Start with 10s backoff\n  \n  private readonly MIN_BACKOFF = 10000;    // 10s\n  private readonly MAX_BACKOFF = 120000;   // 2min\n  private readonly SUCCESS_THRESHOLD = 5;  // 5 successes to close\n  private readonly FAILURE_THRESHOLD = 3;  // 3 failures to open\n  private readonly OPERATION_TIMEOUT = 2000;\n  \n  constructor(\n    private name: string,\n    private onStateChange?: (healthy: boolean) => void,\n    private degradationHandler?: DegradationHandler\n  ) {}\n  \n  async execute<T>(fn: () => Promise<T>): Promise<T> {\n    if (this.state === CircuitState.OPEN) {\n      const timeSinceFailure = Date.now() - this.lastFailureTime;\n      \n      if (timeSinceFailure > this.openDuration) {\n        logger.info('Circuit breaker entering half-open state', { \n          name: this.name,\n          backoff: this.openDuration\n        });\n        this.state = CircuitState.HALF_OPEN;\n        this.successes = 0;\n      } else {\n        metrics.circuitBreakerRejections.inc({ breaker: this.name });\n        throw new Error(`Circuit breaker OPEN for ${this.name}`);\n      }\n    }\n    \n    try {\n      const result = await this.withTimeout(fn(), this.OPERATION_TIMEOUT);\n      \n      this.successes++;\n      \n      if (this.state === CircuitState.HALF_OPEN && \n          this.successes >= this.SUCCESS_THRESHOLD) {\n        logger.info('Circuit breaker closed', { \n          name: this.name,\n          consecutiveSuccesses: this.successes\n        });\n        this.state = CircuitState.CLOSED;\n        this.failures = 0;\n        this.openDuration = this.MIN_BACKOFF; // Reset backoff\n        this.onStateChange?.(true);\n      }\n      \n      metrics.circuitBreakerState.set({ breaker: this.name }, this.state);\n      \n      return result;\n    } catch (error) {\n      this.successes = 0;\n      this.failures++;\n      this.lastFailureTime = Date.now();\n      \n      if (this.failures >= this.FAILURE_THRESHOLD && \n          this.state !== CircuitState.OPEN) {\n        // Exponential backoff with jitter\n        this.openDuration = Math.min(\n          this.MAX_BACKOFF,\n          this.openDuration * 2 + Math.random() * 1000\n        );\n        \n        this.state = CircuitState.OPEN;\n        logger.error('Circuit breaker opened', { \n          name: this.name, \n          failures: this.failures,\n          backoffMs: this.openDuration\n        });\n        metrics.circuitBreakerState.set({ breaker: this.name }, this.state);\n        this.onStateChange?.(false);\n      }\n      \n      throw error;\n    }\n  }\n  \n  private withTimeout<T>(promise: Promise<T>, ms: number): Promise<T> {\n    return Promise.race([\n      promise,\n      new Promise<T>((_, reject) => \n        setTimeout(() => reject(new Error('Operation timeout')), ms)\n      )\n    ]);\n  }\n  \n  getState(): { state: string; failures: number; successes: number; backoff: number } {\n    return {\n      state: CircuitState[this.state],\n      failures: this.failures,\n      successes: this.successes,\n      backoff: this.openDuration\n    };\n  }\n}\n\nenum CircuitState {\n  CLOSED = 0,\n  HALF_OPEN = 1,\n  OPEN = 2\n}\n```\n\n### 6. Deployment Architecture with Read Replicas\n\n```typescript\nconst redis = new Redis({\n  sentinels: [\n    { host: 'sentinel-1', port: 26379 },\n    { host: 'sentinel-2', port: 26379 },\n    { host: 'sentinel-3', port: 26379 }\n  ],\n  name: 'ratelimit-primary',\n  sentinelPassword: process.env.SENTINEL_PASSWORD,\n  password: process.env.REDIS_PASSWORD,\n  db: 0,\n  connectTimeout: 2000,\n  commandTimeout: 2000,\n  maxRetriesPerRequest: 1,\n  enableReadyCheck: true,\n  enableOfflineQueue: false,\n  lazyConnect: false,\n  retryStrategy: (times) => {\n    if (times > 3) return null;\n    return Math.min(times * 50, 200);\n  }\n});\n\nconst l1Cache = new AdaptiveLRUCache({\n  baseTTL: 1500,\n  maxEntries: 100000,\n  maxMemoryBytes: 256 * 1024 * 1024\n});\n\nconst l2Strategy = new ReadReplicaStrategy(\n  {\n    replicaHosts: ['replica-1.local', 'replica-2.local'],\n    port: 6379,\n    password: process.env.REDIS_PASSWORD || ''\n  },\n  redis\n);\n\nconst degradationHandler = new DegradationHandler();\n\nconst circuitBreaker = new CircuitBreaker(\n  'redis-primary',\n  (healthy) => {\n    l1Cache.setHealthState(healthy);\n    if (healthy) {\n      logger.info('Redis primary healthy, cache state updated');\n    } else {\n      logger.warn('Redis primary unhealthy, relying on degradation');\n    }\n  },\n  degradationHandler\n);\n\n// Health monitoring with recovery tracking\nsetInterval(async () => {\n  try {\n    await redis.ping();\n    metrics.redisHealth.set(1);\n    \n    // Record success for all active degraded states\n    // This allows recovery tracking to progress\n    for (const [key, _] of degradationHandler['degradationState'].entries()) {\n      const [_, type, idKey, limitName] = key.split(':');\n      degradationHandler.recordSuccess(\n        { type: type as any, key: idKey },\n        { name: limitName } as any\n      );\n    }\n  } catch (error) {\n    metrics.redisHealth.set(0);\n    logger.error('Redis health check failed', { error });\n  }\n}, 5000);\n\n// Metrics dashboard\nsetInterval(() => {\n  const cacheMetrics = l1Cache.getMetrics();\n  const cbState = circuitBreaker.getState();\n  const recoveryPhase = degradationHandler.getRecoveryPhase();\n  \n  logger.info('Rate limiter health', {\n    cache: {\n      size: cacheMetrics.size,\n      memoryMB: (cacheMetrics.memoryUsage / (1024 * 1024)).toFixed(2),\n      healthy: cacheMetrics.healthy\n    },\n    circuitBreaker: {\n      state: cbState.state,\n      backoffMs: cbState.backoff\n    },\n    degradation: {\n      active: degradationHandler.isInDegradedMode(),\n      recoveryPhase: RecoveryPhase[recoveryPhase]\n    }\n  });\n}, 30000);\n```\n\n### 7. Enhanced Metrics\n\n```typescript\ninterface Metrics {\n  // Core metrics\n  requestsAllowed: Counter;\n  requestsBlocked: Counter;\n  rateLimitCheckDuration: Histogram;\n  \n  // Tiered cache metrics\n  l1CacheHits: Counter;\n  l1CacheMisses: Counter;\n  l2CacheHits: Counter;\n  l2CacheMisses: Counter;\n  cacheSize: Gauge;\n  cacheMemory: Gauge;\n  \n  // Redis metrics\n  redisErrors: Counter;\n  redisLatency: Histogram;\n  redisHealth: Gauge;\n  \n  // Circuit breaker metrics\n  circuitBreakerState: Gauge;\n  circuitBreakerRejections: Counter;\n  \n  // Recovery metrics\n  degradedRequests: Counter;\n  degradedRequestsByTier: Counter;\n  degradationRecoveries: Counter;\n  recoveryPhaseTransitions: Counter;\n  \n  // Business metrics\n  tokenBucketUtilization: Histogram;\n  costDistribution: Histogram;\n  tierRejections: Counter;\n  endpointRejections: Counter;\n}\n```"
}