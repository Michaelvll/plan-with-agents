{
  "initial_prompt": "Design a rate limiting system",
  "status": "debating",
  "rounds": 2,
  "started_at": "2026-01-17T08:44:38.025155",
  "ended_at": "2026-01-17T08:51:53.726388",
  "final_design": "## Design\n\n# Distributed Rate Limiting System\n\n## Architecture Overview\n\nA horizontally scalable, distributed rate limiting system using the **Token Bucket algorithm** with Redis as the shared state backend. The design prioritizes correctness and operational simplicity, with optional performance optimizations that can be enabled based on specific requirements.\n\n## Core Components\n\n### 1. Rate Limiter Service\n\n```typescript\ninterface RateLimiterConfig {\n  limits: RateLimit[];\n  storage: StorageBackend;\n  fallback: FallbackConfig;\n  monitoring: MonitoringConfig;\n  localCache?: LocalCacheConfig;  // Optional performance optimization\n}\n\ninterface RateLimit {\n  name: string;\n  capacity: number;              // Maximum tokens\n  refillRate: number;            // Tokens added per second\n  refillInterval: number;        // Milliseconds between refills\n  keyGenerator: (context: RequestContext) => string;\n  scope: 'global' | 'per_user' | 'per_ip' | 'custom';\n  costFunction?: (context: RequestContext) => number;\n  burstAllowance?: number;       // Additional tokens beyond capacity (default: 0)\n  priority?: 'strict' | 'standard';  // Cache behavior hint\n}\n\ninterface RequestContext {\n  userId?: string;\n  ipAddress?: string;\n  endpoint: string;\n  method: string;\n  customAttributes?: Record<string, string>;\n  requestWeight?: number;\n  timestamp: number;  // Server-generated only\n}\n\nclass RateLimiter {\n  private storage: StorageBackend;\n  private cache?: LocalCacheLayer;\n  private circuitBreaker: CircuitBreaker;\n  private metrics: MetricsCollector;\n  private configProvider: ConfigProvider;\n  \n  constructor(config: RateLimiterConfig) {\n    this.storage = config.storage;\n    this.cache = config.localCache ? new LocalCacheLayer(config.localCache, this.storage) : undefined;\n    this.circuitBreaker = new CircuitBreaker(config.fallback.circuitBreaker);\n    this.metrics = new MetricsCollector(config.monitoring);\n    this.configProvider = new ConfigProvider(config);\n  }\n  \n  async checkLimit(context: RequestContext, limitName: string): Promise<RateLimitResult> {\n    const limit = this.configProvider.getLimit(limitName);\n    const key = this.buildKey(limit, context);\n    const tokens = limit.costFunction?.(context) ?? 1;\n    \n    // Input validation\n    this.validateRequest(key, tokens, limit);\n    \n    return await this.consumeTokensWithFallback(key, tokens, limit, context);\n  }\n  \n  private async consumeTokensWithFallback(\n    key: string, \n    tokens: number, \n    limit: RateLimit,\n    context: RequestContext\n  ): Promise<RateLimitResult> {\n    const startTime = Date.now();\n    \n    try {\n      // Optional cache layer for high-throughput scenarios\n      if (this.cache && this.shouldUseCache(limit)) {\n        const cacheResult = await this.cache.tryConsume(key, tokens, limit);\n        if (cacheResult) {\n          this.metrics.recordRequest(limit.name, cacheResult.allowed, Date.now() - startTime, true);\n          return cacheResult;\n        }\n      }\n      \n      // Authoritative backend check\n      const result = await this.circuitBreaker.execute(() =>\n        this.storage.atomicConsumeTokens(key, tokens, limit, context.timestamp)\n      );\n      \n      this.metrics.recordRequest(limit.name, result.allowed, Date.now() - startTime, false);\n      \n      // Update cache on backend success\n      if (this.cache) {\n        await this.cache.updateFromBackend(key, result);\n      }\n      \n      return result;\n      \n    } catch (error) {\n      this.metrics.recordStorageError(error);\n      return await this.handleFailure(key, tokens, limit, error);\n    }\n  }\n  \n  private shouldUseCache(limit: RateLimit): boolean {\n    // Security-critical operations bypass cache\n    return limit.priority !== 'strict';\n  }\n  \n  private validateRequest(key: string, tokens: number, limit: RateLimit): void {\n    if (key.length > 256) {\n      throw new RateLimitError('Key too long', RateLimitErrorCode.INVALID_KEY);\n    }\n    if (tokens <= 0 || tokens > limit.capacity * 10) {\n      throw new RateLimitError('Invalid token cost', RateLimitErrorCode.INVALID_TOKEN_COST);\n    }\n  }\n  \n  private buildKey(limit: RateLimit, context: RequestContext): string {\n    const identifier = limit.keyGenerator(context);\n    return `ratelimit:${limit.scope}:${identifier}:${limit.name}`;\n  }\n  \n  async resetLimit(key: string): Promise<void> {\n    await this.storage.deleteKey(key);\n    if (this.cache) {\n      await this.cache.invalidate(key);\n    }\n  }\n  \n  async addQuota(key: string, tokens: number): Promise<void> {\n    await this.storage.addTokens(key, tokens);\n    if (this.cache) {\n      await this.cache.invalidate(key);\n    }\n  }\n}\n\ninterface RateLimitResult {\n  allowed: boolean;\n  remaining: number;\n  resetAt: Date;\n  retryAfter?: number;\n  limit: number;\n  waitTimeMs?: number;\n}\n```\n\n### 2. Storage Backend (Redis Implementation)\n\n```typescript\ninterface StorageBackend {\n  atomicConsumeTokens(key: string, tokens: number, config: RateLimit, now: number): Promise<RateLimitResult>;\n  deleteKey(key: string): Promise<void>;\n  addTokens(key: string, tokens: number): Promise<void>;\n  healthCheck(): Promise<boolean>;\n  getStats(): Promise<StorageStats>;\n}\n\ninterface StorageStats {\n  activeKeys: number;\n  memoryUsage: number;\n  operationsPerSecond: number;\n}\n\nclass RedisStorage implements StorageBackend {\n  private pool: RedisConnectionPool;\n  private scriptSha: string;\n  \n  constructor(config: RedisConfig) {\n    this.pool = new RedisConnectionPool({\n      nodes: config.nodes,\n      poolSize: config.poolSize ?? 50,\n      connectionTimeout: config.connectionTimeout ?? 100,\n      operationTimeout: config.operationTimeout ?? 50,\n      retryStrategy: {\n        maxAttempts: 3,\n        backoffMs: [10, 50, 100]\n      }\n    });\n  }\n  \n  async initialize(): Promise<void> {\n    // Pre-load Lua script for performance\n    this.scriptSha = await this.pool.scriptLoad(CONSUME_TOKENS_SCRIPT);\n  }\n  \n  async atomicConsumeTokens(\n    key: string, \n    tokens: number, \n    config: RateLimit,\n    now: number\n  ): Promise<RateLimitResult> {\n    const maxBurst = config.capacity + (config.burstAllowance ?? 0);\n    const ttl = this.calculateTTL(config);\n    \n    try {\n      const result = await this.pool.evalsha(\n        this.scriptSha,\n        1, // number of keys\n        key,\n        config.capacity,\n        config.refillRate,\n        tokens,\n        now,\n        config.refillInterval,\n        maxBurst,\n        ttl\n      );\n      \n      return this.parseScriptResult(result, config);\n      \n    } catch (error) {\n      if (error.message?.includes('NOSCRIPT')) {\n        await this.initialize();\n        return this.atomicConsumeTokens(key, tokens, config, now);\n      }\n      throw error;\n    }\n  }\n  \n  private calculateTTL(config: RateLimit): number {\n    // TTL = 2x the time to refill from 0 to capacity\n    const secondsToFull = config.capacity / config.refillRate;\n    const millisecondsToFull = (secondsToFull * 1000) / (1000 / config.refillInterval);\n    return Math.ceil(millisecondsToFull * 2);\n  }\n  \n  private parseScriptResult(result: any[], config: RateLimit): RateLimitResult {\n    const [allowed, remaining, resetTime, waitMs] = result;\n    \n    return {\n      allowed: allowed === 1,\n      remaining: Math.floor(remaining),\n      resetAt: new Date(resetTime),\n      retryAfter: allowed === 1 ? undefined : Math.ceil(waitMs / 1000),\n      limit: config.capacity,\n      waitTimeMs: allowed === 1 ? undefined : waitMs\n    };\n  }\n  \n  async deleteKey(key: string): Promise<void> {\n    await this.pool.del(key);\n  }\n  \n  async addTokens(key: string, tokens: number): Promise<void> {\n    await this.pool.hincrby(key, 'tokens', tokens);\n  }\n  \n  async healthCheck(): Promise<boolean> {\n    try {\n      await this.pool.ping();\n      return true;\n    } catch {\n      return false;\n    }\n  }\n  \n  async getStats(): Promise<StorageStats> {\n    const info = await this.pool.info();\n    const keyCount = await this.approximateKeyCount();\n    \n    return {\n      activeKeys: keyCount,\n      memoryUsage: this.parseMemoryUsage(info),\n      operationsPerSecond: this.parseOpsPerSecond(info)\n    };\n  }\n  \n  private async approximateKeyCount(): Promise<number> {\n    // Sample-based estimation to avoid blocking KEYS command\n    let cursor = '0';\n    let count = 0;\n    let iterations = 0;\n    const maxIterations = 10;\n    \n    do {\n      const [newCursor, keys] = await this.pool.scan(\n        cursor, \n        'MATCH', \n        'ratelimit:*', \n        'COUNT', \n        1000\n      );\n      cursor = newCursor;\n      count += keys.length;\n      iterations++;\n    } while (cursor !== '0' && iterations < maxIterations);\n    \n    // Extrapolate if we stopped early\n    if (cursor !== '0') {\n      count = Math.ceil(count * (1000 / iterations));\n    }\n    \n    return count;\n  }\n}\n\n// Lua script for atomic token consumption\nconst CONSUME_TOKENS_SCRIPT = `\n  local key = KEYS[1]\n  local capacity = tonumber(ARGV[1])\n  local refill_rate = tonumber(ARGV[2])\n  local requested = tonumber(ARGV[3])\n  local now = tonumber(ARGV[4])\n  local refill_interval = tonumber(ARGV[5])\n  local max_burst = tonumber(ARGV[6])\n  local ttl = tonumber(ARGV[7])\n  \n  -- Input validation\n  if requested > capacity * 10 then\n    return redis.error_reply(\"Token request exceeds reasonable limit\")\n  end\n  \n  -- Get current state\n  local bucket = redis.call('HMGET', key, 'tokens', 'last_refill')\n  local tokens = tonumber(bucket[1])\n  local last_refill = tonumber(bucket[2])\n  \n  -- Initialize new bucket\n  if not tokens then\n    tokens = capacity\n    last_refill = now\n  end\n  \n  -- Calculate refill\n  local elapsed = now - last_refill\n  local refill_periods = math.floor(elapsed / refill_interval)\n  \n  if refill_periods > 0 then\n    local tokens_to_add = refill_periods * refill_rate\n    tokens = math.min(max_burst, tokens + tokens_to_add)\n    last_refill = last_refill + (refill_periods * refill_interval)\n  end\n  \n  -- Attempt consumption\n  if tokens >= requested then\n    tokens = tokens - requested\n    redis.call('HMSET', key, 'tokens', tokens, 'last_refill', last_refill)\n    redis.call('PEXPIRE', key, ttl)\n    \n    local next_refill_at = last_refill + refill_interval\n    return {1, tokens, next_refill_at, 0}\n  else\n    -- Calculate wait time\n    local tokens_needed = requested - tokens\n    local refills_needed = math.ceil(tokens_needed / refill_rate)\n    local wait_ms = (refills_needed * refill_interval) - elapsed + (refill_periods * refill_interval)\n    local reset_at = last_refill + (refills_needed * refill_interval)\n    \n    -- Don't update state on denial, but ensure TTL is set\n    if redis.call('EXISTS', key) == 1 then\n      redis.call('PEXPIRE', key, ttl)\n    end\n    \n    return {0, tokens, reset_at, wait_ms}\n  end\n`;\n```\n\n### 3. Optional Local Cache Layer\n\n```typescript\ninterface LocalCacheConfig {\n  enabled: boolean;\n  maxSize: number;\n  ttlMs: number;\n  accuracyThreshold: number;  // Sync when tokens drop below this % of capacity\n}\n\nclass LocalCacheLayer {\n  private cache: Map<string, CachedBucket>;\n  private backend: StorageBackend;\n  private config: LocalCacheConfig;\n  \n  constructor(config: LocalCacheConfig, backend: StorageBackend) {\n    this.cache = new Map();\n    this.backend = backend;\n    this.config = config;\n    \n    // LRU eviction\n    this.startEvictionTimer();\n  }\n  \n  async tryConsume(key: string, tokens: number, limit: RateLimit): Promise<RateLimitResult | null> {\n    const cached = this.cache.get(key);\n    \n    // Cache miss or stale\n    if (!cached || this.isStale(cached)) {\n      return null;\n    }\n    \n    // Simulate refill\n    const now = Date.now();\n    const elapsed = now - cached.lastRefillTime;\n    const refillPeriods = Math.floor(elapsed / limit.refillInterval);\n    const tokensToAdd = refillPeriods * limit.refillRate;\n    const maxBurst = limit.capacity + (limit.burstAllowance ?? 0);\n    \n    let currentTokens = Math.min(maxBurst, cached.tokens + tokensToAdd);\n    const utilizationPercent = (currentTokens / limit.capacity) * 100;\n    \n    // Force backend sync when approaching limit for accuracy\n    if (utilizationPercent < this.config.accuracyThreshold) {\n      return null;\n    }\n    \n    // Allow optimistically\n    if (currentTokens >= tokens) {\n      currentTokens -= tokens;\n      \n      cached.tokens = currentTokens;\n      cached.lastRefillTime += (refillPeriods * limit.refillInterval);\n      cached.lastAccessTime = now;\n      \n      return {\n        allowed: true,\n        remaining: Math.floor(currentTokens),\n        resetAt: new Date(cached.lastRefillTime + limit.refillInterval),\n        limit: limit.capacity\n      };\n    }\n    \n    // Near-limit denial: force backend check\n    return null;\n  }\n  \n  async updateFromBackend(key: string, result: RateLimitResult): Promise<void> {\n    if (this.cache.size >= this.config.maxSize) {\n      this.evictOldest();\n    }\n    \n    this.cache.set(key, {\n      tokens: result.remaining,\n      lastRefillTime: result.resetAt.getTime(),\n      lastAccessTime: Date.now(),\n      cachedAt: Date.now()\n    });\n  }\n  \n  async invalidate(key: string): Promise<void> {\n    this.cache.delete(key);\n  }\n  \n  private isStale(cached: CachedBucket): boolean {\n    return (Date.now() - cached.cachedAt) > this.config.ttlMs;\n  }\n  \n  private evictOldest(): void {\n    let oldestKey: string | null = null;\n    let oldestTime = Infinity;\n    \n    for (const [key, bucket] of this.cache.entries()) {\n      if (bucket.lastAccessTime < oldestTime) {\n        oldestTime = bucket.lastAccessTime;\n        oldestKey = key;\n      }\n    }\n    \n    if (oldestKey) {\n      this.cache.delete(oldestKey);\n    }\n  }\n  \n  private startEvictionTimer(): void {\n    setInterval(() => {\n      const now = Date.now();\n      for (const [key, bucket] of this.cache.entries()) {\n        if (now - bucket.cachedAt > this.config.ttlMs) {\n          this.cache.delete(key);\n        }\n      }\n    }, this.config.ttlMs);\n  }\n}\n\ninterface CachedBucket {\n  tokens: number;\n  lastRefillTime: number;\n  lastAccessTime: number;\n  cachedAt: number;\n}\n```\n\n### 4. Configuration Provider (Dynamic Config Support)\n\n```typescript\ninterface ConfigProvider {\n  getLimit(name: string): RateLimit;\n  getAllLimits(): RateLimit[];\n  updateLimit(name: string, limit: RateLimit): Promise<void>;\n  reload(): Promise<void>;\n}\n\nclass StaticConfigProvider implements ConfigProvider {\n  private limits: Map<string, RateLimit>;\n  \n  constructor(config: RateLimiterConfig) {\n    this.limits = new Map(config.limits.map(l => [l.name, l]));\n  }\n  \n  getLimit(name: string): RateLimit {\n    const limit = this.limits.get(name);\n    if (!limit) {\n      throw new RateLimitError(`Unknown limit: ${name}`, RateLimitErrorCode.INVALID_CONFIG);\n    }\n    return limit;\n  }\n  \n  getAllLimits(): RateLimit[] {\n    return Array.from(this.limits.values());\n  }\n  \n  async updateLimit(name: string, limit: RateLimit): Promise<void> {\n    throw new Error('Static configuration does not support runtime updates');\n  }\n  \n  async reload(): Promise<void> {\n    // No-op for static config\n  }\n}\n\nclass DynamicConfigProvider implements ConfigProvider {\n  private limits: Map<string, RateLimit>;\n  private configStore: ConfigStore;\n  private refreshInterval: number;\n  \n  constructor(configStore: ConfigStore, refreshInterval: number = 60000) {\n    this.limits = new Map();\n    this.configStore = configStore;\n    this.refreshInterval = refreshInterval;\n    \n    this.startRefreshTimer();\n  }\n  \n  async initialize(): Promise<void> {\n    await this.reload();\n  }\n  \n  getLimit(name: string): RateLimit {\n    const limit = this.limits.get(name);\n    if (!limit) {\n      throw new RateLimitError(`Unknown limit: ${name}`, RateLimitErrorCode.INVALID_CONFIG);\n    }\n    return limit;\n  }\n  \n  getAllLimits(): RateLimit[] {\n    return Array.from(this.limits.values());\n  }\n  \n  async updateLimit(name: string, limit: RateLimit): Promise<void> {\n    await this.configStore.save(name, limit);\n    await this.reload();\n  }\n  \n  async reload(): Promise<void> {\n    const limits = await this.configStore.loadAll();\n    this.limits = new Map(limits.map(l => [l.name, l]));\n  }\n  \n  private startRefreshTimer(): void {\n    setInterval(() => {\n      this.reload().catch(err => {\n        console.error('Failed to reload config:', err);\n      });\n    }, this.refreshInterval);\n  }\n}\n\ninterface ConfigStore {\n  loadAll(): Promise<RateLimit[]>;\n  save(name: string, limit: RateLimit): Promise<void>;\n}\n```\n\n### 5. HTTP Middleware\n\n```typescript\ninterface MiddlewareConfig {\n  rateLimiter: RateLimiter;\n  limitName: string;\n  keyExtractor?: (req: Request) => RequestContext;\n  onLimitExceeded?: (req: Request, result: RateLimitResult) => Response;\n  skipCondition?: (req: Request) => boolean;\n}\n\nfunction createRateLimitMiddleware(config: MiddlewareConfig) {\n  return async (req: Request, res: Response, next: NextFunction) => {\n    if (config.skipCondition?.(req)) {\n      return next();\n    }\n    \n    const context = config.keyExtractor?.(req) ?? {\n      userId: req.user?.id,\n      ipAddress: req.ip,\n      endpoint: req.path,\n      method: req.method,\n      timestamp: Date.now()\n    };\n    \n    try {\n      const result = await config.rateLimiter.checkLimit(context, config.limitName);\n      \n      // Standard rate limit headers\n      res.setHeader('X-RateLimit-Limit', result.limit);\n      res.setHeader('X-RateLimit-Remaining', Math.max(0, result.remaining));\n      res.setHeader('X-RateLimit-Reset', Math.floor(result.resetAt.getTime() / 1000));\n      \n      if (!result.allowed) {\n        res.setHeader('Retry-After', result.retryAfter ?? 60);\n        \n        if (config.onLimitExceeded) {\n          return config.onLimitExceeded(req, result);\n        }\n        \n        return res.status(429).json({\n          error: 'Too Many Requests',\n          retryAfter: result.retryAfter,\n          resetAt: result.resetAt.toISOString()\n        });\n      }\n      \n      next();\n      \n    } catch (error) {\n      // Fail-open on rate limiter failure\n      console.error('Rate limiter error:', error);\n      next();\n    }\n  };\n}\n```\n\n### 6. Circuit Breaker\n\n```typescript\ninterface CircuitBreakerConfig {\n  failureThreshold: number;\n  failureWindowMs: number;\n  resetTimeoutMs: number;\n  halfOpenMaxAttempts: number;\n}\n\nclass CircuitBreaker {\n  private state: 'closed' | 'open' | 'half_open' = 'closed';\n  private failures: Array<number> = [];\n  private halfOpenAttempts = 0;\n  private lastStateChange = Date.now();\n  \n  constructor(private config: CircuitBreakerConfig) {}\n  \n  async execute<T>(operation: () => Promise<T>): Promise<T> {\n    if (this.state === 'open') {\n      if (Date.now() - this.lastStateChange >= this.config.resetTimeoutMs) {\n        this.transitionTo('half_open');\n      } else {\n        throw new RateLimitError(\n          'Circuit breaker open',\n          RateLimitErrorCode.STORAGE_UNAVAILABLE\n        );\n      }\n    }\n    \n    try {\n      const result = await operation();\n      this.recordSuccess();\n      return result;\n      \n    } catch (error) {\n      this.recordFailure();\n      throw error;\n    }\n  }\n  \n  private recordSuccess(): void {\n    if (this.state === 'half_open') {\n      this.halfOpenAttempts++;\n      if (this.halfOpenAttempts >= this.config.halfOpenMaxAttempts) {\n        this.transitionTo('closed');\n      }\n    }\n  }\n  \n  private recordFailure(): void {\n    const now = Date.now();\n    this.failures.push(now);\n    \n    // Remove old failures outside window\n    this.failures = this.failures.filter(\n      time => now - time < this.config.failureWindowMs\n    );\n    \n    if (this.state === 'half_open') {\n      this.transitionTo('open');\n    } else if (this.failures.length >= this.config.failureThreshold) {\n      this.transitionTo('open');\n    }\n  }\n  \n  private transitionTo(newState: 'closed' | 'open' | 'half_open'): void {\n    this.state = newState;\n    this.lastStateChange = Date.now();\n    \n    if (newState === 'closed') {\n      this.failures = [];\n      this.halfOpenAttempts = 0;\n    } else if (newState === 'half_open') {\n      this.halfOpenAttempts = 0;\n    }\n  }\n  \n  getState(): string {\n    return this.state;\n  }\n}\n```\n\n### 7. Fallback Handler\n\n```typescript\ninterface FallbackConfig {\n  strategy: 'fail_open' | 'fail_closed';\n  circuitBreaker: CircuitBreakerConfig;\n}\n\nclass FallbackHandler {\n  constructor(private config: FallbackConfig) {}\n  \n  async handleFailure(\n    key: string,\n    tokens: number,\n    limit: RateLimit,\n    error: Error\n  ): Promise<RateLimitResult> {\n    if (this.config.strategy === 'fail_open') {\n      // Allow request when system is down\n      return {\n        allowed: true,\n        remaining: limit.capacity,\n        resetAt: new Date(Date.now() + limit.refillInterval),\n        limit: limit.capacity\n      };\n    } else {\n      // Deny request when system is down\n      throw new RateLimitError(\n        'Rate limiting service unavailable',\n        RateLimitErrorCode.STORAGE_UNAVAILABLE,\n        60\n      );\n    }\n  }\n}\n```\n\n## Configuration Schema\n\n```yaml\nrate_limits:\n  - name: api_global\n    capacity: 10000\n    refill_rate: 100\n    refill_interval: 1000  # ms\n    scope: global\n    burst_allowance: 0\n    priority: standard\n    \n  - name: api_per_user\n    capacity: 1000\n    refill_rate: 10\n    refill_interval: 1000\n    scope: per_user\n    burst_allowance: 500  # Allow burst to 1500\n    priority: standard\n    \n  - name: login_attempts\n    capacity: 5\n    refill_rate: 1\n    refill_interval: 60000\n    scope: per_ip\n    burst_allowance: 0\n    priority: strict  # Never use cache\n\nstorage:\n  type: redis\n  nodes:\n    - host: redis-1.example.com\n      port: 6379\n    - host: redis-2.example.com\n      port: 6379\n  pool_size: 50\n  connection_timeout_ms: 100\n  operation_timeout_ms: 50\n\nfallback:\n  strategy: fail_open  # or fail_closed\n  circuit_breaker:\n    failure_threshold: 5\n    failure_window_ms: 10000\n    reset_timeout_ms: 30000\n    half_open_max_attempts: 3\n\nlocal_cache:  # Optional\n  enabled: true\n  max_size: 10000\n  ttl_ms: 1000\n  accuracy_threshold: 20  # Force backend sync below 20% tokens\n\nconfig_provider:\n  type: static  # or dynamic\n  # For dynamic:\n  # store: postgres\n  # refresh_interval_ms: 60000\n\nmonitoring:\n  metrics_enabled: true\n  prometheus_port: 9090\n```\n\n## Monitoring\n\n```typescript\ninterface MetricsCollector {\n  recordRequest(limitName: string, allowed: boolean, latencyMs: number, cacheHit: boolean): void;\n  recordStorageError(error: Error): void;\n  recordCacheHit(hit: boolean): void;\n}\n\n// Prometheus metrics\nconst metrics = {\n  requests_total: new Counter({\n    name: 'rate_limit_requests_total',\n    help: 'Total rate limit checks',\n    labelNames: ['limit_name', 'allowed', 'cache_hit']\n  }),\n  \n  check_duration_seconds: new Histogram({\n    name: 'rate_limit_check_duration_seconds',\n    help: 'Rate limit check latency',\n    labelNames: ['limit_name', 'cache_hit'],\n    buckets: [0.001, 0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5]\n  }),\n  \n  storage_errors_total: new Counter({\n    name: 'rate_limit_storage_errors_total',\n    help: 'Storage errors',\n    labelNames: ['error_type']\n  }),\n  \n  circuit_breaker_state: new Gauge({\n    name: 'rate_limit_circuit_breaker_state',\n    help: 'Circuit breaker state (0=closed, 1=open, 2=half_open)'\n  }),\n  \n  active_keys: new Gauge({\n    name: 'rate_limit_active_keys',\n    help: 'Number of active rate limit keys'\n  })\n};\n```\n\n## Error Handling\n\n```typescript\nclass RateLimitError extends Error {\n  constructor(\n    message: string,\n    public code: RateLimitErrorCode,\n    public retryAfter?: number\n  ) {\n    super(message);\n    this.name = 'RateLimitError';\n  }\n}\n\nenum RateLimitErrorCode {\n  STORAGE_UNAVAILABLE = 'STORAGE_UNAVAILABLE',\n  INVALID_CONFIG = 'INVALID_CONFIG',\n  INVALID_KEY = 'INVALID_KEY',\n  INVALID_TOKEN_COST = 'INVALID_TOKEN_COST',\n  STORAGE_TIMEOUT = 'STORAGE_TIMEOUT'\n}\n```\n\n## Deployment Phases\n\n### Phase 1: Core Functionality (Week 1-2)\n**Goal**: Production-ready basic rate limiting\n\n- Token bucket algorithm with Redis backend\n- Atomic Lua script for consistency\n- Circuit breaker for resilience\n- Basic monitoring (Prometheus metrics)\n- Fail-open fallback strategy\n- Static configuration\n- HTTP middleware\n\n**Deliverables**:\n- Single-region deployment\n- 10K req/sec per instance\n- P99 latency <20ms\n- No local cache\n\n### Phase 2: Performance Optimization (Week 3-4)\n**Goal**: Scale to high-throughput scenarios\n\n- Local cache layer (opt-in per limit via `priority` field)\n- Cache accuracy monitoring\n- Script pre-loading (EVALSHA)\n- Connection pooling optimization\n\n**Deliverables**:\n- 200K req/sec per instance (with cache)\n- P99 latency <2ms (cache hits)\n- Cache divergence metrics\n- Performance benchmarks\n\n### Phase 3: Advanced Features (Week 5-6)\n**Goal**: Production operations and flexibility\n\n- Dynamic configuration (database-backed)\n- Admin API (reset limits, add quota)\n- Key cleanup job\n- Variable token costs\n- Burst allowance tuning\n\n**Deliverables**:\n- Runtime config updates\n- Operational playbooks\n- Cost-per-operation support\n\n### Phase 4: Multi-Region (Future)\n**Goal**: Global deployment\n\n- Regional Redis clusters\n- Cross-region sync strategy\n- Geolocation-aware routing"
}