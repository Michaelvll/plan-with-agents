{
  "initial_prompt": "Design a file upload API",
  "status": "debating",
  "rounds": 2,
  "started_at": "2026-01-17T09:01:45.430052",
  "ended_at": "2026-01-17T09:12:19.948535",
  "final_design": "# File Upload API Design\n\n## Architecture Overview\n\n**Pattern**: RESTful API with resumable upload support using native cloud provider multipart capabilities  \n**Transport**: HTTP/HTTPS with streaming upload and chunked transfer encoding  \n**Storage**: Hybrid storage layer leveraging native cloud multipart APIs with fallback to manual assembly  \n**Processing**: Async job queue for post-upload processing with streaming verification  \n**Scale Target**: 1TB files, multi-day upload sessions, 10,000+ concurrent uploads\n\n## API Endpoints\n\n### 1. Initiate Upload\n```\nPOST /api/v1/uploads/initiate\nContent-Type: application/json\nAuthorization: Bearer {token}\n\nRequest:\n{\n  \"filename\": \"large-video.mp4\",\n  \"size\": 107374182400,  // 100GB\n  \"mimeType\": \"video/mp4\",\n  \"metadata\": {\n    \"tags\": [\"presentation\", \"2024\"],\n    \"visibility\": \"private\"\n  },\n  \"chunkSize\": 10485760,  // 10MB recommended for optimal throughput\n  \"checksumAlgorithm\": \"sha256\",\n  \"uploadMode\": \"auto\"  // auto, native, or manual - server chooses best method\n}\n\nResponse: 201 Created\n{\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"uploadToken\": \"tok_secure_random_token\",\n  \"expiresAt\": \"2024-01-26T10:30:00Z\",  // 7 days for large files\n  \"uploadUrl\": \"/api/v1/uploads/upl_7x9k2m3n4p5q6r/chunks\",\n  \"resumable\": true,\n  \"uploadMethod\": \"native_multipart\",  // native_multipart or managed_chunks\n  \"chunkSize\": 10485760,\n  \"minChunkSize\": 5242880,  // 5MB minimum for S3 compatibility\n  \"maxChunkSize\": 104857600,  // 100MB maximum\n  \"expectedChunks\": 10240,\n  \"serverChecksum\": true,\n  \"capabilities\": {\n    \"parallelUploads\": 10,  // Number of chunks that can be uploaded concurrently\n    \"streamingVerification\": true,  // Server verifies checksums during upload\n    \"pauseResume\": true,\n    \"bandwidthThrottling\": true\n  },\n  \"cloudProvider\": {\n    \"type\": \"s3\",  // Exposed for client optimization\n    \"multipartUploadId\": \"mp_aws_xyz123\",  // Native cloud provider upload ID\n    \"region\": \"us-east-1\"\n  }\n}\n```\n\n### 2. Upload Chunk (Parallel-Friendly)\n```\nPUT /api/v1/uploads/{uploadId}/chunks/{chunkIndex}\nContent-Type: application/octet-stream\nContent-Length: 10485760\nContent-Range: bytes 0-10485759/107374182400\nX-Chunk-Checksum: sha256:a1b2c3d4...\nX-Upload-Token: tok_secure_random_token\nAuthorization: Bearer {token}\n\nRequest Body:\n[binary data]\n\nResponse: 200 OK\n{\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"chunkIndex\": 0,\n  \"chunkChecksum\": \"sha256:a1b2c3d4...\",\n  \"checksumMatch\": true,  // Immediate verification during upload\n  \"bytesReceived\": 10485760,\n  \"etag\": \"abc123def456\",  // Cloud provider ETag for native multipart\n  \"uploadedAt\": \"2024-01-19T10:15:30Z\",\n  \"uploadDurationMs\": 1250,\n  \"transferRate\": \"8.39 MB/s\"\n}\n```\n\n### 3. Get Upload Status (Enhanced Resume Information)\n```\nGET /api/v1/uploads/{uploadId}\nAuthorization: Bearer {token}\n\nResponse: 200 OK\n{\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"status\": \"in_progress\",\n  \"bytesReceived\": 1073741824,  // 1GB so far\n  \"bytesTotal\": 107374182400,\n  \"percentComplete\": 1.0,\n  \"chunksCompleted\": [0, 1, 2, 3, ..., 102],  // Can be large array\n  \"chunksCompletedCount\": 103,\n  \"chunksRemaining\": 10137,\n  \"chunksInProgress\": [104, 105, 106],  // Currently being uploaded by client\n  \"estimatedTimeRemainingSeconds\": 86400,  // 24 hours at current rate\n  \"uploadRateMBps\": 1.15,\n  \"lastChunkReceivedAt\": \"2024-01-19T10:15:00Z\",\n  \"createdAt\": \"2024-01-19T10:00:00Z\",\n  \"expiresAt\": \"2024-01-26T10:00:00Z\",\n  \"uploadToken\": \"tok_resume_token\",\n  \"integrityStatus\": {\n    \"chunksVerified\": 103,\n    \"verificationFailures\": [],\n    \"streamingVerificationEnabled\": true\n  },\n  \"cloudProvider\": {\n    \"multipartUploadId\": \"mp_aws_xyz123\",\n    \"uploadedParts\": [  // Native cloud provider part tracking\n      {\"partNumber\": 1, \"etag\": \"abc123\"},\n      {\"partNumber\": 2, \"etag\": \"def456\"}\n    ]\n  },\n  \"extensionRequest\": {\n    \"available\": true,  // Can request extension before expiry\n    \"maxExtensionHours\": 168  // Additional 7 days\n  }\n}\n```\n\n### 4. Complete Upload (Hybrid Verification)\n```\nPOST /api/v1/uploads/{uploadId}/complete\nContent-Type: application/json\nX-Upload-Token: tok_secure_random_token\nAuthorization: Bearer {token}\n\nRequest:\n{\n  \"finalChecksum\": \"sha256:f1n2l3c4...\",\n  \"verificationMode\": \"streaming\",  // streaming or full - streaming skips re-verification\n  \"chunkCount\": 10240  // Simple count check, not full manifest for large files\n}\n\nResponse: 202 Accepted  // Async completion for large files\n{\n  \"fileId\": \"file_9z8y7x6w5v\",\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"status\": \"finalizing\",  // finalizing -> processing -> ready\n  \"completionJobId\": \"job_completion_xyz\",\n  \"estimatedCompletionSeconds\": 120,\n  \"statusUrl\": \"/api/v1/uploads/upl_7x9k2m3n4p5q6r/completion-status\",\n  \"webhookUrl\": \"/api/v1/webhooks/file_9z8y7x6w5v\",\n  \"integrityVerification\": {\n    \"method\": \"streaming\",  // Already verified during upload\n    \"requiresFullScan\": false\n  }\n}\n```\n\n### 5. Get Completion Status\n```\nGET /api/v1/uploads/{uploadId}/completion-status\nAuthorization: Bearer {token}\n\nResponse: 200 OK\n{\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"fileId\": \"file_9z8y7x6w5v\",\n  \"status\": \"processing\",  // finalizing, processing, ready, failed\n  \"completionProgress\": {\n    \"stage\": \"virus_scan\",  // assembly, checksum_verification, virus_scan, metadata_extraction\n    \"percentComplete\": 45,\n    \"estimatedSecondsRemaining\": 30\n  },\n  \"integrityVerified\": true,\n  \"file\": {\n    \"url\": \"/api/v1/files/file_9z8y7x6w5v\",\n    \"size\": 107374182400,\n    \"checksum\": \"sha256:f1n2l3c4...\",\n    \"status\": \"processing\"\n  }\n}\n```\n\n### 6. Request Upload Extension\n```\nPOST /api/v1/uploads/{uploadId}/extend\nContent-Type: application/json\nAuthorization: Bearer {token}\n\nRequest:\n{\n  \"extensionHours\": 168  // Request 7 more days\n}\n\nResponse: 200 OK\n{\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"oldExpiresAt\": \"2024-01-26T10:00:00Z\",\n  \"newExpiresAt\": \"2024-02-02T10:00:00Z\",\n  \"extensionsRemaining\": 2,  // Limit extensions to prevent abuse\n  \"maxTotalUploadDays\": 30\n}\n```\n\n### 7. Pause Upload (Explicit State Management)\n```\nPOST /api/v1/uploads/{uploadId}/pause\nAuthorization: Bearer {token}\n\nResponse: 200 OK\n{\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"status\": \"paused\",\n  \"pausedAt\": \"2024-01-19T15:00:00Z\",\n  \"bytesReceived\": 1073741824,\n  \"chunksCompleted\": 103,\n  \"resumeToken\": \"tok_resume_xyz\",\n  \"pausedExpiresAt\": \"2024-01-26T15:00:00Z\"  // Paused uploads still expire\n}\n```\n\n### 8. Cancel Upload (Enhanced Cleanup)\n```\nDELETE /api/v1/uploads/{uploadId}\nAuthorization: Bearer {token}\nX-Upload-Token: tok_secure_random_token\n\nResponse: 202 Accepted  // Async cleanup for large uploads\n{\n  \"uploadId\": \"upl_7x9k2m3n4p5q6r\",\n  \"status\": \"cancelling\",\n  \"cleanupJobId\": \"job_cleanup_abc\",\n  \"estimatedCleanupSeconds\": 60,\n  \"storageToFree\": 1073741824\n}\n\nHeaders:\n  X-Cleanup-Status: in_progress\n```\n\n### 9. Simple Direct Upload (Small Files <100MB)\n```\nPOST /api/v1/files/direct\nContent-Type: multipart/form-data\nAuthorization: Bearer {token}\n\nRequest:\n- file: binary data\n- metadata: JSON string\n- checksum: sha256 hash\n\nResponse: 201 Created\n{\n  \"fileId\": \"file_9z8y7x6w5v\",\n  \"url\": \"/api/v1/files/file_9z8y7x6w5v\",\n  \"status\": \"processing\",\n  \"integrityVerified\": true,\n  \"size\": 10485760\n}\n```\n\n### 10. Get Signed Download URL (With Resume Support)\n```\nPOST /api/v1/files/{fileId}/download-url\nAuthorization: Bearer {token}\nContent-Type: application/json\n\nRequest:\n{\n  \"expiresIn\": 3600,\n  \"disposition\": \"attachment\",\n  \"supportRangeRequests\": true  // Enable resume for downloads\n}\n\nResponse: 200 OK\n{\n  \"downloadUrl\": \"https://storage.example.com/signed-url?token=...\",\n  \"expiresAt\": \"2024-01-19T11:00:00Z\",\n  \"contentType\": \"video/mp4\",\n  \"size\": 107374182400,\n  \"supportsRangeRequests\": true,\n  \"checksumHeader\": \"x-amz-checksum-sha256\",  // Header to verify download integrity\n  \"recommendedChunkSize\": 10485760\n}\n```\n\n### 11. Batch Operations (Performance Optimization)\n```\nPOST /api/v1/uploads/batch/status\nContent-Type: application/json\nAuthorization: Bearer {token}\n\nRequest:\n{\n  \"uploadIds\": [\"upl_1\", \"upl_2\", \"upl_3\", ...]  // Up to 100 IDs\n}\n\nResponse: 200 OK\n{\n  \"uploads\": [\n    {\"uploadId\": \"upl_1\", \"status\": \"in_progress\", \"percentComplete\": 45, ...},\n    {\"uploadId\": \"upl_2\", \"status\": \"completed\", \"fileId\": \"file_xyz\", ...}\n  ]\n}\n```\n\n## Data Models\n\n### Upload Session (Optimized for Scale)\n```typescript\ninterface UploadSession {\n  uploadId: string;\n  userId: string;\n  filename: string;\n  size: number;\n  mimeType: string;\n  metadata: Record<string, any>;\n  status: 'initiated' | 'in_progress' | 'paused' | 'finalizing' | 'completed' | 'failed' | 'cancelled' | 'quarantined';\n  bytesReceived: number;\n  chunkSize: number;\n  expectedChunks: number;\n  \n  // Hybrid chunk tracking - full list for small uploads, summary for large\n  chunkTrackingMode: 'full' | 'bitmap' | 'summary';\n  chunksCompletedBitmap?: Buffer;  // Efficient bitmap for 10,000+ chunks\n  chunksCompletedCount: number;\n  chunksCompletedRanges?: Array<{start: number, end: number}>;  // Compressed representation\n  lastChunkIndex: number;\n  \n  // Native cloud provider integration\n  uploadMethod: 'native_multipart' | 'managed_chunks';\n  cloudProvider: {\n    type: 's3' | 'gcs' | 'azure';\n    multipartUploadId?: string;  // S3 multipart upload ID\n    uploadedParts: Array<{partNumber: number, etag: string, size: number}>;\n    region: string;\n  };\n  \n  storageKey: string;\n  tempStoragePath?: string;  // Only for managed_chunks mode\n  checksumAlgorithm: 'sha256' | 'sha512' | 'blake3';\n  finalChecksum?: string;\n  \n  // Streaming verification (incremental checksum computation)\n  streamingVerification: {\n    enabled: boolean;\n    currentChecksum: string;  // Running checksum state\n    verifiedBytes: number;\n    lastVerifiedAt: Date;\n  };\n  \n  // Security tokens (simplified for performance)\n  currentUploadToken: string;\n  tokenRotationEnabled: boolean;  // Configurable - false for internal systems\n  tokenExpiresAt: Date;\n  \n  // Concurrency control (lightweight)\n  lockVersion: number;\n  activeChunkUploads: Set<number>;  // Track in-flight chunks\n  maxConcurrentChunks: number;\n  \n  // Lifecycle management\n  createdAt: Date;\n  updatedAt: Date;\n  lastActivityAt: Date;\n  expiresAt: Date;\n  extensionCount: number;\n  pausedAt?: Date;\n  \n  // Performance metrics\n  metrics: {\n    averageChunkUploadMs: number;\n    averageTransferRateMBps: number;\n    totalRetries: number;\n    failedChunks: number;\n  };\n  \n  // Audit (minimal, expanded in separate audit log)\n  clientInfo: {\n    ipAddress: string;\n    userAgent: string;\n    sdkVersion?: string;\n  };\n}\n\ninterface UploadChunk {\n  index: number;\n  size: number;\n  checksum: string;\n  etag?: string;  // Cloud provider ETag\n  receivedAt: Date;\n  verified: boolean;\n  retryCount: number;\n  uploadDurationMs: number;\n}\n```\n\n### File Record (Production-Ready)\n```typescript\ninterface FileRecord {\n  fileId: string;\n  userId: string;\n  filename: string;\n  originalFilename: string;\n  size: number;\n  mimeType: string;\n  detectedMimeType: string;\n  \n  // Storage details\n  storage: {\n    provider: 's3' | 'gcs' | 'azure' | 'local';\n    key: string;\n    bucket: string;\n    region: string;\n    storageClass: 'standard' | 'infrequent' | 'archive';\n    redundancy: 'single-region' | 'multi-region';\n    encryption: {\n      type: 'sse-s3' | 'sse-kms' | 'client-side' | 'none';\n      keyId?: string;\n      algorithm?: string;\n    };\n  };\n  \n  // Integrity\n  checksum: string;\n  checksumAlgorithm: string;\n  verificationMethod: 'streaming' | 'post-upload' | 'deferred';\n  integrityVerifiedAt: Date;\n  \n  metadata: Record<string, any>;\n  \n  // Status and processing\n  status: 'finalizing' | 'processing' | 'ready' | 'failed' | 'quarantined' | 'archived' | 'deleted';\n  \n  securityScan: {\n    status: 'pending' | 'scanning' | 'clean' | 'infected' | 'suspicious' | 'failed' | 'skipped';\n    scannedAt?: Date;\n    scanner: string;\n    scannerVersion?: string;\n    threats?: Array<{name: string, severity: string}>;\n    scanDurationMs?: number;\n  };\n  \n  processingJobs: ProcessingJob[];\n  \n  // Access control\n  accessControl: {\n    visibility: 'private' | 'internal' | 'public';\n    allowedUsers?: string[];\n    allowedGroups?: string[];\n    allowedUntil?: Date;  // Temporary access grants\n  };\n  \n  // URLs (lazy-generated, not stored)\n  // Use signed URL generation endpoint instead\n  \n  // Retention and lifecycle\n  retention: {\n    policy: 'standard' | 'extended' | 'permanent' | 'custom';\n    deleteAt?: Date;\n    archiveAt?: Date;\n    lastAccessedAt?: Date;\n    accessCount: number;\n  };\n  \n  // Derived files (thumbnails, transcodes)\n  derivatives: Array<{\n    type: 'thumbnail' | 'preview' | 'transcode';\n    format: string;\n    size: number;\n    storageKey: string;\n    url: string;\n  }>;\n  \n  // Traceability\n  uploadSessionId: string;\n  sourceIpAddress: string;\n  \n  // Performance metadata\n  uploadDurationSeconds: number;\n  processingDurationSeconds: number;\n  \n  // Timestamps\n  createdAt: Date;\n  updatedAt: Date;\n  completedAt?: Date;\n  deletedAt?: Date;\n  \n  // Audit trail reference (not embedded for performance)\n  auditLogId: string;\n}\n\ninterface ProcessingJob {\n  jobId: string;\n  type: 'assembly' | 'virus_scan' | 'thumbnail' | 'metadata_extract' | 'transcode' | 'checksum_verify';\n  priority: number;  // Higher priority = processed first\n  status: 'queued' | 'running' | 'completed' | 'failed' | 'cancelled';\n  queuedAt: Date;\n  startedAt?: Date;\n  completedAt?: Date;\n  error?: string;\n  retryCount: number;\n  maxRetries: number;\n  progressPercent: number;\n  estimatedDurationSeconds?: number;\n  dependencies: string[];  // Job IDs that must complete first\n}\n```\n\n## Component Architecture\n\n### 1. API Layer\n- **UploadController**: HTTP request handling, validation, auth\n- **FileController**: File retrieval, metadata operations\n- **BatchController**: Bulk operations for performance\n- **Middleware**:\n  - Auth (JWT validation with 15min cache)\n  - Rate limiting (token bucket algorithm, Redis-backed)\n  - Request size validation\n  - CSRF protection (double-submit cookie)\n  - Content-Type validation\n  - Upload token verification (conditional based on config)\n  - Request ID injection (distributed tracing)\n\n### 2. Business Logic Layer\n\n#### Upload Service (Core Orchestration)\n- **UploadOrchestrator**: High-level upload flow management\n  - `initiateUpload()`: Determines native vs managed mode based on file size and provider\n  - `selectUploadStrategy()`: Auto-selects optimal chunk size and concurrency\n  - `routeChunkUpload()`: Routes to native or managed handler\n\n#### Native Multipart Handler (Performance Path)\n- **S3MultipartAdapter**: \n  - Uses `CreateMultipartUpload`, `UploadPart`, `CompleteMultipartUpload` APIs\n  - Each chunk maps to S3 part (1:1 mapping)\n  - Verification: Client checksum + S3 ETag validation\n  - Assembly: S3 native (instant, no data movement)\n  - Trade-off: Less control, maximum performance\n  - Supports up to 10,000 parts (10TB at 1GB chunks)\n  \n- **GCSResumableAdapter**:\n  - Uses GCS resumable upload protocol\n  - Single resumable session for entire file\n  - Verification: MD5 hash + CRC32C\n  - Assembly: Native (automatic)\n  \n- **AzureBlockBlobAdapter**:\n  - Uses Block Blob staging and commitment\n  - Blocks up to 50,000 (4.75TB at 100MB blocks)\n  - Verification: MD5 per block + CRC64 for blob\n\n#### Managed Chunk Handler (Control Path)\n- **ManagedChunkService**:\n  - Manual chunk storage and assembly\n  - Used when: file < 100MB, or native APIs unavailable\n  - Verification: Dual checksum (client + server streaming)\n  - Assembly: Custom concatenation with re-verification\n  - Trade-off: Full control, slower for large files\n\n#### Supporting Services\n- **StreamingChecksumService**: \n  - Computes checksums incrementally during upload (no buffering)\n  - Maintains checksum state in Redis (serializable)\n  - Supports resume without recomputing from start\n  \n- **ChunkTrackerService**:\n  - Bitmap storage for 10,000+ chunk tracking\n  - Range compression (e.g., \"0-500, 750-1000\" vs list of 1001 numbers)\n  - O(1) chunk completion lookup\n  \n- **UploadStrategyService**:\n  - Determines optimal chunk size based on file size and network conditions\n  - Recommends concurrency level (adaptive to user tier and system load)\n  - Auto-tunes based on historical performance metrics\n  \n- **ValidationService**: \n  - MIME type verification (magic numbers + extension)\n  - Filename sanitization (path traversal prevention)\n  - Size and quota enforcement\n  - Content Security Policy checks\n  \n- **LockService**: \n  - Distributed locking (Redis with Redlock algorithm)\n  - Per-upload locks (not per-chunk for performance)\n  - Lock timeouts: 30s default, auto-renewal for long operations\n  \n- **QuarantineService**: \n  - Isolated storage for infected files (separate bucket/container)\n  - Tiered retention: 7 days (confirmed malware) or 30 days (suspicious)\n  - Auto-notification to security team\n\n### 3. Storage Layer\n\n#### Storage Adapter Interface (Hybrid)\n```typescript\ninterface StorageAdapter {\n  // Native multipart operations\n  initiateMultipartUpload(params: InitiateParams): Promise<MultipartUpload>;\n  uploadPart(uploadId: string, partNumber: number, data: Buffer): Promise<PartETag>;\n  completeMultipartUpload(uploadId: string, parts: PartETag[]): Promise<FileKey>;\n  abortMultipartUpload(uploadId: string): Promise<void>;\n  \n  // Managed chunk operations (fallback)\n  writeChunk(uploadId: string, chunkIndex: number, chunk: Buffer): Promise<ChunkMetadata>;\n  assembleChunks(uploadId: string, manifest: ChunkManifest): Promise<FileKey>;\n  \n  // Common operations\n  verifyChecksum(key: string, expectedChecksum: string, algorithm: string): Promise<boolean>;\n  getSignedUrl(key: string, expiresIn: number, options: UrlOptions): Promise<string>;\n  delete(key: string): Promise<void>;\n  getMetadata(key: string): Promise<StorageMetadata>;\n  \n  // Streaming operations\n  getStreamingChecksum(key: string, algorithm: string): Promise<string>;\n  \n  // Capabilities\n  supportsNativeMultipart(): boolean;\n  getMaxPartSize(): number;\n  getMaxParts(): number;\n}\n```\n\n#### Implementations\n- **S3Adapter**: \n  - Native multipart for all files >100MB\n  - Max 10,000 parts \u00d7 5GB = 5TB per file\n  - Uses S3 Transfer Acceleration for cross-region uploads\n  - Server-side encryption (SSE-S3 or SSE-KMS)\n  \n- **GCSAdapter**: \n  - Resumable uploads for files >100MB\n  - Composite objects for managed mode\n  - Customer-managed encryption keys (CMEK)\n  \n- **AzureAdapter**: \n  - Block blobs with staging\n  - Max 50,000 blocks \u00d7 100MB = 4.75TB per file\n  \n- **LocalAdapter**: \n  - Managed chunks only\n  - Direct filesystem with atomic renames\n  - For development and testing\n\n### 4. Processing Layer\n\n#### Job Queue Architecture\n- **Queue System**: Bull (Redis-backed) or RabbitMQ for enterprise\n- **Priority Queues**:\n  - P0 (Critical): Virus scanning - blocks file access\n  - P1 (High): Assembly for large files\n  - P2 (Normal): Metadata extraction\n  - P3 (Low): Thumbnail generation, transcoding\n- **Concurrency**: 10 workers per queue by default\n- **Dead Letter Queue**: 3 retry limit, then DLQ for manual review\n- **Job Deduplication**: Content-addressed (checksum-based)\n\n#### Processors\n- **AssemblyProcessor** (managed chunks only):\n  - Streams chunks directly to final storage (no intermediate copy)\n  - Computes final checksum during assembly\n  - Duration: ~60s per 10GB at 200MB/s\n  \n- **VirusScanProcessor**:\n  - ClamAV for real-time blocking scan\n  - VirusTotal API for deep analysis (async, non-blocking)\n  - Streaming scan (no full file buffering)\n  - Quarantine on detection (moves file atomically)\n  - Duration: ~30s per 1GB\n  \n- **ThumbnailProcessor**:\n  - Generates multiple sizes (small/medium/large)\n  - Timeout: 60s per file\n  - Sandboxed execution (container-based)\n  \n- **MetadataExtractor**:\n  - Extracts EXIF, video codec info, document properties\n  - Sandboxed for security\n  - Timeout: 30s per file\n  \n- **ChecksumVerifier** (post-upload, optional):\n  - For files that used native multipart without streaming verification\n  - Computes full file checksum\n  - Marks file as verified in database\n\n### 5. Consistency and Reliability Layer\n\n#### Transaction Coordinator\n- **UploadTransactionManager**:\n  - Two-phase commit for database + storage updates\n  - Atomic chunk completion (update both Redis tracking and database)\n  - Rollback capability for failed operations\n\n#### State Reconciliation Service\n- **ReconciliationJob** (runs every 30 minutes):\n  - Detects orphaned uploads (chunks in storage, no database record)\n  - Finds incomplete uploads past expiry (cleanup trigger)\n  - Identifies storage vs database mismatches\n  - Auto-heals: Re-creates database records if storage valid + unexpired\n  - Auto-cleans: Deletes storage if database shows cancelled/failed\n  \n- **StorageAudit** (runs daily):\n  - Compares file records with actual storage\n  - Detects missing files (corruption or accidental deletion)\n  - Verifies checksums on sample of files (statistical integrity check)\n  - Alerts on discrepancies\n\n#### Cleanup Service\n- **UploadCleanupJob** (runs hourly):\n  - Deletes expired upload sessions (paused or in-progress)\n  - Removes temporary chunks for completed uploads\n  - Cleans up aborted multipart uploads in cloud providers\n  \n- **QuarantineCleanupJob** (runs daily):\n  - Tiered deletion: 7 days for confirmed threats, 30 days for suspicious\n  - Compression before deletion (forensic archive)\n\n## Error Handling\n\n### Error Response Format\n```typescript\ninterface ErrorResponse {\n  error: {\n    code: string;\n    message: string;\n    details?: Record<string, any>;\n    retryable: boolean;\n    retryAfter?: number;\n    retryStrategy?: 'exponential_backoff' | 'immediate' | 'none';\n    documentation: string;\n    requestId: string;  // For support\n  };\n  timestamp: string;\n}\n```\n\n### Error Categories\n\n#### Retryable Errors (Client should retry)\n- **STORAGE_UNAVAILABLE** (503): Cloud provider downtime \u2192 exponential backoff, max 5 retries\n- **LOCK_TIMEOUT** (409): Concurrent operation \u2192 wait 1s, retry up to 3 times\n- **RATE_LIMIT_EXCEEDED** (429): Quota exceeded \u2192 retry after `retryAfter` seconds\n- **CHECKSUM_COMPUTE_FAILED** (500): Temporary server error \u2192 immediate retry once\n- **INTERNAL_SERVER_ERROR** (500): Generic server error \u2192 exponential backoff\n\n#### Non-Retryable Errors (Client must fix)\n- **CHUNK_CHECKSUM_MISMATCH** (400): Data corruption \u2192 re-upload specific chunk\n- **FILE_SIZE_EXCEEDED** (413): File too large \u2192 cannot proceed\n- **INVALID_MIME_TYPE** (415): Unsupported file type \u2192 cannot proceed\n- **UPLOAD_EXPIRED** (410): Session expired \u2192 initiate new upload\n- **UPLOAD_NOT_FOUND** (404): Invalid upload ID \u2192 verify ID\n- **UNAUTHORIZED** (401): Invalid/expired token \u2192 re-authenticate\n- **QUOTA_EXCEEDED** (403): Storage quota full \u2192 upgrade or delete files\n- **INVALID_CHUNK_INDEX** (400): Chunk out of range \u2192 use correct index\n- **INVALID_TOKEN** (403): Upload token mismatch \u2192 get fresh token\n- **FILE_QUARANTINED** (451): Security block \u2192 file not accessible\n- **MULTIPART_UPLOAD_FAILED** (500): Native cloud API error \u2192 retry initiation with managed mode\n\n#### Edge Cases Handled\n\n**1. Chunk Checksum Mismatch:**\n- Client uploads chunk with checksum `sha256:abc123`\n- Server computes `sha256:def456` during streaming\n- Response: `400 CHUNK_CHECKSUM_MISMATCH` with `chunkIndex: 5`\n- Client action: Re-upload only chunk 5\n- Server: Allows idempotent re-upload of same chunk\n\n**2. Native Multipart Assembly Failure:**\n- S3 `CompleteMultipartUpload` returns error (rare: eventual consistency issue)\n- Server retries 3 times with exponential backoff\n- If all retries fail: Abort multipart upload, switch to managed assembly\n- Client notified: `202 Accepted` with `status: \"finalizing_with_fallback\"`\n- Transparent to client (same completion endpoint)\n\n**3. Upload Token Expiry Mid-Upload:**\n- Token expires during chunk upload\n- Response: `403 INVALID_TOKEN` with `newToken: \"tok_refreshed\"`\n- Client automatically uses new token for next chunk\n- Old chunks remain valid (token only gates new uploads)\n\n**4. Concurrent Chunk Uploads (Same Index):**\n- Two requests upload chunk 5 simultaneously\n- First request acquires lock, succeeds\n- Second request gets `409 LOCK_TIMEOUT`, retries after 1s\n- On retry, chunk already completed \u2192 returns `200 OK` (idempotent)\n\n**5. Partial Upload After Expiry:**\n- Upload expires with 50% complete\n- Client tries to resume: `410 UPLOAD_EXPIRED`\n- Server cleanup job removes chunks after 24h grace period\n- Client must initiate new upload (no recovery possible)\n\n**6. Storage Inconsistency (Missing Chunk):**\n- Database says chunk 5 complete, but storage doesn't have it\n- Detected by reconciliation job\n- Job marks chunk as incomplete in database\n- Client's next status check shows chunk 5 in `chunksRemaining`\n- Client re-uploads chunk 5\n\n**7. Completion Request Before All Chunks Uploaded:**\n- Client calls complete with `chunkCount: 100` but only 98 uploaded\n- Response: `400 INCOMPLETE_UPLOAD` with `chunksRemaining: [45, 67]`\n- Client uploads missing chunks, retries completion\n\n**8. File Download Corruption:**\n- Client downloads file, checksum doesn't match\n- Client reports corruption via `POST /api/v1/files/{fileId}/report-corruption`\n- Server: Re-verifies checksum in storage\n- If mismatch: Marks file as corrupted, triggers re-assembly from chunks (if available)\n- If chunks gone: File marked as unrecoverable, user notified\n\n**9. Virus Detected After Completion:**\n- File initially marked clean\n- VirusTotal async scan (24h later) finds threat\n- File atomically moved to quarantine\n- Status changed to `quarantined`\n- User notified via webhook + email\n- Download URLs return `451 Unavailable For Legal Reasons`\n\n**10. Multi-Day Upload with Network Changes:**\n- User uploads 100GB over 3 days\n- IP address changes mid-upload\n- Token validation: IP binding disabled for long uploads (>24h)\n- User agent fingerprinting used instead\n- Upload continues seamlessly\n\n### Retry Strategy\n\n**Client-Side (SDK Best Practices):**\n```typescript\nconst retryConfig = {\n  maxRetries: 5,\n  baseDelayMs: 1000,\n  maxDelayMs: 32000,\n  retryableStatusCodes: [408, 429, 500, 502, 503, 504],\n  retryableErrorCodes: ['STORAGE_UNAVAILABLE', 'LOCK_TIMEOUT', 'RATE_LIMIT_EXCEEDED'],\n  \n  strategy: (attempt: number, error: ErrorResponse) => {\n    if (error.error.code === 'RATE_LIMIT_EXCEEDED') {\n      return error.error.retryAfter * 1000;  // Use server-provided delay\n    }\n    if (error.error.code === 'CHUNK_CHECKSUM_MISMATCH') {\n      return 0;  // Immediate retry for corrupted chunk\n    }\n    return Math.min(baseDelayMs * Math.pow(2, attempt), maxDelayMs);  // Exponential backoff\n  }\n};\n```\n\n**Server-Side (Processing Jobs):**\n- Virus scan failure: Retry 3 times (10s, 30s, 90s), then fail\n- Thumbnail generation: Retry 2 times (5s, 15s), then skip\n- Assembly failure: Retry 3 times (30s, 90s, 270s), then alert ops team\n- Checksum verification: No retry (deterministic), mark as failed immediately\n\n## Security\n\n### Authentication & Authorization\n1. **JWT Bearer Tokens**: 15-minute access, 7-day refresh, rotation on use\n2. **Upload Token System** (Configurable):\n   - **Strict Mode** (public APIs): Per-chunk rotation, single-use\n   - **Balanced Mode** (default): Per-session token, 24h expiry\n   - **Relaxed Mode** (internal systems): Per-upload token, 7d expiry\n3. **Authorization**: User isolation enforced at database query level (row-level security)\n4. **API Keys**: For service-to-service, HMAC-SHA256 signed requests\n\n### Rate Limiting (Adaptive)\n**Tier-Based Limits:**\n- Free Tier: 10 uploads/day, 1GB total\n- Pro Tier: 100 uploads/day, 100GB total, 5 concurrent\n- Enterprise: Unlimited, 100 concurrent, dedicated bandwidth\n\n**Per-Endpoint Limits:**\n- Initiate: 100/hour per user (prevents session exhaustion)\n- Chunk Upload: 10,000/hour per user (supports 100GB/hour at 10MB chunks)\n- Status Check: 1,000/hour per user\n- Complete: 100/hour per user\n\n**Adaptive Rate Limiting:**\n- Increases limits for users with consistent upload patterns\n- Decreases limits on suspicious activity (rapid session creation)\n- IP-based limits: 10x user limits per IP (multi-user scenarios)\n\n### File Validation (Multi-Layer)\n1. **MIME Type Verification**:\n   - Magic number detection (libmagic)\n   - Extension validation\n   - Content sniffing\n   - Reject if mismatch (strict mode) or warn (permissive mode)\n\n2. **Filename Sanitization**:\n   - Remove: `../`, `..\\\\`, null bytes, control characters\n   - Limit: 255 UTF-8 characters\n   - Generate safe storage key (UUID-based), preserve original for display\n\n3. **Size Limits** (Tier-Based):\n   - Free: 1GB per file, 10GB total storage\n   - Pro: 100GB per file, 1TB total storage\n   - Enterprise: 1TB per file, unlimited storage\n\n4. **Content Security**:\n   - Serve from separate domain (e.g., `files.example.com` vs `api.example.com`)\n   - `Content-Disposition: attachment` default\n   - `X-Content-Type-Options: nosniff`\n   - `Content-Security-Policy: default-src 'none'` for HTML files\n\n### Virus Scanning\n1. **Real-Time** (Blocking): ClamAV scan on completion, <5s for small files\n2. **Deep Scan** (Async): VirusTotal API for files >10MB or suspicious types\n3. **Quarantine**: Atomic move to isolated bucket, access revoked immediately\n4. **Notifications**: User + security team via webhook + email\n\n### Signed URLs (Secure Downloads)\n- HMAC-SHA256 signed with secret key\n- Time-limited: 1h default, 24h max, 5min min\n- Optional: IP binding, single-use, user-agent binding\n- Audit: All URL generations logged\n\n### Encryption\n1. **In Transit**: TLS 1.3, HSTS enabled, strong cipher suites only\n2. **At Rest**: \n   - Server-side: SSE-S3, SSE-KMS, or SSE-C (customer key)\n   - Client-side: Optional, client manages keys\n3. **Key Management**: AWS KMS, Google Cloud KMS, Azure Key Vault integration\n\n## Configuration\n\n```typescript\ninterface SystemConfig {\n  // File size limits\n  files: {\n    maxFileSizeBytes: number;              // 1TB default (1099511627776)\n    directUploadMaxBytes: number;          // 100MB (no chunking)\n    nativeMultipartThresholdBytes: number; // 100MB (use native APIs above this)\n  };\n  \n  // Chunking\n  chunks: {\n    defaultChunkSizeBytes: number;  // 10MB (optimal for most networks)\n    minChunkSizeBytes: number;      // 5MB (S3 requirement)\n    maxChunkSizeBytes: number;      // 100MB (memory limit)\n    autoTuneChunkSize: boolean;     // true (adjust based on file size)\n    maxConcurrentUploads: number;   // 10 per upload session\n  };\n  \n  // Session management\n  sessions: {\n    defaultExpiryHours: number;     // 168 (7 days)\n    maxExpiryHours: number;         // 720 (30 days)\n    allowExtensions: boolean;       // true\n    maxExtensions: number;          // 3\n    pauseSupported: boolean;        // true\n    gracePeriodeleteHours: number; // 24 (before cleanup)\n  };\n  \n  // Upload strategy\n  strategy: {\n    preferNativeMultipart: boolean;  // true (use cloud native APIs)\n    fallbackToManaged: boolean;      // true (if native fails)\n    streamingVerification: boolean;  // true (verify during upload)\n    deferredVerification: boolean;   // false (post-upload verification)\n  };\n  \n  // Storage\n  storage: {\n    provider: 's3' | 'gcs' | 'azure' | 'local';\n    bucket: string;\n    region: string;\n    storageClass: 'standard' | 'infrequent' | 'archive';\n    redundancy: 'single-region' | 'multi-region';\n    encryption: {\n      type: 'sse-s3' | 'sse-kms' | 'client-side';\n      kmsKeyId?: string;\n    };\n    transferAcceleration: boolean;  // S3 only\n    credentials: any;\n  };\n  \n  // Security\n  security: {\n    uploadTokenMode: 'strict' | 'balanced' | 'relaxed';\n    ipBindingEnabled: boolean;          // false (breaks mobile uploads)\n    ipBindingForLongUploads: boolean;   // false (>24h uploads)\n    requireChecksums: boolean;          // true\n    checksumAlgorithm: 'sha256' | 'sha512' | 'blake3';\n    virusScanEnabled: boolean;          // true\n    virusScanBlocking: boolean;         // true\n    allowedMimeTypes: string[];         // [] = all allowed\n    blockedMimeTypes: string[];         // ['application/x-msdownload', ...]\n    blockedExtensions: string[];        // ['.exe', '.bat', '.scr', ...]\n    serveFromSeparateDomain: boolean;   // true (security best practice)\n    separateDomain?: string;\n  };\n  \n  // Quotas (per user tier)\n  quotas: {\n    dailyUploads: number;           // 100\n    dailyBandwidthBytes: number;    // 100GB\n    totalStorageBytes: number;      // 1TB\n    maxConcurrentSessions: number;  // 5\n    maxFileSize: number;            // 100GB (per-tier override)\n  };\n  \n  // Rate limiting\n  rateLimits: {\n    initiate: {perHour: 100, perDay: 500},\n    chunkUpload: {perHour: 10000, perDay: 100000},\n    statusCheck: {perHour: 1000, perDay: 10000},\n    complete: {perHour: 100, perDay: 500},\n    adaptive: boolean;  // true (increase limits for trusted users)\n  };\n  \n  // Processing\n  processing: {\n    virusScanner: 'clamav' | 'virustotal' | 'both';\n    thumbnailGeneration: boolean;       // true for images/videos\n    metadataExtraction: boolean;        // true\n    asyncProcessing: boolean;           // true (don't block completion)\n    jobRetries: {\n      virusScan: 3,\n      thumbnail: 2,\n      metadata: 2,\n      assembly: 3\n    };\n  };\n  \n  // Cleanup\n  cleanup: {\n    failedUploadRetentionHours: number;     // 24\n    completedChunkRetentionHours: number;   // 1 (cleanup after assembly)\n    quarantinedFileRetentionDays: number;   // 7 for confirmed, 30 for suspicious\n    orphanedChunkCleanupHours: number;      // 48\n    auditLogRetentionDays: number;          // 90\n  };\n  \n  // Monitoring\n  monitoring: {\n    metricsEnabled: boolean;           // true\n    detailedLogging: boolean;          // false (performance impact)\n    distributedTracing: boolean;       // true\n    alerting: {\n      failureRateThreshold: number;    // 5% (alert if exceeded)\n      slowUploadThreshold: number;     // 300s per GB\n      storageUsageThreshold: number;   // 90% capacity\n    };\n  };\n}\n```\n\n## Performance Optimizations\n\n### 1. Native Cloud Provider APIs\n- S3: Direct multipart upload (no proxy), instant assembly\n- GCS: Resumable upload protocol, server-side composition\n- Azure: Block blob staging, atomic commitment\n- **Benefit**: 10-100x faster assembly (no data movement), up to 1TB files\n\n### 2. Streaming Verification\n- Checksum computed incrementally during upload (not post-upload)\n- State persisted in Redis (resume without recomputation)\n- **Benefit**: Zero additional latency for verification, enables 100GB+ files\n\n### 3. Chunk Tracking Optimization\n- Bitmap storage: 10,000 chunks = 1.25KB (vs 50KB JSON array)\n- Range compression: \"0-5000\" vs 5000 separate numbers\n- **Benefit**: 40x smaller database payload, faster status checks\n\n### 4. Parallel Chunk Uploads\n- Client uploads 10 chunks concurrently (configurable)\n- Server processes independently (no ordering requirement)\n- **Benefit**: 10x throughput on high-bandwidth connections\n\n### 5. Batch Operations\n- Status checks for multiple uploads in single request\n- **Benefit**: 90% reduction in API calls for multi-file uploads\n\n### 6. Adaptive Chunk Sizing\n- Small files (<100MB): 5MB chunks, direct upload\n- Medium files (100MB-10GB): 10MB chunks, native multipart\n- Large files (>10GB): 50MB chunks, parallel uploads\n- **Benefit**: Optimal throughput across all file sizes\n\n### 7. CDN Integration\n- Static assets (thumbnails, small files) served via CDN\n- Edge caching for download URLs\n- **Benefit**: <100ms download latency globally\n\n## Scalability Targets\n\n### Performance Benchmarks\n- **Small files (<10MB)**: <2s end-to-end (upload + processing)\n- **Medium files (1GB)**: <60s upload on 100Mbps connection\n- **Large files (100GB)**: <3h upload on 100Mbps connection\n- **Very large files (1TB)**: <30h upload on 100Mbps connection\n\n### System Capacity\n- **Concurrent uploads**: 10,000+ simultaneous sessions\n- **Throughput**: 10 PB/day aggregate\n- **File count**: 100M active files\n- **API requests**: 100,000 req/s\n- **Database**: 10M upload sessions, 100M file records\n\n### Horizontal Scaling\n- API servers: Stateless, auto-scale on CPU/memory\n- Job processors: Worker pool scales on queue depth\n- Redis: Cluster mode for session/lock management\n- Database: Read replicas for status checks, sharded by user ID"
}